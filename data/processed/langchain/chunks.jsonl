{"text": "---\ntitle: Agents\n---\n\nAgents combine language models with [tools](/oss/langchain/tools) to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.\n\n:::python\n@[`create_agent`] provides a production-ready agent implementation.\n:::\n:::js\n`createAgent()` provides a production-ready agent implementation.\n:::\n\n[An LLM Agent runs tools in a loop to achieve a goal](https://simonwillison.net/2025/Sep/18/agents/).\nAn agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.\n\n```mermaid\n%%{\n  init: {\n    \"fontFamily\": \"monospace\",\n    \"flowchart\": {\n      \"curve\": \"curve\"\n    },\n    \"themeVariables\": {\"edgeLabelBackground\": \"transparent\"}\n  }\n}%%\ngraph TD\n  %% Outside the agent\n  QUERY([input])\n  LLM{model}\n  TOOL(tools)\n  ANSWER([output])\n\n  %% Main flows (no inline labels)\n  QUERY --> LLM\n  LLM --\"action\"--> TOOL\n  TOOL --\"observation\"--> LLM\n  LLM --\"finish\"--> ANSWER\n\n  classDef blueHighlight fill:#0a1c25,stroke:#0a455f,color:#bae6fd;\n  classDef greenHighlight fill:#0b1e1a,stroke:#0c4c39,color:#9ce4c4;\n  class QUERY blueHighlight;\n  class ANSWER blueHighlight;\n```\n\n<Info>\n\n:::python\n@[`create_agent`] builds a **graph**-based agent runtime using [LangGraph](/oss/langgraph/overview). A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware.\n:::\n:::js\n`createAgent()` builds a **graph**-based agent runtime using [LangGraph](/oss/langgraph/overview). A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware.\n:::\n\nLearn more about the [Graph API](/oss/langgraph/graph-api).\n\n</Info>\n\n## Core components\n\n### Model\n\nThe [model](/oss/langchain/models) is the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection.\n\n#### Static model\n\nStatic models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach.\n\nTo initialize a static model from a <Tooltip tip=\"A string that follows the format `provider:model` (e.g. openai:gpt-5)\" cta=\"See mappings\" href=\"https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model(model)\">model identifier string</Tooltip>:\n\n:::python\n```python", "metadata": {"source": "agents.mdx"}}
{"text": ":\n\nLearn more about the [Graph API](/oss/langgraph/graph-api).\n\n</Info>\n\n## Core components\n\n### Model\n\nThe [model](/oss/langchain/models) is the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection.\n\n#### Static model\n\nStatic models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach.\n\nTo initialize a static model from a <Tooltip tip=\"A string that follows the format `provider:model` (e.g. openai:gpt-5)\" cta=\"See mappings\" href=\"https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model(model)\">model identifier string</Tooltip>:\n\n:::python\n```python wrap\nfrom langchain.agents import create_agent\n\nagent = create_agent(\"openai:gpt-5\", tools=tools)\n```\n:::\n:::js\n```ts wrap\nimport { createAgent } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"openai:gpt-5\",\n  tools: []\n});\n```\n:::\n\n:::python\n<Tip>\n    Model identifier strings support automatic inference (e.g., `\"gpt-5\"` will be inferred as `\"openai:gpt-5\"`). Refer to the @[reference][init_chat_model(model)] to see a full list of model identifier string mappings.\n</Tip>\n\nFor more control over the model configuration, initialize a model instance directly using the provider package. In this example, we use @[`ChatOpenAI`]. See [Chat models](/oss/integrations/chat) for other available chat model classes.\n\n```python wrap\nfrom langchain.agents import create_agent\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(\n    model=\"gpt-5\",\n    temperature=0.1,\n    max_tokens=1000,\n    timeout=30\n    # ... (other params)\n)\nagent = create_agent(model, tools=tools)\n```\n\nModel instances give you complete control over configuration. Use them when you need to set specific [parameters](/oss/langchain/models#parameters) like `temperature`, `max_tokens`, `timeouts`, `base_url`, and other provider-specific settings. Refer to the [reference](/oss/integrations/providers/all_providers) to see available params and methods on your model.\n:::\n:::js\nModel identifier strings use the format `provider:model` (e.g. `\"openai:gpt-5\"`). You may want more control over the model configuration, in which case you can initialize a model instance directly using the provider package:\n\n```ts wrap\nimport { createAgent } from \"langchain\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst model = new ChatOpenAI({\n  model: \"gpt-4.1\",\n  temperature: 0.1,\n  maxTokens: 1000,\n  timeout: 30\n});\n\nconst agent = createAgent({\n  model,\n  tools: []\n});\n```\n\nModel instances give you complete control over configuration. Use them when you need to set specific parameters like", "metadata": {"source": "agents.mdx"}}
{"text": "/providers/all_providers) to see available params and methods on your model.\n:::\n:::js\nModel identifier strings use the format `provider:model` (e.g. `\"openai:gpt-5\"`). You may want more control over the model configuration, in which case you can initialize a model instance directly using the provider package:\n\n```ts wrap\nimport { createAgent } from \"langchain\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst model = new ChatOpenAI({\n  model: \"gpt-4.1\",\n  temperature: 0.1,\n  maxTokens: 1000,\n  timeout: 30\n});\n\nconst agent = createAgent({\n  model,\n  tools: []\n});\n```\n\nModel instances give you complete control over configuration. Use them when you need to set specific parameters like `temperature`, `max_tokens`, `timeouts`, or configure API keys, `base_url`, and other provider-specific settings. Refer to the [API reference](/oss/integrations/providers/) to see available params and methods on your model.\n:::\n\n#### Dynamic model\n\nDynamic models are selected at <Tooltip tip=\"The execution environment of your agent, containing immutable configuration and contextual data that persists throughout the agent's execution (e.g., user IDs, session details, or application-specific configuration).\">runtime</Tooltip> based on the current <Tooltip tip=\"The data that flows through your agent's execution, including messages, custom fields, and any information that needs to be tracked and potentially modified during processing (e.g., user preferences or tool usage stats).\">state</Tooltip> and context. This enables sophisticated routing logic and cost optimization.\n\n:::python\n\nTo use a dynamic model, create middleware using the @[`@wrap_model_call`] decorator that modifies the model in the request:\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n\n\nbasic_model = ChatOpenAI(model=\"gpt-4.1-mini\")\nadvanced_model = ChatOpenAI(model=\"gpt-4.1\")\n\n@wrap_model_call\ndef dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\n    \"\"\"Choose model based on conversation complexity.\"\"\"\n    message_count = len(request.state[\"messages\"])\n\n    if message_count > 10:\n        # Use an advanced model for longer conversations\n        model = advanced_model\n    else:\n        model = basic_model\n\n    return handler(request.override(model=model))\n\nagent = create_agent(\n    model=basic_model,  # Default model\n    tools=tools,\n    middleware=[dynamic_model_selection]\n)\n```\n\n<Warning>\nPre-bound models (models with @[`bind_tools`][BaseChatModel.bind_tools] already called) are not supported when using structured output. If you need dynamic model selection with structured output, ensure the models passed to the middleware are not pre-bound.\n</Warning>\n\n:::\n:::js\n\nTo use a dynamic model, create middleware with `wrapModelCall` that modifies the model in the request:", "metadata": {"source": "agents.mdx"}}
{"text": " conversations\n        model = advanced_model\n    else:\n        model = basic_model\n\n    return handler(request.override(model=model))\n\nagent = create_agent(\n    model=basic_model,  # Default model\n    tools=tools,\n    middleware=[dynamic_model_selection]\n)\n```\n\n<Warning>\nPre-bound models (models with @[`bind_tools`][BaseChatModel.bind_tools] already called) are not supported when using structured output. If you need dynamic model selection with structured output, ensure the models passed to the middleware are not pre-bound.\n</Warning>\n\n:::\n:::js\n\nTo use a dynamic model, create middleware with `wrapModelCall` that modifies the model in the request:\n\n```ts\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { createAgent, createMiddleware } from \"langchain\";\n\nconst basicModel = new ChatOpenAI({ model: \"gpt-4.1-mini\" });\nconst advancedModel = new ChatOpenAI({ model: \"gpt-4.1\" });\n\nconst dynamicModelSelection = createMiddleware({\n  name: \"DynamicModelSelection\",\n  wrapModelCall: (request, handler) => {\n    // Choose model based on conversation complexity\n    const messageCount = request.messages.length;\n\n    return handler({\n        ...request,\n        model: messageCount > 10 ? advancedModel : basicModel,\n    });\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-4.1-mini\", // Base model (used when messageCount \u2264 10)\n  tools,\n  middleware: [dynamicModelSelection],\n});\n```\n\nFor more details on middleware and advanced patterns, see the [middleware documentation](/oss/langchain/middleware).\n:::\n\n<Tip>\nFor model configuration details, see [Models](/oss/langchain/models). For dynamic model selection patterns, see [Dynamic model in middleware](/oss/langchain/middleware#dynamic-model).\n</Tip>\n\n### Tools\n\nTools give agents the ability to take actions. Agents go beyond simple model-only tool binding by facilitating:\n\n- Multiple tool calls in sequence (triggered by a single prompt)\n- Parallel tool calls when appropriate\n- Dynamic tool selection based on previous results\n- Tool retry logic and error handling\n- State persistence across tool calls\n\nFor more information, see [Tools](/oss/langchain/tools).\n\n#### Defining tools\n\nPass a list of tools to the agent.\n\n:::python\n\n<Tip>\nTools can be specified as plain Python functions or <Tooltip tip=\"A method that can suspend execution and resume at a later time\">coroutines</Tooltip>.\n\nThe [tool decorator](/oss/langchain/tools#create-tools) can be used to customize tool names, descriptions, argument schemas, and other properties.\n</Tip>\n\n```python wrap\nfrom langchain.tools import tool\nfrom langchain.agents import create_agent\n\n\n@tool\ndef search(query: str) -> str:\n    \"\"\"Search for information.\"\"\"\n    return f\"Results for: {query}\"\n\n@tool\ndef get_weather", "metadata": {"source": "agents.mdx"}}
{"text": "\n- State persistence across tool calls\n\nFor more information, see [Tools](/oss/langchain/tools).\n\n#### Defining tools\n\nPass a list of tools to the agent.\n\n:::python\n\n<Tip>\nTools can be specified as plain Python functions or <Tooltip tip=\"A method that can suspend execution and resume at a later time\">coroutines</Tooltip>.\n\nThe [tool decorator](/oss/langchain/tools#create-tools) can be used to customize tool names, descriptions, argument schemas, and other properties.\n</Tip>\n\n```python wrap\nfrom langchain.tools import tool\nfrom langchain.agents import create_agent\n\n\n@tool\ndef search(query: str) -> str:\n    \"\"\"Search for information.\"\"\"\n    return f\"Results for: {query}\"\n\n@tool\ndef get_weather(location: str) -> str:\n    \"\"\"Get weather information for a location.\"\"\"\n    return f\"Weather in {location}: Sunny, 72\u00b0F\"\n\nagent = create_agent(model, tools=[search, get_weather])\n```\n:::\n:::js\n```ts wrap\nimport * as z from \"zod\";\nimport { createAgent, tool } from \"langchain\";\n\nconst search = tool(\n  ({ query }) => `Results for: ${query}`,\n  {\n    name: \"search\",\n    description: \"Search for information\",\n    schema: z.object({\n      query: z.string().describe(\"The query to search for\"),\n    }),\n  }\n);\n\nconst getWeather = tool(\n  ({ location }) => `Weather in ${location}: Sunny, 72\u00b0F`,\n  {\n    name: \"get_weather\",\n    description: \"Get weather information for a location\",\n    schema: z.object({\n      location: z.string().describe(\"The location to get weather for\"),\n    }),\n  }\n);\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [search, getWeather],\n});\n```\n:::\n\nIf an empty tool list is provided, the agent will consist of a single LLM node without tool-calling capabilities.\n\n#### Tool error handling\n\n:::python\n\nTo customize how tool errors are handled, use the @[`@wrap_tool_call`] decorator to create middleware:\n\n```python wrap\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import wrap_tool_call\nfrom langchain.messages import ToolMessage\n\n\n@wrap_tool_call\ndef handle_tool_errors(request, handler):\n    \"\"\"Handle tool execution errors with custom messages.\"\"\"\n    try:\n        return handler(request)\n    except Exception as e:\n        # Return a custom error message to the model\n        return ToolMessage(\n            content=f\"Tool error: Please check your input and try again. ({str(e)})\",\n            tool_call_id=request.tool_call[\"id\"]\n        )\n\nagent = create_agent(\n    model=\"gpt", "metadata": {"source": "agents.mdx"}}
{"text": "from langchain.agents.middleware import wrap_tool_call\nfrom langchain.messages import ToolMessage\n\n\n@wrap_tool_call\ndef handle_tool_errors(request, handler):\n    \"\"\"Handle tool execution errors with custom messages.\"\"\"\n    try:\n        return handler(request)\n    except Exception as e:\n        # Return a custom error message to the model\n        return ToolMessage(\n            content=f\"Tool error: Please check your input and try again. ({str(e)})\",\n            tool_call_id=request.tool_call[\"id\"]\n        )\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[search, get_weather],\n    middleware=[handle_tool_errors]\n)\n```\n\nThe agent will return a @[`ToolMessage`] with the custom error message when a tool fails:\n\n```python\n[\n    ...\n    ToolMessage(\n        content=\"Tool error: Please check your input and try again. (division by zero)\",\n        tool_call_id=\"...\"\n    ),\n    ...\n]\n```\n\n:::\n:::js\n\nTo customize how tool errors are handled, use the `wrapToolCall` hook in a custom middleware:\n\n```ts wrap\nimport { createAgent, createMiddleware, ToolMessage } from \"langchain\";\n\nconst handleToolErrors = createMiddleware({\n  name: \"HandleToolErrors\",\n  wrapToolCall: async (request, handler) => {\n    try {\n      return await handler(request);\n    } catch (error) {\n      // Return a custom error message to the model\n      return new ToolMessage({\n        content: `Tool error: Please check your input and try again. (${error})`,\n        tool_call_id: request.toolCall.id!,\n      });\n    }\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [\n    /* ... */\n  ],\n  middleware: [handleToolErrors],\n});\n```\n\nThe agent will return a @[`ToolMessage`] with the custom error message when a tool fails.\n:::\n\n#### Tool use in the ReAct loop\n\nAgents follow the ReAct (\"Reasoning + Acting\") pattern, alternating between brief reasoning steps with targeted tool calls and feeding the resulting observations into subsequent decisions until they can deliver a final answer.\n\n<Accordion title=\"Example of ReAct loop\">\n**Prompt:** Identify the current most popular wireless headphones and verify availability.\n\n```\n================================ Human Message =================================\n\nFind the most popular wireless headphones right now and check if they're in stock\n```\n\n* **Reasoning**: \"Popularity is time-sensitive, I need to use the provided search tool.\"\n* **Acting**: Call `search_products(\"wireless headphones\")`\n\n```\n================================== Ai Message ==================================\nTool Calls:\n  search_products (call", "metadata": {"source": "agents.mdx"}}
{"text": " will return a @[`ToolMessage`] with the custom error message when a tool fails.\n:::\n\n#### Tool use in the ReAct loop\n\nAgents follow the ReAct (\"Reasoning + Acting\") pattern, alternating between brief reasoning steps with targeted tool calls and feeding the resulting observations into subsequent decisions until they can deliver a final answer.\n\n<Accordion title=\"Example of ReAct loop\">\n**Prompt:** Identify the current most popular wireless headphones and verify availability.\n\n```\n================================ Human Message =================================\n\nFind the most popular wireless headphones right now and check if they're in stock\n```\n\n* **Reasoning**: \"Popularity is time-sensitive, I need to use the provided search tool.\"\n* **Acting**: Call `search_products(\"wireless headphones\")`\n\n```\n================================== Ai Message ==================================\nTool Calls:\n  search_products (call_abc123)\n Call ID: call_abc123\n  Args:\n    query: wireless headphones\n```\n```\n================================= Tool Message =================================\n\nFound 5 products matching \"wireless headphones\". Top 5 results: WH-1000XM5, ...\n```\n\n* **Reasoning**: \"I need to confirm availability for the top-ranked item before answering.\"\n* **Acting**: Call `check_inventory(\"WH-1000XM5\")`\n\n```\n================================== Ai Message ==================================\nTool Calls:\n  check_inventory (call_def456)\n Call ID: call_def456\n  Args:\n    product_id: WH-1000XM5\n```\n```\n================================= Tool Message =================================\n\nProduct WH-1000XM5: 10 units in stock\n```\n\n* **Reasoning**: \"I have the most popular model and its stock status. I can now answer the user's question.\"\n* **Acting**: Produce final answer\n\n```\n================================== Ai Message ==================================\n\nI found wireless headphones (model WH-1000XM5) with 10 units in stock...\n```\n</Accordion>\n\n#### Dynamic tools\n\nIn some scenarios, you need to modify the set of tools available to the agent at runtime rather than defining them all upfront. There are two approaches depending on whether tools are known ahead of time:\n\n<Tabs>\n  <Tab title=\"Filtering pre-registered tools\">\n\n    When all possible tools are known at agent creation time, you can pre-register them and dynamically filter which ones are exposed to the model based on state, permissions, or context.\n\n    :::python\n\n    ```python\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from typing import Callable\n\n    @wrap_model_call\n    def filter_tools(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse],\n    ) -> ModelResponse:\n        \"\"\"Filter tools based on user permissions.\"\"\"\n        user_role = request.runtime.context.user_role\n\n        if user_role == \"admin\":\n            # Admins get all tools\n            tools = request.tools\n        else:\n          ", "metadata": {"source": "agents.mdx"}}
{"text": "  from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from typing import Callable\n\n    @wrap_model_call\n    def filter_tools(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse],\n    ) -> ModelResponse:\n        \"\"\"Filter tools based on user permissions.\"\"\"\n        user_role = request.runtime.context.user_role\n\n        if user_role == \"admin\":\n            # Admins get all tools\n            tools = request.tools\n        else:\n            # Regular users get read-only tools\n            tools = [t for t in request.tools if t.name.startswith(\"read_\")]\n\n        return handler(request.override(tools=tools))\n\n    agent = create_agent(\n        model=\"gpt-4o\",\n        tools=[read_data, write_data, delete_data],  # All tools pre-registered\n        middleware=[filter_tools],\n    )\n    ```\n\n    :::\n\n    :::js\n\n    ```typescript\n    import { createAgent, createMiddleware } from \"langchain\";\n\n    const filterTools = createMiddleware({\n      name: \"FilterTools\",\n      wrapModelCall: (request, handler) => {\n        const userRole = request.runtime.context.userRole;\n\n        let tools;\n        if (userRole === \"admin\") {\n          // Admins get all tools\n          tools = request.tools;\n        } else {\n          // Regular users get read-only tools\n          tools = request.tools.filter((t) => t.name.startsWith(\"read_\"));\n        }\n\n        return handler({ ...request, tools });\n      },\n    });\n\n    const agent = createAgent({\n      model: \"gpt-4o\",\n      tools: [readData, writeData, deleteData], // All tools pre-registered\n      middleware: [filterTools],\n    });\n    ```\n\n    :::\n\n    This approach is best when:\n    - All possible tools are known at compile/startup time\n    - You want to filter based on permissions, feature flags, or conversation state\n    - Tools are static but their availability is dynamic\n\n    See [Dynamically selecting tools](/oss/langchain/middleware/custom#dynamically-selecting-tools) for more examples.\n\n  </Tab>\n\n  <Tab title=\"Runtime tool registration\">\n\n    When tools", "metadata": {"source": "agents.mdx"}}
{"text": " },\n    });\n\n    const agent = createAgent({\n      model: \"gpt-4o\",\n      tools: [readData, writeData, deleteData], // All tools pre-registered\n      middleware: [filterTools],\n    });\n    ```\n\n    :::\n\n    This approach is best when:\n    - All possible tools are known at compile/startup time\n    - You want to filter based on permissions, feature flags, or conversation state\n    - Tools are static but their availability is dynamic\n\n    See [Dynamically selecting tools](/oss/langchain/middleware/custom#dynamically-selecting-tools) for more examples.\n\n  </Tab>\n\n  <Tab title=\"Runtime tool registration\">\n\n    When tools are discovered or created at runtime (e.g., loaded from an MCP server, generated based on user data, or fetched from a remote registry), you need to both register the tools and handle their execution dynamically.\n\n    This requires two middleware hooks:\n    1. `wrap_model_call` - Add the dynamic tools to the request\n    2. `wrap_tool_call` - Handle execution of the dynamically added tools\n\n    :::python\n\n    ```python\n    from langchain.tools import tool\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import AgentMiddleware, ModelRequest, ToolCallRequest\n\n    # A tool that will be added dynamically at runtime\n    @tool\n    def calculate_tip(bill_amount: float, tip_percentage: float = 20.0) -> str:\n        \"\"\"Calculate the tip amount for a bill.\"\"\"\n        tip = bill_amount * (tip_percentage / 100)\n        return f\"Tip: ${tip:.2f}, Total: ${bill_amount + tip:.2f}\"\n\n    class DynamicToolMiddleware(AgentMiddleware):\n        \"\"\"Middleware that registers and handles dynamic tools.\"\"\"\n\n        def wrap_model_call(self, request: ModelRequest, handler):\n            # Add dynamic tool to the request\n            # This could be loaded from an MCP server, database, etc.\n            updated = request.override(tools=[*request.tools, calculate_tip])\n            return handler(updated)\n\n        def wrap_tool_call(self, request: ToolCallRequest, handler):\n            # Handle execution of the dynamic tool\n            if request.tool_call[\"name\"] == \"calculate_tip\":\n                return handler(request.override(tool=calculate_tip))\n            return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4o\",\n        tools=[get_weather],  # Only static tools", "metadata": {"source": "agents.mdx"}}
{"text": " request.override(tools=[*request.tools, calculate_tip])\n            return handler(updated)\n\n        def wrap_tool_call(self, request: ToolCallRequest, handler):\n            # Handle execution of the dynamic tool\n            if request.tool_call[\"name\"] == \"calculate_tip\":\n                return handler(request.override(tool=calculate_tip))\n            return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4o\",\n        tools=[get_weather],  # Only static tools registered here\n        middleware=[DynamicToolMiddleware()],\n    )\n\n    # The agent can now use both get_weather AND calculate_tip\n    result = agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": \"Calculate a 20% tip on $85\"}]\n    })\n    ```\n\n    :::\n\n    :::js\n\n    ```typescript\n    import { createAgent, createMiddleware, tool } from \"langchain\";\n    import * as z from \"zod\";\n\n    // A tool that will be added dynamically at runtime\n    const calculateTip = tool(\n      ({ billAmount, tipPercentage = 20 }) => {\n        const tip = billAmount * (tipPercentage / 100);\n        return `Tip: $${tip.toFixed(2)}, Total: $${(billAmount + tip).toFixed(2)}`;\n      },\n      {\n        name: \"calculate_tip\",\n        description: \"Calculate the tip amount for a bill\",\n        schema: z.object({\n          billAmount: z.number().describe(\"The bill amount\"),\n          tipPercentage: z.number().default(20).describe(\"Tip percentage\"),\n        }),\n      }\n    );\n\n    const dynamicToolMiddleware = createMiddleware({\n      name: \"DynamicToolMiddleware\",\n      wrapModelCall: (request, handler) => {\n        // Add dynamic tool to the request\n        // This could be loaded from an MCP server, database, etc.\n        return handler({\n          ...request,\n          tools: [...request.tools, calculateTip],\n        });\n      },\n      wrapToolCall: (request, handler) => {\n        // Handle execution of the dynamic tool\n        if (request.toolCall.name === \"calculate_tip\") {\n   ", "metadata": {"source": "agents.mdx"}}
{"text": " dynamicToolMiddleware = createMiddleware({\n      name: \"DynamicToolMiddleware\",\n      wrapModelCall: (request, handler) => {\n        // Add dynamic tool to the request\n        // This could be loaded from an MCP server, database, etc.\n        return handler({\n          ...request,\n          tools: [...request.tools, calculateTip],\n        });\n      },\n      wrapToolCall: (request, handler) => {\n        // Handle execution of the dynamic tool\n        if (request.toolCall.name === \"calculate_tip\") {\n          return handler({ ...request, tool: calculateTip });\n        }\n        return handler(request);\n      },\n    });\n\n    const agent = createAgent({\n      model: \"gpt-4o\",\n      tools: [getWeather], // Only static tools registered here\n      middleware: [dynamicToolMiddleware],\n    });\n\n    // The agent can now use both getWeather AND calculateTip\n    const result = await agent.invoke({\n      messages: [{ role: \"user\", content: \"Calculate a 20% tip on $85\" }],\n    });\n    ```\n\n    :::\n\n    This approach is best when:\n    - Tools are discovered at runtime (e.g., from an MCP server)\n    - Tools are generated dynamically based on user data or configuration\n    - You're integrating with external tool registries\n\n    <Note>\n    The `wrap_tool_call` hook is required for runtime-registered tools because the agent needs to know how to execute tools that weren't in the original tool list. Without it, the agent won't know how to invoke the dynamically added tool.\n    </Note>\n\n  </Tab>\n</Tabs>\n\n<Tip>\nTo learn more about tools, see [Tools](/oss/langchain/tools).\n</Tip>\n\n### System prompt\n\n:::python\nYou can shape how your agent approaches tasks by providing a prompt. The @[`system_prompt`] parameter can be provided as a string:\n:::\n\n:::js\nYou can shape how your agent approaches tasks by providing a prompt. The `systemPrompt` parameter can be provided as a string:\n:::\n\n:::python\n```python wrap\nagent = create_agent(\n    model,\n    tools,\n    system_prompt=\"You are a helpful assistant. Be concise and accurate.\"\n)\n```\n:::\n:::js\n```ts wrap\nconst agent = createAgent({\n  model,\n  tools,\n  systemPrompt: \"You are a helpful assistant. Be concise and accurate.\",\n});\n```\n:::\n\n:::python\nWhen no @[`system_prompt`] is provided, the agent will infer its task from the messages directly.\n\nThe @[`system_prompt`] parameter accepts either a `str` or a @[`", "metadata": {"source": "agents.mdx"}}
{"text": ":::\n\n:::js\nYou can shape how your agent approaches tasks by providing a prompt. The `systemPrompt` parameter can be provided as a string:\n:::\n\n:::python\n```python wrap\nagent = create_agent(\n    model,\n    tools,\n    system_prompt=\"You are a helpful assistant. Be concise and accurate.\"\n)\n```\n:::\n:::js\n```ts wrap\nconst agent = createAgent({\n  model,\n  tools,\n  systemPrompt: \"You are a helpful assistant. Be concise and accurate.\",\n});\n```\n:::\n\n:::python\nWhen no @[`system_prompt`] is provided, the agent will infer its task from the messages directly.\n\nThe @[`system_prompt`] parameter accepts either a `str` or a @[`SystemMessage`]. Using a `SystemMessage` gives you more control over the prompt structure, which is useful for provider-specific features like [Anthropic's prompt caching](/oss/integrations/chat/anthropic#prompt-caching):\n\n```python wrap\nfrom langchain.agents import create_agent\nfrom langchain.messages import SystemMessage, HumanMessage\n\nliterary_agent = create_agent(\n    model=\"anthropic:claude-sonnet-4-5\",\n    system_prompt=SystemMessage(\n        content=[\n            {\n                \"type\": \"text\",\n                \"text\": \"You are an AI assistant tasked with analyzing literary works.\",\n            },\n            {\n                \"type\": \"text\",\n                \"text\": \"<the entire contents of 'Pride and Prejudice'>\",\n                \"cache_control\": {\"type\": \"ephemeral\"}\n            }\n        ]\n    )\n)\n\nresult = literary_agent.invoke(\n    {\"messages\": [HumanMessage(\"Analyze the major themes in 'Pride and Prejudice'.\")]}\n)\n```\n\nThe `cache_control` field with `{\"type\": \"ephemeral\"}` tells Anthropic to cache that content block, reducing latency and costs for repeated requests that use the same system prompt.\n:::\n\n:::js\nWhen no `systemPrompt` is provided, the agent will infer its task from the messages directly.\n\nThe `systemPrompt` parameter accepts either a `string` or a `SystemMessage`. Using a `SystemMessage` gives you more control over the prompt structure, which is useful for provider-specific features like [Anthropic's prompt caching](/oss/integrations/chat/anthropic#prompt-caching):\n\n```ts wrap\nimport { createAgent } from \"langchain\";\nimport { SystemMessage, HumanMessage } from \"@langchain/core/messages\";\n\nconst literaryAgent = createAgent({\n  model: \"anthropic:claude-sonnet-4-5\",\n  systemPrompt: new SystemMessage({", "metadata": {"source": "agents.mdx"}}
{"text": "al\"}` tells Anthropic to cache that content block, reducing latency and costs for repeated requests that use the same system prompt.\n:::\n\n:::js\nWhen no `systemPrompt` is provided, the agent will infer its task from the messages directly.\n\nThe `systemPrompt` parameter accepts either a `string` or a `SystemMessage`. Using a `SystemMessage` gives you more control over the prompt structure, which is useful for provider-specific features like [Anthropic's prompt caching](/oss/integrations/chat/anthropic#prompt-caching):\n\n```ts wrap\nimport { createAgent } from \"langchain\";\nimport { SystemMessage, HumanMessage } from \"@langchain/core/messages\";\n\nconst literaryAgent = createAgent({\n  model: \"anthropic:claude-sonnet-4-5\",\n  systemPrompt: new SystemMessage({\n    content: [\n      {\n        type: \"text\",\n        text: \"You are an AI assistant tasked with analyzing literary works.\",\n      },\n      {\n        type: \"text\",\n        text: \"<the entire contents of 'Pride and Prejudice'>\",\n        cache_control: { type: \"ephemeral\" }\n      }\n    ]\n  })\n});\n\nconst result = await literaryAgent.invoke({\n  messages: [new HumanMessage(\"Analyze the major themes in 'Pride and Prejudice'.\")]\n});\n```\n\nThe `cache_control` field with `{ type: \"ephemeral\" }` tells Anthropic to cache that content block, reducing latency and costs for repeated requests that use the same system prompt.\n:::\n\n#### Dynamic system prompt\n\nFor more advanced use cases where you need to modify the system prompt based on runtime context or agent state, you can use [middleware](/oss/langchain/middleware).\n\n:::python\n\nThe @[`@dynamic_prompt`] decorator creates middleware that generates system prompts based on the model request:\n\n```python wrap\nfrom typing import TypedDict\n\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\n\n\nclass Context(TypedDict):\n    user_role: str\n\n@dynamic_prompt\ndef user_role_prompt(request: ModelRequest) -> str:\n    \"\"\"Generate system prompt based on user role.\"\"\"\n    user_role = request.runtime.context.get(\"user_role\", \"user\")\n    base_prompt = \"You are a helpful assistant.\"\n\n    if user_role == \"expert\":\n        return f\"{base_prompt} Provide detailed technical responses.\"\n    elif user_role == \"beginner\":\n        return f\"{base_prompt} Explain concepts simply and avoid jargon.\"\n\n    return base_prompt\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[web_search],\n    middleware=[user_role_prompt],\n    context_schema=Context\n)\n\n# The system prompt will be set dynamically based on context\nresult = agent", "metadata": {"source": "agents.mdx"}}
{"text": " on user role.\"\"\"\n    user_role = request.runtime.context.get(\"user_role\", \"user\")\n    base_prompt = \"You are a helpful assistant.\"\n\n    if user_role == \"expert\":\n        return f\"{base_prompt} Provide detailed technical responses.\"\n    elif user_role == \"beginner\":\n        return f\"{base_prompt} Explain concepts simply and avoid jargon.\"\n\n    return base_prompt\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[web_search],\n    middleware=[user_role_prompt],\n    context_schema=Context\n)\n\n# The system prompt will be set dynamically based on context\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Explain machine learning\"}]},\n    context={\"user_role\": \"expert\"}\n)\n```\n:::\n\n:::js\n```typescript wrap\nimport * as z from \"zod\";\nimport { createAgent, dynamicSystemPromptMiddleware } from \"langchain\";\n\nconst contextSchema = z.object({\n  userRole: z.enum([\"expert\", \"beginner\"]),\n});\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [/* ... */],\n  contextSchema,\n  middleware: [\n    dynamicSystemPromptMiddleware<z.infer<typeof contextSchema>>((state, runtime) => {\n      const userRole = runtime.context.userRole || \"user\";\n      const basePrompt = \"You are a helpful assistant.\";\n\n      if (userRole === \"expert\") {\n        return `${basePrompt} Provide detailed technical responses.`;\n      } else if (userRole === \"beginner\") {\n        return `${basePrompt} Explain concepts simply and avoid jargon.`;\n      }\n      return basePrompt;\n    }),\n  ],\n});\n\n// The system prompt will be set dynamically based on context\nconst result = await agent.invoke(\n  { messages: [{ role: \"user\", content: \"Explain machine learning\" }] },\n  { context: { userRole: \"expert\" } }\n);\n```\n:::\n\n<Tip>\nFor more details on message types and formatting, see [Messages](/oss/langchain/messages). For comprehensive middleware documentation, see [Middleware](/oss/langchain/middleware).\n</Tip>\n\n## Invocation\n\nYou can invoke an agent by passing an update to its [`State`](/oss/langgraph/graph-api#state). All agents include a [sequence of messages](/oss/langgraph/use-graph-api#messagesstate) in their state; to invoke the agent, pass a new message:\n\n:::python\n```python\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in San Francisco?\"}]}\n)\n```\n:::\n:::js\n```typescript\n", "metadata": {"source": "agents.mdx"}}
{"text": " }\n);\n```\n:::\n\n<Tip>\nFor more details on message types and formatting, see [Messages](/oss/langchain/messages). For comprehensive middleware documentation, see [Middleware](/oss/langchain/middleware).\n</Tip>\n\n## Invocation\n\nYou can invoke an agent by passing an update to its [`State`](/oss/langgraph/graph-api#state). All agents include a [sequence of messages](/oss/langgraph/use-graph-api#messagesstate) in their state; to invoke the agent, pass a new message:\n\n:::python\n```python\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in San Francisco?\"}]}\n)\n```\n:::\n:::js\n```typescript\nawait agent.invoke({\n  messages: [{ role: \"user\", content: \"What's the weather in San Francisco?\" }],\n})\n```\n:::\n\nFor streaming steps and / or tokens from the agent, refer to the [streaming](/oss/langchain/streaming) guide.\n\nOtherwise, the agent follows the LangGraph [Graph API](/oss/langgraph/use-graph-api) and supports all associated methods, such as `stream` and `invoke`.\n\n## Advanced concepts\n\n### Structured output\n\n:::python\n\nIn some situations, you may want the agent to return an output in a specific format. LangChain provides strategies for structured output via the @[`response_format`][create_agent(response_format)] parameter.\n\n#### ToolStrategy\n\n`ToolStrategy` uses artificial tool calling to generate structured output. This works with any model that supports tool calling. `ToolStrategy` should be used when provider-native structured output (via [`ProviderStrategy`](#ProviderStrategy)) is not available or reliable.\n\n```python wrap\nfrom pydantic import BaseModel\nfrom langchain.agents import create_agent\nfrom langchain.agents.structured_output import ToolStrategy\n\n\nclass ContactInfo(BaseModel):\n    name: str\n    email: str\n    phone: str\n\nagent = create_agent(\n    model=\"gpt-4.1-mini\",\n    tools=[search_tool],\n    response_format=ToolStrategy(ContactInfo)\n)\n\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\n})\n\nresult[\"structured_response\"]\n# ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')\n```\n\n#### ProviderStrategy\n\n`ProviderStrategy` uses the model provider's native structured output generation. This is more reliable but only works with providers that support native structured output:\n\n```python wrap\nfrom langchain.agents.structured_output import ProviderStrategy\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    response_format=ProviderStrategy(ContactInfo)\n)\n```\n\n<Note>\nAs of `langchain 1.0`, simply passing a schema (e.g., `response_format=ContactInfo`) will default to `ProviderStrategy` if the model supports native structured output.", "metadata": {"source": "agents.mdx"}}
{"text": " (555) 123-4567\"}]\n})\n\nresult[\"structured_response\"]\n# ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')\n```\n\n#### ProviderStrategy\n\n`ProviderStrategy` uses the model provider's native structured output generation. This is more reliable but only works with providers that support native structured output:\n\n```python wrap\nfrom langchain.agents.structured_output import ProviderStrategy\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    response_format=ProviderStrategy(ContactInfo)\n)\n```\n\n<Note>\nAs of `langchain 1.0`, simply passing a schema (e.g., `response_format=ContactInfo`) will default to `ProviderStrategy` if the model supports native structured output. It will fall back to `ToolStrategy` otherwise.\n</Note>\n\n:::\n:::js\nIn some situations, you may want the agent to return an output in a specific format. LangChain provides a simple, universal way to do this with the `responseFormat` parameter.\n\n```ts wrap\nimport * as z from \"zod\";\nimport { createAgent } from \"langchain\";\n\nconst ContactInfo = z.object({\n  name: z.string(),\n  email: z.string(),\n  phone: z.string(),\n});\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  responseFormat: ContactInfo,\n});\n\nconst result = await agent.invoke({\n  messages: [\n    {\n      role: \"user\",\n      content: \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\",\n    },\n  ],\n});\n\nconsole.log(result.structuredResponse);\n// {\n//   name: 'John Doe',\n//   email: 'john@example.com',\n//   phone: '(555) 123-4567'\n// }\n```\n:::\n<Tip>\n    To learn about structured output, see [Structured output](/oss/langchain/structured-output).\n</Tip>\n\n### Memory\n\nAgents maintain conversation history automatically through the message state. You can also configure the agent to use a custom state schema to remember additional information during the conversation.\n\nInformation stored in the state can be thought of as the [short-term memory](/oss/langchain/short-term-memory) of the agent:\n\n:::python\n\nCustom state schemas must extend @[`AgentState`] as a `TypedDict`.\n\nThere are two ways to define custom state:\n1. Via [middleware](/oss/langchain/middleware) (preferred)\n2. Via @[`state_schema`] on @[`create_agent`]\n\n#### Defining state via middleware\n\nUse middleware to define custom state when your custom state needs to be accessed by specific middleware hooks and tools attached to said middleware.\n\n```python\nfrom langchain.agents import AgentState\nfrom langchain.agents.middleware import AgentMiddleware\nfrom typing import Any\n\n\nclass CustomState(AgentState):\n    user_preferences: dict\n\nclass CustomMiddleware(AgentMiddleware):\n    state_schema = CustomState\n    tools = [tool1", "metadata": {"source": "agents.mdx"}}
{"text": "python\n\nCustom state schemas must extend @[`AgentState`] as a `TypedDict`.\n\nThere are two ways to define custom state:\n1. Via [middleware](/oss/langchain/middleware) (preferred)\n2. Via @[`state_schema`] on @[`create_agent`]\n\n#### Defining state via middleware\n\nUse middleware to define custom state when your custom state needs to be accessed by specific middleware hooks and tools attached to said middleware.\n\n```python\nfrom langchain.agents import AgentState\nfrom langchain.agents.middleware import AgentMiddleware\nfrom typing import Any\n\n\nclass CustomState(AgentState):\n    user_preferences: dict\n\nclass CustomMiddleware(AgentMiddleware):\n    state_schema = CustomState\n    tools = [tool1, tool2]\n\n    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n        ...\n\nagent = create_agent(\n    model,\n    tools=tools,\n    middleware=[CustomMiddleware()]\n)\n\n# The agent can now track additional state beyond messages\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\n    \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\n})\n```\n\n#### Defining state via `state_schema`\n\nUse the @[`state_schema`] parameter as a shortcut to define custom state that is only used in tools.\n\n```python\nfrom langchain.agents import AgentState\n\n\nclass CustomState(AgentState):\n    user_preferences: dict\n\nagent = create_agent(\n    model,\n    tools=[tool1, tool2],\n    state_schema=CustomState\n)\n# The agent can now track additional state beyond messages\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\n    \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\n})\n```\n\n<Note>\nAs of `langchain 1.0`, custom state schemas **must** be `TypedDict` types. Pydantic models and dataclasses are no longer supported. See the [v1 migration guide](/oss/migrate/langchain-v1#state-type-restrictions) for more details.\n</Note>\n\n:::\n:::js\n```ts wrap\nimport { z } from \"zod/v4\";\nimport { StateSchema, MessagesValue } from \"@langchain/langgraph\";\nimport { createAgent } from \"langchain\";\n\nconst CustomAgentState = new StateSchema({\n  messages: MessagesValue,\n  userPreferences: z.record(z.string(), z.string()),\n});\n\nconst customAgent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [],\n  stateSchema: CustomAgentState,\n});\n```\n:::\n\n:::python\n<Note>\n    Defining custom state via middleware is preferred over defining it via @[`state_schema`] on @[`create_agent", "metadata": {"source": "agents.mdx"}}
{"text": "1#state-type-restrictions) for more details.\n</Note>\n\n:::\n:::js\n```ts wrap\nimport { z } from \"zod/v4\";\nimport { StateSchema, MessagesValue } from \"@langchain/langgraph\";\nimport { createAgent } from \"langchain\";\n\nconst CustomAgentState = new StateSchema({\n  messages: MessagesValue,\n  userPreferences: z.record(z.string(), z.string()),\n});\n\nconst customAgent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [],\n  stateSchema: CustomAgentState,\n});\n```\n:::\n\n:::python\n<Note>\n    Defining custom state via middleware is preferred over defining it via @[`state_schema`] on @[`create_agent`] because it allows you to keep state extensions conceptually scoped to the relevant middleware and tools.\n\n    @[`state_schema`] is still supported for backwards compatibility on @[`create_agent`].\n</Note>\n:::\n\n<Tip>\n    To learn more about memory, see [Memory](/oss/concepts/memory). For information on implementing long-term memory that persists across sessions, see [Long-term memory](/oss/langchain/long-term-memory).\n</Tip>\n\n### Streaming\n\nWe've seen how the agent can be called with `invoke` to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n\n:::python\n```python\nfor chunk in agent.stream({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Search for AI news and summarize the findings\"}]\n}, stream_mode=\"values\"):\n    # Each chunk contains the full state at that point\n    latest_message = chunk[\"messages\"][-1]\n    if latest_message.content:\n        print(f\"Agent: {latest_message.content}\")\n    elif latest_message.tool_calls:\n        print(f\"Calling tools: {[tc['name'] for tc in latest_message.tool_calls]}\")\n```\n:::\n:::js\n```ts\nconst stream = await agent.stream(\n  {\n    messages: [{\n      role: \"user\",\n      content: \"Search for AI news and summarize the findings\"\n    }],\n  },\n  { streamMode: \"values\" }\n);\n\nfor await (const chunk of stream) {\n  // Each chunk contains the full state at that point\n  const latestMessage = chunk.messages.at(-1);\n  if (latestMessage?.content) {\n    console.log(`Agent: ${latestMessage.content}`);\n  } else if (latestMessage?.tool_calls) {\n    const toolCallNames = latestMessage.tool_calls.map((tc) => tc.name);\n    console.log(`Calling tools: ${toolCallNames.join(\", \")}`);\n  }\n}\n```\n:::\n\n<Tip>\nFor more details on streaming, see [Streaming](/oss/langchain/streaming).\n</Tip>\n", "metadata": {"source": "agents.mdx"}}
{"text": " the findings\"\n    }],\n  },\n  { streamMode: \"values\" }\n);\n\nfor await (const chunk of stream) {\n  // Each chunk contains the full state at that point\n  const latestMessage = chunk.messages.at(-1);\n  if (latestMessage?.content) {\n    console.log(`Agent: ${latestMessage.content}`);\n  } else if (latestMessage?.tool_calls) {\n    const toolCallNames = latestMessage.tool_calls.map((tc) => tc.name);\n    console.log(`Calling tools: ${toolCallNames.join(\", \")}`);\n  }\n}\n```\n:::\n\n<Tip>\nFor more details on streaming, see [Streaming](/oss/langchain/streaming).\n</Tip>\n\n### Middleware\n\n[Middleware](/oss/langchain/middleware) provides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to:\n\n- Process state before the model is called (e.g., message trimming, context injection)\n- Modify or validate the model's response (e.g., guardrails, content filtering)\n- Handle tool execution errors with custom logic\n- Implement dynamic model selection based on state or context\n- Add custom logging, monitoring, or analytics\n\nMiddleware integrates seamlessly into the agent's execution, allowing you to intercept and modify data flow at key points without changing the core agent logic.\n\n:::python\n<Tip>\nFor comprehensive middleware documentation including decorators like @[`@before_model`], @[`@after_model`], and @[`@wrap_tool_call`], see [Middleware](/oss/langchain/middleware).\n</Tip>\n:::\n\n:::js\n<Tip>\nFor comprehensive middleware documentation including hooks like `beforeModel`, `afterModel`, and `wrapToolCall`, see [Middleware](/oss/langchain/middleware).\n</Tip>\n:::\n", "metadata": {"source": "agents.mdx"}}
{"text": "---\ntitle: LangSmith Studio\nsidebarTitle: LangSmith Studio\n---\n\nimport Studio from '/snippets/oss/studio.mdx';\n\n<Studio/>\n\n<Tip>\nFor more information about local and deployed agents, see [Set up local Agent Server](/oss/langchain/studio#setup-local-agent-server) and [Deploy](/oss/langchain/deploy).\n</Tip>\n", "metadata": {"source": "studio.mdx"}}
{"text": "---\ntitle: Philosophy\ndescription: LangChain exists to be the easiest place to start building with LLMs, while also being flexible and production-ready.\nmode: wide\n---\n\nLangChain is driven by a few core beliefs:\n\n- Large Language Models (LLMs) are great, powerful new technology.\n- LLMs are even better when you combine them with external sources of data.\n- LLMs will transform what the applications of the future look like. Specifically, the applications of the future will look more and more agentic.\n- It is still very early on in that transformation.\n- While it's easy to build a prototype of those agentic applications, it's still really hard to build agents that are reliable enough to put into production.\n\nWith LangChain, we have two core focuses:\n\n<Steps>\n    <Step title=\"We want to enable developers to build with the best models.\">\n        Different providers expose different APIs, with different model parameters and different message formats.\n        Standardizing these model inputs and outputs is a core focus, making it easy for developer to easily change to the most recent state-of-the-art model, avoiding lock-in.\n    </Step>\n    <Step title=\"We want to make it easy to use models to orchestrate more complex flows that interact with other data and computation.\">\n        Models should be used for more than just *text generation* - they should also be used to orchestrate more complex flows that interact with other data. LangChain makes it easy to define [tools](/oss/langchain/tools) that LLMs can use dynamically, as well as help with parsing of and access to unstructured data.\n    </Step>\n</Steps>\n\n## History\n\nGiven the constant rate of change in the field, LangChain has also evolved over time. Below is a brief timeline of how LangChain has changed over the years, evolving alongside what it means to build with LLMs:\n\n<Update label=\"2022-10-24\" description=\"v0.0.1\">\n    A month before ChatGPT, **LangChain was launched as a Python package**. It consisted of two main components:\n\n    - LLM abstractions\n    - \"Chains\", or predetermined steps of computation to run, for common use cases. For example - RAG: run a retrieval step, then run a generation step.\n\n    The name LangChain comes from \"Language\" (like Language models) and \"Chains\".\n</Update>\n\n<Update label=\"2022-12\">\n    The first general purpose agents were added to LangChain.\n\n    These general purpose agents were based on the [ReAct paper](https://arxiv.org/abs/2210.03629) (ReAct standing for Reasoning and Acting). They used LLMs to generate JSON that represented tool calls, and then parsed that JSON to determine what tools to call.\n</Update>\n\n<Update label=\"2023-01\">\n    OpenAI releases a 'Chat Completion' API.\n\n    Previously, models took in strings and returned a string. In the ChatCompletions API, they evolved to take in a list of messages and return a message. Other model providers followed suit, and LangChain updated to work with lists of messages.\n</Update>\n\n<Update label=\"2023-01\">\n    LangChain releases a JavaScript version.\n\n    LLMs and agents will change how applications are built and JavaScript is the language of application developers.", "metadata": {"source": "philosophy.mdx"}}
{"text": "\n\n    These general purpose agents were based on the [ReAct paper](https://arxiv.org/abs/2210.03629) (ReAct standing for Reasoning and Acting). They used LLMs to generate JSON that represented tool calls, and then parsed that JSON to determine what tools to call.\n</Update>\n\n<Update label=\"2023-01\">\n    OpenAI releases a 'Chat Completion' API.\n\n    Previously, models took in strings and returned a string. In the ChatCompletions API, they evolved to take in a list of messages and return a message. Other model providers followed suit, and LangChain updated to work with lists of messages.\n</Update>\n\n<Update label=\"2023-01\">\n    LangChain releases a JavaScript version.\n\n    LLMs and agents will change how applications are built and JavaScript is the language of application developers.\n</Update>\n\n<Update label=\"2023-02\">\n    **LangChain Inc. was formed as a company** around the open source LangChain project.\n\n    The main goal was to \"make intelligent agents ubiquitous\". The team recognized that while LangChain was a key part (LangChain made it simple to get started with LLMs), there was also a need for other components.\n</Update>\n\n<Update label=\"2023-03\">\n    OpenAI releases 'function calling' in their API.\n\n    This allowed the API to explicitly generate payloads that represented tool calls. Other model providers followed suit, and LangChain was updated to use this as the preferred method for tool calling (rather than parsing JSON).\n</Update>\n\n<Update label=\"2023-06\">\n    **LangSmith is released** as closed source platform by LangChain Inc., providing observability and evals.\n\n    The main issue with building agents is getting them to be reliable, and LangSmith, which provides observability and evals, was built to solve that need. LangChain was updated to integrate seamlessly with LangSmith.\n</Update>\n\n<Update label=\"2024-01\" description=\"v0.1.0\">\n    **LangChain releases 0.1.0**, its first non-0.0.x.\n\n    The industry matured from prototypes to production, and as such, LangChain increased its focus on stability.\n</Update>\n\n<Update label=\"2024-02\">\n    **LangGraph is released** as an open-source library.\n\n    The original LangChain had two focuses: LLM abstractions, and high-level interfaces for getting started with common applications; however, it was missing a low-level orchestration layer that allowed developers to control the exact flow of their agent. Enter: LangGraph.\n\n    When building LangGraph, we learned from lessons when building LangChain and added functionality we discovered was needed: streaming, durable execution, short-term memory, human-in-the-loop, and more.\n</Update>\n\n<Update label=\"2024-06\">\n    **LangChain has over 700 integrations.**\n\n    :::python\n    Integrations were split out of the core LangChain package, and either moved into their own standalone packages (for the core integrations) or `langchain-community`.\n    :::\n    :::js\n    Integrations were split out of the core LangChain package, and either moved into their own standalone packages (for the core integrations) or `@langchain/community`.\n    :::\n</Update>", "metadata": {"source": "philosophy.mdx"}}
{"text": " that allowed developers to control the exact flow of their agent. Enter: LangGraph.\n\n    When building LangGraph, we learned from lessons when building LangChain and added functionality we discovered was needed: streaming, durable execution, short-term memory, human-in-the-loop, and more.\n</Update>\n\n<Update label=\"2024-06\">\n    **LangChain has over 700 integrations.**\n\n    :::python\n    Integrations were split out of the core LangChain package, and either moved into their own standalone packages (for the core integrations) or `langchain-community`.\n    :::\n    :::js\n    Integrations were split out of the core LangChain package, and either moved into their own standalone packages (for the core integrations) or `@langchain/community`.\n    :::\n</Update>\n\n<Update label=\"2024-10\">\n    LangGraph becomes the preferred way to build any AI application that is more than a single LLM call.\n\n    As developers tried to improve the reliability of their applications, they needed more control than the high-level interfaces provided. LangGraph provided that low-level flexibility. Most chains and agents were marked as deprecated in LangChain with guides on how to migrate them to LangGraph. There is still one high-level abstraction created in LangGraph: an agent abstraction. It is built on top of low-level LangGraph and has the same interface as the ReAct agents from LangChain.\n</Update>\n\n<Update label=\"2025-04\">\n    Model APIs become more multimodal.\n\n    :::python\n    Models started to accept files, images, videos, and more. We updated the `langchain-core` message format accordingly to allow developers to specify these multimodal inputs in a standard way.\n    :::\n    :::js\n    Models started to accept files, images, videos, and more. We updated the `@langchain/core` message format accordingly to allow developers to specify these multimodal inputs in a standard way.\n    :::\n</Update>\n\n<Update label=\"2025-10-20\" description=\"v1.0.0\">\n    **LangChain releases 1.0** with two major changes:\n\n    1. Complete revamp of all chains and agents in `langchain`. All chains and agents are now replaced with only one high level abstraction: an agent abstraction built on top of LangGraph. This was the high-level abstraction that was originally created in LangGraph, but just moved to LangChain.\n\n        :::python\n        For users still using old LangChain chains/agents who do NOT want to upgrade (note: we recommend you do), you can continue using old LangChain by installing the `langchain-classic` package.\n        :::\n        :::js\n        For users still using old LangChain chains/agents who do NOT want to upgrade (note: we recommend you do), you can continue using old LangChain by installing the `@langchain/classic` package.\n        :::\n\n    2. A standard message content format: Model APIs evolved from returning messages with a simple content string to more complex output types - reasoning blocks, citations, server-side tool calls, etc. LangChain evolved its message formats to standardize these across providers.\n</Update>\n", "metadata": {"source": "philosophy.mdx"}}
{"text": "---\ntitle: Tools\n---\n\nTools extend what [agents](/oss/langchain/agents) can do\u2014letting them fetch real-time data, execute code, query external databases, and take actions in the world.\n\nUnder the hood, tools are callable functions with well-defined inputs and outputs that get passed to a [chat model](/oss/langchain/models). The model decides when to invoke a tool based on the conversation context, and what input arguments to provide.\n\n<Tip>\n    For details on how models handle tool calls, see [Tool calling](/oss/langchain/models#tool-calling).\n</Tip>\n\n## Create tools\n\n### Basic tool definition\n\n:::python\nThe simplest way to create a tool is with the @[`@tool`] decorator. By default, the function's docstring becomes the tool's description that helps the model understand when to use it:\n\n```python\nfrom langchain.tools import tool\n\n@tool\ndef search_database(query: str, limit: int = 10) -> str:\n    \"\"\"Search the customer database for records matching the query.\n\n    Args:\n        query: Search terms to look for\n        limit: Maximum number of results to return\n    \"\"\"\n    return f\"Found {limit} results for '{query}'\"\n```\n\nType hints are **required** as they define the tool's input schema. The docstring should be informative and concise to help the model understand the tool's purpose.\n:::\n\n:::js\nThe simplest way to create a tool is by importing the `tool` function from the `langchain` package. You can use [zod](https://zod.dev/) to define the tool's input schema:\n\n```ts\nimport * as z from \"zod\"\nimport { tool } from \"langchain\"\n\nconst searchDatabase = tool(\n  ({ query, limit }) => `Found ${limit} results for '${query}'`,\n  {\n    name: \"search_database\",\n    description: \"Search the customer database for records matching the query.\",\n    schema: z.object({\n      query: z.string().describe(\"Search terms to look for\"),\n      limit: z.number().describe(\"Maximum number of results to return\"),\n    }),\n  }\n);\n```\n:::\n\n<Note>\n    **Server-side tool use:** Some chat models feature built-in tools (web search, code interpreters) that are executed server-side. See [Server-side tool use](#server-side-tool-use) for details.\n</Note>\n\n:::python\n### Customize tool properties\n\n#### Custom tool name\n\nBy default, the tool name comes from the function name. Override it when you need something more descriptive:\n\n```python\n@tool(\"web_search\")  # Custom name\ndef search(query: str) -> str:\n    \"\"\"Search the web for information.\"\"\"\n    return f\"Results for: {query}\"\n\nprint(search.name)  # web_search\n```\n\n#### Custom tool description\n\nOverride the auto-generated tool description for clearer model guidance:\n\n```python\n@tool(\"calculator\", description=\"Performs arithmetic calculations. Use this for any math problems.\")\ndef calc(expression: str) -> str:\n    \"\"\"", "metadata": {"source": "tools.mdx"}}
{"text": "-side. See [Server-side tool use](#server-side-tool-use) for details.\n</Note>\n\n:::python\n### Customize tool properties\n\n#### Custom tool name\n\nBy default, the tool name comes from the function name. Override it when you need something more descriptive:\n\n```python\n@tool(\"web_search\")  # Custom name\ndef search(query: str) -> str:\n    \"\"\"Search the web for information.\"\"\"\n    return f\"Results for: {query}\"\n\nprint(search.name)  # web_search\n```\n\n#### Custom tool description\n\nOverride the auto-generated tool description for clearer model guidance:\n\n```python\n@tool(\"calculator\", description=\"Performs arithmetic calculations. Use this for any math problems.\")\ndef calc(expression: str) -> str:\n    \"\"\"Evaluate mathematical expressions.\"\"\"\n    return str(eval(expression))\n```\n\n### Advanced schema definition\n\nDefine complex inputs with Pydantic models or JSON schemas:\n\n<CodeGroup>\n    ```python Pydantic model\n    from pydantic import BaseModel, Field\n    from typing import Literal\n\n    class WeatherInput(BaseModel):\n        \"\"\"Input for weather queries.\"\"\"\n        location: str = Field(description=\"City name or coordinates\")\n        units: Literal[\"celsius\", \"fahrenheit\"] = Field(\n            default=\"celsius\",\n            description=\"Temperature unit preference\"\n        )\n        include_forecast: bool = Field(\n            default=False,\n            description=\"Include 5-day forecast\"\n        )\n\n    @tool(args_schema=WeatherInput)\n    def get_weather(location: str, units: str = \"celsius\", include_forecast: bool = False) -> str:\n        \"\"\"Get current weather and optional forecast.\"\"\"\n        temp = 22 if units == \"celsius\" else 72\n        result = f\"Current weather in {location}: {temp} degrees {units[0].upper()}\"\n        if include_forecast:\n            result += \"\\nNext 5 days: Sunny\"\n        return result\n    ```\n\n    ```python JSON Schema\n    weather_schema = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\"type\": \"string\"},\n            \"units\": {\"type\": \"string\"},\n            \"include_forecast\": {\"type\": \"boolean\"}\n        },\n        \"required\": [\"location\", \"units\", \"include_forecast\"]\n    }\n\n    @tool(args_schema=weather_schema)\n ", "metadata": {"source": "tools.mdx"}}
{"text": "      result += \"\\nNext 5 days: Sunny\"\n        return result\n    ```\n\n    ```python JSON Schema\n    weather_schema = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\"type\": \"string\"},\n            \"units\": {\"type\": \"string\"},\n            \"include_forecast\": {\"type\": \"boolean\"}\n        },\n        \"required\": [\"location\", \"units\", \"include_forecast\"]\n    }\n\n    @tool(args_schema=weather_schema)\n    def get_weather(location: str, units: str = \"celsius\", include_forecast: bool = False) -> str:\n        \"\"\"Get current weather and optional forecast.\"\"\"\n        temp = 22 if units == \"celsius\" else 72\n        result = f\"Current weather in {location}: {temp} degrees {units[0].upper()}\"\n        if include_forecast:\n            result += \"\\nNext 5 days: Sunny\"\n        return result\n    ```\n</CodeGroup>\n\n### Reserved argument names\n\nThe following parameter names are reserved and cannot be used as tool arguments. Using these names will cause runtime errors.\n\n| Parameter name | Purpose |\n|----------------|---------|\n| `config` | Reserved for passing `RunnableConfig` to tools internally |\n| `runtime` | Reserved for `ToolRuntime` parameter (accessing state, context, store) |\n\nTo access runtime information, use the @[`ToolRuntime`] parameter instead of naming your own arguments `config` or `runtime`.\n:::\n\n## Access context\n\nTools are most powerful when they can access runtime information like conversation history, user data, and persistent memory. This section covers how to access and update this information from within your tools.\n\n:::python\nTools can access runtime information through the @[`ToolRuntime`] parameter, which provides:\n\n| Component | Description | Use case |\n|-----------|-------------|----------|\n| **State** | Short-term memory - mutable data that exists for the current conversation (messages, counters, custom fields) | Access conversation history, track tool call counts |\n| **Context** | Immutable configuration passed at invocation time (user IDs, session info) | Personalize responses based on user identity |\n| **Store** | Long-term memory - persistent data that survives across conversations | Save user preferences, maintain knowledge base |\n| **Stream Writer** | Emit real-time updates during tool execution | Show progress for long-running operations |\n| **Config** | @[`RunnableConfig`] for the execution | Access callbacks, tags, and metadata |\n| **Tool Call ID** | Unique identifier for the current tool invocation | Correlate tool calls for logs and model invocations |\n\n```mermaid\ngraph LR\n    %% Runtime Context\n    subgraph \"\ud83d\udd27 Tool Runtime Context\"\n        A[Tool Call] --> B[ToolRuntime]\n        B --> C[State", "metadata": {"source": "tools.mdx"}}
{"text": " counts |\n| **Context** | Immutable configuration passed at invocation time (user IDs, session info) | Personalize responses based on user identity |\n| **Store** | Long-term memory - persistent data that survives across conversations | Save user preferences, maintain knowledge base |\n| **Stream Writer** | Emit real-time updates during tool execution | Show progress for long-running operations |\n| **Config** | @[`RunnableConfig`] for the execution | Access callbacks, tags, and metadata |\n| **Tool Call ID** | Unique identifier for the current tool invocation | Correlate tool calls for logs and model invocations |\n\n```mermaid\ngraph LR\n    %% Runtime Context\n    subgraph \"\ud83d\udd27 Tool Runtime Context\"\n        A[Tool Call] --> B[ToolRuntime]\n        B --> C[State Access]\n        B --> D[Context Access]\n        B --> E[Store Access]\n        B --> F[Stream Writer]\n    end\n\n    %% Available Resources\n    subgraph \"\ud83d\udcca Available Resources\"\n        C --> G[Messages]\n        C --> H[Custom State]\n        D --> I[User ID]\n        D --> J[Session Info]\n        E --> K[Long-term Memory]\n        E --> L[User Preferences]\n    end\n\n    %% Tool Capabilities\n    subgraph \"\u26a1 Enhanced Tool Capabilities\"\n        M[Context-Aware Tools]\n        N[Stateful Tools]\n        O[Memory-Enabled Tools]\n        P[Streaming Tools]\n    end\n\n    %% Connections\n    G --> M\n    H --> N\n    I --> M\n    J --> M\n    K --> O\n    L --> O\n    F --> P\n```\n\n### Short-term memory (State)\n\nState represents short-term memory that exists for the duration of a conversation. It includes the message history and any custom fields you define in your [graph state](/oss/langgraph/graph-api#state).\n\n<Info>\n    Add `runtime: ToolRuntime` to your tool signature to access state. This parameter is automatically injected and hidden from the LLM - it won't appear in the tool's schema.\n</Info>\n\n#### Access state\n\nTools can access the current conversation state using `runtime.state`:\n\n```python\nfrom langchain.tools import tool, ToolRuntime\nfrom langchain.messages import HumanMessage\n\n@tool\ndef get_last_user_message(runtime: ToolRuntime) -> str:\n    \"\"\"Get the most recent message from the user.\"\"\"\n    messages = runtime.state[\"messages\"]\n\n    # Find the last human message\n    for message in reversed(messages):\n        if isinstance(message, HumanMessage):\n            return message.content\n\n    return \"No user messages found\"\n\n# Access custom state fields\n@tool\ndef get_user_", "metadata": {"source": "tools.mdx"}}
{"text": " and hidden from the LLM - it won't appear in the tool's schema.\n</Info>\n\n#### Access state\n\nTools can access the current conversation state using `runtime.state`:\n\n```python\nfrom langchain.tools import tool, ToolRuntime\nfrom langchain.messages import HumanMessage\n\n@tool\ndef get_last_user_message(runtime: ToolRuntime) -> str:\n    \"\"\"Get the most recent message from the user.\"\"\"\n    messages = runtime.state[\"messages\"]\n\n    # Find the last human message\n    for message in reversed(messages):\n        if isinstance(message, HumanMessage):\n            return message.content\n\n    return \"No user messages found\"\n\n# Access custom state fields\n@tool\ndef get_user_preference(\n    pref_name: str,\n    runtime: ToolRuntime\n) -> str:\n    \"\"\"Get a user preference value.\"\"\"\n    preferences = runtime.state.get(\"user_preferences\", {})\n    return preferences.get(pref_name, \"Not set\")\n```\n\n<Warning>\n    The `runtime` parameter is hidden from the model. For the example above, the model only sees `pref_name` in the tool schema.\n</Warning>\n\n#### Update state\n\nUse @[`Command`] to update the agent's state. This is useful for tools that need to update custom state fields:\n\n```python\nfrom langgraph.types import Command\nfrom langchain.tools import tool\n\n@tool\ndef set_user_name(new_name: str) -> Command:\n    \"\"\"Set the user's name in the conversation state.\"\"\"\n    return Command(update={\"user_name\": new_name})\n```\n\n<Tip>\n    When tools update state variables, consider defining a [reducer](/oss/langgraph/graph-api#reducers) for those fields. Since LLMs can call multiple tools in parallel, a reducer determines how to resolve conflicts when the same state field is updated by concurrent tool calls.\n</Tip>\n:::\n\n### Context\n\nContext provides immutable configuration data that is passed at invocation time. Use it for user IDs, session details, or application-specific settings that shouldn't change during a conversation.\n\n:::python\nAccess context through `runtime.context`:\n\n```python\nfrom dataclasses import dataclass\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool, ToolRuntime\n\n\nUSER_DATABASE = {\n    \"user123\": {\n        \"name\": \"Alice Johnson\",\n        \"account_type\": \"Premium\",\n        \"balance\": 5000,\n        \"email\": \"alice@example.com\"\n    },\n    \"user456\": {\n        \"name\": \"Bob Smith\",\n        \"account_type\": \"Standard\",\n        \"balance\": 1200,\n        \"email\": \"bob@example.com\"\n    }\n}\n\n@dataclass\nclass UserContext:\n    user_id: str", "metadata": {"source": "tools.mdx"}}
{"text": " create_agent\nfrom langchain.tools import tool, ToolRuntime\n\n\nUSER_DATABASE = {\n    \"user123\": {\n        \"name\": \"Alice Johnson\",\n        \"account_type\": \"Premium\",\n        \"balance\": 5000,\n        \"email\": \"alice@example.com\"\n    },\n    \"user456\": {\n        \"name\": \"Bob Smith\",\n        \"account_type\": \"Standard\",\n        \"balance\": 1200,\n        \"email\": \"bob@example.com\"\n    }\n}\n\n@dataclass\nclass UserContext:\n    user_id: str\n\n@tool\ndef get_account_info(runtime: ToolRuntime[UserContext]) -> str:\n    \"\"\"Get the current user's account information.\"\"\"\n    user_id = runtime.context.user_id\n\n    if user_id in USER_DATABASE:\n        user = USER_DATABASE[user_id]\n        return f\"Account holder: {user['name']}\\nType: {user['account_type']}\\nBalance: ${user['balance']}\"\n    return \"User not found\"\n\nmodel = ChatOpenAI(model=\"gpt-4.1\")\nagent = create_agent(\n    model,\n    tools=[get_account_info],\n    context_schema=UserContext,\n    system_prompt=\"You are a financial assistant.\"\n)\n\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my current balance?\"}]},\n    context=UserContext(user_id=\"user123\")\n)\n```\n:::\n\n:::js\nTools can access an agent's runtime context through the `config` parameter:\n\n```ts\nimport * as z from \"zod\"\nimport { ChatOpenAI } from \"@langchain/openai\"\nimport { createAgent } from \"langchain\"\n\nconst getUserName = tool(\n  (_, config) => {\n    return config.context.user_name\n  },\n  {\n    name: \"get_user_name\",\n    description: \"Get the user's name.\",\n    schema: z.object({}),\n  }\n);\n\nconst contextSchema = z.object({\n  user_name: z.string(),\n});\n\nconst agent = createAgent({\n  model: new ChatOpenAI({ model: \"gpt-4.1\" }),\n  tools: [getUserName],\n  contextSchema,\n});\n\nconst result = await agent.invoke(\n  {\n    messages: [{ role: \"user\", content: \"What is my name?\" }]\n  },\n  {\n    context: { user_name: \"John Smith\" }\n  }\n);\n```\n:::\n\n### Long-term memory (Store)\n\nThe @[`BaseStore`] provides persistent storage that survives across conversations. Unlike state (short-term memory), data saved to the store remains available in future sessions.\n\n::", "metadata": {"source": "tools.mdx"}}
{"text": ": z.object({}),\n  }\n);\n\nconst contextSchema = z.object({\n  user_name: z.string(),\n});\n\nconst agent = createAgent({\n  model: new ChatOpenAI({ model: \"gpt-4.1\" }),\n  tools: [getUserName],\n  contextSchema,\n});\n\nconst result = await agent.invoke(\n  {\n    messages: [{ role: \"user\", content: \"What is my name?\" }]\n  },\n  {\n    context: { user_name: \"John Smith\" }\n  }\n);\n```\n:::\n\n### Long-term memory (Store)\n\nThe @[`BaseStore`] provides persistent storage that survives across conversations. Unlike state (short-term memory), data saved to the store remains available in future sessions.\n\n:::python\nAccess the store through `runtime.store`. The store uses a namespace/key pattern to organize data:\n\n<Tip>\n    For production deployments, use a persistent store implementation like @[`PostgresStore`] instead of `InMemoryStore`. See the [memory documentation](/oss/langgraph/memory) for setup details.\n</Tip>\n\n```python expandable\nfrom typing import Any\nfrom langgraph.store.memory import InMemoryStore\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool, ToolRuntime\n\n\n# Access memory\n@tool\ndef get_user_info(user_id: str, runtime: ToolRuntime) -> str:\n    \"\"\"Look up user info.\"\"\"\n    store = runtime.store\n    user_info = store.get((\"users\",), user_id)\n    return str(user_info.value) if user_info else \"Unknown user\"\n\n# Update memory\n@tool\ndef save_user_info(user_id: str, user_info: dict[str, Any], runtime: ToolRuntime) -> str:\n    \"\"\"Save user info.\"\"\"\n    store = runtime.store\n    store.put((\"users\",), user_id, user_info)\n    return \"Successfully saved user info.\"\n\nstore = InMemoryStore()\nagent = create_agent(\n    model,\n    tools=[get_user_info, save_user_info],\n    store=store\n)\n\n# First session: save user info\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Save the following user: userid: abc123, name: Foo, age: 25, email: foo@langchain.dev\"}]\n})\n\n# Second session: get user info\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Get user info for user with id 'abc123'\"}]\n})\n# Here is the user info for user with ID \"abc123\":\n# - Name: Foo\n# - Age: 25\n# - Email: foo@langchain.dev\n```\n:::\n\n:::js\nAccess the store through `config.store`. The store uses a namespace/key pattern to organize data:\n\n```ts expandable\nimport * as z from \"zod\";\nimport { createAgent, tool } from \"langchain\";\nimport { InMemoryStore } from \"@langchain/langgraph\";\nimport { ChatOpenAI } from \"@langchain/openai\";", "metadata": {"source": "tools.mdx"}}
{"text": ", name: Foo, age: 25, email: foo@langchain.dev\"}]\n})\n\n# Second session: get user info\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Get user info for user with id 'abc123'\"}]\n})\n# Here is the user info for user with ID \"abc123\":\n# - Name: Foo\n# - Age: 25\n# - Email: foo@langchain.dev\n```\n:::\n\n:::js\nAccess the store through `config.store`. The store uses a namespace/key pattern to organize data:\n\n```ts expandable\nimport * as z from \"zod\";\nimport { createAgent, tool } from \"langchain\";\nimport { InMemoryStore } from \"@langchain/langgraph\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst store = new InMemoryStore();\n\n// Access memory\nconst getUserInfo = tool(\n  async ({ user_id }) => {\n    const value = await store.get([\"users\"], user_id);\n    console.log(\"get_user_info\", user_id, value);\n    return value;\n  },\n  {\n    name: \"get_user_info\",\n    description: \"Look up user info.\",\n    schema: z.object({\n      user_id: z.string(),\n    }),\n  }\n);\n\n// Update memory\nconst saveUserInfo = tool(\n  async ({ user_id, name, age, email }) => {\n    console.log(\"save_user_info\", user_id, name, age, email);\n    await store.put([\"users\"], user_id, { name, age, email });\n    return \"Successfully saved user info.\";\n  },\n  {\n    name: \"save_user_info\",\n    description: \"Save user info.\",\n    schema: z.object({\n      user_id: z.string(),\n      name: z.string(),\n      age: z.number(),\n      email: z.string(),\n    }),\n  }\n);\n\nconst agent = createAgent({\n  model: new ChatOpenAI({ model: \"gpt-4.1\" }),\n  tools: [getUserInfo, saveUserInfo],\n  store,\n});\n\n// First session: save user info\nawait agent.invoke({\n  messages: [\n    {\n      role: \"user\",\n      content: \"Save the following user: userid: abc123, name: Foo, age: 25, email: foo@langchain.dev\",\n    },\n  ],\n});\n\n// Second session: get user info\nconst result = await agent.invoke({\n  messages: [\n    { role: \"user\", content: \"Get user info for user with id 'abc123'\" },\n  ],\n});\n\nconsole.log(result);\n// Here is the user info for user with ID \"abc123\":\n// - Name: Foo\n// - Age: 25\n// - Email: foo@langchain.dev\n```\n:::\n\n### Stream writer\n\nStream real-time updates from tools during execution. This is useful for providing progress feedback to users during", "metadata": {"source": "tools.mdx"}}
{"text": "  messages: [\n    {\n      role: \"user\",\n      content: \"Save the following user: userid: abc123, name: Foo, age: 25, email: foo@langchain.dev\",\n    },\n  ],\n});\n\n// Second session: get user info\nconst result = await agent.invoke({\n  messages: [\n    { role: \"user\", content: \"Get user info for user with id 'abc123'\" },\n  ],\n});\n\nconsole.log(result);\n// Here is the user info for user with ID \"abc123\":\n// - Name: Foo\n// - Age: 25\n// - Email: foo@langchain.dev\n```\n:::\n\n### Stream writer\n\nStream real-time updates from tools during execution. This is useful for providing progress feedback to users during long-running operations.\n\n:::python\nUse `runtime.stream_writer` to emit custom updates:\n\n```python\nfrom langchain.tools import tool, ToolRuntime\n\n@tool\ndef get_weather(city: str, runtime: ToolRuntime) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    writer = runtime.stream_writer\n\n    # Stream custom updates as the tool executes\n    writer(f\"Looking up data for city: {city}\")\n    writer(f\"Acquired data for city: {city}\")\n\n    return f\"It's always sunny in {city}!\"\n```\n\n<Note>\nIf you use `runtime.stream_writer` inside your tool, the tool must be invoked within a LangGraph execution context. See [Streaming](/oss/langchain/streaming) for more details.\n</Note>\n:::\n\n:::js\nUse `config.writer` to emit custom updates:\n\n```ts\nimport * as z from \"zod\";\nimport { tool, ToolRuntime } from \"langchain\";\n\nconst getWeather = tool(\n  ({ city }, config: ToolRuntime) => {\n    const writer = config.writer;\n\n    // Stream custom updates as the tool executes\n    if (writer) {\n      writer(`Looking up data for city: ${city}`);\n      writer(`Acquired data for city: ${city}`);\n    }\n\n    return `It's always sunny in ${city}!`;\n  },\n  {\n    name: \"get_weather\",\n    description: \"Get weather for a given city.\",\n    schema: z.object({\n      city: z.string(),\n    }),\n  }\n);\n```\n:::\n\n## ToolNode\n\n@[`ToolNode`] is a prebuilt node that executes tools in LangGraph workflows. It handles parallel tool execution, error handling, and state injection automatically.\n\n<Info>\n    For custom workflows where you need fine-grained control over tool execution patterns, use @[`ToolNode`] instead of @[`create_agent`]. It's the building block that powers agent tool execution.\n</Info>\n\n### Basic usage\n\n:::python\n```python\nfrom langchain.tools import tool\nfrom langgraph.prebuilt import ToolNode\nfrom langgraph.graph import StateGraph, MessagesState, START, END\n\n@tool\ndef search(query:", "metadata": {"source": "tools.mdx"}}
{"text": " city.\",\n    schema: z.object({\n      city: z.string(),\n    }),\n  }\n);\n```\n:::\n\n## ToolNode\n\n@[`ToolNode`] is a prebuilt node that executes tools in LangGraph workflows. It handles parallel tool execution, error handling, and state injection automatically.\n\n<Info>\n    For custom workflows where you need fine-grained control over tool execution patterns, use @[`ToolNode`] instead of @[`create_agent`]. It's the building block that powers agent tool execution.\n</Info>\n\n### Basic usage\n\n:::python\n```python\nfrom langchain.tools import tool\nfrom langgraph.prebuilt import ToolNode\nfrom langgraph.graph import StateGraph, MessagesState, START, END\n\n@tool\ndef search(query: str) -> str:\n    \"\"\"Search for information.\"\"\"\n    return f\"Results for: {query}\"\n\n@tool\ndef calculator(expression: str) -> str:\n    \"\"\"Evaluate a math expression.\"\"\"\n    return str(eval(expression))\n\n# Create the ToolNode with your tools\ntool_node = ToolNode([search, calculator])\n\n# Use in a graph\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(\"tools\", tool_node)\n# ... add other nodes and edges\n```\n:::\n\n:::js\n```typescript\nimport { ToolNode } from \"@langchain/langgraph/prebuilt\";\nimport { tool } from \"@langchain/core/tools\";\nimport * as z from \"zod\";\n\nconst search = tool(\n  ({ query }) => `Results for: ${query}`,\n  {\n    name: \"search\",\n    description: \"Search for information.\",\n    schema: z.object({ query: z.string() }),\n  }\n);\n\nconst calculator = tool(\n  ({ expression }) => String(eval(expression)),\n  {\n    name: \"calculator\",\n    description: \"Evaluate a math expression.\",\n    schema: z.object({ expression: z.string() }),\n  }\n);\n\n// Create the ToolNode with your tools\nconst toolNode = new ToolNode([search, calculator]);\n```\n:::\n\n### Error handling\n\nConfigure how tool errors are handled. See the @[`ToolNode`] API reference for all options.\n\n:::python\n```python\nfrom langgraph.prebuilt import ToolNode\n\n# Default: catch invocation errors, re-raise execution errors\ntool_node = ToolNode(tools)\n\n# Catch all errors and return error message to LLM\ntool_node = ToolNode(tools, handle_tool_errors=True)\n\n# Custom error message\ntool_node = ToolNode(tools, handle_tool_errors=\"Something went wrong, please try again.\")\n\n# Custom error handler\ndef handle_error(e: ValueError) -> str:\n    return f\"Invalid input: {e}\"\n\ntool_node = ToolNode(tools, handle_tool_errors=handle_error)\n\n# Only catch specific exception types\ntool_node = ToolNode(tools, handle_tool_errors=(ValueError, TypeError))\n```\n:::\n\n:::js\n```typescript\nimport { ToolNode } from \"@langchain/langgraph/", "metadata": {"source": "tools.mdx"}}
{"text": "\n\n# Default: catch invocation errors, re-raise execution errors\ntool_node = ToolNode(tools)\n\n# Catch all errors and return error message to LLM\ntool_node = ToolNode(tools, handle_tool_errors=True)\n\n# Custom error message\ntool_node = ToolNode(tools, handle_tool_errors=\"Something went wrong, please try again.\")\n\n# Custom error handler\ndef handle_error(e: ValueError) -> str:\n    return f\"Invalid input: {e}\"\n\ntool_node = ToolNode(tools, handle_tool_errors=handle_error)\n\n# Only catch specific exception types\ntool_node = ToolNode(tools, handle_tool_errors=(ValueError, TypeError))\n```\n:::\n\n:::js\n```typescript\nimport { ToolNode } from \"@langchain/langgraph/prebuilt\";\n\n// Default behavior\nconst toolNode = new ToolNode(tools);\n\n// Catch all errors\nconst toolNode = new ToolNode(tools, { handleToolErrors: true });\n\n// Custom error message\nconst toolNode = new ToolNode(tools, {\n  handleToolErrors: \"Something went wrong, please try again.\"\n});\n```\n:::\n\n### Route with tools_condition\n\nUse @[`tools_condition`] for conditional routing based on whether the LLM made tool calls:\n\n:::python\n```python\nfrom langgraph.prebuilt import ToolNode, tools_condition\nfrom langgraph.graph import StateGraph, MessagesState, START, END\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(\"llm\", call_llm)\nbuilder.add_node(\"tools\", ToolNode(tools))\n\nbuilder.add_edge(START, \"llm\")\nbuilder.add_conditional_edges(\"llm\", tools_condition)  # Routes to \"tools\" or END\nbuilder.add_edge(\"tools\", \"llm\")\n\ngraph = builder.compile()\n```\n:::\n\n:::js\n```typescript\nimport { ToolNode, toolsCondition } from \"@langchain/langgraph/prebuilt\";\nimport { StateGraph, MessagesAnnotation } from \"@langchain/langgraph\";\n\nconst builder = new StateGraph(MessagesAnnotation)\n  .addNode(\"llm\", callLlm)\n  .addNode(\"tools\", new ToolNode(tools))\n  .addEdge(\"__start__\", \"llm\")\n  .addConditionalEdges(\"llm\", toolsCondition)  // Routes to \"tools\" or \"__end__\"\n  .addEdge(\"tools\", \"llm\");\n\nconst graph = builder.compile();\n```\n:::\n\n### State injection\n\nTools can access the current graph state through @[`ToolRuntime`]:\n\n:::python\n```python\nfrom langchain.tools import tool, ToolRuntime\nfrom langgraph.prebuilt import ToolNode\n\n@tool\ndef get_message_count(runtime: ToolRuntime) -> str:\n    \"\"\"Get the number of messages in the conversation.\"\"\"\n    messages = runtime.state[\"messages\"]\n    return f\"There are {len(messages)} messages.\"\n\ntool_node = ToolNode([get_message_count])\n```\n:::\n\nFor more details on accessing state, context, and long-term memory from tools, see [Access context](#access-context).\n\n##", "metadata": {"source": "tools.mdx"}}
{"text": "__\"\n  .addEdge(\"tools\", \"llm\");\n\nconst graph = builder.compile();\n```\n:::\n\n### State injection\n\nTools can access the current graph state through @[`ToolRuntime`]:\n\n:::python\n```python\nfrom langchain.tools import tool, ToolRuntime\nfrom langgraph.prebuilt import ToolNode\n\n@tool\ndef get_message_count(runtime: ToolRuntime) -> str:\n    \"\"\"Get the number of messages in the conversation.\"\"\"\n    messages = runtime.state[\"messages\"]\n    return f\"There are {len(messages)} messages.\"\n\ntool_node = ToolNode([get_message_count])\n```\n:::\n\nFor more details on accessing state, context, and long-term memory from tools, see [Access context](#access-context).\n\n## Prebuilt tools\n\nLangChain provides a large collection of prebuilt tools and toolkits for common tasks like web search, code interpretation, database access, and more. These ready-to-use tools can be directly integrated into your agents without writing custom code.\n\nSee the [tools and toolkits](/oss/integrations/tools) integration page for a complete list of available tools organized by category.\n\n## Server-side tool use\n\nSome chat models feature built-in tools that are executed server-side by the model provider. These include capabilities like web search and code interpreters that don't require you to define or host the tool logic.\n\nRefer to the individual [chat model integration pages](/oss/integrations/providers) and the [tool calling documentation](/oss/langchain/models#server-side-tool-use) for details on enabling and using these built-in tools.\n", "metadata": {"source": "tools.mdx"}}
{"text": "---\ntitle: Build a RAG agent with LangChain\nsidebarTitle: RAG agent\n---\n\nimport ChatModelTabsPy from '/snippets/chat-model-tabs.mdx';\nimport ChatModelTabsJS from '/snippets/chat-model-tabs-js.mdx';\nimport EmbeddingsTabsPy from '/snippets/embeddings-tabs-py.mdx';\nimport EmbeddingsTabsJS from '/snippets/embeddings-tabs-js.mdx';\nimport VectorstoreTabsPy from '/snippets/vectorstore-tabs-py.mdx';\nimport VectorstoreTabsJS from '/snippets/vectorstore-tabs-js.mdx';\n\n## Overview\n\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or [RAG](/oss/langchain/retrieval/).\n\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\n\n1. A RAG [agent](#rag-agents) that executes searches with a simple tool. This is a good general-purpose implementation.\n2. A two-step RAG [chain](#rag-chains) that uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\n### Concepts\n\nWe will cover the following concepts:\n\n- **Indexing**: a pipeline for ingesting data from a source and indexing it. *This usually happens in a separate process.*\n\n- **Retrieval and generation**: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n\nOnce we've indexed our data, we will use an [agent](/oss/langchain/agents) as our orchestration framework to implement the retrieval and generation steps.\n\n<Note>\n    The indexing portion of this tutorial will largely follow the [semantic search tutorial](/oss/langchain/knowledge-base).\n\n    If your data is already available for search (i.e., you have a function to execute a search), or you're comfortable with the content from that tutorial, feel free to skip to the section on [retrieval and generation](#2-retrieval-and-generation)\n</Note>\n\n### Preview\n\nIn this guide we'll build an app that answers questions about the website's content. The specific website we will use is the [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\n\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\n\n<Accordion title=\"Expand for full code snippet\">\n\n:::python\n```python\nimport bs4\nfrom langchain.agents import AgentState, create_agent\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain.messages import MessageLikeRepresentation\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Load and chunk contents of the blog\nloader = WebBaseLoader(\n    web_paths=(\"https://lilianweng", "metadata": {"source": "rag.mdx"}}
{"text": "LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\n\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\n\n<Accordion title=\"Expand for full code snippet\">\n\n:::python\n```python\nimport bs4\nfrom langchain.agents import AgentState, create_agent\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain.messages import MessageLikeRepresentation\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Load and chunk contents of the blog\nloader = WebBaseLoader(\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n    bs_kwargs=dict(\n        parse_only=bs4.SoupStrainer(\n            class_=(\"post-content\", \"post-title\", \"post-header\")\n        )\n    ),\n)\ndocs = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nall_splits = text_splitter.split_documents(docs)\n\n# Index chunks\n_ = vector_store.add_documents(documents=all_splits)\n\n# Construct a tool for retrieving context\n@tool(response_format=\"content_and_artifact\")\ndef retrieve_context(query: str):\n    \"\"\"Retrieve information to help answer a query.\"\"\"\n    retrieved_docs = vector_store.similarity_search(query, k=2)\n    serialized = \"\\n\\n\".join(\n        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n        for doc in retrieved_docs\n    )\n    return serialized, retrieved_docs\n\ntools = [retrieve_context]\n# If desired, specify custom instructions\nprompt = (\n    \"You have access to a tool that retrieves context from a blog post. \"\n    \"Use the tool to help answer user queries.\"\n)\nagent = create_agent(model, tools, system_prompt=prompt)\n```\n\n```python\nquery = \"What is task decomposition?\"\nfor step in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n    stream_mode=\"values\",\n):\n    step[\"messages\"][-1].pretty_print()\n```\n\n```\n================================ Human Message =================================\n\nWhat is task decomposition?\n================================== Ai Message ==================================\nTool Calls:\n  retrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\n Call ID: call_xTkJr8njRY0geNz43ZvGkX0R\n  Args:\n    query: task decomposition\n================================= Tool Message =================================\nName: retrieve_context\n\nSource: {'source': 'https://l", "metadata": {"source": "rag.mdx"}}
{"text": "```\n\n```python\nquery = \"What is task decomposition?\"\nfor step in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n    stream_mode=\"values\",\n):\n    step[\"messages\"][-1].pretty_print()\n```\n\n```\n================================ Human Message =================================\n\nWhat is task decomposition?\n================================== Ai Message ==================================\nTool Calls:\n  retrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\n Call ID: call_xTkJr8njRY0geNz43ZvGkX0R\n  Args:\n    query: task decomposition\n================================= Tool Message =================================\nName: retrieve_context\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done by...\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\n\nTask decomposition refers to...\n```\n:::\n:::js\n```typescript\nimport \"cheerio\";\nimport { createAgent, tool } from \"langchain\";\nimport { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\";\nimport { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\nimport * as z from \"zod\";\n\n// Load and chunk contents of blog\nconst pTagSelector = \"p\";\nconst cheerioLoader = new CheerioWebBaseLoader(\n  \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n  {\n    selector: pTagSelector\n  }\n);\n\nconst docs = await cheerioLoader.load();\n\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 1000,\n  chunkOverlap: 200\n});\nconst allSplits = await splitter.splitDocuments(docs);\n\n// Index chunks\nawait vectorStore.addDocuments(allSplits)\n\n// Construct a tool for retrieving context\nconst retrieveSchema = z.object({ query: z.string() });\n\nconst retrieve = tool(\n  async ({ query }) => {\n    const retrievedDocs = await vectorStore.similaritySearch(query, 2);\n    const serialized = retrievedDocs\n      .map(\n        (doc) => `Source: ${doc.metadata.source}\\nContent: ${doc.pageContent}`\n      )\n      .join(\"\\n\");\n    return [serialized, retrievedDocs];\n  },\n  {\n    name: \"retrieve\",\n    description: \"Retrieve information related to a query.\",\n    schema: retrieveSchema,\n    responseFormat: \"content_and_artifact\",\n  }\n);\n\nconst agent = createAgent({ model: \"gpt-5\", tools: [retrieve] });\n```\n```typescript\nlet inputMessage = `What is Task Decomposition?`;\n\nlet agentInputs = { messages: [{ role", "metadata": {"source": "rag.mdx"}}
{"text": " serialized = retrievedDocs\n      .map(\n        (doc) => `Source: ${doc.metadata.source}\\nContent: ${doc.pageContent}`\n      )\n      .join(\"\\n\");\n    return [serialized, retrievedDocs];\n  },\n  {\n    name: \"retrieve\",\n    description: \"Retrieve information related to a query.\",\n    schema: retrieveSchema,\n    responseFormat: \"content_and_artifact\",\n  }\n);\n\nconst agent = createAgent({ model: \"gpt-5\", tools: [retrieve] });\n```\n```typescript\nlet inputMessage = `What is Task Decomposition?`;\n\nlet agentInputs = { messages: [{ role: \"user\", content: inputMessage }] };\n\nfor await (const step of await agent.stream(agentInputs, {\n  streamMode: \"values\",\n})) {\n  const lastMessage = step.messages[step.messages.length - 1];\n  prettyPrint(lastMessage);\n  console.log(\"-----\\n\");\n}\n```\n:::\n\nCheck out the [LangSmith trace](https://smith.langchain.com/public/a117a1f8-c96c-4c16-a285-00b85646118e/r).\n\n</Accordion>\n\n## Setup\n\n\n### Installation\n\nThis tutorial requires these langchain dependencies:\n\n:::python\n<CodeGroup>\n```bash pip\npip install langchain langchain-text-splitters langchain-community bs4\n```\n```bash uv\nuv add langchain langchain-text-splitters langchain-community bs4\n```\n</CodeGroup>\n:::\n:::js\n\n<CodeGroup>\n```bash npm\nnpm i langchain @langchain/community @langchain/textsplitters\n```\n```bash yarn\nyarn add langchain @langchain/community @langchain/textsplitters\n```\n```bash pnpm\npnpm add langchain @langchain/community @langchain/textsplitters\n```\n</CodeGroup>\n\n:::\n\nFor more details, see our [Installation guide](/oss/langchain/install).\n\n### LangSmith\n\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith](https://smith.langchain.com).\n\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\n\n```shell\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n```\n\n:::python\nOr, set them in Python:\n\n```python\nimport getpass\nimport os\n\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```\n:::\n\n### Components\n\nWe will need to select three components from LangChain's suite of integrations.\n\nSelect a chat model:\n:::python\n<ChatModelTabsPy", "metadata": {"source": "rag.mdx"}}
{"text": " is going on inside your chain or agent. The best way to do this is with [LangSmith](https://smith.langchain.com).\n\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\n\n```shell\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n```\n\n:::python\nOr, set them in Python:\n\n```python\nimport getpass\nimport os\n\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```\n:::\n\n### Components\n\nWe will need to select three components from LangChain's suite of integrations.\n\nSelect a chat model:\n:::python\n<ChatModelTabsPy />\n:::\n:::js\n<ChatModelTabsJS />\n:::\n\nSelect an embeddings model:\n:::python\n<EmbeddingsTabsPy />\n:::\n:::js\n<EmbeddingsTabsJS />\n:::\n\nSelect a vector store:\n:::python\n<VectorstoreTabsPy />\n:::\n:::js\n<VectorstoreTabsJS />\n:::\n\n\n## 1. Indexing\n\n<Note>\n**This section is an abbreviated version of the content in the [semantic search tutorial](/oss/langchain/knowledge-base).**\n\nIf your data is already indexed and available for search (i.e., you have a function to execute a search), or if you're comfortable with [document loaders](/oss/langchain/retrieval#document_loaders), [embeddings](/oss/langchain/retrieval#embedding_models), and [vector stores](/oss/langchain/retrieval#vectorstores), feel free to skip to the next section on [retrieval and generation](/oss/langchain/rag#2-retrieval-and-generation).\n\n</Note>\n\nIndexing commonly works as follows:\n\n1. **Load**: First we need to load our data. This is done with [Document Loaders](/oss/langchain/retrieval#document_loaders).\n2. **Split**: [Text splitters](/oss/langchain/retrieval#text_splitters) break large `Documents` into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.\n3. **Store**: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a [VectorStore](/oss/langchain/retrieval#vectorstores) and [Embeddings](/oss/langchain/retrieval#embedding_models) model.\n\n![index_diagram](/images/rag_indexing.png)\n\n### Loading documents\n\nWe need to first load the blog post contents. We can use [DocumentLoaders](/oss/langchain/retrieval#document_loaders) for this, which are objects that load in data from a source and return a list of @[Document] objects.\n\n:::python\nIn this case we'll use the [`WebBaseLoader`](/oss/integrations/document_loaders/web_base), which uses `urllib` to load HTML from web URLs and `BeautifulSoup` to", "metadata": {"source": "rag.mdx"}}
{"text": " store and index our splits, so that they can be searched over later. This is often done using a [VectorStore](/oss/langchain/retrieval#vectorstores) and [Embeddings](/oss/langchain/retrieval#embedding_models) model.\n\n![index_diagram](/images/rag_indexing.png)\n\n### Loading documents\n\nWe need to first load the blog post contents. We can use [DocumentLoaders](/oss/langchain/retrieval#document_loaders) for this, which are objects that load in data from a source and return a list of @[Document] objects.\n\n:::python\nIn this case we'll use the [`WebBaseLoader`](/oss/integrations/document_loaders/web_base), which uses `urllib` to load HTML from web URLs and `BeautifulSoup` to parse it to text. We can customize the HTML -\\> text parsing by passing in parameters into the `BeautifulSoup` parser via `bs_kwargs` (see [BeautifulSoup docs](https://beautiful-soup-4.readthedocs.io/en/latest/#beautifulsoup)). In this case only HTML tags with class \u201cpost-content\u201d, \u201cpost-title\u201d, or \u201cpost-header\u201d are relevant, so we'll remove all others.\n\n```python\nimport bs4\nfrom langchain_community.document_loaders import WebBaseLoader\n\n# Only keep post title, headers, and content from the full HTML.\nbs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\nloader = WebBaseLoader(\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n    bs_kwargs={\"parse_only\": bs4_strainer},\n)\ndocs = loader.load()\n\nassert len(docs) == 1\nprint(f\"Total characters: {len(docs[0].page_content)}\")\n```\n```text\nTotal characters: 43131\n```\n\n```python\nprint(docs[0].page_content[:500])\n```\n```text\n      LLM Powered Autonomous Agents\n\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n\n\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn\n```\n:::\n:::js\n```typescript\nimport \"cheerio\";\nimport { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\";\n\nconst pTagSelector = \"p\";\nconst cheerioLoader = new CheerioWebBaseLoader(\n  \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n  {\n    selector: pTagSelector,\n  }\n);\n\nconst docs = await cheerioLoader.load();\n\nconsole.assert", "metadata": {"source": "rag.mdx"}}
{"text": " demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn\n```\n:::\n:::js\n```typescript\nimport \"cheerio\";\nimport { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\";\n\nconst pTagSelector = \"p\";\nconst cheerioLoader = new CheerioWebBaseLoader(\n  \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n  {\n    selector: pTagSelector,\n  }\n);\n\nconst docs = await cheerioLoader.load();\n\nconsole.assert(docs.length === 1);\nconsole.log(`Total characters: ${docs[0].pageContent.length}`);\n```\n```\nTotal characters: 22360\n```\n```typescript\nconsole.log(docs[0].pageContent.slice(0, 500));\n```\n```\nBuilding agents with LLM (large language model) as its core controller is...\n```\n:::\n**Go deeper**\n\n`DocumentLoader`: Object that loads data from a source as list of `Documents`.\n\n- [Integrations](/oss/integrations/document_loaders/): 160+ integrations to choose from.\n- @[`BaseLoader`]: API reference for the base interface.\n\n### Splitting documents\n\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\n\nTo handle this we'll split the @[`Document`] into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\n\nAs in the [semantic search tutorial](/oss/langchain/knowledge-base), we use a `RecursiveCharacterTextSplitter`, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\n\n:::python\n```python\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,  # chunk size (characters)\n    chunk_overlap=200,  # chunk overlap (characters)\n    add_start_index=True,  # track index in original document\n)\nall_splits = text_splitter.split_documents(docs)\n\nprint(f\"Split blog post into {len(all_splits)} sub-documents.\")\n```\n```text\nSplit blog post into 66 sub-documents.\n```\n\n**Go deeper**\n\n`TextSplitter`: Object that splits a list of @[`Document`] objects into smaller\nchunks for storage and retrieval.\n\n- [Integrations](/oss/integrations/splitters/)\n- [Interface](https://python.langchain.com/api_reference/text_splitters/base/langchain_text_splitters.base.TextSplitter.html): API reference for the base interface.\n\n:::\n:::js\n``", "metadata": {"source": "rag.mdx"}}
{"text": " chunk overlap (characters)\n    add_start_index=True,  # track index in original document\n)\nall_splits = text_splitter.split_documents(docs)\n\nprint(f\"Split blog post into {len(all_splits)} sub-documents.\")\n```\n```text\nSplit blog post into 66 sub-documents.\n```\n\n**Go deeper**\n\n`TextSplitter`: Object that splits a list of @[`Document`] objects into smaller\nchunks for storage and retrieval.\n\n- [Integrations](/oss/integrations/splitters/)\n- [Interface](https://python.langchain.com/api_reference/text_splitters/base/langchain_text_splitters.base.TextSplitter.html): API reference for the base interface.\n\n:::\n:::js\n```typescript\nimport { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\n\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 1000,\n  chunkOverlap: 200,\n});\nconst allSplits = await splitter.splitDocuments(docs);\nconsole.log(`Split blog post into ${allSplits.length} sub-documents.`);\n```\n```\nSplit blog post into 29 sub-documents.\n```\n:::\n\n\n### Storing documents\n\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the [semantic search tutorial](/oss/langchain/knowledge-base), our approach is to [embed](/oss/langchain/retrieval#embedding_models/) the contents of each document split and insert these embeddings into a [vector store](/oss/langchain/retrieval#vectorstores/). Given an input query, we can then use vector search to retrieve relevant documents.\n\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the [start of the tutorial](/oss/langchain/rag#components).\n\n:::python\n```python\ndocument_ids = vector_store.add_documents(documents=all_splits)\n\nprint(document_ids[:3])\n```\n```python\n['07c18af6-ad58-479a-bfb1-d508033f9c64', '9000bf8e-1993-446f-8d4d-f4e507ba4b8f', 'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6']\n```\n:::\n:::js\n```typescript\nawait vectorStore.addDocuments(allSplits);\n```\n:::\n**Go deeper**\n\n`Embeddings`: Wrapper around a text embedding model, used for converting text to embeddings.\n\n- [Integrations](/oss/integrations/text_embedding/): 30+ integrations to choose from.\n- @[Interface][Embeddings]: API reference for the base interface.\n\n`VectorStore`: Wrapper around a vector database, used for storing and querying embeddings.\n\n- [Integrations](/oss/integrations/vectorstores/): 40+ integrations to choose from.\n- [Interface](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.Vector", "metadata": {"source": "rag.mdx"}}
{"text": "88aedc2e6']\n```\n:::\n:::js\n```typescript\nawait vectorStore.addDocuments(allSplits);\n```\n:::\n**Go deeper**\n\n`Embeddings`: Wrapper around a text embedding model, used for converting text to embeddings.\n\n- [Integrations](/oss/integrations/text_embedding/): 30+ integrations to choose from.\n- @[Interface][Embeddings]: API reference for the base interface.\n\n`VectorStore`: Wrapper around a vector database, used for storing and querying embeddings.\n\n- [Integrations](/oss/integrations/vectorstores/): 40+ integrations to choose from.\n- [Interface](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStore.html): API reference for the base interface.\n\nThis completes the **Indexing** portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\n\n## 2. Retrieval and generation\n\nRAG applications commonly work as follows:\n\n1. **Retrieve**: Given a user input, relevant splits are retrieved from storage using a [Retriever](/oss/langchain/retrieval#retrievers).\n2. **Generate**: A [model](/oss/langchain/models) produces an answer using a prompt that includes both the question with the retrieved data\n\n![retrieval_diagram](/images/rag_retrieval_generation.png)\n\nNow let's write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\n\nWe will demonstrate:\n\n1. A RAG [agent](#rag-agents) that executes searches with a simple tool. This is a good general-purpose implementation.\n2. A two-step RAG [chain](#rag-chains) that uses just a single LLM call per query. This is a fast and effective method for simple queries.\n\n### RAG agents\n\nOne formulation of a RAG application is as a simple [agent](/oss/langchain/agents) with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a [tool](/oss/langchain/tools) that wraps our vector store:\n\n:::python\n```python\nfrom langchain.tools import tool\n\n@tool(response_format=\"content_and_artifact\")\ndef retrieve_context(query: str):\n    \"\"\"Retrieve information to help answer a query.\"\"\"\n    retrieved_docs = vector_store.similarity_search(query, k=2)\n    serialized = \"\\n\\n\".join(\n        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n        for doc in retrieved_docs\n    )\n    return serialized, retrieved_docs\n```\n\n<Tip>\n\nHere we use the @[tool decorator][@tool] to configure the tool to attach raw documents as [artifacts](/oss/langchain/messages#param-artifact) to each [ToolMessage](/oss/langchain/messages#tool-message", "metadata": {"source": "rag.mdx"}}
{"text": "(response_format=\"content_and_artifact\")\ndef retrieve_context(query: str):\n    \"\"\"Retrieve information to help answer a query.\"\"\"\n    retrieved_docs = vector_store.similarity_search(query, k=2)\n    serialized = \"\\n\\n\".join(\n        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n        for doc in retrieved_docs\n    )\n    return serialized, retrieved_docs\n```\n\n<Tip>\n\nHere we use the @[tool decorator][@tool] to configure the tool to attach raw documents as [artifacts](/oss/langchain/messages#param-artifact) to each [ToolMessage](/oss/langchain/messages#tool-message). This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\n\n</Tip>\n:::\n:::js\n```typescript\nimport * as z from \"zod\";\nimport { tool } from \"@langchain/core/tools\";\n\nconst retrieveSchema = z.object({ query: z.string() });\n\nconst retrieve = tool(\n  async ({ query }) => {\n    const retrievedDocs = await vectorStore.similaritySearch(query, 2);\n    const serialized = retrievedDocs\n      .map(\n        (doc) => `Source: ${doc.metadata.source}\\nContent: ${doc.pageContent}`\n      )\n      .join(\"\\n\");\n    return [serialized, retrievedDocs];\n  },\n  {\n    name: \"retrieve\",\n    description: \"Retrieve information related to a query.\",\n    schema: retrieveSchema,\n    responseFormat: \"content_and_artifact\",\n  }\n);\n```\n<Tip>\n    Here we specify the `responseFormat` to `content_and_artifact` to confiugre the tool to attach raw documents as [artifacts](/oss/langchain/messages#param-artifact) to each [ToolMessage](/oss/langchain/messages#tool-message). This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\n</Tip>\n:::\n\n:::python\n<Tip>\n    Retrieval tools are not limited to a single string `query` argument, as in the above example. You can\n    force the LLM to specify additional search parameters by adding arguments\u2014 for example, a category:\n\n    ```python\n    from typing import Literal\n\n    def retrieve_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]):\n    ```\n</Tip>\n:::\n\nGiven our tool, we can construct the agent:\n\n:::python\n```python\nfrom langchain.agents import create_agent\n\n\ntools = [retrieve_context]\n# If desired, specify custom instructions\nprompt = (\n    \"You have access to a tool that retrieves context from a blog post. \"\n    \"Use the tool to help answer user queries.\"\n)\nagent = create_agent(model, tools, system_prompt=prompt)", "metadata": {"source": "rag.mdx"}}
{"text": " argument, as in the above example. You can\n    force the LLM to specify additional search parameters by adding arguments\u2014 for example, a category:\n\n    ```python\n    from typing import Literal\n\n    def retrieve_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]):\n    ```\n</Tip>\n:::\n\nGiven our tool, we can construct the agent:\n\n:::python\n```python\nfrom langchain.agents import create_agent\n\n\ntools = [retrieve_context]\n# If desired, specify custom instructions\nprompt = (\n    \"You have access to a tool that retrieves context from a blog post. \"\n    \"Use the tool to help answer user queries.\"\n)\nagent = create_agent(model, tools, system_prompt=prompt)\n```\n:::\n:::js\n\n\n```typescript\nimport { createAgent } from \"langchain\";\n\nconst tools = [retrieve];\nconst systemPrompt = new SystemMessage(\n    \"You have access to a tool that retrieves context from a blog post. \" +\n    \"Use the tool to help answer user queries.\"\n)\n\nconst agent = createAgent({ model: \"gpt-5\", tools, systemPrompt });\n```\n:::\n\nLet's test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\n\n:::python\n```python\nquery = (\n    \"What is the standard method for Task Decomposition?\\n\\n\"\n    \"Once you get the answer, look up common extensions of that method.\"\n)\n\nfor event in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n    stream_mode=\"values\",\n):\n    event[\"messages\"][-1].pretty_print()\n```\n```\n================================ Human Message =================================\n\nWhat is the standard method for Task Decomposition?\n\nOnce you get the answer, look up common extensions of that method.\n================================== Ai Message ==================================\nTool Calls:\n  retrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\n Call ID: call_d6AVxICMPQYwAKj9lgH4E337\n  Args:\n    query: standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTool Calls:\n  retrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\n Call ID: call_0dbMOw7266jvETbXWn4JqWpR\n  Args:\n    query: common extensions of the standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'", "metadata": {"source": "rag.mdx"}}
{"text": "eng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTool Calls:\n  retrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\n Call ID: call_0dbMOw7266jvETbXWn4JqWpR\n  Args:\n    query: common extensions of the standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\n\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\n```\n:::\n:::js\n```typescript\nlet inputMessage = `What is the standard method for Task Decomposition?\nOnce you get the answer, look up common extensions of that method.`;\n\nlet agentInputs = { messages: [{ role: \"user\", content: inputMessage }] };\n\nconst stream = await agent.stream(agentInputs, {\n  streamMode: \"values\",\n});\nfor await (const step of stream) {\n  const lastMessage = step.messages[step.messages.length - 1];\n  console.log(`[${lastMessage.role}]: ${lastMessage.content}`);\n  console.log(\"-----\\n\");\n}\n```\n```\n[human]: What is the standard method for Task Decomposition?\nOnce you get the answer, look up common extensions of that method.\n-----\n\n[ai]:\nTools:\n- retrieve({\"query\":\"standard method for Task Decomposition\"})\n-----\n\n[tool]: Source: https://lilianweng.github.io/posts/2023-06-23-agent/\nContent: hard tasks into smaller and simpler steps...\nSource: https://lilianweng.github.io/posts/2023-06-23-agent/\nContent: System message:Think step by step and reason yourself...\n-----\n\n[ai]:\nTools:\n- retrieve({\"query\":\"common extensions of Task Decomposition method\"})\n-----\n\n[tool]: Source: https://lilianweng.github.io/posts/2023-06-23-agent/\nContent: hard tasks into smaller and simpler steps...\nSource: https://lilianweng.github.io/posts/2023-06-23-agent/\nContent: be provided by other developers (as in Plugins) or self-defined...\n-----\n\n[ai]: ### Standard Method for Task Decomposition\n\nThe standard method for task decomposition involves...\n-----\n```\n:::\nNote that the agent:\n\n1. Generates a query to search for a standard method for task decomposition;\n2. Receiving the answer, generates a second query to search for common extensions of it;\n3. Having received all necessary context, answers the question.\n\nWe can see the full sequence", "metadata": {"source": "rag.mdx"}}
{"text": "({\"query\":\"common extensions of Task Decomposition method\"})\n-----\n\n[tool]: Source: https://lilianweng.github.io/posts/2023-06-23-agent/\nContent: hard tasks into smaller and simpler steps...\nSource: https://lilianweng.github.io/posts/2023-06-23-agent/\nContent: be provided by other developers (as in Plugins) or self-defined...\n-----\n\n[ai]: ### Standard Method for Task Decomposition\n\nThe standard method for task decomposition involves...\n-----\n```\n:::\nNote that the agent:\n\n1. Generates a query to search for a standard method for task decomposition;\n2. Receiving the answer, generates a second query to search for common extensions of it;\n3. Having received all necessary context, answers the question.\n\nWe can see the full sequence of steps, along with latency and other metadata, in the [LangSmith trace](https://smith.langchain.com/public/7b42d478-33d2-4631-90a4-7cb731681e88/r).\n\n<Tip>\n    You can add a deeper level of control and customization using the [LangGraph](/oss/langgraph/overview) framework directly\u2014 for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph's [Agentic RAG tutorial](/oss/langgraph/agentic-rag) for more advanced formulations.\n</Tip>\n\n\n### RAG chains\n\nIn the above [agentic RAG](#rag-agents) formulation we allow the LLM to use its discretion in generating a [tool call](/oss/langchain/models#tool-calling) to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\n\n| \u2705 Benefits                                                                 | \u26a0\ufe0f Drawbacks                                                                 |\n|-----------------------------------------------------------------------------|----------------------------------------------------------------------------|\n| **Search only when needed** \u2013 The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches. | **Two inference calls** \u2013 When a search is performed, it requires one call to generate the query and another to produce the final response. |\n| **Contextual search queries** \u2013 By treating search as a tool with a `query` input, the LLM crafts its own queries that incorporate conversational context. | **Reduced control** \u2013 The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary. |\n| **Multiple searches allowed** \u2013 The LLM can execute several searches in support of a single user query. |                                                                            |\n\n\nAnother common approach is a two-step chain, in", "metadata": {"source": "rag.mdx"}}
{"text": "** \u2013 When a search is performed, it requires one call to generate the query and another to produce the final response. |\n| **Contextual search queries** \u2013 By treating search as a tool with a `query` input, the LLM crafts its own queries that incorporate conversational context. | **Reduced control** \u2013 The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary. |\n| **Multiple searches allowed** \u2013 The LLM can execute several searches in support of a single user query. |                                                                            |\n\n\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\n\nIn this approach we no longer call the model in a loop, but instead make a single pass.\n\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\n\n:::python\n```python\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\n\n@dynamic_prompt\ndef prompt_with_context(request: ModelRequest) -> str:\n    \"\"\"Inject context into state messages.\"\"\"\n    last_query = request.state[\"messages\"][-1].text\n    retrieved_docs = vector_store.similarity_search(last_query)\n\n    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n\n    system_message = (\n        \"You are a helpful assistant. Use the following context in your response:\"\n        f\"\\n\\n{docs_content}\"\n    )\n\n    return system_message\n\n\nagent = create_agent(model, tools=[], middleware=[prompt_with_context])\n```\n:::\n:::js\n```typescript\nimport { createAgent, dynamicSystemPromptMiddleware } from \"langchain\";\nimport { SystemMessage } from \"@langchain/core/messages\";\n\nconst agent = createAgent({\n  model,\n  tools: [],\n  middleware: [\n    dynamicSystemPromptMiddleware(async (state) => {\n        const lastQuery = state.messages[state.messages.length - 1].content;\n\n        const retrievedDocs = await vectorStore.similaritySearch(lastQuery, 2);\n\n        const docsContent = retrievedDocs\n        .map((doc) => doc.pageContent)\n        .join(\"\\n\\n\");\n\n        // Build system message\n        const systemMessage = new SystemMessage(\n        `You are a helpful assistant. Use the following context in your response:\\n\\n${docsContent}`\n        );\n\n        // Return system + existing messages\n        return [system", "metadata": {"source": "rag.mdx"}}
{"text": ") => {\n        const lastQuery = state.messages[state.messages.length - 1].content;\n\n        const retrievedDocs = await vectorStore.similaritySearch(lastQuery, 2);\n\n        const docsContent = retrievedDocs\n        .map((doc) => doc.pageContent)\n        .join(\"\\n\\n\");\n\n        // Build system message\n        const systemMessage = new SystemMessage(\n        `You are a helpful assistant. Use the following context in your response:\\n\\n${docsContent}`\n        );\n\n        // Return system + existing messages\n        return [systemMessage, ...state.messages];\n    })\n  ]\n});\n```\n:::\n\nLet's try this out:\n:::python\n```python\nquery = \"What is task decomposition?\"\nfor step in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n    stream_mode=\"values\",\n):\n    step[\"messages\"][-1].pretty_print()\n```\n```\n================================ Human Message =================================\n\nWhat is task decomposition?\n================================== Ai Message ==================================\n\nTask decomposition is...\n```\n:::\n:::js\n```typescript\nlet inputMessage = `What is Task Decomposition?`;\n\nlet chainInputs = { messages: [{ role: \"user\", content: inputMessage }] };\n\nconst stream = await agent.stream(chainInputs, {\n  streamMode: \"values\",\n})\nfor await (const step of stream) {\n  const lastMessage = step.messages[step.messages.length - 1];\n  prettyPrint(lastMessage);\n  console.log(\"-----\\n\");\n}\n```\n:::\nIn the [LangSmith trace](https://smith.langchain.com/public/0322904b-bc4c-4433-a568-54c6b31bbef4/r/9ef1c23e-380e-46bf-94b3-d8bb33df440c) we can see the retrieved context incorporated into the model prompt.\n\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\n\n<Accordion title=\"Returning source documents\">\n\nThe above [RAG chain](#rag-chains) incorporates retrieved context into a single system message for that run.\n\nAs in the [agentic RAG](#rag-agents) formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\n\n1. Adding a key to the state to store the retrieved documents\n2. Adding a new node via a [pre-model hook](/oss/langchain/agents#pre-model-hook) to populate that key (as well as inject the context).\n\n:::python\n```python\nfrom typing import Any\nfrom langchain_core.documents import Document\nfrom langchain.agents.middleware import AgentMiddleware, AgentState\n\n\nclass State(AgentState):\n    context: list[Document]\n\n\nclass", "metadata": {"source": "rag.mdx"}}
{"text": " title=\"Returning source documents\">\n\nThe above [RAG chain](#rag-chains) incorporates retrieved context into a single system message for that run.\n\nAs in the [agentic RAG](#rag-agents) formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\n\n1. Adding a key to the state to store the retrieved documents\n2. Adding a new node via a [pre-model hook](/oss/langchain/agents#pre-model-hook) to populate that key (as well as inject the context).\n\n:::python\n```python\nfrom typing import Any\nfrom langchain_core.documents import Document\nfrom langchain.agents.middleware import AgentMiddleware, AgentState\n\n\nclass State(AgentState):\n    context: list[Document]\n\n\nclass RetrieveDocumentsMiddleware(AgentMiddleware[State]):\n    state_schema = State\n\n    def before_model(self, state: AgentState) -> dict[str, Any] | None:\n        last_message = state[\"messages\"][-1]\n        retrieved_docs = vector_store.similarity_search(last_message.text)\n\n        docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n\n        augmented_message_content = (\n            f\"{last_message.text}\\n\\n\"\n            \"Use the following context to answer the query:\\n\"\n            f\"{docs_content}\"\n        )\n        return {\n            \"messages\": [last_message.model_copy(update={\"content\": augmented_message_content})],\n            \"context\": retrieved_docs,\n        }\n\n\nagent = create_agent(\n    model,\n    tools=[],\n    middleware=[RetrieveDocumentsMiddleware()],\n)\n```\n:::\n:::js\n```typescript\nimport { createMiddleware, Document, createAgent } from \"langchain\";\nimport { StateSchema, MessagesValue } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst CustomState = new StateSchema({\n  messages: MessagesValue,\n  context: z.array(z.custom<Document>()),\n});\n\nconst retrieveDocumentsMiddleware = createMiddleware({\n  stateSchema: CustomState,\n  beforeModel: async (state) => {\n    const lastMessage = state.messages[state.messages.length - 1].content;\n    const retrievedDocs = await vectorStore.similaritySearch(lastMessage, 2);\n\n    const docsContent = retrievedDocs\n      .map((doc) => doc.pageContent)\n      .join(\"\\n\\n\");\n\n    const augmentedMessageContent = [\n        ...lastMessage.content,\n        { type: \"text\", text: `Use the following context to answer the query:\\n", "metadata": {"source": "rag.mdx"}}
{"text": " StateSchema({\n  messages: MessagesValue,\n  context: z.array(z.custom<Document>()),\n});\n\nconst retrieveDocumentsMiddleware = createMiddleware({\n  stateSchema: CustomState,\n  beforeModel: async (state) => {\n    const lastMessage = state.messages[state.messages.length - 1].content;\n    const retrievedDocs = await vectorStore.similaritySearch(lastMessage, 2);\n\n    const docsContent = retrievedDocs\n      .map((doc) => doc.pageContent)\n      .join(\"\\n\\n\");\n\n    const augmentedMessageContent = [\n        ...lastMessage.content,\n        { type: \"text\", text: `Use the following context to answer the query:\\n\\n${docsContent}` }\n    ]\n\n    // Below we augment each input message with context, but we could also\n    // modify just the system message, as before.\n    return {\n      messages: [{\n        ...lastMessage,\n        content: augmentedMessageContent,\n      }]\n      context: retrievedDocs,\n    }\n  },\n});\n\nconst agent = createAgent({\n  model,\n  tools: [],\n  middleware: [retrieveDocumentsMiddleware],\n});\n```\n:::\n</Accordion>\n\n\n## Next steps\n\n:::python\n\nNow that we've implemented a simple RAG application via @[`create_agent`], we can easily incorporate new features and go deeper:\n\n:::\n:::js\n\nNow that we've implemented a simple RAG application via @[`createAgent`], we can easily incorporate new features and go deeper:\n\n:::\n\n- [Stream](/oss/langchain/streaming) tokens and other information for responsive user experiences\n- Add [conversational memory](/oss/langchain/short-term-memory) to support multi-turn interactions\n- Add [long-term memory](/oss/langchain/long-term-memory) to support memory across conversational threads\n- Add [structured responses](/oss/langchain/structured-output)\n- Deploy your application with [LangSmith Deployment](/langsmith/deployments)\n", "metadata": {"source": "rag.mdx"}}
{"text": "---\ntitle: Guardrails\ndescription: Implement safety checks and content filtering for your agents\n---\n\nGuardrails help you build safe, compliant AI applications by validating and filtering content at key points in your agent's execution. They can detect sensitive information, enforce content policies, validate outputs, and prevent unsafe behaviors before they cause problems.\n\nCommon use cases include:\n- Preventing PII leakage\n- Detecting and blocking prompt injection attacks\n- Blocking inappropriate or harmful content\n- Enforcing business rules and compliance requirements\n- Validating output quality and accuracy\n\nYou can implement guardrails using [middleware](/oss/langchain/middleware) to intercept execution at strategic points - before the agent starts, after it completes, or around model and tool calls.\n\n<div style={{ display: \"flex\", justifyContent: \"center\" }}>\n    <img\n        src=\"/oss/images/middleware_final.png\"\n        alt=\"Middleware flow diagram\"\n        className=\"rounded-lg\"\n    />\n</div>\n\nGuardrails can be implemented using two complementary approaches:\n\n<CardGroup cols={2}>\n    <Card title=\"Deterministic guardrails\" icon=\"list-check\">\n        Use rule-based logic like regex patterns, keyword matching, or explicit checks. Fast, predictable, and cost-effective, but may miss nuanced violations.\n    </Card>\n    <Card title=\"Model-based guardrails\" icon=\"brain\">\n        Use LLMs or classifiers to evaluate content with semantic understanding. Catch subtle issues that rules miss, but are slower and more expensive.\n    </Card>\n</CardGroup>\n\nLangChain provides both built-in guardrails (e.g., [PII detection](#pii-detection), [human-in-the-loop](#human-in-the-loop)) and a flexible middleware system for building custom guardrails using either approach.\n\n## Built-in guardrails\n\n### PII detection\n\nLangChain provides built-in middleware for detecting and handling Personally Identifiable Information (PII) in conversations. This middleware can detect common PII types like emails, credit cards, IP addresses, and more.\n\nPII detection middleware is helpful for cases such as health care and financial applications with compliance requirements, customer service agents that need to sanitize logs, and generally any application handling sensitive user data.\n\nThe PII middleware supports multiple strategies for handling detected PII:\n\n| Strategy | Description | Example |\n|----------|-------------|---------|\n| `redact` | Replace with `[REDACTED_{PII_TYPE}]` | `[REDACTED_EMAIL]` |\n| `mask` | Partially obscure (e.g., last 4 digits) | `****-****-****-1234` |\n| `hash` | Replace with deterministic hash | `a8f5f167...` |\n| `block` | Raise exception when detected | Error thrown |\n\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import PIIMiddleware\n\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[customer_service_tool, email_tool],\n    middleware=[\n        # Redact emails in user input before sending", "metadata": {"source": "guardrails.mdx"}}
{"text": " |\n|----------|-------------|---------|\n| `redact` | Replace with `[REDACTED_{PII_TYPE}]` | `[REDACTED_EMAIL]` |\n| `mask` | Partially obscure (e.g., last 4 digits) | `****-****-****-1234` |\n| `hash` | Replace with deterministic hash | `a8f5f167...` |\n| `block` | Raise exception when detected | Error thrown |\n\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import PIIMiddleware\n\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[customer_service_tool, email_tool],\n    middleware=[\n        # Redact emails in user input before sending to model\n        PIIMiddleware(\n            \"email\",\n            strategy=\"redact\",\n            apply_to_input=True,\n        ),\n        # Mask credit cards in user input\n        PIIMiddleware(\n            \"credit_card\",\n            strategy=\"mask\",\n            apply_to_input=True,\n        ),\n        # Block API keys - raise error if detected\n        PIIMiddleware(\n            \"api_key\",\n            detector=r\"sk-[a-zA-Z0-9]{32}\",\n            strategy=\"block\",\n            apply_to_input=True,\n        ),\n    ],\n)\n\n# When user provides PII, it will be handled according to the strategy\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"My email is john.doe@example.com and card is 5105-1051-0510-5100\"}]\n})\n```\n:::\n\n:::js\n```typescript\nimport { createAgent, piiRedactionMiddleware } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [customerServiceTool, emailTool],\n  middleware: [\n    // Redact emails in user input before sending to model\n    piiRedactionMiddleware({\n      piiType: \"email\",\n      strategy: \"redact\",\n      applyToInput: true,\n    }),\n    // Mask credit cards in user input\n    piiRedactionMiddleware({\n      piiType: \"credit_card\",\n      strategy: \"mask\",\n      applyToInput: true,\n    }),\n    // Block API keys - raise error if detected\n    piiRedactionMiddle", "metadata": {"source": "guardrails.mdx"}}
{"text": " \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [customerServiceTool, emailTool],\n  middleware: [\n    // Redact emails in user input before sending to model\n    piiRedactionMiddleware({\n      piiType: \"email\",\n      strategy: \"redact\",\n      applyToInput: true,\n    }),\n    // Mask credit cards in user input\n    piiRedactionMiddleware({\n      piiType: \"credit_card\",\n      strategy: \"mask\",\n      applyToInput: true,\n    }),\n    // Block API keys - raise error if detected\n    piiRedactionMiddleware({\n      piiType: \"api_key\",\n      detector: /sk-[a-zA-Z0-9]{32}/,\n      strategy: \"block\",\n      applyToInput: true,\n    }),\n  ],\n});\n\n// When user provides PII, it will be handled according to the strategy\nconst result = await agent.invoke({\n  messages: [{\n    role: \"user\",\n    content: \"My email is john.doe@example.com and card is 5105-1051-0510-5100\"\n  }]\n});\n```\n:::\n\n<Accordion title=\"Built-in PII types and configuration\">\n\n**Built-in PII types:**\n- `email` - Email addresses\n- `credit_card` - Credit card numbers (Luhn validated)\n- `ip` - IP addresses\n- `mac_address` - MAC addresses\n- `url` - URLs\n\n**Configuration options:**\n\n:::python\nParameter | Description | Default\n-----------|-------------|---------\n`pii_type` | Type of PII to detect (built-in or custom) | Required\n`strategy` | How to handle detected PII (`\"block\"`, `\"redact\"`, `\"mask\"`, `\"hash\"`) | `\"redact\"`\n`detector` | Custom detector function or regex pattern | `None` (uses built-in)\n`apply_to_input` | Check user messages before model call | `True`\n`apply_to_output` | Check AI messages after model call | `False`\n`apply_to_tool_results` | Check tool result messages after execution | `False`\n:::\n\n:::js\nParameter | Description | Default\n-----------|-------------|---------\n`piiType` | Type of PII to detect (built-in or custom) | Required\n`strategy` | How to handle detected PII (`\"block\"`, `\"redact\"`, `\"mask\"`, `\"hash\"`) | `\"redact\"`\n`detector` | Custom detector regex pattern | `undefined` (uses built-in)\n`applyToInput` | Check user messages before model call | `true`\n`applyToOutput` | Check AI messages after model call | `false`\n`applyToToolResults` | Check tool result messages after execution | `false`\n:::\n\n</Accordion>\n\nSee the [middleware documentation](/oss/langchain/middleware#pii-detection", "metadata": {"source": "guardrails.mdx"}}
{"text": " Check tool result messages after execution | `False`\n:::\n\n:::js\nParameter | Description | Default\n-----------|-------------|---------\n`piiType` | Type of PII to detect (built-in or custom) | Required\n`strategy` | How to handle detected PII (`\"block\"`, `\"redact\"`, `\"mask\"`, `\"hash\"`) | `\"redact\"`\n`detector` | Custom detector regex pattern | `undefined` (uses built-in)\n`applyToInput` | Check user messages before model call | `true`\n`applyToOutput` | Check AI messages after model call | `false`\n`applyToToolResults` | Check tool result messages after execution | `false`\n:::\n\n</Accordion>\n\nSee the [middleware documentation](/oss/langchain/middleware#pii-detection) for complete details on PII detection capabilities.\n\n### Human-in-the-loop\n\nLangChain provides built-in middleware for requiring human approval before executing sensitive operations. This is one of the most effective guardrails for high-stakes decisions.\n\nHuman-in-the-loop middleware is helpful for cases such as financial transactions and transfers, deleting or modifying production data, sending communications to external parties, and any operation with significant business impact.\n\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.types import Command\n\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[search_tool, send_email_tool, delete_database_tool],\n    middleware=[\n        HumanInTheLoopMiddleware(\n            interrupt_on={\n                # Require approval for sensitive operations\n                \"send_email\": True,\n                \"delete_database\": True,\n                # Auto-approve safe operations\n                \"search\": False,\n            }\n        ),\n    ],\n    # Persist the state across interrupts\n    checkpointer=InMemorySaver(),\n)\n\n# Human-in-the-loop requires a thread ID for persistence\nconfig = {\"configurable\": {\"thread_id\": \"some_id\"}}\n\n# Agent will pause and wait for approval before executing sensitive tools\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Send an email to the team\"}]},\n    config=config\n)\n\nresult = agent.invoke(\n    Command(resume={\"decisions\": [{\"type\": \"approve\"}]}),\n    config=config  # Same thread ID to resume the paused conversation\n)\n```\n:::\n\n:::js\n```typescript\nimport { createAgent, humanInTheLoopMiddleware } from \"langchain\";\nimport { MemorySaver, Command } from \"@langchain/langgraph\";\n\nconst agent =", "metadata": {"source": "guardrails.mdx"}}
{"text": ")\n\n# Human-in-the-loop requires a thread ID for persistence\nconfig = {\"configurable\": {\"thread_id\": \"some_id\"}}\n\n# Agent will pause and wait for approval before executing sensitive tools\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Send an email to the team\"}]},\n    config=config\n)\n\nresult = agent.invoke(\n    Command(resume={\"decisions\": [{\"type\": \"approve\"}]}),\n    config=config  # Same thread ID to resume the paused conversation\n)\n```\n:::\n\n:::js\n```typescript\nimport { createAgent, humanInTheLoopMiddleware } from \"langchain\";\nimport { MemorySaver, Command } from \"@langchain/langgraph\";\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [searchTool, sendEmailTool, deleteDatabaseTool],\n  middleware: [\n    humanInTheLoopMiddleware({\n      interruptOn: {\n        // Require approval for sensitive operations\n        send_email: { allowAccept: true, allowEdit: true, allowRespond: true },\n        delete_database: { allowAccept: true, allowEdit: true, allowRespond: true },\n        // Auto-approve safe operations\n        search: false,\n      }\n    }),\n  ],\n  checkpointer: new MemorySaver(),\n});\n\n// Human-in-the-loop requires a thread ID for persistence\nconst config = { configurable: { thread_id: \"some_id\" } };\n\n// Agent will pause and wait for approval before executing sensitive tools\nlet result = await agent.invoke(\n  { messages: [{ role: \"user\", content: \"Send an email to the team\" }] },\n  config\n);\n\nresult = await agent.invoke(\n  new Command({ resume: { decisions: [{ type: \"approve\" }] } }),\n  config  // Same thread ID to resume the paused conversation\n);\n```\n:::\n\n<Tip>\n    See the [human-in-the-loop documentation](/oss/langchain/human-in-the-loop) for complete details on implementing approval workflows.\n</Tip>\n\n## Custom guardrails\n\nFor more sophisticated guardrails, you can create custom middleware that runs before or after the agent executes. This gives you full control over validation logic, content filtering, and safety checks.\n\n### Before agent guardrails\n\nUse \"before agent\" hooks to validate requests once at the start of each invocation. This is useful for session-level checks like authentication, rate limiting, or blocking inappropriate requests before any processing begins.\n\n:::python\n\n<CodeGroup>\n\n```python title=\"Class syntax\"\nfrom typing import Any\n\nfrom langchain.agents.middleware import AgentMiddleware, AgentState, hook_config\nfrom langgraph.runtime import Runtime\n\nclass ContentFilterMiddleware(AgentMiddleware):\n    \"\"\"Deterministic guardrail: Block requests containing banned keywords.\"\"\"\n\n    def __init__(self, banned_keywords: list[str]):\n        super().__init__()\n      ", "metadata": {"source": "guardrails.mdx"}}
{"text": " before or after the agent executes. This gives you full control over validation logic, content filtering, and safety checks.\n\n### Before agent guardrails\n\nUse \"before agent\" hooks to validate requests once at the start of each invocation. This is useful for session-level checks like authentication, rate limiting, or blocking inappropriate requests before any processing begins.\n\n:::python\n\n<CodeGroup>\n\n```python title=\"Class syntax\"\nfrom typing import Any\n\nfrom langchain.agents.middleware import AgentMiddleware, AgentState, hook_config\nfrom langgraph.runtime import Runtime\n\nclass ContentFilterMiddleware(AgentMiddleware):\n    \"\"\"Deterministic guardrail: Block requests containing banned keywords.\"\"\"\n\n    def __init__(self, banned_keywords: list[str]):\n        super().__init__()\n        self.banned_keywords = [kw.lower() for kw in banned_keywords]\n\n    @hook_config(can_jump_to=[\"end\"])\n    def before_agent(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n        # Get the first user message\n        if not state[\"messages\"]:\n            return None\n\n        first_message = state[\"messages\"][0]\n        if first_message.type != \"human\":\n            return None\n\n        content = first_message.content.lower()\n\n        # Check for banned keywords\n        for keyword in self.banned_keywords:\n            if keyword in content:\n                # Block execution before any processing\n                return {\n                    \"messages\": [{\n                        \"role\": \"assistant\",\n                        \"content\": \"I cannot process requests containing inappropriate content. Please rephrase your request.\"\n                    }],\n                    \"jump_to\": \"end\"\n                }\n\n        return None\n\n# Use the custom guardrail\nfrom langchain.agents import create_agent\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[search_tool, calculator_tool],\n    middleware=[\n        ContentFilterMiddleware(\n            banned_keywords=[\"hack\", \"exploit\", \"malware\"]\n        ),\n    ],\n)\n\n# This request will be blocked before any processing\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user", "metadata": {"source": "guardrails.mdx"}}
{"text": "                 \"jump_to\": \"end\"\n                }\n\n        return None\n\n# Use the custom guardrail\nfrom langchain.agents import create_agent\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[search_tool, calculator_tool],\n    middleware=[\n        ContentFilterMiddleware(\n            banned_keywords=[\"hack\", \"exploit\", \"malware\"]\n        ),\n    ],\n)\n\n# This request will be blocked before any processing\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"How do I hack into a database?\"}]\n})\n```\n\n```python title=\"Decorator syntax\"\nfrom typing import Any\n\nfrom langchain.agents.middleware import before_agent, AgentState, hook_config\nfrom langgraph.runtime import Runtime\n\nbanned_keywords = [\"hack\", \"exploit\", \"malware\"]\n\n@before_agent(can_jump_to=[\"end\"])\ndef content_filter(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n    \"\"\"Deterministic guardrail: Block requests containing banned keywords.\"\"\"\n    # Get the first user message\n    if not state[\"messages\"]:\n        return None\n\n    first_message = state[\"messages\"][0]\n    if first_message.type != \"human\":\n        return None\n\n    content = first_message.content.lower()\n\n    # Check for banned keywords\n    for keyword in banned_keywords:\n        if keyword in content:\n            # Block execution before any processing\n            return {\n                \"messages\": [{\n                    \"role\": \"assistant\",\n                    \"content\": \"I cannot process requests containing inappropriate content. Please rephrase your request.\"\n                }],\n                \"jump_to\": \"end\"\n            }\n\n    return None\n\n# Use the custom guardrail\nfrom langchain.agents import create_agent\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[search_tool, calculator_tool],\n    middleware=[content_filter],\n)\n\n# This request will be blocked before any processing\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"How do I hack into a database?\"}]\n})\n```\n</CodeGroup>\n\n:::\n\n:::js\n```typescript\nimport { createMiddleware, AIMessage } from \"langchain\";\n", "metadata": {"source": "guardrails.mdx"}}
{"text": " }],\n                \"jump_to\": \"end\"\n            }\n\n    return None\n\n# Use the custom guardrail\nfrom langchain.agents import create_agent\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[search_tool, calculator_tool],\n    middleware=[content_filter],\n)\n\n# This request will be blocked before any processing\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"How do I hack into a database?\"}]\n})\n```\n</CodeGroup>\n\n:::\n\n:::js\n```typescript\nimport { createMiddleware, AIMessage } from \"langchain\";\n\nconst contentFilterMiddleware = (bannedKeywords: string[]) => {\n  const keywords = bannedKeywords.map(kw => kw.toLowerCase());\n\n  return createMiddleware({\n    name: \"ContentFilterMiddleware\",\n    beforeAgent: {\n      hook: (state) => {\n        // Get the first user message\n        if (!state.messages || state.messages.length === 0) {\n          return;\n        }\n\n        const firstMessage = state.messages[0];\n        if (firstMessage._getType() !== \"human\") {\n          return;\n        }\n\n        const content = firstMessage.content.toString().toLowerCase();\n\n        // Check for banned keywords\n        for (const keyword of keywords) {\n          if (content.includes(keyword)) {\n            // Block execution before any processing\n            return {\n              messages: [\n                new AIMessage(\n                  \"I cannot process requests containing inappropriate content. Please rephrase your request.\"\n                )\n              ],\n              jumpTo: \"end\",\n            };\n          }\n        }\n\n        return;\n      },\n      canJumpTo: ['end']\n    }\n  });\n};\n\n// Use the custom guardrail\nimport { createAgent } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [searchTool, calculatorTool],\n  middleware: [\n    contentFilterMiddleware([\"hack\", \"exploit\", \"malware\"]),\n  ],\n});\n\n// This request will be blocked before any processing\nconst result = await agent.", "metadata": {"source": "guardrails.mdx"}}
{"text": "       ],\n              jumpTo: \"end\",\n            };\n          }\n        }\n\n        return;\n      },\n      canJumpTo: ['end']\n    }\n  });\n};\n\n// Use the custom guardrail\nimport { createAgent } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [searchTool, calculatorTool],\n  middleware: [\n    contentFilterMiddleware([\"hack\", \"exploit\", \"malware\"]),\n  ],\n});\n\n// This request will be blocked before any processing\nconst result = await agent.invoke({\n  messages: [{ role: \"user\", content: \"How do I hack into a database?\" }]\n});\n```\n:::\n\n### After agent guardrails\n\nUse \"after agent\" hooks to validate final outputs once before returning to the user. This is useful for model-based safety checks, quality validation, or final compliance scans on the complete agent response.\n\n:::python\n\n<CodeGroup>\n\n```python title=\"Class syntax\"\nfrom langchain.agents.middleware import AgentMiddleware, AgentState, hook_config\nfrom langgraph.runtime import Runtime\nfrom langchain.messages import AIMessage\nfrom langchain.chat_models import init_chat_model\nfrom typing import Any\n\nclass SafetyGuardrailMiddleware(AgentMiddleware):\n    \"\"\"Model-based guardrail: Use an LLM to evaluate response safety.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.safety_model = init_chat_model(\"gpt-4.1-mini\")\n\n    @hook_config(can_jump_to=[\"end\"])\n    def after_agent(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n        # Get the final AI response\n        if not state[\"messages\"]:\n            return None\n\n        last_message = state[\"messages\"][-1]\n        if not isinstance(last_message, AIMessage):\n            return None\n\n        # Use a model to evaluate safety\n        safety_prompt = f\"\"\"Evaluate if this response is safe and appropriate.\n        Respond with only 'SAFE' or 'UNSAFE'.\n\n        Response: {last_message.content}\"\"\"\n\n        result = self.safety_model.invoke([{\"role\": \"user\", \"content\": safety_prompt}])\n\n        if \"UNSAFE\" in result.content:\n            last_message.content = \"I cannot provide that response. Please rephrase your request.\"\n\n        return None\n\n# Use the safety guardrail\nfrom langchain.agents import create_agent\n\nagent = create_agent(\n", "metadata": {"source": "guardrails.mdx"}}
{"text": "\n\n        # Use a model to evaluate safety\n        safety_prompt = f\"\"\"Evaluate if this response is safe and appropriate.\n        Respond with only 'SAFE' or 'UNSAFE'.\n\n        Response: {last_message.content}\"\"\"\n\n        result = self.safety_model.invoke([{\"role\": \"user\", \"content\": safety_prompt}])\n\n        if \"UNSAFE\" in result.content:\n            last_message.content = \"I cannot provide that response. Please rephrase your request.\"\n\n        return None\n\n# Use the safety guardrail\nfrom langchain.agents import create_agent\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[search_tool, calculator_tool],\n    middleware=[SafetyGuardrailMiddleware()],\n)\n\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"How do I make explosives?\"}]\n})\n```\n\n```python title=\"Decorator syntax\"\nfrom langchain.agents.middleware import after_agent, AgentState, hook_config\nfrom langgraph.runtime import Runtime\nfrom langchain.messages import AIMessage\nfrom langchain.chat_models import init_chat_model\nfrom typing import Any\n\nsafety_model = init_chat_model(\"gpt-4.1-mini\")\n\n@after_agent(can_jump_to=[\"end\"])\ndef safety_guardrail(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n    \"\"\"Model-based guardrail: Use an LLM to evaluate response safety.\"\"\"\n    # Get the final AI response\n    if not state[\"messages\"]:\n        return None\n\n    last_message = state[\"messages\"][-1]\n    if not isinstance(last_message, AIMessage):\n        return None\n\n    # Use a model to evaluate safety\n    safety_prompt = f\"\"\"Evaluate if this response is safe and appropriate.\n    Respond with only 'SAFE' or 'UNSAFE'.\n\n    Response: {last_message.content}\"\"\"\n\n    result = safety_model.invoke([{\"role\": \"user\", \"content\": safety_prompt}])\n\n    if \"UNSAFE\" in result.content:\n        last_message.content = \"I cannot provide that response. Please rephrase your request.\"\n\n    return None\n\n# Use the safety guardrail\nfrom langchain.agents import create_agent\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[search_tool, calculator_tool],\n    middleware=[safety_guardrail],\n)\n\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"How do I make explosives?\"}]\n})\n```\n\n</CodeGroup>\n\n:::\n\n:::js\n```typescript\nimport { createMiddleware, AIMessage, initChatModel } from \"langchain\";", "metadata": {"source": "guardrails.mdx"}}
{"text": "prompt}])\n\n    if \"UNSAFE\" in result.content:\n        last_message.content = \"I cannot provide that response. Please rephrase your request.\"\n\n    return None\n\n# Use the safety guardrail\nfrom langchain.agents import create_agent\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[search_tool, calculator_tool],\n    middleware=[safety_guardrail],\n)\n\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"How do I make explosives?\"}]\n})\n```\n\n</CodeGroup>\n\n:::\n\n:::js\n```typescript\nimport { createMiddleware, AIMessage, initChatModel } from \"langchain\";\n\nconst safetyGuardrailMiddleware = () => {\n  const safetyModel = initChatModel(\"gpt-4.1-mini\");\n\n  return createMiddleware({\n    name: \"SafetyGuardrailMiddleware\",\n    afterAgent: {\n      hook: async (state) => {\n        // Get the final AI response\n        if (!state.messages || state.messages.length === 0) {\n          return;\n        }\n\n        const lastMessage = state.messages[state.messages.length - 1];\n        if (lastMessage._getType() !== \"ai\") {\n          return;\n        }\n\n        // Use a model to evaluate safety\n        const safetyPrompt = `Evaluate if this response is safe and appropriate.\n        Respond with only 'SAFE' or 'UNSAFE'.\n\n        Response: ${lastMessage.content.toString()}`;\n\n        const result = await safetyModel.invoke([\n          { role: \"user\", content: safetyPrompt }\n        ]);\n\n        if (result.content.toString().includes(\"UNSAFE\")) {\n          return {\n            messages: [\n              new AIMessage(\n                \"I cannot provide that response. Please rephrase your request.\"\n              )\n            ],\n            jumpTo: \"end\",\n          };\n        }\n\n        return;\n      },\n      canJumpTo: ['end']\n    }\n  });\n};\n\n// Use the safety guardrail\nimport { createAgent } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [searchTool, calculatorTool],\n  middleware: [safetyGuardrailMiddleware", "metadata": {"source": "guardrails.mdx"}}
{"text": " AIMessage(\n                \"I cannot provide that response. Please rephrase your request.\"\n              )\n            ],\n            jumpTo: \"end\",\n          };\n        }\n\n        return;\n      },\n      canJumpTo: ['end']\n    }\n  });\n};\n\n// Use the safety guardrail\nimport { createAgent } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [searchTool, calculatorTool],\n  middleware: [safetyGuardrailMiddleware()],\n});\n\nconst result = await agent.invoke({\n  messages: [{ role: \"user\", content: \"How do I make explosives?\" }]\n});\n```\n:::\n\n### Combine multiple guardrails\n\nYou can stack multiple guardrails by adding them to the middleware array. They execute in order, allowing you to build layered protection:\n\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import PIIMiddleware, HumanInTheLoopMiddleware\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[search_tool, send_email_tool],\n    middleware=[\n        # Layer 1: Deterministic input filter (before agent)\n        ContentFilterMiddleware(banned_keywords=[\"hack\", \"exploit\"]),\n\n        # Layer 2: PII protection (before and after model)\n        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_input=True),\n        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_output=True),\n\n        # Layer 3: Human approval for sensitive tools\n        HumanInTheLoopMiddleware(interrupt_on={\"send_email\": True}),\n\n        # Layer 4: Model-based safety check (after agent)\n        SafetyGuardrailMiddleware(),\n    ],\n)\n```\n:::\n\n:::js\n```typescript\nimport { createAgent, piiRedactionMiddleware, humanInTheLoopMiddleware } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [searchTool, sendEmailTool],\n  middleware: [\n    // Layer 1: Deterministic input filter (before agent)\n    contentFilterMiddleware([\"hack\", \"exploit\"]),\n\n    // Layer 2: PII protection (before and after model)\n    piiRedactionMiddleware({\n      piiType: \"email\",\n      strategy: \"redact\",\n      applyToInput: true,\n    }),\n    piiRedactionMiddleware({\n      piiType: \"email\",\n      strategy: \"redact\",\n  ", "metadata": {"source": "guardrails.mdx"}}
{"text": " piiRedactionMiddleware, humanInTheLoopMiddleware } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [searchTool, sendEmailTool],\n  middleware: [\n    // Layer 1: Deterministic input filter (before agent)\n    contentFilterMiddleware([\"hack\", \"exploit\"]),\n\n    // Layer 2: PII protection (before and after model)\n    piiRedactionMiddleware({\n      piiType: \"email\",\n      strategy: \"redact\",\n      applyToInput: true,\n    }),\n    piiRedactionMiddleware({\n      piiType: \"email\",\n      strategy: \"redact\",\n      applyToOutput: true,\n    }),\n\n    // Layer 3: Human approval for sensitive tools\n    humanInTheLoopMiddleware({\n      interruptOn: {\n        send_email: { allowAccept: true, allowEdit: true, allowRespond: true },\n      }\n    }),\n\n    // Layer 4: Model-based safety check (after agent)\n    safetyGuardrailMiddleware(),\n  ],\n});\n```\n:::\n\n## Additional resources\n\n- [Middleware documentation](/oss/langchain/middleware) - Complete guide to custom middleware\n- [Middleware API reference](https://reference.langchain.com/python/langchain/middleware/) - Complete guide to custom middleware\n- [Human-in-the-loop](/oss/langchain/human-in-the-loop) - Add human review for sensitive operations\n- [Testing agents](/oss/langchain/test) - Strategies for testing safety mechanisms\n", "metadata": {"source": "guardrails.mdx"}}
{"text": "---\ntitle: Build a SQL agent\nsidebarTitle: SQL agent\n---\n\nimport ChatModelTabsPy from '/snippets/chat-model-tabs.mdx';\nimport ChatModelTabsJS from '/snippets/chat-model-tabs-js.mdx';\n\n## Overview\n\nIn this tutorial, you will learn how to build an agent that can answer questions about a SQL database using LangChain [agents](/oss/langchain/agents).\n\nAt a high level, the agent will:\n\n<Steps>\n<Step title=\"Fetch the available tables and schemas from the database\" />\n<Step title=\"Decide which tables are relevant to the question\" />\n<Step title=\"Fetch the schemas for the relevant tables\" />\n<Step title=\"Generate a query based on the question and information from the schemas\" />\n<Step title=\"Double-check the query for common mistakes using an LLM\" />\n<Step title=\"Execute the query and return the results\" />\n<Step title=\"Correct mistakes surfaced by the database engine until the query is successful\" />\n<Step title=\"Formulate a response based on the results\" />\n</Steps>\n\n<Warning>\nBuilding Q&A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your agent's needs. This will mitigate, though not eliminate, the risks of building a model-driven system.\n</Warning>\n\n### Concepts\n\nWe will cover the following concepts:\n\n- [Tools](/oss/langchain/tools) for reading from SQL databases\n- LangChain [agents](/oss/langchain/agents)\n- [Human-in-the-loop](/oss/langchain/human-in-the-loop) processes\n\n\n## Setup\n\n### Installation\n\n    :::python\n    <CodeGroup>\n    ```bash pip\n    pip install langchain  langgraph  langchain-community\n    ```\n    </CodeGroup>\n    :::\n    :::js\n    <CodeGroup>\n    ```bash npm\n    npm i langchain @langchain/core typeorm sqlite3 zod\n    ```\n    ```bash yarn\n    yarn add langchain @langchain/core typeorm sqlite3 zod\n    ```\n    ```bash pnpm\n    pnpm add langchain @langchain/core typeorm sqlite3 zod\n    ```\n    </CodeGroup>\n    :::\n\n### LangSmith\nSet up [LangSmith](https://smith.langchain.com) to inspect what is happening inside your chain or agent. Then set the following environment variables:\n\n    ```shell\n    export LANGSMITH_TRACING=\"true\"\n    export LANGSMITH_API_KEY=\"...\"\n    ```\n\n## 1. Select an LLM\n\n:::python\nSelect a model that supports [tool-calling](/oss/integrations/providers/overview):\n\n<ChatModelTabsPy />\n:::\n:::js\n\nSelect a model that supports [tool-calling](/oss/integrations/providers/overview):\n<ChatModelTabsJS />\n:::\n\nThe output shown in the examples below used OpenAI.", "metadata": {"source": "sql-agent.mdx"}}
{"text": "`\n    </CodeGroup>\n    :::\n\n### LangSmith\nSet up [LangSmith](https://smith.langchain.com) to inspect what is happening inside your chain or agent. Then set the following environment variables:\n\n    ```shell\n    export LANGSMITH_TRACING=\"true\"\n    export LANGSMITH_API_KEY=\"...\"\n    ```\n\n## 1. Select an LLM\n\n:::python\nSelect a model that supports [tool-calling](/oss/integrations/providers/overview):\n\n<ChatModelTabsPy />\n:::\n:::js\n\nSelect a model that supports [tool-calling](/oss/integrations/providers/overview):\n<ChatModelTabsJS />\n:::\n\nThe output shown in the examples below used OpenAI.\n\n## 2. Configure the database\n\nYou will be creating a [SQLite database](https://www.sqlitetutorial.net/sqlite-sample-database/) for this tutorial. SQLite is a lightweight database that is easy to set up and use. We will be loading the `chinook` database, which is a sample database that represents a digital media store.\n\nFor convenience, we have hosted the database (`Chinook.db`) on a public GCS bucket.\n\n:::python\n```python\nimport requests, pathlib\n\nurl = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\"\nlocal_path = pathlib.Path(\"Chinook.db\")\n\nif local_path.exists():\n    print(f\"{local_path} already exists, skipping download.\")\nelse:\n    response = requests.get(url)\n    if response.status_code == 200:\n        local_path.write_bytes(response.content)\n        print(f\"File downloaded and saved as {local_path}\")\n    else:\n        print(f\"Failed to download the file. Status code: {response.status_code}\")\n```\n:::\n:::js\n```typescript\nimport fs from \"node:fs/promises\";\nimport path from \"node:path\";\n\nconst url = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\";\nconst localPath = path.resolve(\"Chinook.db\");\n\nasync function resolveDbPath() {\n  if (await fs.exists(localPath)) {\n    return localPath;\n  }\n  const resp = await fetch(url);\n  if (!resp.ok) throw new Error(`Failed to download DB. Status code: ${resp.status}`);\n  const buf = Buffer.from(await resp.arrayBuffer());\n  await fs.writeFile(localPath, buf);\n  return localPath;\n}\n```\n:::\n:::python\nWe will use a handy SQL database wrapper available in the `langchain_community` package to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:\n\n```python\nfrom langchain_community.utilities import SQLDatabase\n\ndb = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n\nprint(f\"Dialect: {db.dialect}", "metadata": {"source": "sql-agent.mdx"}}
{"text": "await fs.exists(localPath)) {\n    return localPath;\n  }\n  const resp = await fetch(url);\n  if (!resp.ok) throw new Error(`Failed to download DB. Status code: ${resp.status}`);\n  const buf = Buffer.from(await resp.arrayBuffer());\n  await fs.writeFile(localPath, buf);\n  return localPath;\n}\n```\n:::\n:::python\nWe will use a handy SQL database wrapper available in the `langchain_community` package to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:\n\n```python\nfrom langchain_community.utilities import SQLDatabase\n\ndb = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n\nprint(f\"Dialect: {db.dialect}\")\nprint(f\"Available tables: {db.get_usable_table_names()}\")\nprint(f'Sample output: {db.run(\"SELECT * FROM Artist LIMIT 5;\")}')\n```\n```\nDialect: sqlite\nAvailable tables: ['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']\nSample output: [(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains')]\n```\n:::\n\n## 3. Add tools for database interactions\n:::python\nUse the `SQLDatabase` wrapper available in the `langchain_community` package to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:\n\n```python\nfrom langchain_community.agent_toolkits import SQLDatabaseToolkit\n\ntoolkit = SQLDatabaseToolkit(db=db, llm=model)\n\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(f\"{tool.name}: {tool.description}\\n\")\n```\n```\nsql_db_query: Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. If you encounter an issue with Unknown column 'xxxx' in 'field list', use sql_db_schema to query the correct table fields.\n\nsql_db_schema: Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling sql_db_list_tables first! Example Input: table1, table2, table3\n\nsql_db_list_tables: Input is an empty string, output is a comma-separated list of tables in the database.\n\nsql_db_query_checker: Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with sql_db_query!\n```\n:::\n:::js\n\nUse the `SqlDatabase` wrapper available in the `langchain/sql_db` to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:\n```typescript\nimport { SqlDatabase } from \"@langchain/classic/sql_db\";\nimport { DataSource } from", "metadata": {"source": "sql-agent.mdx"}}
{"text": "ated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling sql_db_list_tables first! Example Input: table1, table2, table3\n\nsql_db_list_tables: Input is an empty string, output is a comma-separated list of tables in the database.\n\nsql_db_query_checker: Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with sql_db_query!\n```\n:::\n:::js\n\nUse the `SqlDatabase` wrapper available in the `langchain/sql_db` to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:\n```typescript\nimport { SqlDatabase } from \"@langchain/classic/sql_db\";\nimport { DataSource } from \"typeorm\";\n\nlet db: SqlDatabase | undefined;\nasync function getDb() {\n  if (!db) {\n    const dbPath = await resolveDbFile();\n    const datasource = new DataSource({ type: \"sqlite\", database: dbPath });\n    db = await SqlDatabase.fromDataSourceParams({ appDataSource: datasource });\n  }\n  return db;\n}\n\nasync function getSchema() {\n  const db = await getDb();\n  return await db.getTableInfo();\n}\n```\n:::\n\n:::python\n## 4. Use `create_agent`\n\nUse @[`create_agent`] to build a [ReAct agent](https://arxiv.org/pdf/2210.03629) with minimal code. The agent will interpret the request and generate a SQL command, which the tools will execute. If the command has an error, the error message is returned to the model. The model can then examine the original request and the new error message and generate a new command. This can continue until the LLM generates the command successfully or reaches an end count. This pattern of providing a model with feedback - error messages in this case - is very powerful.\n\nInitialize the agent with a descriptive system prompt to customize its behavior:\n\n```python\nsystem_prompt = \"\"\"\nYou are an agent designed to interact with a SQL database.\nGiven an input question, create a syntactically correct {dialect} query to run,\nthen look at the results of the query and return the answer. Unless the user\nspecifies a specific number of examples they wish to obtain, always limit your\nquery to at most {top_k} results.\n\nYou can order the results by a relevant column to return the most interesting\nexamples in the database. Never query for all the columns from a specific table,\nonly ask for the relevant columns given the question.\n\nYou MUST double check your query before executing it. If you get an error while\nexecuting a query, rewrite the query and try again.\n\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the\ndatabase.\n\nTo start you should ALWAYS look at the tables in the database to see what you\ncan query. Do NOT skip this step.\n\nThen you should query the schema of the most relevant tables.\n\"\"\".format(\n    dialect=db.dialect,\n    top_k=5,\n)\n```\n\nNow, create an agent with the model, tools, and prompt:\n\n```python\nfrom langchain.agents import create_agent\n\n\nagent = create_", "metadata": {"source": "sql-agent.mdx"}}
{"text": " most interesting\nexamples in the database. Never query for all the columns from a specific table,\nonly ask for the relevant columns given the question.\n\nYou MUST double check your query before executing it. If you get an error while\nexecuting a query, rewrite the query and try again.\n\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the\ndatabase.\n\nTo start you should ALWAYS look at the tables in the database to see what you\ncan query. Do NOT skip this step.\n\nThen you should query the schema of the most relevant tables.\n\"\"\".format(\n    dialect=db.dialect,\n    top_k=5,\n)\n```\n\nNow, create an agent with the model, tools, and prompt:\n\n```python\nfrom langchain.agents import create_agent\n\n\nagent = create_agent(\n    model,\n    tools,\n    system_prompt=system_prompt,\n)\n```\n\n## 5. Run the agent\n\nRun the agent on a sample query and observe its behavior:\n\n```python\nquestion = \"Which genre on average has the longest tracks?\"\n\nfor step in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": question}]},\n    stream_mode=\"values\",\n):\n    step[\"messages\"][-1].pretty_print()\n```\n\n```\n================================ Human Message =================================\n\nWhich genre on average has the longest tracks?\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_list_tables (call_BQsWg8P65apHc8BTJ1NPDvnM)\n Call ID: call_BQsWg8P65apHc8BTJ1NPDvnM\n  Args:\n================================= Tool Message =================================\nName: sql_db_list_tables\n\nAlbum, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_schema (call_i89tjKECFSeERbuACYm4w0cU)\n Call ID: call_i89tjKECFSeERbuACYm4w0cU\n  Args:\n    table_names: Track, Genre\n================================= Tool Message =================================\nName: sql_db_schema\n\n\nCREATE TABLE \"Genre\" (\n\t\"GenreId\" INTEGER NOT NULL,\n\t\"Name\" NVARCHAR(120),\n\tPRIMARY KEY (\"GenreId\")\n)\n\n/*\n3 rows from Genre table:\nGenreId\tName\n1\tRock\n2\tJazz\n3\tMetal\n*/\n\n\nCREATE TABLE \"Track\" (\n\t\"TrackId\" INTEGER NOT NULL,\n\t\"Name\" NVARCHAR(200) NOT NULL,\n\t\"AlbumId\" INTEGER,\n\t\"MediaTypeId\" INTEGER NOT NULL,\n\t\"GenreId\" INTEGER,\n\t\"Composer\" NVARCHAR(220),\n\t\"Milliseconds\" INTEGER NOT NULL,\n\t\"Bytes\" INTEGER,\n\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL,\n\tPRIMARY KEY (\"TrackId\"),\n\tFOREIGN KEY(\"MediaTypeId\") REFERENC", "metadata": {"source": "sql-agent.mdx"}}
{"text": "\t\"Name\" NVARCHAR(120),\n\tPRIMARY KEY (\"GenreId\")\n)\n\n/*\n3 rows from Genre table:\nGenreId\tName\n1\tRock\n2\tJazz\n3\tMetal\n*/\n\n\nCREATE TABLE \"Track\" (\n\t\"TrackId\" INTEGER NOT NULL,\n\t\"Name\" NVARCHAR(200) NOT NULL,\n\t\"AlbumId\" INTEGER,\n\t\"MediaTypeId\" INTEGER NOT NULL,\n\t\"GenreId\" INTEGER,\n\t\"Composer\" NVARCHAR(220),\n\t\"Milliseconds\" INTEGER NOT NULL,\n\t\"Bytes\" INTEGER,\n\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL,\n\tPRIMARY KEY (\"TrackId\"),\n\tFOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"),\n\tFOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"),\n\tFOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\")\n)\n\n/*\n3 rows from Track table:\nTrackId\tName\tAlbumId\tMediaTypeId\tGenreId\tComposer\tMilliseconds\tBytes\tUnitPrice\n1\tFor Those About To Rock (We Salute You)\t1\t1\t1\tAngus Young, Malcolm Young, Brian Johnson\t343719\t11170334\t0.99\n2\tBalls to the Wall\t2\t2\t1\tU. Dirkschneider, W. Hoffmann, H. Frank, P. Baltes, S. Kaufmann, G. Hoffmann\t342562\t5510424\t0.99\n3\tFast As a Shark\t3\t2\t1\tF. Baltes, S. Kaufman, U. Dirkscneider & W. Hoffman\t230619\t3990994\t0.99\n*/\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_query_checker (call_G64yYm6R6UauiVPCXJZMA49b)\n Call ID: call_G64yYm6R6UauiVPCXJZMA49b\n  Args:\n    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AverageLength FROM Track INNER JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AverageLength DESC LIMIT 5;\n================================= Tool Message =================================\nName: sql_db_query_checker\n\nSELECT Genre.Name, AVG(Track.Milliseconds) AS AverageLength FROM Track INNER JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AverageLength DESC LIMIT 5;\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_query (call_AnO3SrhD0ODJBxh6dHMwvHwZ)\n Call ID: call_AnO3SrhD0ODJBxh6dHMwvHwZ\n  Args:\n    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AverageLength FROM Track INNER JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AverageLength DESC LIMIT 5;\n================================= Tool Message =================================\nName: sql_db_query\n", "metadata": {"source": "sql-agent.mdx"}}
{"text": "SELECT Genre.Name, AVG(Track.Milliseconds) AS AverageLength FROM Track INNER JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AverageLength DESC LIMIT 5;\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_query (call_AnO3SrhD0ODJBxh6dHMwvHwZ)\n Call ID: call_AnO3SrhD0ODJBxh6dHMwvHwZ\n  Args:\n    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AverageLength FROM Track INNER JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AverageLength DESC LIMIT 5;\n================================= Tool Message =================================\nName: sql_db_query\n\n[('Sci Fi & Fantasy', 2911783.0384615385), ('Science Fiction', 2625549.076923077), ('Drama', 2575283.78125), ('TV Shows', 2145041.0215053763), ('Comedy', 1585263.705882353)]\n================================== Ai Message ==================================\n\nOn average, the genre with the longest tracks is \"Sci Fi & Fantasy\" with an average track length of approximately 2,911,783 milliseconds. This is followed by \"Science Fiction,\" \"Drama,\" \"TV Shows,\" and \"Comedy.\"\n```\n\nThe agent correctly wrote a query, checked the query, and ran it to inform its final response.\n\n<Note>\n    You can inspect all aspects of the above run, including steps taken, tools invoked, what prompts were seen by the LLM, and more in the [LangSmith trace](https://smith.langchain.com/public/cd2ce887-388a-4bb1-a29d-48208ce50d15/r).\n</Note>\n\n### (Optional) Use Studio\n\n[Studio](/langsmith/studio) provides a \"client side\" loop as well as memory so you can run this as a chat interface and query the database. You can ask questions like \"Tell me the scheme of the database\" or \"Show me the invoices for the 5 top customers\". You will see the SQL command that is generated and the resulting output. The details of how to get that started are below.\n<Accordion title=\"Run your agent in Studio\">\n\nIn addition to the previously mentioned packages, you will need to:\n\n```shell\npip install -U langgraph-cli[inmem]>=0.4.0\n```\n\nIn directory you will run in, you will need a `langgraph.json` file with the following contents:\n\n```json\n{\n  \"dependencies\": [\".\"],\n  \"graphs\": {\n      \"agent\": \"./sql_agent.py:agent\",\n      \"graph\": \"./sql_agent_langgraph.py:graph\"\n  },\n  \"env\": \".env\"\n}\n```\n\nCreate a file `sql_agent.py` and insert this:\n\n```python\n#sql_agent.py for studio\nimport pathlib\n\nfrom langchain.agents import create_agent\nfrom langchain.chat_models import init_chat_model\nfrom langchain_community.agent_toolkits import SQLDatabaseToolkit\nfrom langchain_community.utilities import SQLDatabase\nimport requests\n\n\n# Initialize an LLM\n", "metadata": {"source": "sql-agent.mdx"}}
{"text": " will run in, you will need a `langgraph.json` file with the following contents:\n\n```json\n{\n  \"dependencies\": [\".\"],\n  \"graphs\": {\n      \"agent\": \"./sql_agent.py:agent\",\n      \"graph\": \"./sql_agent_langgraph.py:graph\"\n  },\n  \"env\": \".env\"\n}\n```\n\nCreate a file `sql_agent.py` and insert this:\n\n```python\n#sql_agent.py for studio\nimport pathlib\n\nfrom langchain.agents import create_agent\nfrom langchain.chat_models import init_chat_model\nfrom langchain_community.agent_toolkits import SQLDatabaseToolkit\nfrom langchain_community.utilities import SQLDatabase\nimport requests\n\n\n# Initialize an LLM\nmodel = init_chat_model(\"gpt-4.1\")\n\n# Get the database, store it locally\nurl = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\"\nlocal_path = pathlib.Path(\"Chinook.db\")\n\nif local_path.exists():\n    print(f\"{local_path} already exists, skipping download.\")\nelse:\n    response = requests.get(url)\n    if response.status_code == 200:\n        local_path.write_bytes(response.content)\n        print(f\"File downloaded and saved as {local_path}\")\n    else:\n        print(f\"Failed to download the file. Status code: {response.status_code}\")\n\ndb = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n\n# Create the tools\ntoolkit = SQLDatabaseToolkit(db=db, llm=model)\n\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(f\"{tool.name}: {tool.description}\\n\")\n\n# Use create_agent\nsystem_prompt = \"\"\"\nYou are an agent designed to interact with a SQL database.\nGiven an input question, create a syntactically correct {dialect} query to run,\nthen look at the results of the query and return the answer. Unless the user\nspecifies a specific number of examples they wish to obtain, always limit your\nquery to at most {top_k} results.\n\nYou can order the results by a relevant column to return the most interesting\nexamples in the database. Never query for all the columns from a specific table,\nonly ask for the relevant columns given the question.\n\nYou MUST double check your query before executing it. If you get an error while\nexecuting a query, rewrite the query and try again.\n\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the\ndatabase.\n\nTo start you should ALWAYS look at the tables in the database to see what you\ncan query. Do NOT skip this step.\n\nThen you should query the schema of the most relevant tables.\n\"\"\".format(\n    dialect=db.dialect,\n    top_k=5,\n)\n\nagent = create_agent(\n    model,\n    tools,\n    system_prompt=system_prompt,\n)\n```\n</Accordion>\n", "metadata": {"source": "sql-agent.mdx"}}
{"text": " the database. Never query for all the columns from a specific table,\nonly ask for the relevant columns given the question.\n\nYou MUST double check your query before executing it. If you get an error while\nexecuting a query, rewrite the query and try again.\n\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the\ndatabase.\n\nTo start you should ALWAYS look at the tables in the database to see what you\ncan query. Do NOT skip this step.\n\nThen you should query the schema of the most relevant tables.\n\"\"\".format(\n    dialect=db.dialect,\n    top_k=5,\n)\n\nagent = create_agent(\n    model,\n    tools,\n    system_prompt=system_prompt,\n)\n```\n</Accordion>\n\n:::\n\n## 6. Implement human-in-the-loop review\n\nIt can be prudent to check the agent's SQL queries before they are executed for any unintended actions or inefficiencies.\n\nLangChain agents feature support for built-in [human-in-the-loop middleware](/oss/langchain/human-in-the-loop) to add oversight to agent tool calls. Let's configure the agent to pause for human review on calling the `sql_db_query` tool:\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware # [!code highlight]\nfrom langgraph.checkpoint.memory import InMemorySaver # [!code highlight]\n\n\nagent = create_agent(\n    model,\n    tools,\n    system_prompt=system_prompt,\n    middleware=[ # [!code highlight]\n        HumanInTheLoopMiddleware( # [!code highlight]\n            interrupt_on={\"sql_db_query\": True}, # [!code highlight]\n            description_prefix=\"Tool execution pending approval\", # [!code highlight]\n        ), # [!code highlight]\n    ], # [!code highlight]\n    checkpointer=InMemorySaver(), # [!code highlight]\n)\n```\n<Note>\nWe've added a [checkpointer](/oss/langchain/short-term-memory) to our agent to allow execution to be paused and resumed. See the [human-in-the-loop guide](/oss/langchain/human-in-the-loop) for detalis on this as well as available middleware configurations.\n</Note>\n\nOn running the agent, it will now pause for review before executing the `sql_db_query` tool:\n```python\nquestion = \"Which genre on average has the longest tracks?\"\nconfig = {\"configurable\": {\"thread_id\": \"1\"}} # [!code highlight]\n\nfor step in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": question}]},\n    config, # [!code highlight]\n    stream_mode=\"values\",\n):\n    if \"__interrupt__\" in step: # [!code highlight]\n        print(\"INTERRUPTED:\") # [!code highlight]\n        interrupt = step[\"__interrupt__\"", "metadata": {"source": "sql-agent.mdx"}}
{"text": "-the-loop) for detalis on this as well as available middleware configurations.\n</Note>\n\nOn running the agent, it will now pause for review before executing the `sql_db_query` tool:\n```python\nquestion = \"Which genre on average has the longest tracks?\"\nconfig = {\"configurable\": {\"thread_id\": \"1\"}} # [!code highlight]\n\nfor step in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": question}]},\n    config, # [!code highlight]\n    stream_mode=\"values\",\n):\n    if \"__interrupt__\" in step: # [!code highlight]\n        print(\"INTERRUPTED:\") # [!code highlight]\n        interrupt = step[\"__interrupt__\"][0] # [!code highlight]\n        for request in interrupt.value[\"action_requests\"]: # [!code highlight]\n            print(request[\"description\"]) # [!code highlight]\n    elif \"messages\" in step:\n        step[\"messages\"][-1].pretty_print()\n    else:\n        pass\n```\n```\n...\n\nINTERRUPTED:\nTool execution pending approval\n\nTool: sql_db_query\nArgs: {'query': 'SELECT g.Name AS Genre, AVG(t.Milliseconds) AS AvgTrackLength FROM Track t JOIN Genre g ON t.GenreId = g.GenreId GROUP BY g.Name ORDER BY AvgTrackLength DESC LIMIT 1;'}\n```\nWe can resume execution, in this case accepting the query, using [Command](/oss/langgraph/use-graph-api#combine-control-flow-and-state-updates-with-command):\n```python\nfrom langgraph.types import Command # [!code highlight]\n\nfor step in agent.stream(\n    Command(resume={\"decisions\": [{\"type\": \"approve\"}]}), # [!code highlight]\n    config,\n    stream_mode=\"values\",\n):\n    if \"messages\" in step:\n        step[\"messages\"][-1].pretty_print()\n    elif \"__interrupt__\" in step:\n        print(\"INTERRUPTED:\")\n        interrupt = step[\"__interrupt__\"][0]\n        for request in interrupt.value[\"action_requests\"]:\n            print(request[\"description\"])\n    else:\n        pass\n```\n```\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_query (call_7oz86Epg7lYRqi9rQHbZPS1U)\n Call ID: call_7oz86Epg7lYRqi9rQHbZPS1U\n  Args:\n    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgDuration FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AvgDuration DESC LIMIT 5", "metadata": {"source": "sql-agent.mdx"}}
{"text": "  interrupt = step[\"__interrupt__\"][0]\n        for request in interrupt.value[\"action_requests\"]:\n            print(request[\"description\"])\n    else:\n        pass\n```\n```\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_query (call_7oz86Epg7lYRqi9rQHbZPS1U)\n Call ID: call_7oz86Epg7lYRqi9rQHbZPS1U\n  Args:\n    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgDuration FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AvgDuration DESC LIMIT 5;\n================================= Tool Message =================================\nName: sql_db_query\n\n[('Sci Fi & Fantasy', 2911783.0384615385), ('Science Fiction', 2625549.076923077), ('Drama', 2575283.78125), ('TV Shows', 2145041.0215053763), ('Comedy', 1585263.705882353)]\n================================== Ai Message ==================================\n\nThe genre with the longest average track length is \"Sci Fi & Fantasy\" with an average duration of about 2,911,783 milliseconds, followed by \"Science Fiction\" and \"Drama.\"\n```\n\nRefer to the [human-in-the-loop guide](/oss/langchain/human-in-the-loop) for details.\n\n:::js\n## 4. Execute SQL queries\n\nBefore running the command, do a check to check the LLM generated command in ` _safe_sql`:\n\n```typescript\n\nconst DENY_RE = /\\b(INSERT|UPDATE|DELETE|ALTER|DROP|CREATE|REPLACE|TRUNCATE)\\b/i;\nconst HAS_LIMIT_TAIL_RE = /\\blimit\\b\\s+\\d+(\\s*,\\s*\\d+)?\\s*;?\\s*$/i;\n\nfunction sanitizeSqlQuery(q) {\n  let query = String(q ?? \"\").trim();\n\n  // block multiple statements (allow one optional trailing ;)\n  const semis = [...query].filter((c) => c === \";\").length;\n  if (semis > 1 || (query.endsWith(\";\") && query.slice(0, -1).includes(\";\"))) {\n    throw new Error(\"multiple statements are not allowed.\")\n  }\n  query = query.replace(/;+\\s*$/g, \"\").trim();\n\n  // read-only gate\n  if (!query.toLowerCase().startsWith(\"select\")) {\n    throw new Error(\"Only SELECT statements are allowed\")\n  }\n  if (DENY_RE.test(query)) {\n    throw new Error(\"DML/DDL detected. Only read-only queries are permitted.\")\n  }\n\n  // append LIMIT only if not already present\n  if (!HAS_LIMIT_TAIL_RE.test(query)) {\n    query += \" LIMIT 5\";\n  }\n  return query;\n}\n\n```\n\nThen, use `run` from `SQLDatabase` to execute commands with an `execute_sql` tool", "metadata": {"source": "sql-agent.mdx"}}
{"text": "   throw new Error(\"multiple statements are not allowed.\")\n  }\n  query = query.replace(/;+\\s*$/g, \"\").trim();\n\n  // read-only gate\n  if (!query.toLowerCase().startsWith(\"select\")) {\n    throw new Error(\"Only SELECT statements are allowed\")\n  }\n  if (DENY_RE.test(query)) {\n    throw new Error(\"DML/DDL detected. Only read-only queries are permitted.\")\n  }\n\n  // append LIMIT only if not already present\n  if (!HAS_LIMIT_TAIL_RE.test(query)) {\n    query += \" LIMIT 5\";\n  }\n  return query;\n}\n\n```\n\nThen, use `run` from `SQLDatabase` to execute commands with an `execute_sql` tool:\n\n```typescript\nimport { tool } from \"langchain\"\nimport * as z from \"zod\";\n\nconst executeSql = tool(\n  async ({ query }) => {\n    const q = sanitizeSqlQuery(query);\n    try {\n      const result = await db.run(q);\n      return typeof result === \"string\" ? result : JSON.stringify(result, null, 2);\n    } catch (e) {\n      throw new Error(e?.message ?? String(e))\n    }\n  },\n  {\n    name: \"execute_sql\",\n    description: \"Execute a READ-ONLY SQLite SELECT query and return results.\",\n    schema: z.object({\n      query: z.string().describe(\"SQLite SELECT query to execute (read-only).\"),\n    }),\n  }\n);\n\n```\n\n## 5. Use `createAgent`\n\nUse `createAgent` to build a [ReAct agent](https://arxiv.org/pdf/2210.03629) with minimal code. The agent will interpret the request and generate a SQL command. The tools will check the command for safety and then try to execute the command. If the command has an error, the error message is returned to the model. The model can then examine the original request and the new error message and generate a new command. This can continue until the LLM generates the command successfully or reaches an end count. This pattern of providing a model with feedback - error messages in this case - is very powerful.\n\nInitialize the agent with a descriptive system prompt to customize its behavior:\n\n```typescript\nimport { SystemMessage } from \"langchain\";\n\nconst getSystemPrompt = async () => new SystemMessage(`You are a careful SQLite analyst.\n\nAuthoritative schema (do not invent columns/tables):\n${await getSchema()}\n\nRules:\n- Think step-by-step.\n- When you need data, call the tool \\`execute_sql\\` with ONE SELECT query.\n- Read-only only; no INSERT/UPDATE/DELETE/ALTER/DROP/CREATE/REPLACE/TRUNCATE.\n- Limit to 5 rows unless user explicitly asks otherwise.\n- If the tool returns 'Error:', revise the SQL and try again.\n- Limit the number of attempts to 5.\n- If you are not successful after 5 attempts, return a note to the user.\n- Prefer explicit column lists; avoid SELECT *.\n", "metadata": {"source": "sql-agent.mdx"}}
{"text": "typescript\nimport { SystemMessage } from \"langchain\";\n\nconst getSystemPrompt = async () => new SystemMessage(`You are a careful SQLite analyst.\n\nAuthoritative schema (do not invent columns/tables):\n${await getSchema()}\n\nRules:\n- Think step-by-step.\n- When you need data, call the tool \\`execute_sql\\` with ONE SELECT query.\n- Read-only only; no INSERT/UPDATE/DELETE/ALTER/DROP/CREATE/REPLACE/TRUNCATE.\n- Limit to 5 rows unless user explicitly asks otherwise.\n- If the tool returns 'Error:', revise the SQL and try again.\n- Limit the number of attempts to 5.\n- If you are not successful after 5 attempts, return a note to the user.\n- Prefer explicit column lists; avoid SELECT *.\n`);\n```\n\nNow, create an agent with the model, tools, and prompt:\n\n```typescript\nimport { createAgent } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-5\",\n  tools: [executeSql],\n  systemPrompt: getSystemPrompt,\n});\n\n```\n\n## 6. Run the agent\n\nRun the agent on a sample query and observe its behavior:\n\n```typescript\nconst question = \"Which genre, on average, has the longest tracks?\";\nconst stream = await agent.stream(\n  { messages: [{ role: \"user\", content: question }] },\n  { streamMode: \"values\" }\n);\nfor await (const step of stream) {\n  const message = step.messages.at(-1);\n  console.log(`${message.role}: ${JSON.stringify(message.content, null, 2)}`);\n}\n```\n\n```\nhuman: Which genre, on average, has the longest tracks?\nai:\ntool: [{\"Genre\":\"Sci Fi & Fantasy\",\"AvgMilliseconds\":2911783.0384615385}]\nai: Sci Fi & Fantasy \u2014 average track length \u2248 48.5 minutes (about 2,911,783 ms).\n```\n\nThe agent correctly wrote a query, checked the query, and ran it to inform its final response.\n\n<Note>\n    You can inspect all aspects of the above run, including steps taken, tools invoked, what prompts were seen by the LLM, and more in the [LangSmith trace](https://smith.langchain.com/public/653d218b-af67-4854-95ca-6abecb9b2520/r).\n</Note>\n\n#### (Optional) Use Studio\n\n[Studio](/langsmith/studio) provides a \"client side\" loop as well as memory so you can run this as a chat interface and query the database. You can ask questions like \"Tell me the scheme of the database\" or \"Show me the invoices for the 5 top customers\". You will see the SQL command that is generated and the resulting output. The details of how to get that started are below.\n<Accordion title=\"Run your agent in Studio\">\n\nIn addition to the previously mentioned packages, you will need to:\n\n```shell\nnpm i -g langgraph-cli@latest\n```\n\nIn directory you will run in, you will need a `langgraph.json` file with the following contents:\n\n```json\n{\n  \"dependencies\": [\".\"],\n", "metadata": {"source": "sql-agent.mdx"}}
{"text": "b9b2520/r).\n</Note>\n\n#### (Optional) Use Studio\n\n[Studio](/langsmith/studio) provides a \"client side\" loop as well as memory so you can run this as a chat interface and query the database. You can ask questions like \"Tell me the scheme of the database\" or \"Show me the invoices for the 5 top customers\". You will see the SQL command that is generated and the resulting output. The details of how to get that started are below.\n<Accordion title=\"Run your agent in Studio\">\n\nIn addition to the previously mentioned packages, you will need to:\n\n```shell\nnpm i -g langgraph-cli@latest\n```\n\nIn directory you will run in, you will need a `langgraph.json` file with the following contents:\n\n```json\n{\n  \"dependencies\": [\".\"],\n  \"graphs\": {\n      \"agent\": \"./sqlAgent.ts:agent\",\n      \"graph\": \"./sqlAgentLanggraph.ts:graph\"\n  },\n  \"env\": \".env\"\n}\n```\n\n```typescript\nimport fs from \"node:fs/promises\";\nimport path from \"node:path\";\nimport { SqlDatabase } from \"@langchain/classic/sql_db\";\nimport { DataSource } from \"typeorm\";\nimport { SystemMessage, createAgent, tool } from \"langchain\"\nimport * as z from \"zod\";\n\nconst url = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\";\nconst localPath = path.resolve(\"Chinook.db\");\n\nasync function resolveDbPath() {\n  if (await fs.exists(localPath)) {\n    return localPath;\n  }\n  const resp = await fetch(url);\n  if (!resp.ok) throw new Error(`Failed to download DB. Status code: ${resp.status}`);\n  const buf = Buffer.from(await resp.arrayBuffer());\n  await fs.writeFile(localPath, buf);\n  return localPath;\n}\n\nlet db: SqlDatabase | undefined;\nasync function getDb() {\n  if (!db) {\n    const dbPath = await resolveDbPath();\n    const datasource = new DataSource({ type: \"sqlite\", database: dbPath });\n    db = await SqlDatabase.fromDataSourceParams({ appDataSource: datasource });\n  }\n  return db;\n}\n\nasync function getSchema() {\n  const db = await getDb();\n  return await db.getTableInfo();\n}\n\nconst DENY_RE = /\\b(INSERT|UPDATE|DELETE|ALTER|DROP|CREATE|REPLACE|TRUNCATE)\\b/i;\nconst HAS_LIMIT_TAIL_RE = /\\blimit\\b\\s+\\d+(\\s*,\\s*\\d+)?\\s*;?\\s*$/i;\n\nfunction sanitizeSqlQuery(q) {\n  let query = String(q ?? \"\").trim();\n\n  // block multiple statements (allow one optional trailing ;)\n  const semis = [...query].filter((c) => c === \";\").length;\n  if (semis > 1 || (query.endsWith(\";\") && query", "metadata": {"source": "sql-agent.mdx"}}
{"text": "async function getSchema() {\n  const db = await getDb();\n  return await db.getTableInfo();\n}\n\nconst DENY_RE = /\\b(INSERT|UPDATE|DELETE|ALTER|DROP|CREATE|REPLACE|TRUNCATE)\\b/i;\nconst HAS_LIMIT_TAIL_RE = /\\blimit\\b\\s+\\d+(\\s*,\\s*\\d+)?\\s*;?\\s*$/i;\n\nfunction sanitizeSqlQuery(q) {\n  let query = String(q ?? \"\").trim();\n\n  // block multiple statements (allow one optional trailing ;)\n  const semis = [...query].filter((c) => c === \";\").length;\n  if (semis > 1 || (query.endsWith(\";\") && query.slice(0, -1).includes(\";\"))) {\n    throw new Error(\"multiple statements are not allowed.\")\n  }\n  query = query.replace(/;+\\s*$/g, \"\").trim();\n\n  // read-only gate\n  if (!query.toLowerCase().startsWith(\"select\")) {\n    throw new Error(\"Only SELECT statements are allowed\")\n  }\n  if (DENY_RE.test(query)) {\n    throw new Error(\"DML/DDL detected. Only read-only queries are permitted.\")\n  }\n\n  // append LIMIT only if not already present\n  if (!HAS_LIMIT_TAIL_RE.test(query)) {\n    query += \" LIMIT 5\";\n  }\n  return query;\n}\n\nconst executeSql = tool(\n  async ({ query }) => {\n    const q = sanitizeSqlQuery(query);\n    try {\n      const result = await db.run(q);\n      return typeof result === \"string\" ? result : JSON.stringify(result, null, 2);\n    } catch (e) {\n      throw new Error(e?.message ?? String(e))\n    }\n  },\n  {\n    name: \"execute_sql\",\n    description: \"Execute a READ-ONLY SQLite SELECT query and return results.\",\n    schema: z.object({\n      query: z.string().describe(\"SQLite SELECT query to execute (read-only).\"),\n    }),\n  }\n);\n\nconst getSystemPrompt = async () => new SystemMessage(`You are a careful SQLite analyst.\n\nAuthoritative schema (do not invent columns/tables):\n${await getSchema()}\n\nRules:\n- Think step-by-step.\n- When you need data, call the tool \\`execute_sql\\` with ONE SELECT query.\n- Read-only only; no INSERT/UPDATE/DELETE/ALTER/DROP/CREATE/REPLACE/TRUNCATE.\n- Limit to 5 rows unless user explicitly asks otherwise.\n- If the tool returns 'Error:', revise the SQL and try again.\n- Limit the number of attempts to 5.\n- If you are not successful after 5 attempts, return a note to the user.\n- Prefer explicit column lists; avoid SELECT *.\n`);\n\nexport const agent = createAgent({\n  model: \"gpt-5\",\n ", "metadata": {"source": "sql-agent.mdx"}}
{"text": " () => new SystemMessage(`You are a careful SQLite analyst.\n\nAuthoritative schema (do not invent columns/tables):\n${await getSchema()}\n\nRules:\n- Think step-by-step.\n- When you need data, call the tool \\`execute_sql\\` with ONE SELECT query.\n- Read-only only; no INSERT/UPDATE/DELETE/ALTER/DROP/CREATE/REPLACE/TRUNCATE.\n- Limit to 5 rows unless user explicitly asks otherwise.\n- If the tool returns 'Error:', revise the SQL and try again.\n- Limit the number of attempts to 5.\n- If you are not successful after 5 attempts, return a note to the user.\n- Prefer explicit column lists; avoid SELECT *.\n`);\n\nexport const agent = createAgent({\n  model: \"gpt-5\",\n  tools: [executeSql],\n  systemPrompt: getSystemPrompt,\n});\n```\n</Accordion>\n:::\n\n## Next steps\n\nFor deeper customization, check out [this tutorial](/oss/langgraph/sql-agent) for implementing a SQL agent directly using LangGraph primitives.\n", "metadata": {"source": "sql-agent.mdx"}}
{"text": "---\ntitle: Retrieval\n---\n\nLarge Language Models (LLMs) are powerful, but they have two key limitations:\n\n* **Finite context** \u2014 they can\u2019t ingest entire corpora at once.\n* **Static knowledge** \u2014 their training data is frozen at a point in time.\n\nRetrieval addresses these problems by fetching relevant external knowledge at query time. This is the foundation of **Retrieval-Augmented Generation (RAG)**: enhancing an LLM\u2019s answers with context-specific information.\n\n\n## Building a knowledge base\n\nA **knowledge base** is a repository of documents or structured data used during retrieval.\n\nIf you need a custom knowledge base, you can use LangChain\u2019s document loaders and vector stores to build one from your own data.\n\n<Note>\n    If you already have a knowledge base (e.g., a SQL database, CRM, or internal documentation system), you do **not** need to rebuild it. You can:\n    - Connect it as a **tool** for an agent in Agentic RAG.\n    - Query it and supply the retrieved content as context to the LLM [(2-Step RAG)](#2-step-rag).\n</Note>\n\nSee the following tutorial to build a searchable knowledge base and minimal RAG workflow:\n\n<Card\n    title=\"Tutorial: Semantic search\"\n    icon=\"database\"\n    href=\"/oss/langchain/knowledge-base\"\n    arrow cta=\"Learn more\"\n>\n    Learn how to create a searchable knowledge base from your own data using LangChain\u2019s document loaders, embeddings, and vector stores.\n    In this tutorial, you\u2019ll build a search engine over a PDF, enabling retrieval of passages relevant to a query. You\u2019ll also implement a minimal RAG workflow on top of this engine to see how external knowledge can be integrated into LLM reasoning.\n</Card>\n\n### From retrieval to RAG\n\nRetrieval allows LLMs to access relevant context at runtime. But most real-world applications go one step further: they **integrate retrieval with generation** to produce grounded, context-aware answers.\n\nThis is the core idea behind **Retrieval-Augmented Generation (RAG)**. The retrieval pipeline becomes a foundation for a broader system that combines search with generation.\n\n### Retrieval pipeline\n\nA typical retrieval workflow looks like this:\n\n```mermaid\nflowchart LR\n  S([\"Sources<br>(Google Drive, Slack, Notion, etc.)\"]) --> L[Document Loaders]\n  L --> A([Documents])\n  A --> B[Split into chunks]\n  B --> C[Turn into embeddings]\n  C --> D[(Vector Store)]\n  Q([User Query]) --> E[Query embedding]\n  E --> D\n  D --> F[Retriever]\n  F --> G[LLM uses retrieved info]\n  G --> H([Answer])\n```\n\nEach component is modular: you can swap loaders, splitters, embeddings, or vector stores without rewriting the app\u2019s logic.\n\n### Building blocks\n\n<Columns cols={2}>\n    <Card\n        title=\"Document loaders\"\n        icon=\"file-import\"\n        href=\"/oss/integrations/document_loaders\"\n        arrow cta=\"Learn more\"", "metadata": {"source": "retrieval.mdx"}}
{"text": "  A --> B[Split into chunks]\n  B --> C[Turn into embeddings]\n  C --> D[(Vector Store)]\n  Q([User Query]) --> E[Query embedding]\n  E --> D\n  D --> F[Retriever]\n  F --> G[LLM uses retrieved info]\n  G --> H([Answer])\n```\n\nEach component is modular: you can swap loaders, splitters, embeddings, or vector stores without rewriting the app\u2019s logic.\n\n### Building blocks\n\n<Columns cols={2}>\n    <Card\n        title=\"Document loaders\"\n        icon=\"file-import\"\n        href=\"/oss/integrations/document_loaders\"\n        arrow cta=\"Learn more\"\n    >\n        Ingest data from external sources (Google Drive, Slack, Notion, etc.), returning standardized @[`Document`] objects.\n    </Card>\n\n    :::python\n    <Card\n        title=\"Text splitters\"\n        icon=\"scissors\"\n        href=\"/oss/integrations/splitters\"\n        arrow\n        cta=\"Learn more\"\n    >\n        Break large docs into smaller chunks that will be retrievable individually and fit within a model's context window.\n    </Card>\n    :::\n    <Card\n        title=\"Embedding models\"\n        icon=\"diagram-project\"\n        href=\"/oss/integrations/text_embedding\"\n        arrow\n        cta=\"Learn more\"\n    >\n        An embedding model turns text into a vector of numbers so that texts with similar meaning land close together in that vector space.\n    </Card>\n\n    <Card\n        title=\"Vector stores\"\n        icon=\"database\"\n        href=\"/oss/integrations/vectorstores/\"\n        arrow\n        cta=\"Learn more\"\n    >\n        Specialized databases for storing and searching embeddings.\n    </Card>\n\n    <Card\n        title=\"Retrievers\"\n        icon=\"binoculars\"\n        href=\"/oss/integrations/retrievers/\"\n        arrow\n        cta=\"Learn more\"\n    >\n        A retriever is an interface that returns documents given an unstructured query.\n    </Card>\n</Columns>\n\n## RAG architectures\n\nRAG can be implemented in multiple ways, depending on your system's needs. We outline each type in the sections below.\n\n| Architecture            | Description                               ", "metadata": {"source": "retrieval.mdx"}}
{"text": " </Card>\n\n    <Card\n        title=\"Retrievers\"\n        icon=\"binoculars\"\n        href=\"/oss/integrations/retrievers/\"\n        arrow\n        cta=\"Learn more\"\n    >\n        A retriever is an interface that returns documents given an unstructured query.\n    </Card>\n</Columns>\n\n## RAG architectures\n\nRAG can be implemented in multiple ways, depending on your system's needs. We outline each type in the sections below.\n\n| Architecture            | Description                                                                | Control   | Flexibility | Latency        | Example Use Case                                   |\n|-------------------------|----------------------------------------------------------------------------|-----------|-------------|----------------|----------------------------------------------------|\n| **2-Step RAG**          | Retrieval always happens before generation. Simple and predictable         | \u2705 High    | \u274c Low       | \u26a1 Fast         | FAQs, documentation bots                           |\n| **Agentic RAG**         | An LLM-powered agent decides *when* and *how* to retrieve during reasoning | \u274c Low     | \u2705 High      | \u23f3 Variable     | Research assistants with access to multiple tools  |\n| **Hybrid**              | Combines characteristics of both approaches with validation steps          | \u2696\ufe0f Medium | \u2696\ufe0f Medium   | \u23f3 Variable     | Domain-specific Q&A with quality validation        |\n\n<Info>\n**Latency**: Latency is generally more **predictable** in **2-Step RAG**, as the maximum number of LLM calls is known and capped. This predictability assumes that LLM inference time is the dominant factor. However, real-world latency may also be affected by the performance of retrieval steps\u2014such as API response times, network delays, or database queries\u2014which can vary based on the tools and infrastructure in use.\n</Info>\n\n### 2-step RAG\n\nIn **2-Step RAG**, the retrieval step is always executed before the generation step. This architecture is straightforward and predictable, making it suitable for many applications where the retrieval of relevant documents is a clear prerequisite for generating an answer.\n\n```mermaid\ngraph LR\n    A[User Question] --> B[\"Retrieve Relevant Documents\"]\n    B --> C[\"Generate Answer\"]\n    C --> D[Return Answer to User]\n\n    %% Styling\n    classDef startend fill:#2e7d32,stroke:#1b5e20,stroke", "metadata": {"source": "retrieval.mdx"}}
{"text": "ability assumes that LLM inference time is the dominant factor. However, real-world latency may also be affected by the performance of retrieval steps\u2014such as API response times, network delays, or database queries\u2014which can vary based on the tools and infrastructure in use.\n</Info>\n\n### 2-step RAG\n\nIn **2-Step RAG**, the retrieval step is always executed before the generation step. This architecture is straightforward and predictable, making it suitable for many applications where the retrieval of relevant documents is a clear prerequisite for generating an answer.\n\n```mermaid\ngraph LR\n    A[User Question] --> B[\"Retrieve Relevant Documents\"]\n    B --> C[\"Generate Answer\"]\n    C --> D[Return Answer to User]\n\n    %% Styling\n    classDef startend fill:#2e7d32,stroke:#1b5e20,stroke-width:2px,color:#fff\n    classDef process fill:#1976d2,stroke:#0d47a1,stroke-width:1.5px,color:#fff\n\n    class A,D startend\n    class B,C process\n```\n\n<Card\n    title=\"Tutorial: Retrieval-Augmented Generation (RAG)\"\n    icon=\"robot\"\n    href=\"/oss/langchain/rag#rag-chains\"\n    arrow cta=\"Learn more\"\n>\n    See how to build a Q&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation.\n    This tutorial walks through two approaches:\n    * A **RAG agent** that runs searches with a flexible tool\u2014great for general-purpose use.\n    * A **2-step RAG** chain that requires just one LLM call per query\u2014fast and efficient for simpler tasks.\n</Card>\n\n### Agentic RAG\n\n**Agentic Retrieval-Augmented Generation (RAG)** combines the strengths of Retrieval-Augmented Generation with agent-based reasoning. Instead of retrieving documents before answering, an agent (powered by an LLM) reasons step-by-step and decides **when** and **how** to retrieve information during the interaction.\n\n<Tip>\nThe only thing an agent needs to enable RAG behavior is access to one or more **tools** that can fetch external knowledge \u2014 such as documentation loaders, web APIs, or database queries.\n</Tip>\n\n```mermaid\ngraph LR\n    A[User Input / Question] --> B[\"Agent (LLM)\"]\n    B --> C{Need external info?}\n    C -- Yes --> D[\"Search using tool(s)\"]\n    D --> H{Enough to answer?}\n    H -- No --> B\n    H -- Yes --> I[Generate final answer]\n    C -- No --> I\n    I --> J[Return to user]\n\n    %% Dark-mode friendly styling\n    classDef startend fill:#2e7d32,stroke:#1b5e20,stroke-width:2px,color:#fff\n    classDef decision fill:#f9a825,stroke:#f57f17,stroke-width:2px,color:#000\n    classDef process fill:#1976d2,stroke:#0d47a1,stroke-width:1.5px,color:#fff\n\n    class A,J startend\n    class B,D,I", "metadata": {"source": "retrieval.mdx"}}
{"text": "  C -- Yes --> D[\"Search using tool(s)\"]\n    D --> H{Enough to answer?}\n    H -- No --> B\n    H -- Yes --> I[Generate final answer]\n    C -- No --> I\n    I --> J[Return to user]\n\n    %% Dark-mode friendly styling\n    classDef startend fill:#2e7d32,stroke:#1b5e20,stroke-width:2px,color:#fff\n    classDef decision fill:#f9a825,stroke:#f57f17,stroke-width:2px,color:#000\n    classDef process fill:#1976d2,stroke:#0d47a1,stroke-width:1.5px,color:#fff\n\n    class A,J startend\n    class B,D,I process\n    class C,H decision\n```\n\n:::python\n```python\nimport requests\nfrom langchain.tools import tool\nfrom langchain.chat_models import init_chat_model\nfrom langchain.agents import create_agent\n\n\n@tool\ndef fetch_url(url: str) -> str:\n    \"\"\"Fetch text content from a URL\"\"\"\n    response = requests.get(url, timeout=10.0)\n    response.raise_for_status()\n    return response.text\n\nsystem_prompt = \"\"\"\\\nUse fetch_url when you need to fetch information from a web-page; quote relevant snippets.\n\"\"\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[fetch_url], # A tool for retrieval [!code highlight]\n    system_prompt=system_prompt,\n)\n```\n:::\n\n:::js\n```typescript\nimport { tool, createAgent } from \"langchain\";\n\nconst fetchUrl = tool(\n    (url: string) => {\n        return `Fetched content from ${url}`;\n    },\n    { name: \"fetch_url\", description: \"Fetch text content from a URL\" }\n);\n\nconst agent = createAgent({\n    model: \"claude-sonnet-4-0\",\n    tools: [fetchUrl],\n    systemPrompt,\n});\n```\n:::\n\n<Expandable title=\"Extended example: Agentic RAG for LangGraph's llms.txt\">\n\nThis example implements an **Agentic RAG system** to assist users in querying LangGraph documentation. The agent begins by loading [llms.txt](https://llmstxt.org/), which lists available documentation URLs, and can then dynamically use a `fetch_documentation` tool to retrieve and process the relevant content based on the user\u2019s question.\n\n:::python\n```python\nimport requests\nfrom langchain.agents import create_agent\nfrom langchain.messages import HumanMessage\nfrom langchain.tools import tool\nfrom markdownify import markdownify\n\n\nALLOWED_DOMAINS = [\"https://langchain-ai.github.io/\"]\nLLMS_TXT = 'https://langchain-ai.github.io/langgraph/llms.txt'\n\n\n@tool\ndef fetch_documentation(url: str) -> str:", "metadata": {"source": "retrieval.mdx"}}
{"text": ".txt\">\n\nThis example implements an **Agentic RAG system** to assist users in querying LangGraph documentation. The agent begins by loading [llms.txt](https://llmstxt.org/), which lists available documentation URLs, and can then dynamically use a `fetch_documentation` tool to retrieve and process the relevant content based on the user\u2019s question.\n\n:::python\n```python\nimport requests\nfrom langchain.agents import create_agent\nfrom langchain.messages import HumanMessage\nfrom langchain.tools import tool\nfrom markdownify import markdownify\n\n\nALLOWED_DOMAINS = [\"https://langchain-ai.github.io/\"]\nLLMS_TXT = 'https://langchain-ai.github.io/langgraph/llms.txt'\n\n\n@tool\ndef fetch_documentation(url: str) -> str:  # [!code highlight]\n    \"\"\"Fetch and convert documentation from a URL\"\"\"\n    if not any(url.startswith(domain) for domain in ALLOWED_DOMAINS):\n        return (\n            \"Error: URL not allowed. \"\n            f\"Must start with one of: {', '.join(ALLOWED_DOMAINS)}\"\n        )\n    response = requests.get(url, timeout=10.0)\n    response.raise_for_status()\n    return markdownify(response.text)\n\n\n# We will fetch the content of llms.txt, so this can\n# be done ahead of time without requiring an LLM request.\nllms_txt_content = requests.get(LLMS_TXT).text\n\n# System prompt for the agent\nsystem_prompt = f\"\"\"\nYou are an expert Python developer and technical assistant.\nYour primary role is to help users with questions about LangGraph and related tools.\n\nInstructions:\n\n1. If a user asks a question you're unsure about \u2014 or one that likely involves API usage,\n   behavior, or configuration \u2014 you MUST use the `fetch_documentation` tool to consult the relevant docs.\n2. When citing documentation, summarize clearly and include relevant context from the content.\n3. Do not use any URLs outside of the allowed domain.\n4. If a documentation fetch fails, tell the user and proceed with your best expert understanding.\n\nYou can access official documentation from the following approved sources:\n\n{llms_txt_content}\n\nYou MUST consult the documentation to get up to date documentation\nbefore answering a user's question about LangGraph.\n\nYour answers should be clear, concise, and technically accurate.\n\"\"\"\n\ntools = [fetch_documentation]\n\nmodel = init_chat_model(\"claude-sonnet-4-0\", max_tokens=32_000)\n\nagent = create_agent(\n    model=model,\n    tools=tools,  # [!code highlight]\n    system_prompt=system_prompt,  # [!code highlight]\n    name=\"Agentic RAG\",\n)\n\nresponse = agent.invoke({\n    'messages': [\n        HumanMessage(content=(\n            \"Write a short example of a langgraph agent using the \"\n           ", "metadata": {"source": "retrieval.mdx"}}
{"text": "before answering a user's question about LangGraph.\n\nYour answers should be clear, concise, and technically accurate.\n\"\"\"\n\ntools = [fetch_documentation]\n\nmodel = init_chat_model(\"claude-sonnet-4-0\", max_tokens=32_000)\n\nagent = create_agent(\n    model=model,\n    tools=tools,  # [!code highlight]\n    system_prompt=system_prompt,  # [!code highlight]\n    name=\"Agentic RAG\",\n)\n\nresponse = agent.invoke({\n    'messages': [\n        HumanMessage(content=(\n            \"Write a short example of a langgraph agent using the \"\n            \"prebuilt create react agent. the agent should be able \"\n            \"to look up stock pricing information.\"\n        ))\n    ]\n})\n\nprint(response['messages'][-1].content)\n```\n:::\n:::js\n```typescript\nimport { tool, createAgent, HumanMessage } from \"langchain\";\nimport * as z from \"zod\";\n\nconst ALLOWED_DOMAINS = [\"https://langchain-ai.github.io/\"];\nconst LLMS_TXT = \"https://langchain-ai.github.io/langgraph/llms.txt\";\n\nconst fetchDocumentation = tool(\n  async (input) => {  // [!code highlight]\n    if (!ALLOWED_DOMAINS.some((domain) => input.url.startsWith(domain))) {\n      return `Error: URL not allowed. Must start with one of: ${ALLOWED_DOMAINS.join(\", \")}`;\n    }\n    const response = await fetch(input.url);\n    if (!response.ok) {\n      throw new Error(`HTTP error! status: ${response.status}`);\n    }\n    return response.text();\n  },\n  {\n    name: \"fetch_documentation\",\n    description: \"Fetch and convert documentation from a URL\",\n    schema: z.object({\n      url: z.string().describe(\"The URL of the documentation to fetch\"),\n    }),\n  }\n);\n\nconst llmsTxtResponse = await fetch(LLMS_TXT);\nconst llmsTxtContent = await llmsTxtResponse.text();\n\nconst systemPrompt = `\nYou are an expert TypeScript developer and technical assistant.\nYour primary role is to help users with questions about LangGraph and related tools.\n\nInstructions:\n\n1. If a user asks a question you're unsure about \u2014 or one that likely involves API usage,\n   behavior, or configuration \u2014 you MUST use the \\`fetch_documentation\\` tool to consult the relevant docs.\n2. When citing documentation, summarize clearly and include relevant context from the content.\n3. Do not use any URLs outside of the allowed domain.\n4. If a documentation fetch fails, tell the user and proceed with your best expert understanding.\n\nYou can access official documentation from the following approved sources:\n\n${llmsTxtContent}\n\nYou MUST", "metadata": {"source": "retrieval.mdx"}}
{"text": "TxtResponse = await fetch(LLMS_TXT);\nconst llmsTxtContent = await llmsTxtResponse.text();\n\nconst systemPrompt = `\nYou are an expert TypeScript developer and technical assistant.\nYour primary role is to help users with questions about LangGraph and related tools.\n\nInstructions:\n\n1. If a user asks a question you're unsure about \u2014 or one that likely involves API usage,\n   behavior, or configuration \u2014 you MUST use the \\`fetch_documentation\\` tool to consult the relevant docs.\n2. When citing documentation, summarize clearly and include relevant context from the content.\n3. Do not use any URLs outside of the allowed domain.\n4. If a documentation fetch fails, tell the user and proceed with your best expert understanding.\n\nYou can access official documentation from the following approved sources:\n\n${llmsTxtContent}\n\nYou MUST consult the documentation to get up to date documentation\nbefore answering a user's question about LangGraph.\n\nYour answers should be clear, concise, and technically accurate.\n`;\n\nconst tools = [fetchDocumentation];\n\nconst agent = createAgent({\n  model: \"claude-sonnet-4-0\"\n  tools,  // [!code highlight]\n  systemPrompt,  // [!code highlight]\n  name: \"Agentic RAG\",\n});\n\nconst response = await agent.invoke({\n  messages: [\n    new HumanMessage(\n      \"Write a short example of a langgraph agent using the \" +\n      \"prebuilt create react agent. the agent should be able \" +\n      \"to look up stock pricing information.\"\n    ),\n  ],\n});\n\nconsole.log(response.messages.at(-1)?.content);\n```\n:::\n</Expandable>\n\n<Card\n    title=\"Tutorial: Retrieval-Augmented Generation (RAG)\"\n    icon=\"robot\"\n    href=\"/oss/langchain/rag\"\n    arrow cta=\"Learn more\"\n>\n    See how to build a Q&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation.\n    This tutorial walks through two approaches:\n    * A **RAG agent** that runs searches with a flexible tool\u2014great for general-purpose use.\n    * A **2-step RAG** chain that requires just one LLM call per query\u2014fast and efficient for simpler tasks.\n</Card>\n\n### Hybrid RAG\n\nHybrid RAG combines characteristics of both 2-Step and Agentic RAG. It introduces intermediate steps such as query preprocessing, retrieval validation, and post-generation checks. These systems offer more flexibility than fixed pipelines while maintaining some control over execution.\n\nTypical components include:\n\n* **Query enhancement**: Modify the input question to improve retrieval quality. This can involve rewriting unclear queries, generating multiple variations, or expanding queries with additional context.\n* **Retrieval validation**: Evaluate whether retrieved documents are relevant and sufficient. If not, the system may refine the query and retrieve again.\n* **Answer validation**: Check the generated answer for accuracy, completeness, and alignment with source content. If needed, the system can regenerate or revise the answer.\n\nThe architecture often supports multiple iterations between these steps:\n\n```mermaid\ngraph LR\n    A[User Question] --> B[Query Enhancement]\n    B --> C[Retrieve", "metadata": {"source": "retrieval.mdx"}}
{"text": " both 2-Step and Agentic RAG. It introduces intermediate steps such as query preprocessing, retrieval validation, and post-generation checks. These systems offer more flexibility than fixed pipelines while maintaining some control over execution.\n\nTypical components include:\n\n* **Query enhancement**: Modify the input question to improve retrieval quality. This can involve rewriting unclear queries, generating multiple variations, or expanding queries with additional context.\n* **Retrieval validation**: Evaluate whether retrieved documents are relevant and sufficient. If not, the system may refine the query and retrieve again.\n* **Answer validation**: Check the generated answer for accuracy, completeness, and alignment with source content. If needed, the system can regenerate or revise the answer.\n\nThe architecture often supports multiple iterations between these steps:\n\n```mermaid\ngraph LR\n    A[User Question] --> B[Query Enhancement]\n    B --> C[Retrieve Documents]\n    C --> D{Sufficient Info?}\n    D -- No --> E[Refine Query]\n    E --> C\n    D -- Yes --> F[Generate Answer]\n    F --> G{Answer Quality OK?}\n    G -- No --> H{Try Different Approach?}\n    H -- Yes --> E\n    H -- No --> I[Return Best Answer]\n    G -- Yes --> I\n    I --> J[Return to User]\n\n    classDef startend fill:#2e7d32,stroke:#1b5e20,stroke-width:2px,color:#fff\n    classDef decision fill:#f9a825,stroke:#f57f17,stroke-width:2px,color:#000\n    classDef process fill:#1976d2,stroke:#0d47a1,stroke-width:1.5px,color:#fff\n\n    class A,J startend\n    class B,C,E,F,I process\n    class D,G,H decision\n```\n\nThis architecture is suitable for:\n\n* Applications with ambiguous or underspecified queries\n* Systems that require validation or quality control steps\n* Workflows involving multiple sources or iterative refinement\n\n<Card\n    title=\"Tutorial: Agentic RAG with Self-Correction\"\n    icon=\"robot\"\n    href=\"/oss/langgraph/agentic-rag\"\n    arrow cta=\"Learn more\"\n>\n    An example of **Hybrid RAG** that combines agentic reasoning with retrieval and self-correction.\n</Card>\n", "metadata": {"source": "retrieval.mdx"}}
{"text": "---\ntitle: LangChain overview\nsidebarTitle: Overview\ndescription: LangChain is an open source framework with a pre-built agent architecture and integrations for any model or tool \u2014 so you can build agents that adapt as fast as the ecosystem evolves\n---\n\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and [more](/oss/integrations/providers/overview). LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\n\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use [LangGraph](/oss/langgraph/overview), our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\n\nLangChain [agents](/oss/langchain/agents) are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n\n## <Icon icon=\"wand-magic-sparkles\" /> Create an agent\n\n:::python\n```python\n# pip install -qU langchain \"langchain[anthropic]\"\nfrom langchain.agents import create_agent\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[get_weather],\n    system_prompt=\"You are a helpful assistant\",\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n)\n```\n:::\n\n:::js\n```ts\nimport * as z from \"zod\";\n// npm install @langchain/anthropic to call the model\nimport { createAgent, tool } from \"langchain\";\n\nconst getWeather = tool(\n  ({ city }) => `It's always sunny in ${city}!`,\n  {\n    name: \"get_weather\",\n    description: \"Get the weather for a given city\",\n    schema: z.object({\n      city: z.string(),\n    }),\n  },\n);\n\nconst agent = createAgent({\n  model: \"claude-sonnet-4-5-20250929\",\n  tools: [getWeather],\n});\n\nconsole.log(\n  await agent.invoke({\n    messages: [{ role: \"user\", content: \"What's the weather in Tokyo?\" }],\n  })\n);\n```\n:::\n\nSee the [Installation instructions](/oss/langchain/install) and [Quickstart guide](/oss/langchain/quickstart) to get started building your own agents and applications with LangChain.\n\n## <Icon icon=\"star\" size={20} /> Core benefits\n\n<Columns cols={2}>\n    <Card title=\"Standard model interface\" icon=\"arrows-rotate\" href=\"/oss/langchain/models\" arrow cta=\"Learn more", "metadata": {"source": "overview.mdx"}}
{"text": "),\n  },\n);\n\nconst agent = createAgent({\n  model: \"claude-sonnet-4-5-20250929\",\n  tools: [getWeather],\n});\n\nconsole.log(\n  await agent.invoke({\n    messages: [{ role: \"user\", content: \"What's the weather in Tokyo?\" }],\n  })\n);\n```\n:::\n\nSee the [Installation instructions](/oss/langchain/install) and [Quickstart guide](/oss/langchain/quickstart) to get started building your own agents and applications with LangChain.\n\n## <Icon icon=\"star\" size={20} /> Core benefits\n\n<Columns cols={2}>\n    <Card title=\"Standard model interface\" icon=\"arrows-rotate\" href=\"/oss/langchain/models\" arrow cta=\"Learn more\">\n        Different providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\n    </Card>\n\n    <Card title=\"Easy to use, highly flexible agent\" icon=\"wand-magic-sparkles\" href=\"/oss/langchain/agents\" arrow cta=\"Learn more\">\n        LangChain's agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\n    </Card>\n\n    <Card title=\"Built on top of LangGraph\" icon=\"circle-nodes\" href=\"/oss/langgraph/overview\" arrow cta=\"Learn more\">\n        LangChain's agents are built on top of LangGraph. This allows us to take advantage of LangGraph's durable execution, human-in-the-loop support, persistence, and more.\n    </Card>\n\n    <Card title=\"Debug with LangSmith\" icon=\"eye\" href=\"/langsmith/home\" arrow cta=\"Learn more\">\n        Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\n    </Card>\n</Columns>\n", "metadata": {"source": "overview.mdx"}}
{"text": "---\ntitle: LangSmith Deployment\nsidebarTitle: Deployment\n---\n\nimport deploy from '/snippets/oss/deploy.mdx';\n\nWhen you're ready to deploy your LangChain agent to production, LangSmith provides a managed hosting platform designed for agent workloads. Traditional hosting platforms are built for stateless, short-lived web applications, while LangGraph is **purpose-built for stateful, long-running agents** that require persistent state and background execution. LangSmith handles the infrastructure, scaling, and operational concerns so you can deploy directly from your repository.\n\n## Prerequisites\n\nBefore you begin, ensure you have the following:\n\n- A [GitHub account](https://github.com/)\n- A [LangSmith account](https://smith.langchain.com/) (free to sign up)\n\n## Deploy your agent\n\n### 1. Create a repository on GitHub\n\nYour application's code must reside in a GitHub repository to be deployed on LangSmith. Both public and private repositories are supported. For this quickstart, first make sure your app is LangGraph-compatible by following the [local server setup guide](/oss/langchain/studio#setup-local-agent-server). Then, push your code to the repository.\n\n<deploy />\n", "metadata": {"source": "deploy.mdx"}}
{"text": "---\ntitle: LangSmith Observability\nsidebarTitle: Observability\n---\n\nimport observability from '/snippets/oss/observability.mdx';\n\n:::python\n\nAs you build and run agents with LangChain, you need visibility into how they behave: which [tools](/oss/langchain/tools) they call, what prompts they generate, and how they make decisions. LangChain agents built with @[`create_agent`] automatically support tracing through [LangSmith](/langsmith/home), a platform for capturing, debugging, evaluating, and monitoring LLM application behavior.\n\n:::\n:::js\n\nAs you build and run agents with LangChain, you need visibility into how they behave: which [tools](/oss/langchain/tools) they call, what prompts they generate, and how they make decisions. LangChain agents built with @[`createAgent`] automatically support tracing through [LangSmith](/langsmith/home), a platform for capturing, debugging, evaluating, and monitoring LLM application behavior.\n\n:::\n\n[_Traces_](/langsmith/observability-concepts#traces) record every step of your agent's execution, from the initial user input to the final response, including all tool calls, model interactions, and decision points. This execution data helps you debug issues, evaluate performance across different inputs, and monitor usage patterns in production.\n\nThis guide shows you how to enable tracing for your LangChain agents and use LangSmith to analyze their execution.\n\n## Prerequisites\n\nBefore you begin, ensure you have the following:\n\n- **A LangSmith account**: Sign up (for free) or log in at [smith.langchain.com](https://smith.langchain.com).\n- **A LangSmith API key**: Follow the [Create an API key](/langsmith/create-account-api-key#create-an-api-key) guide.\n\n## Enable tracing\n\nAll LangChain agents automatically support LangSmith tracing. To enable it, set the following environment variables:\n\n```bash\nexport LANGSMITH_TRACING=true\nexport LANGSMITH_API_KEY=<your-api-key>\n```\n\n## Quickstart\n\nNo extra code is needed to log a trace to LangSmith. Just run your agent code as you normally would:\n\n:::python\n```python\nfrom langchain.agents import create_agent\n\n\ndef send_email(to: str, subject: str, body: str):\n    \"\"\"Send an email to a recipient.\"\"\"\n    # ... email sending logic\n    return f\"Email sent to {to}\"\n\ndef search_web(query: str):\n    \"\"\"Search the web for information.\"\"\"\n    # ... web search logic\n    return f\"Search results for: {query}\"\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[send_email, search_web],\n    system_prompt=\"You are a helpful assistant that can send emails and search the web.\"\n)\n\n# Run the agent - all steps will be traced automatically\nresponse = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Search for the latest AI news and email a summary to john@example.com\"}]\n})\n```\n:::\n\n:::js\n```ts\nimport { createAgent } from \"@langchain/agents\";\n\nfunction sendEmail(to: string, subject: string, body", "metadata": {"source": "observability.mdx"}}
{"text": ": str):\n    \"\"\"Search the web for information.\"\"\"\n    # ... web search logic\n    return f\"Search results for: {query}\"\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[send_email, search_web],\n    system_prompt=\"You are a helpful assistant that can send emails and search the web.\"\n)\n\n# Run the agent - all steps will be traced automatically\nresponse = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Search for the latest AI news and email a summary to john@example.com\"}]\n})\n```\n:::\n\n:::js\n```ts\nimport { createAgent } from \"@langchain/agents\";\n\nfunction sendEmail(to: string, subject: string, body: string): string {\n    // ... email sending logic\n    return `Email sent to ${to}`;\n}\n\nfunction searchWeb(query: string): string {\n    // ... web search logic\n    return `Search results for: ${query}`;\n}\n\nconst agent = createAgent({\n    model: \"gpt-4.1\",\n    tools: [sendEmail, searchWeb],\n    systemPrompt: \"You are a helpful assistant that can send emails and search the web.\"\n});\n\n// Run the agent - all steps will be traced automatically\nconst response = await agent.invoke({\n    messages: [{ role: \"user\", content: \"Search for the latest AI news and email a summary to john@example.com\" }]\n});\n```\n:::\n\nBy default, the trace will be logged to the project with the name `default`. To configure a custom project name, see [Log to a project](#log-to-a-project).\n\n<observability />\n", "metadata": {"source": "observability.mdx"}}
{"text": "---\ntitle: Short-term memory\n---\n\n## Overview\n\nMemory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction.\n\nShort term memory lets your application remember previous interactions within a single thread or conversation.\n\n<Note>\n    A thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.\n</Note>\n\nConversation history is the most common form of short-term memory. Long conversations pose a challenge to today's LLMs; a full history may not fit inside an LLM's context window, resulting in an context loss or errors.\n\nEven if your model supports the full context length, most LLMs still perform poorly over long contexts. They get \"distracted\" by stale or off-topic content, all while suffering from slower response times and higher costs.\n\nChat models accept context using [messages](/oss/langchain/messages), which include instructions (a system message) and inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited, many applications can benefit from using techniques to remove or \"forget\" stale information.\n\n## Usage\n\nTo add short-term memory (thread-level persistence) to an agent, you need to specify a `checkpointer` when creating an agent.\n\n<Info>\n    LangChain's agent manages short-term memory as a part of your agent's state.\n\n    By storing these in the graph's state, the agent can access the full context for a given conversation while maintaining separation between different threads.\n\n    State is persisted to a database (or memory) using a checkpointer so the thread can be resumed at any time.\n\n    Short-term memory updates when the agent is invoked or a step (like a tool call) is completed, and the state is read at the start of each step.\n</Info>\n\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom langgraph.checkpoint.memory import InMemorySaver  # [!code highlight]\n\n\nagent = create_agent(\n    \"gpt-5\",\n    tools=[get_user_info],\n    checkpointer=InMemorySaver(),  # [!code highlight]\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! My name is Bob.\"}]},\n    {\"configurable\": {\"thread_id\": \"1\"}},  # [!code highlight]\n)\n```\n:::\n:::js\n```ts {highlight={2,4, 9,14}}\nimport { createAgent } from \"langchain\";\nimport { MemorySaver } from \"@langchain/langgraph\";\n\nconst checkpointer = new MemorySaver();\n\nconst agent = createAgent({\n    model: \"claude-sonnet-4-5-20250929\",\n    tools: [],\n    checkpointer,\n});\n\nawait agent.invoke(\n    { messages: [{ role: \"user\", content: \"hi! i am Bob\" }] },\n    { configurable: { thread_id: \"1\" } }\n);\n```\n:::\n\n### In production\n\n", "metadata": {"source": "short-term-memory.mdx"}}
{"text": " {\"configurable\": {\"thread_id\": \"1\"}},  # [!code highlight]\n)\n```\n:::\n:::js\n```ts {highlight={2,4, 9,14}}\nimport { createAgent } from \"langchain\";\nimport { MemorySaver } from \"@langchain/langgraph\";\n\nconst checkpointer = new MemorySaver();\n\nconst agent = createAgent({\n    model: \"claude-sonnet-4-5-20250929\",\n    tools: [],\n    checkpointer,\n});\n\nawait agent.invoke(\n    { messages: [{ role: \"user\", content: \"hi! i am Bob\" }] },\n    { configurable: { thread_id: \"1\" } }\n);\n```\n:::\n\n### In production\n\nIn production, use a checkpointer backed by a database:\n\n\n:::python\n```shell\npip install langgraph-checkpoint-postgres\n```\n\n```python\nfrom langchain.agents import create_agent\n\nfrom langgraph.checkpoint.postgres import PostgresSaver  # [!code highlight]\n\n\nDB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\nwith PostgresSaver.from_conn_string(DB_URI) as checkpointer:\n    checkpointer.setup() # auto create tables in PostgresSql\n    agent = create_agent(\n        \"gpt-5\",\n        tools=[get_user_info],\n        checkpointer=checkpointer,  # [!code highlight]\n    )\n```\n:::\n:::js\n```ts\nimport { PostgresSaver } from \"@langchain/langgraph-checkpoint-postgres\";\n\nconst DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\";\nconst checkpointer = PostgresSaver.fromConnString(DB_URI);\n```\n:::\n\n<Note>\n    For more checkpointer options including SQLite, Postgres, and Azure Cosmos DB, see the [list of checkpointer libraries](/oss/langgraph/persistence#checkpointer-libraries) in the Persistence documentation.\n</Note>\n\n## Customizing agent memory\n\n:::python\nBy default, agents use @[`AgentState`] to manage short term memory, specifically the conversation history via a `messages` key.\n\nYou can extend @[`AgentState`] to add additional fields. Custom state schemas are passed to @[`create_agent`] using the @[`state_schema`] parameter.\n\n```python\nfrom langchain.agents import create_agent, AgentState\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n\nclass CustomAgentState(AgentState):  # [!code highlight]\n    user_id: str  # [!code highlight]\n    preferences: dict  # [!code highlight]\n\nagent = create_agent(\n    \"gpt-5\",\n    tools=[get_user_info],\n    state_schema=CustomAgentState,  # [!code highlight]\n    checkpointer=InMemorySaver(),\n)\n\n# Custom state can be passed in", "metadata": {"source": "short-term-memory.mdx"}}
{"text": "` key.\n\nYou can extend @[`AgentState`] to add additional fields. Custom state schemas are passed to @[`create_agent`] using the @[`state_schema`] parameter.\n\n```python\nfrom langchain.agents import create_agent, AgentState\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n\nclass CustomAgentState(AgentState):  # [!code highlight]\n    user_id: str  # [!code highlight]\n    preferences: dict  # [!code highlight]\n\nagent = create_agent(\n    \"gpt-5\",\n    tools=[get_user_info],\n    state_schema=CustomAgentState,  # [!code highlight]\n    checkpointer=InMemorySaver(),\n)\n\n# Custom state can be passed in invoke\nresult = agent.invoke(\n    {\n        \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}],\n        \"user_id\": \"user_123\",  # [!code highlight]\n        \"preferences\": {\"theme\": \"dark\"}  # [!code highlight]\n    },\n    {\"configurable\": {\"thread_id\": \"1\"}})\n```\n:::\n\n:::js\nYou can extend the agent state by creating custom middleware with a state schema. Custom state schemas can be passed using the `stateSchema` parameter in middleware. Use the `StateSchema` class for state definitions preferably (plain Zod objects are also supported).\n\n```typescript\nimport { createAgent, createMiddleware } from \"langchain\";\nimport { StateSchema, MemorySaver } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\nconst CustomState = new StateSchema({  // [!code highlight]\n    userId: z.string(),  // [!code highlight]\n    preferences: z.record(z.string(), z.any()),  // [!code highlight]\n});  // [!code highlight]\n\nconst stateExtensionMiddleware = createMiddleware({\n    name: \"StateExtension\",\n    stateSchema: CustomState,  // [!code highlight]\n});\n\nconst checkpointer = new MemorySaver();\nconst agent = createAgent({\n    model: \"gpt-5\",\n    tools: [],\n    middleware: [stateExtensionMiddleware],  // [!code highlight]\n    checkpointer,\n});\n\n// Custom state can be passed in invoke\nconst result = await agent.invoke({\n    messages: [{ role: \"user\", content: \"Hello\" }],\n    userId: \"user_123\",  // [!code highlight]\n    preferences: { theme: \"dark\" },  // [!code highlight]\n});\n```\n:::\n\n## Common patterns\n\nWith [short-term memory](#add-short-term-memory) enabled, long conversations can exceed the LLM's context window. Common solutions are:\n\n<CardGroup cols={2}>\n    <Card title=\"Trim messages\" icon=\"scissors\" href=\"#trim-messages\" arrow>\n        Remove first or last N messages (before calling LLM)\n", "metadata": {"source": "short-term-memory.mdx"}}
{"text": " // [!code highlight]\n    checkpointer,\n});\n\n// Custom state can be passed in invoke\nconst result = await agent.invoke({\n    messages: [{ role: \"user\", content: \"Hello\" }],\n    userId: \"user_123\",  // [!code highlight]\n    preferences: { theme: \"dark\" },  // [!code highlight]\n});\n```\n:::\n\n## Common patterns\n\nWith [short-term memory](#add-short-term-memory) enabled, long conversations can exceed the LLM's context window. Common solutions are:\n\n<CardGroup cols={2}>\n    <Card title=\"Trim messages\" icon=\"scissors\" href=\"#trim-messages\" arrow>\n        Remove first or last N messages (before calling LLM)\n    </Card>\n    <Card title=\"Delete messages\" icon=\"trash\" href=\"#delete-messages\" arrow>\n        Delete messages from LangGraph state permanently\n    </Card>\n    <Card title=\"Summarize messages\" icon=\"layer-group\" href=\"#summarize-messages\" arrow>\n        Summarize earlier messages in the history and replace them with a summary\n    </Card>\n    <Card title=\"Custom strategies\" icon=\"gears\">\n        Custom strategies (e.g., message filtering, etc.)\n    </Card>\n</CardGroup>\n\nThis allows the agent to keep track of the conversation without exceeding the LLM's context window.\n\n### Trim messages\n\nMost LLMs have a maximum supported context window (denominated in tokens).\n\n:::python\nOne way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you're using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the `strategy` (e.g., keep the last `max_tokens`) to use for handling the boundary.\n:::\n:::js\nOne way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you're using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the `strategy` (e.g., keep the last `maxTokens`) to use for handling the boundary.\n:::\n\n:::python\nTo trim message history in an agent, use the @[`@before_model`] middleware decorator:\n\n```python\nfrom langchain.messages import RemoveMessage\nfrom langgraph.graph.message import REMOVE_ALL_MESSAGES\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langchain.agents import create_agent, AgentState\nfrom langchain.agents.middleware import before_model\nfrom langgraph.runtime import Runtime\nfrom langchain_core.runnables import RunnableConfig\nfrom typing import Any\n\n\n@before_model\ndef trim_messages(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n    \"\"\"Keep only the last few messages to fit context window.\"\"\"\n    messages = state[\"messages\"]\n\n    if len(messages) <= 3:\n        return", "metadata": {"source": "short-term-memory.mdx"}}
{"text": " use the @[`@before_model`] middleware decorator:\n\n```python\nfrom langchain.messages import RemoveMessage\nfrom langgraph.graph.message import REMOVE_ALL_MESSAGES\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langchain.agents import create_agent, AgentState\nfrom langchain.agents.middleware import before_model\nfrom langgraph.runtime import Runtime\nfrom langchain_core.runnables import RunnableConfig\nfrom typing import Any\n\n\n@before_model\ndef trim_messages(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n    \"\"\"Keep only the last few messages to fit context window.\"\"\"\n    messages = state[\"messages\"]\n\n    if len(messages) <= 3:\n        return None  # No changes needed\n\n    first_msg = messages[0]\n    recent_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:]\n    new_messages = [first_msg] + recent_messages\n\n    return {\n        \"messages\": [\n            RemoveMessage(id=REMOVE_ALL_MESSAGES),\n            *new_messages\n        ]\n    }\n\nagent = create_agent(\n    your_model_here,\n    tools=your_tools_here,\n    middleware=[trim_messages],\n    checkpointer=InMemorySaver(),\n)\n\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\nagent.invoke({\"messages\": \"hi, my name is bob\"}, config)\nagent.invoke({\"messages\": \"write a short poem about cats\"}, config)\nagent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\nfinal_response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n\nfinal_response[\"messages\"][-1].pretty_print()\n\"\"\"\n================================== Ai Message ==================================\n\nYour name is Bob. You told me that earlier.\nIf you'd like me to call you a nickname or use a different name, just say the word.\n\"\"\"\n```\n:::\n\n:::js\nTo trim message history in an agent, use @[`createMiddleware`] with a `beforeModel` hook:\n\n```typescript\nimport { RemoveMessage } from \"@langchain/core/messages\";\nimport { createAgent, createMiddleware } from \"langchain\";\nimport { MemorySaver, REMOVE_ALL_MESSAGES } from \"@langchain/langgraph\";\n\nconst trimMessages = createMiddleware({\n  name: \"TrimMessages\",\n  beforeModel: (state) => {\n    const messages = state.messages;\n\n    if (messages.length <= 3) {\n      return; // No changes needed\n    }\n\n    const firstMsg = messages[0];\n    const recentMessages =\n      messages.length % 2 === 0 ? messages.slice(-3) : messages.slice(-4);\n    const newMessages =", "metadata": {"source": "short-term-memory.mdx"}}
{"text": "beforeModel` hook:\n\n```typescript\nimport { RemoveMessage } from \"@langchain/core/messages\";\nimport { createAgent, createMiddleware } from \"langchain\";\nimport { MemorySaver, REMOVE_ALL_MESSAGES } from \"@langchain/langgraph\";\n\nconst trimMessages = createMiddleware({\n  name: \"TrimMessages\",\n  beforeModel: (state) => {\n    const messages = state.messages;\n\n    if (messages.length <= 3) {\n      return; // No changes needed\n    }\n\n    const firstMsg = messages[0];\n    const recentMessages =\n      messages.length % 2 === 0 ? messages.slice(-3) : messages.slice(-4);\n    const newMessages = [firstMsg, ...recentMessages];\n\n    return {\n      messages: [\n        new RemoveMessage({ id: REMOVE_ALL_MESSAGES }),\n        ...newMessages,\n      ],\n    };\n  },\n});\n\nconst checkpointer = new MemorySaver();\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [],\n  middleware: [trimMessages],\n  checkpointer,\n});\n```\n:::\n\n### Delete messages\n\nYou can delete messages from the graph state to manage the message history.\n\nThis is useful when you want to remove specific messages or clear the entire message history.\n\n:::python\nTo delete messages from the graph state, you can use the `RemoveMessage`.\n\nFor `RemoveMessage` to work, you need to use a state key with @[`add_messages`] [reducer](/oss/langgraph/graph-api#reducers).\n\nThe default @[`AgentState`] provides this.\n\nTo remove specific messages:\n\n```python\nfrom langchain.messages import RemoveMessage  # [!code highlight]\n\ndef delete_messages(state):\n    messages = state[\"messages\"]\n    if len(messages) > 2:\n        # remove the earliest two messages\n        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}  # [!code highlight]\n```\n\nTo remove **all** messages:\n\n```python\nfrom langgraph.graph.message import REMOVE_ALL_MESSAGES  # [!code highlight]\n\ndef delete_messages(state):\n    return {\"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES)]}  # [!code highlight]\n```\n:::\n\n:::js\nTo delete messages from the graph state, you can use the `RemoveMessage`. For `RemoveMessage` to work, you need to use a state key with @[`messagesStateReducer`][messagesStateReducer] [reducer](/oss/langgraph/graph-api#reducers), like `MessagesValue`.\n\nTo remove specific messages:\n\n```typescript\nimport { RemoveMessage } from \"@langchain/core/messages\";\n\nconst deleteMessages = (state) => {\n    const messages = state.mess", "metadata": {"source": "short-term-memory.mdx"}}
{"text": ".message import REMOVE_ALL_MESSAGES  # [!code highlight]\n\ndef delete_messages(state):\n    return {\"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES)]}  # [!code highlight]\n```\n:::\n\n:::js\nTo delete messages from the graph state, you can use the `RemoveMessage`. For `RemoveMessage` to work, you need to use a state key with @[`messagesStateReducer`][messagesStateReducer] [reducer](/oss/langgraph/graph-api#reducers), like `MessagesValue`.\n\nTo remove specific messages:\n\n```typescript\nimport { RemoveMessage } from \"@langchain/core/messages\";\n\nconst deleteMessages = (state) => {\n    const messages = state.messages;\n    if (messages.length > 2) {\n        // remove the earliest two messages\n        return {\n        messages: messages\n            .slice(0, 2)\n            .map((m) => new RemoveMessage({ id: m.id })),\n        };\n    }\n};\n```\n:::\n\n<Warning>\n    When deleting messages, **make sure** that the resulting message history is valid. Check the limitations of the LLM provider you're using. For example:\n\n    * Some providers expect message history to start with a `user` message\n    * Most providers require `assistant` messages with tool calls to be followed by corresponding `tool` result messages.\n</Warning>\n\n:::python\n\n```python\nfrom langchain.messages import RemoveMessage\nfrom langchain.agents import create_agent, AgentState\nfrom langchain.agents.middleware import after_model\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.runtime import Runtime\nfrom langchain_core.runnables import RunnableConfig\n\n\n@after_model\ndef delete_old_messages(state: AgentState, runtime: Runtime) -> dict | None:\n    \"\"\"Remove old messages to keep conversation manageable.\"\"\"\n    messages = state[\"messages\"]\n    if len(messages) > 2:\n        # remove the earliest two messages\n        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}\n    return None\n\n\nagent = create_agent(\n    \"gpt-5-nano\",\n    tools=[],\n    system_prompt=\"Please be concise and to the point.\",\n    middleware=[delete_old_messages],\n    checkpointer=InMemorySaver(),\n)\n\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\nfor event in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n    config,\n    stream_mode=\"values\",\n):\n    print([(message.type, message.content) for message in event[\"messages\"]])\n\nfor event in agent.stream(\n    {\"mess", "metadata": {"source": "short-term-memory.mdx"}}
{"text": " messages[:2]]}\n    return None\n\n\nagent = create_agent(\n    \"gpt-5-nano\",\n    tools=[],\n    system_prompt=\"Please be concise and to the point.\",\n    middleware=[delete_old_messages],\n    checkpointer=InMemorySaver(),\n)\n\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\nfor event in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n    config,\n    stream_mode=\"values\",\n):\n    print([(message.type, message.content) for message in event[\"messages\"]])\n\nfor event in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n    config,\n    stream_mode=\"values\",\n):\n    print([(message.type, message.content) for message in event[\"messages\"]])\n```\n\n```\n[('human', \"hi! I'm bob\")]\n[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.')]\n[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.'), ('human', \"what's my name?\")]\n[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.'), ('human', \"what's my name?\"), ('ai', 'Your name is Bob. How can I help you today, Bob?')]\n[('human', \"what's my name?\"), ('ai', 'Your name is Bob. How can I help you today, Bob?')]\n```\n:::\n\n:::js\n```typescript\nimport { RemoveMessage } from \"@langchain/core/messages\";\nimport { createAgent, createMiddleware } from \"langchain\";\nimport { MemorySaver } from \"@langchain/langgraph\";\n\nconst deleteOldMessages = createMiddleware({\n  name: \"DeleteOldMessages\",\n  afterModel: (state) => {\n    const messages = state.messages;\n    if (messages.length > 2) {\n      // remove the earliest two messages\n      return {\n        messages: messages\n          .slice(0, 2)\n          .map((m) => new RemoveMessage({ id: m.id! })),\n      };\n    }\n    return;\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [],\n  systemPrompt: \"Please be concise and to the point.\",\n  middleware: [deleteOldMessages],\n  checkpointer: new MemorySaver(),\n});\n\nconst config = { configurable: {", "metadata": {"source": "short-term-memory.mdx"}}
{"text": " {\n    const messages = state.messages;\n    if (messages.length > 2) {\n      // remove the earliest two messages\n      return {\n        messages: messages\n          .slice(0, 2)\n          .map((m) => new RemoveMessage({ id: m.id! })),\n      };\n    }\n    return;\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [],\n  systemPrompt: \"Please be concise and to the point.\",\n  middleware: [deleteOldMessages],\n  checkpointer: new MemorySaver(),\n});\n\nconst config = { configurable: { thread_id: \"1\" } };\n\nconst streamA = await agent.stream(\n  { messages: [{ role: \"user\", content: \"hi! I'm bob\" }] },\n  { ...config, streamMode: \"values\" }\n);\nfor await (const event of streamA) {\n  const messageDetails = event.messages.map((message) => [\n    message.getType(),\n    message.content,\n  ]);\n  console.log(messageDetails);\n}\n\nconst streamB = await agent.stream(\n  {\n    messages: [{ role: \"user\", content: \"what's my name?\" }],\n  },\n  { ...config, streamMode: \"values\" }\n);\nfor await (const event of streamB) {\n  const messageDetails = event.messages.map((message) => [\n    message.getType(),\n    message.content,\n  ]);\n  console.log(messageDetails);\n}\n```\n\n```\n[[ \"human\", \"hi! I'm bob\" ]]\n[[ \"human\", \"hi! I'm bob\" ], [ \"ai\", \"Hello, Bob! How can I assist you today?\" ]]\n[[ \"human\", \"hi! I'm bob\" ], [ \"ai\", \"Hello, Bob! How can I assist you today?\" ]]\n[[ \"human\", \"hi! I'm bob\" ], [ \"ai\", \"Hello, Bob! How can I assist you today\" ], [\"human\", \"what's my name?\" ]]\n[[ \"human\", \"hi! I'm bob\" ], [ \"ai\", \"Hello, Bob! How can I assist you today?\" ], [\"human\", \"what's my name?\"], [ \"ai\", \"Your name is Bob, as you mentioned. How can I help you further?\" ]]\n[[ \"human\", \"what's my name?\" ], [ \"ai\", \"Your name is Bob, as you mentioned. How can I help you further?\" ]]\n```\n:::\n\n### Summarize messages\n\nThe problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue.\nBecause of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.\n\n![Summary](/oss/images/summary.png)\n\n\n:::python\nTo summarize message history in an agent, use the built-in [``SummarizationMiddleware``](/oss/langchain/middleware#summarization):\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.", "metadata": {"source": "short-term-memory.mdx"}}
{"text": "ai\", \"Your name is Bob, as you mentioned. How can I help you further?\" ]]\n[[ \"human\", \"what's my name?\" ], [ \"ai\", \"Your name is Bob, as you mentioned. How can I help you further?\" ]]\n```\n:::\n\n### Summarize messages\n\nThe problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue.\nBecause of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.\n\n![Summary](/oss/images/summary.png)\n\n\n:::python\nTo summarize message history in an agent, use the built-in [``SummarizationMiddleware``](/oss/langchain/middleware#summarization):\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import SummarizationMiddleware\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langchain_core.runnables import RunnableConfig\n\n\ncheckpointer = InMemorySaver()\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[],\n    middleware=[\n        SummarizationMiddleware(\n            model=\"gpt-4.1-mini\",\n            trigger=(\"tokens\", 4000),\n            keep=(\"messages\", 20)\n        )\n    ],\n    checkpointer=checkpointer,\n)\n\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\nagent.invoke({\"messages\": \"hi, my name is bob\"}, config)\nagent.invoke({\"messages\": \"write a short poem about cats\"}, config)\nagent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\nfinal_response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n\nfinal_response[\"messages\"][-1].pretty_print()\n\"\"\"\n================================== Ai Message ==================================\n\nYour name is Bob!\n\"\"\"\n```\n\nSee [`SummarizationMiddleware`](/oss/langchain/middleware#summarization) for more configuration options.\n:::\n:::js\nTo summarize message history in an agent, use the built-in [`summarizationMiddleware`](/oss/langchain/middleware#summarization):\n\n```typescript\nimport { createAgent, summarizationMiddleware } from \"langchain\";\nimport { MemorySaver } from \"@langchain/langgraph\";\n\nconst checkpointer = new MemorySaver();\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [],\n  middleware: [\n    summarizationMiddleware({\n      model: \"gpt-4.1-mini\",\n      trigger: { tokens: 4000 },\n      keep: { messages: 20 },\n    }),\n  ],\n  checkpointer,\n});\n\nconst config = { configurable: { thread_id: \"1\" } };\nawait agent.invoke({ messages: \"hi, my name is bob\" }, config);\nawait agent.invoke({ messages: \"write a", "metadata": {"source": "short-term-memory.mdx"}}
{"text": "\n```typescript\nimport { createAgent, summarizationMiddleware } from \"langchain\";\nimport { MemorySaver } from \"@langchain/langgraph\";\n\nconst checkpointer = new MemorySaver();\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [],\n  middleware: [\n    summarizationMiddleware({\n      model: \"gpt-4.1-mini\",\n      trigger: { tokens: 4000 },\n      keep: { messages: 20 },\n    }),\n  ],\n  checkpointer,\n});\n\nconst config = { configurable: { thread_id: \"1\" } };\nawait agent.invoke({ messages: \"hi, my name is bob\" }, config);\nawait agent.invoke({ messages: \"write a short poem about cats\" }, config);\nawait agent.invoke({ messages: \"now do the same but for dogs\" }, config);\nconst finalResponse = await agent.invoke({ messages: \"what's my name?\" }, config);\n\nconsole.log(finalResponse.messages.at(-1)?.content);\n// Your name is Bob!\n```\n\nSee [`summarizationMiddleware`](/oss/langchain/middleware#summarization) for more configuration options.\n:::\n\n## Access memory\n\nYou can access and modify the short-term memory (state) of an agent in several ways:\n\n### Tools\n\n#### Read short-term memory in a tool\n\nAccess short term memory (state) in a tool using the `runtime` parameter (typed as `ToolRuntime`).\n\nThe `runtime` parameter is hidden from the tool signature (so the model doesn't see it), but the tool can access the state through it.\n\n:::python\n```python\nfrom langchain.agents import create_agent, AgentState\nfrom langchain.tools import tool, ToolRuntime\n\n\nclass CustomState(AgentState):\n    user_id: str\n\n@tool\ndef get_user_info(\n    runtime: ToolRuntime\n) -> str:\n    \"\"\"Look up user info.\"\"\"\n    user_id = runtime.state[\"user_id\"]\n    return \"User is John Smith\" if user_id == \"user_123\" else \"Unknown user\"\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[get_user_info],\n    state_schema=CustomState,\n)\n\nresult = agent.invoke({\n    \"messages\": \"look up user information\",\n    \"user_id\": \"user_123\"\n})\nprint(result[\"messages\"][-1].content)\n# > User is John Smith.\n```\n:::\n:::js\n```typescript\nimport { createAgent, tool, type ToolRuntime } from \"langchain\";\nimport { StateSchema } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\nconst CustomState = new StateSchema({\n  userId: z.string(),\n});\n\nconst getUserInfo = tool(\n  async (_, config: ToolRuntime<typeof CustomState.State>) => {\n    const userId = config.state.userId;\n    return userId === \"user_123\" ? \"John Doe\" : \"Unknown User\";\n  },\n", "metadata": {"source": "short-term-memory.mdx"}}
{"text": "   \"messages\": \"look up user information\",\n    \"user_id\": \"user_123\"\n})\nprint(result[\"messages\"][-1].content)\n# > User is John Smith.\n```\n:::\n:::js\n```typescript\nimport { createAgent, tool, type ToolRuntime } from \"langchain\";\nimport { StateSchema } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\nconst CustomState = new StateSchema({\n  userId: z.string(),\n});\n\nconst getUserInfo = tool(\n  async (_, config: ToolRuntime<typeof CustomState.State>) => {\n    const userId = config.state.userId;\n    return userId === \"user_123\" ? \"John Doe\" : \"Unknown User\";\n  },\n  {\n    name: \"get_user_info\",\n    description: \"Get user info\",\n    schema: z.object({}),\n  }\n);\n\nconst agent = createAgent({\n  model: \"gpt-5-nano\",\n  tools: [getUserInfo],\n  stateSchema: CustomState,\n});\n\nconst result = await agent.invoke(\n  {\n    messages: [{ role: \"user\", content: \"what's my name?\" }],\n    userId: \"user_123\",\n  },\n  {\n    context: {},\n  }\n);\n\nconsole.log(result.messages.at(-1)?.content);\n// Outputs: \"Your name is John Doe.\"\n```\n:::\n\n#### Write short-term memory from tools\n\nTo modify the agent's short-term memory (state) during execution, you can return state updates directly from the tools.\n\nThis is useful for persisting intermediate results or making information accessible to subsequent tools or prompts.\n\n:::python\n```python\nfrom langchain.tools import tool, ToolRuntime\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain.messages import ToolMessage\nfrom langchain.agents import create_agent, AgentState\nfrom langgraph.types import Command\nfrom pydantic import BaseModel\n\n\nclass CustomState(AgentState):  # [!code highlight]\n    user_name: str\n\nclass CustomContext(BaseModel):\n    user_id: str\n\n@tool\ndef update_user_info(\n    runtime: ToolRuntime[CustomContext, CustomState],\n) -> Command:\n    \"\"\"Look up and update user info.\"\"\"\n    user_id = runtime.context.user_id\n    name = \"John Smith\" if user_id == \"user_123\" else \"Unknown user\"\n    return Command(update={  # [!code highlight]\n        \"user_name\": name,\n        # update the message history\n        \"messages\": [\n            ToolMessage(\n                \"Successfully looked up user information\",\n                tool_call_id=runtime.tool_call_id\n            )\n        ]\n    })\n\n", "metadata": {"source": "short-term-memory.mdx"}}
{"text": " and update user info.\"\"\"\n    user_id = runtime.context.user_id\n    name = \"John Smith\" if user_id == \"user_123\" else \"Unknown user\"\n    return Command(update={  # [!code highlight]\n        \"user_name\": name,\n        # update the message history\n        \"messages\": [\n            ToolMessage(\n                \"Successfully looked up user information\",\n                tool_call_id=runtime.tool_call_id\n            )\n        ]\n    })\n\n@tool\ndef greet(\n    runtime: ToolRuntime[CustomContext, CustomState]\n) -> str | Command:\n    \"\"\"Use this to greet the user once you found their info.\"\"\"\n    user_name = runtime.state.get(\"user_name\", None)\n    if user_name is None:\n       return Command(update={\n            \"messages\": [\n                ToolMessage(\n                    \"Please call the 'update_user_info' tool it will get and update the user's name.\",\n                    tool_call_id=runtime.tool_call_id\n                )\n            ]\n        })\n    return f\"Hello {user_name}!\"\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[update_user_info, greet],\n    state_schema=CustomState, # [!code highlight]\n    context_schema=CustomContext,\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"greet the user\"}]},\n    context=CustomContext(user_id=\"user_123\"),\n)\n```\n:::\n\n:::js\n```typescript\nimport { tool, createAgent, ToolMessage, type ToolRuntime } from \"langchain\";\nimport { Command, StateSchema } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\nconst CustomState = new StateSchema({\n  userId: z.string().optional(),\n});\n\nconst updateUserInfo = tool(\n  async (_, config: ToolRuntime<typeof CustomState.State>) => {\n    const userId = config.state.userId;\n    const name = userId === \"user_123\" ? \"John Smith\" : \"Unknown user\";\n    return new Command({\n      update: {\n        userName: name,\n        // update the message history\n        messages: [\n          new ToolMessage({\n   ", "metadata": {"source": "short-term-memory.mdx"}}
{"text": " type ToolRuntime } from \"langchain\";\nimport { Command, StateSchema } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\nconst CustomState = new StateSchema({\n  userId: z.string().optional(),\n});\n\nconst updateUserInfo = tool(\n  async (_, config: ToolRuntime<typeof CustomState.State>) => {\n    const userId = config.state.userId;\n    const name = userId === \"user_123\" ? \"John Smith\" : \"Unknown user\";\n    return new Command({\n      update: {\n        userName: name,\n        // update the message history\n        messages: [\n          new ToolMessage({\n            content: \"Successfully looked up user information\",\n            tool_call_id: config.toolCall?.id ?? \"\",\n          }),\n        ],\n      },\n    });\n  },\n  {\n    name: \"update_user_info\",\n    description: \"Look up and update user info.\",\n    schema: z.object({}),\n  }\n);\n\nconst greet = tool(\n  async (_, config) => {\n    const userName = config.context?.userName;\n    return `Hello ${userName}!`;\n  },\n  {\n    name: \"greet\",\n    description: \"Use this to greet the user once you found their info.\",\n    schema: z.object({}),\n  }\n);\n\nconst agent = createAgent({\n  model: \"openai:gpt-5-mini\",\n  tools: [updateUserInfo, greet],\n  stateSchema: CustomState,\n});\n\nconst result = await agent.invoke({\n  messages: [{ role: \"user\", content: \"greet the user\" }],\n  userId: \"user_123\",\n});\n\nconsole.log(result.messages.at(-1)?.content);\n// Output: \"Hello! I\u2019m here to help \u2014 what would you like to do today?\"\n```\n:::\n\n### Prompt\n\nAccess short term memory (state) in middleware to create dynamic prompts based on conversation history or custom state fields.\n\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom typing import TypedDict\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\n\n\nclass CustomContext(TypedDict):\n    user_name: str\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a city.\"\"\"\n    return f\"The weather in {city} is always sunny!\"\n\n\n@dynamic_prompt\ndef dynamic_system_prompt(request: ModelRequest) -> str:\n    user_name = request.runtime.context[\"user_name\"]\n    system_prompt = f\"You are a helpful assistant. Address the user as {user_name}.\"\n    return system_prompt\n\n\nagent = create_agent(\n    model=\"gpt-5-nano\",", "metadata": {"source": "short-term-memory.mdx"}}
{"text": "\nfrom langchain.agents import create_agent\nfrom typing import TypedDict\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\n\n\nclass CustomContext(TypedDict):\n    user_name: str\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a city.\"\"\"\n    return f\"The weather in {city} is always sunny!\"\n\n\n@dynamic_prompt\ndef dynamic_system_prompt(request: ModelRequest) -> str:\n    user_name = request.runtime.context[\"user_name\"]\n    system_prompt = f\"You are a helpful assistant. Address the user as {user_name}.\"\n    return system_prompt\n\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[get_weather],\n    middleware=[dynamic_system_prompt],\n    context_schema=CustomContext,\n)\n\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n    context=CustomContext(user_name=\"John Smith\"),\n)\nfor msg in result[\"messages\"]:\n    msg.pretty_print()\n\n```\n\n```shell title=\"Output\"\n================================ Human Message =================================\n\nWhat is the weather in SF?\n================================== Ai Message ==================================\nTool Calls:\n  get_weather (call_WFQlOGn4b2yoJrv7cih342FG)\n Call ID: call_WFQlOGn4b2yoJrv7cih342FG\n  Args:\n    city: San Francisco\n================================= Tool Message =================================\nName: get_weather\n\nThe weather in San Francisco is always sunny!\n================================== Ai Message ==================================\n\nHi John Smith, the weather in San Francisco is always sunny!\n```\n:::\n:::js\n```typescript\nimport * as z from \"zod\";\nimport { createAgent, tool, dynamicSystemPromptMiddleware } from \"langchain\";\n\nconst contextSchema = z.object({\n  userName: z.string(),\n});\ntype ContextSchema = z.infer<typeof contextSchema>;\n\nconst getWeather = tool(\n  async ({ city }) => {\n    return `The weather in ${city} is always sunny!`;\n  },\n  {\n    name: \"get_weather\",\n    description: \"Get user info\",\n    schema: z.object({\n      city: z.string(),\n    }),\n  }\n);\n\nconst agent = createAgent({\n  model: \"gpt-5-nano\",\n  tools: [getWeather],\n  contextSchema,\n  middleware: [\n    dynamicSystemPromptMiddleware<ContextSchema>((_, config) => {\n      return `You are a helpful assistant. Address the user as ${config.context?.userName}.`;\n    }),\n  ],\n});\n\nconst result = await agent.invoke(\n  {\n    messages: [{ role: \"user\", content: \"What is the weather in SF?\" }],\n  },\n  {\n    context: {\n      userName", "metadata": {"source": "short-term-memory.mdx"}}
{"text": " \"Get user info\",\n    schema: z.object({\n      city: z.string(),\n    }),\n  }\n);\n\nconst agent = createAgent({\n  model: \"gpt-5-nano\",\n  tools: [getWeather],\n  contextSchema,\n  middleware: [\n    dynamicSystemPromptMiddleware<ContextSchema>((_, config) => {\n      return `You are a helpful assistant. Address the user as ${config.context?.userName}.`;\n    }),\n  ],\n});\n\nconst result = await agent.invoke(\n  {\n    messages: [{ role: \"user\", content: \"What is the weather in SF?\" }],\n  },\n  {\n    context: {\n      userName: \"John Smith\",\n    },\n  }\n);\n\nfor (const message of result.messages) {\n  console.log(message);\n}\n/**\n * HumanMessage {\n *   \"content\": \"What is the weather in SF?\",\n *   // ...\n * }\n * AIMessage {\n *   // ...\n *   \"tool_calls\": [\n *     {\n *       \"name\": \"get_weather\",\n *       \"args\": {\n *         \"city\": \"San Francisco\"\n *       },\n *       \"type\": \"tool_call\",\n *       \"id\": \"call_tCidbv0apTpQpEWb3O2zQ4Yx\"\n *     }\n *   ],\n *   // ...\n * }\n * ToolMessage {\n *   \"content\": \"The weather in San Francisco is always sunny!\",\n *   \"tool_call_id\": \"call_tCidbv0apTpQpEWb3O2zQ4Yx\"\n *   // ...\n * }\n * AIMessage {\n *   \"content\": \"John Smith, here's the latest: The weather in San Francisco is always sunny!\\n\\nIf you'd like more details (temperature, wind, humidity) or a forecast for the next few days, I can pull that up. What would you like?\",\n *   // ...\n * }\n */\n```\n:::\n\n### Before model\n\n:::python\nAccess short term memory (state) in @[`@before_model`] middleware to process messages before model calls.\n:::\n\n\n```mermaid\n%%{\n    init: {\n        \"fontFamily\": \"monospace\",\n        \"flowchart\": {\n        \"curve\": \"basis\"\n        },\n        \"themeVariables\": {\"edgeLabelBackground\": \"transparent\"}\n    }\n}%%\ngraph TD\n    S([\"\\_\\_start\\_\\_\"])\n    PRE(before_model)\n    MODEL(model)\n    TOOLS(tools)\n    END([\"\\_\\_end\\_\\_\"])\n    S --> PRE\n    PRE -->", "metadata": {"source": "short-term-memory.mdx"}}
{"text": "`@before_model`] middleware to process messages before model calls.\n:::\n\n\n```mermaid\n%%{\n    init: {\n        \"fontFamily\": \"monospace\",\n        \"flowchart\": {\n        \"curve\": \"basis\"\n        },\n        \"themeVariables\": {\"edgeLabelBackground\": \"transparent\"}\n    }\n}%%\ngraph TD\n    S([\"\\_\\_start\\_\\_\"])\n    PRE(before_model)\n    MODEL(model)\n    TOOLS(tools)\n    END([\"\\_\\_end\\_\\_\"])\n    S --> PRE\n    PRE --> MODEL\n    MODEL -.-> TOOLS\n    MODEL -.-> END\n    TOOLS --> PRE\n    classDef blueHighlight fill:#0a1c25,stroke:#0a455f,color:#bae6fd;\n    class S blueHighlight;\n    class END blueHighlight;\n```\n\n:::python\n```python\nfrom langchain.messages import RemoveMessage\nfrom langgraph.graph.message import REMOVE_ALL_MESSAGES\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langchain.agents import create_agent, AgentState\nfrom langchain.agents.middleware import before_model\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.runtime import Runtime\nfrom typing import Any\n\n\n@before_model\ndef trim_messages(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n    \"\"\"Keep only the last few messages to fit context window.\"\"\"\n    messages = state[\"messages\"]\n\n    if len(messages) <= 3:\n        return None  # No changes needed\n\n    first_msg = messages[0]\n    recent_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:]\n    new_messages = [first_msg] + recent_messages\n\n    return {\n        \"messages\": [\n            RemoveMessage(id=REMOVE_ALL_MESSAGES),\n            *new_messages\n        ]\n    }\n\n\nagent = create_agent(\n    \"gpt-5-nano\",\n    tools=[],\n    middleware=[trim_messages],\n    checkpointer=InMemorySaver()\n)\n\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\nagent.invoke({\"messages\": \"hi, my name is bob\"}, config)\nagent.invoke({\"messages\": \"write a short poem about cats\"}, config)\nagent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\nfinal_response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n\nfinal_response[\"messages\"][-1].pretty_print()\n\"\"\"\n==================================", "metadata": {"source": "short-term-memory.mdx"}}
{"text": "  *new_messages\n        ]\n    }\n\n\nagent = create_agent(\n    \"gpt-5-nano\",\n    tools=[],\n    middleware=[trim_messages],\n    checkpointer=InMemorySaver()\n)\n\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\nagent.invoke({\"messages\": \"hi, my name is bob\"}, config)\nagent.invoke({\"messages\": \"write a short poem about cats\"}, config)\nagent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\nfinal_response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n\nfinal_response[\"messages\"][-1].pretty_print()\n\"\"\"\n================================== Ai Message ==================================\n\nYour name is Bob. You told me that earlier.\nIf you'd like me to call you a nickname or use a different name, just say the word.\n\"\"\"\n```\n:::\n:::js\n```typescript\nimport { RemoveMessage } from \"@langchain/core/messages\";\nimport { createAgent, createMiddleware, trimMessages } from \"langchain\";\nimport { MemorySaver } from \"@langchain/langgraph\";\nimport { REMOVE_ALL_MESSAGES } from \"@langchain/langgraph\";\n\nconst trimMessageHistory = createMiddleware({\n  name: \"TrimMessages\",\n  beforeModel: async (state) => {\n    const trimmed = await trimMessages(state.messages, {\n      maxTokens: 384,\n      strategy: \"last\",\n      startOn: \"human\",\n      endOn: [\"human\", \"tool\"],\n      tokenCounter: (msgs) => msgs.length,\n    });\n    return {\n      messages: [new RemoveMessage({ id: REMOVE_ALL_MESSAGES }), ...trimmed],\n    };\n  },\n});\n\nconst checkpointer = new MemorySaver();\nconst agent = createAgent({\n  model: \"gpt-5-nano\",\n  tools: [],\n  middleware: [trimMessageHistory],\n  checkpointer,\n});\n```\n:::\n\n### After model\n\n:::python\nAccess short term memory (state) in @[`@after_model`] middleware to process messages after model calls.\n:::\n\n```mermaid\n%%{\n    init: {\n        \"fontFamily\": \"monospace\",\n        \"flowchart\": {\n        \"curve\": \"basis\"\n        },\n        \"themeVariables\": {\"edgeLabelBackground\": \"transparent\"}\n    }\n}%%\ngraph TD\n    S([\"\\_\\_start\\_\\_\"])\n    MODEL(model)\n    POST(after_model)\n    TOOLS(tools)\n    END([\"\\_\\_end\\_\\_\"])\n    S --> MODEL\n    MODEL --> POST\n    POST -.-> END\n    POST -", "metadata": {"source": "short-term-memory.mdx"}}
{"text": ":\n\n```mermaid\n%%{\n    init: {\n        \"fontFamily\": \"monospace\",\n        \"flowchart\": {\n        \"curve\": \"basis\"\n        },\n        \"themeVariables\": {\"edgeLabelBackground\": \"transparent\"}\n    }\n}%%\ngraph TD\n    S([\"\\_\\_start\\_\\_\"])\n    MODEL(model)\n    POST(after_model)\n    TOOLS(tools)\n    END([\"\\_\\_end\\_\\_\"])\n    S --> MODEL\n    MODEL --> POST\n    POST -.-> END\n    POST -.-> TOOLS\n    TOOLS --> MODEL\n    classDef blueHighlight fill:#0a1c25,stroke:#0a455f,color:#bae6fd;\n    class S blueHighlight;\n    class END blueHighlight;\n    class POST greenHighlight;\n```\n\n:::python\n```python\nfrom langchain.messages import RemoveMessage\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langchain.agents import create_agent, AgentState\nfrom langchain.agents.middleware import after_model\nfrom langgraph.runtime import Runtime\n\n\n@after_model\ndef validate_response(state: AgentState, runtime: Runtime) -> dict | None:\n    \"\"\"Remove messages containing sensitive words.\"\"\"\n    STOP_WORDS = [\"password\", \"secret\"]\n    last_message = state[\"messages\"][-1]\n    if any(word in last_message.content for word in STOP_WORDS):\n        return {\"messages\": [RemoveMessage(id=last_message.id)]}\n    return None\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[],\n    middleware=[validate_response],\n    checkpointer=InMemorySaver(),\n)\n```\n:::\n:::js\n```typescript\nimport { RemoveMessage } from \"@langchain/core/messages\";\nimport { createAgent, createMiddleware } from \"langchain\";\nimport { REMOVE_ALL_MESSAGES } from \"@langchain/langgraph\";\n\nconst validateResponse = createMiddleware({\n  name: \"ValidateResponse\",\n  afterModel: (state) => {\n    const lastMessage = state.messages.at(-1)?.content;\n    if (\n      typeof lastMessage === \"string\" &&\n      lastMessage.toLowerCase().includes(\"confidential\")\n    ) {\n      return {\n        messages: [\n          new RemoveMessage({ id: REMOVE_ALL_MESSAGES }),\n        ],\n      };\n    }\n    return;\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-5-nano\",\n  tools: [],\n  middleware: [", "metadata": {"source": "short-term-memory.mdx"}}
{"text": " createMiddleware({\n  name: \"ValidateResponse\",\n  afterModel: (state) => {\n    const lastMessage = state.messages.at(-1)?.content;\n    if (\n      typeof lastMessage === \"string\" &&\n      lastMessage.toLowerCase().includes(\"confidential\")\n    ) {\n      return {\n        messages: [\n          new RemoveMessage({ id: REMOVE_ALL_MESSAGES }),\n        ],\n      };\n    }\n    return;\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-5-nano\",\n  tools: [],\n  middleware: [validateResponse],\n});\n```\n:::\n\n\n", "metadata": {"source": "short-term-memory.mdx"}}
{"text": "---\ntitle: Get help\n---\n\nConnect with the LangChain community, access learning resources, and get the support you need to build with confidence.\n\n## Learning resources\n\nStart your journey or deepen your knowledge with our comprehensive learning materials.\n\n- **[Chat LangChain](https://chat.langchain.com/)**: Ask the docs anything about LangChain, powered by real-time docs\n- **[API Reference](https://reference.langchain.com/python/)**: Complete documentation for all LangChain packages\n\n## Community support\n\nGet help from fellow developers and the LangChain team through our active community channels.\n\n- **[Community Forum](https://forum.langchain.com/)**: Ask questions, share solutions, and discuss best practices\n- **[Community Slack](https://www.langchain.com/join-community)**: Connect with other builders and get quick help\n\n## Professional support\n\nFor enterprise needs and critical applications, access dedicated support channels.\n\n- **[Support portal](https://support.langchain.com/)**: Submit tickets and track support requests\n- **[LangSmith status](https://status.smith.langchain.com/)**: Real-time status of LangSmith services and APIs\n\n## Contribute\n\nHelp us improve LangChain for everyone. Whether you're fixing bugs, adding features, or improving documentation, we welcome your contributions.\n\n- **[Contributing Guide](/oss/contributing/overview)**: Everything you need to know about contributing to LangChain\n\n## Stay connected\n\nFollow us for the latest updates, announcements, and community highlights.\n\n- **[X (Twitter)](https://x.com/langchain)**: Daily updates and community spotlights\n- **[LinkedIn](https://www.linkedin.com/company/langchain/)**: Professional network and company updates\n", "metadata": {"source": "get-help.mdx"}}
{"text": "---\ntitle: Build a semantic search engine with LangChain\nsidebarTitle: Semantic search\n---\n\nimport EmbeddingsTabsPy from '/snippets/embeddings-tabs-py.mdx';\nimport EmbeddingsTabsJS from '/snippets/embeddings-tabs-js.mdx';\nimport VectorstoreTabsPy from '/snippets/vectorstore-tabs-py.mdx';\nimport VectorstoreTabsJS from '/snippets/vectorstore-tabs-js.mdx';\n\n## Overview\n\nThis tutorial will familiarize you with LangChain's [document loader](/oss/langchain/retrieval#document-loaders), [embedding](/oss/langchain/retrieval#embedding-models), and [vector store](/oss/langchain/retrieval#vector-store) abstractions. These abstractions are designed to support retrieval of data--  from (vector) databases and other sources -- for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or [RAG](/oss/langchain/retrieval).\n\nHere we will build a search engine over a PDF document. This will allow us to retrieve passages in the PDF that are similar to an input query. The guide also includes a minimal RAG implementation on top of the search engine.\n\n### Concepts\n\nThis guide focuses on retrieval of text data. We will cover the following concepts:\n\n- [Documents and document loaders](/oss/integrations/document_loaders);\n- [Text splitters](/oss/integrations/splitters);\n- [Embeddings](/oss/integrations/text_embedding);\n- [Vector stores](/oss/integrations/vectorstores) and [retrievers](/oss/integrations/retrievers).\n\n## Setup\n\n### Installation\n\n:::python\n\nThis tutorial requires the `langchain-community` and `pypdf` packages:\n\n<CodeGroup>\n```bash pip\npip install langchain-community pypdf\n```\n```bash conda\nconda install langchain-community pypdf -c conda-forge\n```\n</CodeGroup>\n\n:::\n\n:::js\n\nThis guide requires `@langchain/community` and `pdf-parse`:\n\n<CodeGroup>\n```bash npm\nnpm i @langchain/community pdf-parse\n```\n```bash yarn\nyarn add @langchain/community pdf-parse\n```\n```bash pnpm\npnpm add @langchain/community pdf-parse\n```\n</CodeGroup>\n\n:::\n\nFor more details, see our [Installation guide](/oss/langchain/install).\n\n### LangSmith\n\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\nThe best way to do this is with [LangSmith](https://smith.langchain.com).\n\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\n\n```shell\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n```\n\n:::python\n\nOr, if in a notebook, you can set them with:\n", "metadata": {"source": "knowledge-base.mdx"}}
{"text": "pm add @langchain/community pdf-parse\n```\n</CodeGroup>\n\n:::\n\nFor more details, see our [Installation guide](/oss/langchain/install).\n\n### LangSmith\n\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\nThe best way to do this is with [LangSmith](https://smith.langchain.com).\n\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\n\n```shell\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n```\n\n:::python\n\nOr, if in a notebook, you can set them with:\n\n```python\nimport getpass\nimport os\n\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```\n\n:::\n\n## 1. Documents and document loaders\n\nLangChain implements a @[Document] abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:\n\n:::python\n- `page_content`: a string representing the content;\n- `metadata`: a dict containing arbitrary metadata;\n- `id`: (optional) a string identifier for the document.\n:::\n:::js\n- `pageContent`: a string representing the content;\n- `metadata`: a dict containing arbitrary metadata;\n- `id`: (optional) a string identifier for the document.\n:::\n\nThe `metadata` attribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual @[`Document`] object often represents a chunk of a larger document.\n\nWe can generate sample documents when desired:\n\n:::python\n```python\nfrom langchain_core.documents import Document\n\ndocuments = [\n    Document(\n        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n        metadata={\"source\": \"mammal-pets-doc\"},\n    ),\n    Document(\n        page_content=\"Cats are independent pets that often enjoy their own space.\",\n        metadata={\"source\": \"mammal-pets-doc\"},\n    ),\n]\n```\n:::\n:::js\n```typescript\nimport { Document } from \"@langchain/core/documents\";\n\nconst documents = [\n  new Document({\n    pageContent:\n      \"Dogs are great companions, known for their loyalty and friendliness.\",\n    metadata: { source: \"mammal-pets-doc\" },\n  }),\n  new Document({\n    pageContent: \"Cats are independent pets that often enjoy their own space.\",\n    metadata: { source: \"mammal-pets-doc\" },\n  }),\n];\n```\n:::\n\nHowever, the LangChain ecosystem implements [document loaders](/oss/langchain/retrieval#document-loaders) that [integrate with hundreds of common sources](/oss/integrations/document_", "metadata": {"source": "knowledge-base.mdx"}}
{"text": "\n]\n```\n:::\n:::js\n```typescript\nimport { Document } from \"@langchain/core/documents\";\n\nconst documents = [\n  new Document({\n    pageContent:\n      \"Dogs are great companions, known for their loyalty and friendliness.\",\n    metadata: { source: \"mammal-pets-doc\" },\n  }),\n  new Document({\n    pageContent: \"Cats are independent pets that often enjoy their own space.\",\n    metadata: { source: \"mammal-pets-doc\" },\n  }),\n];\n```\n:::\n\nHowever, the LangChain ecosystem implements [document loaders](/oss/langchain/retrieval#document-loaders) that [integrate with hundreds of common sources](/oss/integrations/document_loaders/). This makes it easy to incorporate data from these sources into your AI application.\n\n### Loading documents\n\nLet's load a PDF into a sequence of @[`Document`] objects. [Here is a sample PDF](https://github.com/langchain-ai/langchain/blob/v0.3/docs/docs/example_data/nke-10k-2023.pdf) -- a 10-k filing for Nike from 2023. We can consult the LangChain documentation for [available PDF document loaders](/oss/integrations/document_loaders/#pdfs).\n\n:::python\n```python\nfrom langchain_community.document_loaders import PyPDFLoader\n\nfile_path = \"../example_data/nke-10k-2023.pdf\"\nloader = PyPDFLoader(file_path)\n\ndocs = loader.load()\n\nprint(len(docs))\n```\n```text\n107\n```\n\n`PyPDFLoader` loads one @[`Document`] object per PDF page. For each, we can easily access:\n\n- The string content of the page;\n- Metadata containing the file name and page number.\n:::\n:::js\n```typescript\nimport { PDFLoader } from \"@langchain/community/document_loaders/fs/pdf\";\n\nconst loader = new PDFLoader(\"../../data/nke-10k-2023.pdf\");\n\nconst docs = await loader.load();\nconsole.log(docs.length);\n```\n```text\n107\n```\n\n`PDFLoader` loads one @[`Document`] object per PDF page. For each, we can easily access:\n\n- The string content of the page;\n- Metadata containing the file name and page number.\n:::\n\n:::python\n```python\nprint(f\"{docs[0].page_content[:200]}\\n\")\nprint(docs[0].metadata)\n```\n```python\nTable of Contents\nUNITED STATES\nSECURITIES AND EXCHANGE COMMISSION\nWashington, D.C. 20549\nFORM 10-K\n(Mark One)\n\u2611 ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934\nFO\n\n{'source': '../example_data/nke-10k-2023.pdf', 'page': 0}\n```\n:::\n:::js\n```typescript\nconsole.log(docs[0].pageContent.slice(0, 200));\n```\n", "metadata": {"source": "knowledge-base.mdx"}}
{"text": " The string content of the page;\n- Metadata containing the file name and page number.\n:::\n\n:::python\n```python\nprint(f\"{docs[0].page_content[:200]}\\n\")\nprint(docs[0].metadata)\n```\n```python\nTable of Contents\nUNITED STATES\nSECURITIES AND EXCHANGE COMMISSION\nWashington, D.C. 20549\nFORM 10-K\n(Mark One)\n\u2611 ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934\nFO\n\n{'source': '../example_data/nke-10k-2023.pdf', 'page': 0}\n```\n:::\n:::js\n```typescript\nconsole.log(docs[0].pageContent.slice(0, 200));\n```\n```text\nTable of Contents\nUNITED STATES\nSECURITIES AND EXCHANGE COMMISSION\nWashington, D.C. 20549\nFORM 10-K\n(Mark One)\n\u2611 ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934\nFO\n```\n```typescript\nconsole.log(docs[0].metadata);\n```\n```javascript\n{\n  source: '../../data/nke-10k-2023.pdf',\n  pdf: {\n    version: '1.10.100',\n    info: {\n      PDFFormatVersion: '1.4',\n      IsAcroFormPresent: false,\n      IsXFAPresent: false,\n      Title: '0000320187-23-000039',\n      Author: 'EDGAR Online, a division of Donnelley Financial Solutions',\n      Subject: 'Form 10-K filed on 2023-07-20 for the period ending 2023-05-31',\n      Keywords: '0000320187-23-000039; ; 10-K',\n      Creator: 'EDGAR Filing HTML Converter',\n      Producer: 'EDGRpdf Service w/ EO.Pdf 22.0.40.0',\n      CreationDate: \"D:20230720162200-04'00'\",\n      ModDate: \"D:20230720162208-04'00'\"\n    },\n    metadata: null,\n    totalPages: 107\n  },\n  loc: { pageNumber: 1 }\n}\n```\n:::\n\n### Splitting\n\nFor both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve @[`Document`] objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not \"washed out\" by surrounding text.\n\nWe can use [text splitters](/oss/langchain/retrieval#text_splitters) for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters\nwith 200 characters of overlap between chunks. The overlap helps\nmitigate the possibility of separating a statement from important\ncontext related to it. We use the\n`RecursiveCharacterTextSplitter`,\nwhich will recursively split", "metadata": {"source": "knowledge-base.mdx"}}
{"text": "  },\n  loc: { pageNumber: 1 }\n}\n```\n:::\n\n### Splitting\n\nFor both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve @[`Document`] objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not \"washed out\" by surrounding text.\n\nWe can use [text splitters](/oss/langchain/retrieval#text_splitters) for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters\nwith 200 characters of overlap between chunks. The overlap helps\nmitigate the possibility of separating a statement from important\ncontext related to it. We use the\n`RecursiveCharacterTextSplitter`,\nwhich will recursively split the document using common separators like\nnew lines until each chunk is the appropriate size. This is the\nrecommended text splitter for generic text use cases.\n\n:::python\nWe set `add_start_index=True` so that the character index where each\nsplit Document starts within the initial Document is preserved as\nmetadata attribute \u201cstart_index\u201d.\n\n```python\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000, chunk_overlap=200, add_start_index=True\n)\nall_splits = text_splitter.split_documents(docs)\n\nprint(len(all_splits))\n```\n:::\n:::js\n```typescript\nimport { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\n\nconst textSplitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 1000,\n  chunkOverlap: 200,\n});\n\nconst allSplits = await textSplitter.splitDocuments(docs);\n\nconsole.log(allSplits.length);\n```\n:::\n\n```text\n514\n```\n\n\n## 2. Embeddings\n\nVector search is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text. Given a query, we can [embed](/oss/langchain/retrieval#embedding_models) it as a vector of the same dimension and use vector similarity metrics (such as cosine similarity) to identify related text.\n\nLangChain supports embeddings from [dozens of providers](/oss/integrations/text_embedding/). These models specify how text should be converted into a numeric vector. Let's select a model:\n\n:::python\n<EmbeddingsTabsPy />\n\n```python\nvector_1 = embeddings.embed_query(all_splits[0].page_content)\nvector_2 = embeddings.embed_query(all_splits[1].page_content)\n\nassert len(vector_1) == len(vector_2)\nprint(f\"Generated vectors of length {len(vector_1)}\\n\")\nprint(vector_1[:10])\n```\n:::\n:::js\n<EmbeddingsTabsJS />\n\n```typescript\nconst vector1 = await embeddings.embedQuery(allSplits[0].pageContent);\nconst vector2 = await embeddings.embedQuery(allSpl", "metadata": {"source": "knowledge-base.mdx"}}
{"text": "_embedding/). These models specify how text should be converted into a numeric vector. Let's select a model:\n\n:::python\n<EmbeddingsTabsPy />\n\n```python\nvector_1 = embeddings.embed_query(all_splits[0].page_content)\nvector_2 = embeddings.embed_query(all_splits[1].page_content)\n\nassert len(vector_1) == len(vector_2)\nprint(f\"Generated vectors of length {len(vector_1)}\\n\")\nprint(vector_1[:10])\n```\n:::\n:::js\n<EmbeddingsTabsJS />\n\n```typescript\nconst vector1 = await embeddings.embedQuery(allSplits[0].pageContent);\nconst vector2 = await embeddings.embedQuery(allSplits[1].pageContent);\n\nassert vector1.length === vector2.length;\nconsole.log(`Generated vectors of length ${vector1.length}\\n`);\nconsole.log(vector1.slice(0, 10));\n```\n:::\n\n```text\nGenerated vectors of length 1536\n\n[-0.008586574345827103, -0.03341241180896759, -0.008936782367527485, -0.0036674530711025, 0.010564599186182022, 0.009598285891115665, -0.028587326407432556, -0.015824200585484505, 0.0030416189692914486, -0.012899317778646946]\n```\nArmed with a model for generating text embeddings, we can next store them in a special data structure that supports efficient similarity search.\n\n## 3. Vector stores\n\nLangChain @[VectorStore] objects contain methods for adding text and @[`Document`] objects to the store, and querying them using various similarity metrics. They are often initialized with [embedding](/oss/langchain/retrieval#embedding_models) models, which determine how text data is translated to numeric vectors.\n\nLangChain includes a suite of [integrations](/oss/integrations/vectorstores) with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as [Postgres](/oss/integrations/vectorstores/pgvector)) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads. Let's select a vector store:\n\n:::python\n<VectorstoreTabsPy />\n:::\n:::js\n<VectorstoreTabsJS />\n:::\n\nHaving instantiated our vector store, we can now index the documents.\n\n:::python\n```python\nids = vector_store.add_documents(documents=all_splits)\n```\n:::\n:::js\n```typescript\nawait vectorStore.addDocuments(allSplits);\n```\n:::\n\nNote that most vector store implementations will allow you to connect to an existing vector store--  e.g., by providing a client, index name, or other information. See the documentation for a specific [integration](/oss/integrations/vectorstores) for more detail.\n\n:::python\n\nOnce we've instantiated a @[`VectorStore`] that", "metadata": {"source": "knowledge-base.mdx"}}
{"text": "s. Let's select a vector store:\n\n:::python\n<VectorstoreTabsPy />\n:::\n:::js\n<VectorstoreTabsJS />\n:::\n\nHaving instantiated our vector store, we can now index the documents.\n\n:::python\n```python\nids = vector_store.add_documents(documents=all_splits)\n```\n:::\n:::js\n```typescript\nawait vectorStore.addDocuments(allSplits);\n```\n:::\n\nNote that most vector store implementations will allow you to connect to an existing vector store--  e.g., by providing a client, index name, or other information. See the documentation for a specific [integration](/oss/integrations/vectorstores) for more detail.\n\n:::python\n\nOnce we've instantiated a @[`VectorStore`] that contains documents, we can query it. @[VectorStore] includes methods for querying:\n- Synchronously and asynchronously;\n- By string query and by vector;\n- With and without returning similarity scores;\n- By similarity and @[maximum marginal relevance][VectorStore.max_marginal_relevance_search] (to balance similarity with query to diversity in retrieved results).\n\n:::\n\n:::js\n\nOnce we've instantiated a @[`VectorStore`] that contains documents, we can query it. @[VectorStore] includes methods for querying:\n- Synchronously and asynchronously;\n- By string query and by vector;\n- With and without returning similarity scores;\n- By similarity and @[maximum marginal relevance][VectorStore.maxMarginalRelevanceSearch] (to balance similarity with query to diversity in retrieved results).\n\n:::\n\nThe methods will generally include a list of @[Document] objects in their outputs.\n\n**Usage**\n\nEmbeddings typically represent text as a \"dense\" vector such that texts with similar meanings are geometrically close. This lets us retrieve relevant information just by passing in a question, without knowledge of any specific key-terms used in the document.\n\nReturn documents based on similarity to a string query:\n\n:::python\n```python\nresults = vector_store.similarity_search(\n    \"How many distribution centers does Nike have in the US?\"\n)\n\nprint(results[0])\n```\n```python\npage_content='direct to consumer operations sell products through the following number of retail stores in the United States:\nU.S. RETAIL STORES NUMBER\nNIKE Brand factory stores 213\nNIKE Brand in-line stores (including employee-only stores) 74\nConverse stores (including factory stores) 82\nTOTAL 369\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\n2023 FORM 10-K 2' metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}\n```\n:::\n:::js\n```typescript\nconst results1 = await vectorStore.similaritySearch(\n  \"When was Nike incorporated?\"\n);\n\nconsole.log(results1[0]);\n```\n```javascript\nDocument {\n    pageContent: 'direct to consumer operations sell products...',\n    metadata: {'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}\n}\n```\n::", "metadata": {"source": "knowledge-base.mdx"}}
{"text": "including factory stores) 82\nTOTAL 369\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\n2023 FORM 10-K 2' metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}\n```\n:::\n:::js\n```typescript\nconst results1 = await vectorStore.similaritySearch(\n  \"When was Nike incorporated?\"\n);\n\nconsole.log(results1[0]);\n```\n```javascript\nDocument {\n    pageContent: 'direct to consumer operations sell products...',\n    metadata: {'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}\n}\n```\n:::\n\n:::python\nAsync query:\n\n```python\nresults = await vector_store.asimilarity_search(\"When was Nike incorporated?\")\n\nprint(results[0])\n```\n```python\npage_content='Table of Contents\nPART I\nITEM 1. BUSINESS\nGENERAL\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales' metadata={'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}\n```\n:::\n\nReturn scores:\n\n:::python\n```python\n# Note that providers implement different scores; the score here\n# is a distance metric that varies inversely with similarity.\n\nresults = vector_store.similarity_search_with_score(\"What was Nike's revenue in 2023?\")\ndoc, score = results[0]\nprint(f\"Score: {score}\\n\")\nprint(doc)\n```\n```python\nScore: 0.23699893057346344\n\npage_content='Table of Contents\nFISCAL 2023 NIKE BRAND REVENUE HIGHLIGHTS\nThe following tables present NIKE Brand revenues disaggregated by reportable operating segment, distribution channel and major product line:\nFISCAL 2023 COMPARED TO FISCAL 2022\n\u2022NIKE, Inc. Revenues were $51.2 billion in fiscal 2023, which increased 10% and 16% compared to fiscal 2022 on a reported and currency-neutral basis, respectively.\nThe increase was due to higher revenues in North America, Europe, Middle East & Africa (\"EMEA\"), APLA and Greater China, which contributed approximately 7, 6,\n2 and 1 percentage points to NIKE, Inc. Revenues, respectively.\n\u2022NIKE Brand revenues, which represented over 90% of NIKE, Inc. Revenues, increased 10% and 16% on a reported and currency-neutral basis,", "metadata": {"source": "knowledge-base.mdx"}}
{"text": "6344\n\npage_content='Table of Contents\nFISCAL 2023 NIKE BRAND REVENUE HIGHLIGHTS\nThe following tables present NIKE Brand revenues disaggregated by reportable operating segment, distribution channel and major product line:\nFISCAL 2023 COMPARED TO FISCAL 2022\n\u2022NIKE, Inc. Revenues were $51.2 billion in fiscal 2023, which increased 10% and 16% compared to fiscal 2022 on a reported and currency-neutral basis, respectively.\nThe increase was due to higher revenues in North America, Europe, Middle East & Africa (\"EMEA\"), APLA and Greater China, which contributed approximately 7, 6,\n2 and 1 percentage points to NIKE, Inc. Revenues, respectively.\n\u2022NIKE Brand revenues, which represented over 90% of NIKE, Inc. Revenues, increased 10% and 16% on a reported and currency-neutral basis, respectively. This\nincrease was primarily due to higher revenues in Men's, the Jordan Brand, Women's and Kids' which grew 17%, 35%,11% and 10%, respectively, on a wholesale\nequivalent basis.' metadata={'page': 35, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}\n```\n:::\n:::js\n```typescript\nconst results2 = await vectorStore.similaritySearchWithScore(\n  \"What was Nike's revenue in 2023?\"\n);\n\nconsole.log(results2[0]);\n```\n```javascript\nScore: 0.23699893057346344\n\nDocument {\n    pageContent: 'Table of Contents...',\n    metadata: {'page': 35, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}\n}\n```\n:::\n\nReturn documents based on similarity to an embedded query:\n\n:::python\n```python\nembedding = embeddings.embed_query(\"How were Nike's margins impacted in 2023?\")\n\nresults = vector_store.similarity_search_by_vector(embedding)\nprint(results[0])\n```\n```python\npage_content='Table of Contents\nGROSS MARGIN\nFISCAL 2023 COMPARED TO FISCAL 2022\nFor fiscal 2023, our consolidated gross profit increased 4% to $22,292 million compared to $21,479 million for fiscal 2022. Gross margin decreased 250 basis points to\n43.5% for fiscal 2023 compared to 46.0% for fiscal 2022 due to the following:\n*Wholesale equivalent\nThe decrease in gross margin for fiscal 2023 was primarily due to:\n\u2022Higher NIKE Brand product costs, on a wholesale equivalent basis, primarily due to higher input costs and elevated inbound freight and logistics costs as well as\nproduct mix;\n\u2022Lower margin in our NIKE Direct business, driven by higher promotional activity to liquidate inventory in the current period compared to lower promotional activity in\nthe prior period resulting from lower available inventory supply;\n\u2022Unfavorable changes in net foreign currency exchange rates, including hedges; and\n\u2022Lower off-price margin, on a wholesale equivalent basis.\nThis was partially offset by:' metadata={'page': 36, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}\n```\n:::\n:::js\n```typescript\nconst embedding = await embeddings.embedQuery(\n  \"How were Nike's margins impacted in 2023?\"\n", "metadata": {"source": "knowledge-base.mdx"}}
{"text": " 2023 was primarily due to:\n\u2022Higher NIKE Brand product costs, on a wholesale equivalent basis, primarily due to higher input costs and elevated inbound freight and logistics costs as well as\nproduct mix;\n\u2022Lower margin in our NIKE Direct business, driven by higher promotional activity to liquidate inventory in the current period compared to lower promotional activity in\nthe prior period resulting from lower available inventory supply;\n\u2022Unfavorable changes in net foreign currency exchange rates, including hedges; and\n\u2022Lower off-price margin, on a wholesale equivalent basis.\nThis was partially offset by:' metadata={'page': 36, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}\n```\n:::\n:::js\n```typescript\nconst embedding = await embeddings.embedQuery(\n  \"How were Nike's margins impacted in 2023?\"\n);\n\nconst results3 = await vectorStore.similaritySearchVectorWithScore(\n  embedding,\n  1\n);\n\nconsole.log(results3[0]);\n```\n```javascript\nDocument {\n    pageContent: 'FISCAL 2023 COMPARED TO FISCAL 2022...',\n    metadata: {\n        'page': 36,\n        'source': '../example_data/nke-10k-2023.pdf',\n        'start_index': 0\n    }\n}\n```\n:::\n\nLearn more:\n\n- @[API Reference][VectorStore]\n- [Integration-specific docs](/oss/integrations/vectorstores)\n\n## 4. Retrievers\n\nLangChain @[`VectorStore`] objects do not subclass @[Runnable]. LangChain @[Retrievers] are Runnables, so they implement a standard set of methods (e.g., synchronous and asynchronous `invoke` and `batch` operations). Although we can construct retrievers from vector stores, retrievers can interface with non-vector store sources of data, as well (such as external APIs).\n\n:::python\nWe can create a simple version of this ourselves, without subclassing `Retriever`. If we choose what method we wish to use to retrieve documents, we can create a runnable easily. Below we will build one around the `similarity_search` method:\n\n\n```python\nfrom typing import List\n\nfrom langchain_core.documents import Document\nfrom langchain_core.runnables import chain\n\n\n@chain\ndef retriever(query: str) -> List[Document]:\n    return vector_store.similarity_search(query, k=1)\n\n\nretriever.batch(\n    [\n        \"How many distribution centers does Nike have in the US?\",\n        \"When was Nike incorporated?\",\n    ],\n)\n```\n\n\n\n```text\n[[Document(metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}, page_content='direct to consumer operations sell products through the following number of retail stores in the United States:\\nU.S. RETAIL STORES NUMBER\\nNIKE Brand factory stores 213 \\nNIKE Brand in-line stores (including employee-only stores) 74 \\nConverse stores (including factory stores) 82 \\nTOTAL 369 \\nIn the United States, NIKE", "metadata": {"source": "knowledge-base.mdx"}}
{"text": "_store.similarity_search(query, k=1)\n\n\nretriever.batch(\n    [\n        \"How many distribution centers does Nike have in the US?\",\n        \"When was Nike incorporated?\",\n    ],\n)\n```\n\n\n\n```text\n[[Document(metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}, page_content='direct to consumer operations sell products through the following number of retail stores in the United States:\\nU.S. RETAIL STORES NUMBER\\nNIKE Brand factory stores 213 \\nNIKE Brand in-line stores (including employee-only stores) 74 \\nConverse stores (including factory stores) 82 \\nTOTAL 369 \\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\\n2023 FORM 10-K 2')],\n [Document(metadata={'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}, page_content='Table of Contents\\nPART I\\nITEM 1. BUSINESS\\nGENERAL\\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales')]]\n```\n:::\n\nVectorstores implement an `as_retriever` method that will generate a Retriever, specifically a [`VectorStoreRetriever`](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStoreRetriever.html). These retrievers include specific `search_type` and `search_kwargs` attributes that identify what methods of the underlying vector store to call, and how to parameterize them. For instance, we can replicate the above with the following:\n\n:::python\n```python\nretriever = vector_store.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs={\"k\": 1},\n)\n\nretriever.batch(\n    [\n        \"How many distribution centers does Nike have in the US?\",\n        \"When was Nike incorporated?\",\n    ],\n)\n```\n```text\n[[Document(metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}, page_content='direct to consumer operations sell products through the following number of retail stores in the United States:\\nU.S. RETAIL STORES NUMBER\\nNIKE Brand factory stores 213 \\nNIKE Brand in-line stores (", "metadata": {"source": "knowledge-base.mdx"}}
{"text": ":python\n```python\nretriever = vector_store.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs={\"k\": 1},\n)\n\nretriever.batch(\n    [\n        \"How many distribution centers does Nike have in the US?\",\n        \"When was Nike incorporated?\",\n    ],\n)\n```\n```text\n[[Document(metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}, page_content='direct to consumer operations sell products through the following number of retail stores in the United States:\\nU.S. RETAIL STORES NUMBER\\nNIKE Brand factory stores 213 \\nNIKE Brand in-line stores (including employee-only stores) 74 \\nConverse stores (including factory stores) 82 \\nTOTAL 369 \\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\\n2023 FORM 10-K 2')],\n [Document(metadata={'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}, page_content='Table of Contents\\nPART I\\nITEM 1. BUSINESS\\nGENERAL\\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales')]]\n```\n\n`VectorStoreRetriever` supports search types of `\"similarity\"` (default), `\"mmr\"` (maximum marginal relevance, described above), and `\"similarity_score_threshold\"`. We can use the latter to threshold documents output by the retriever by similarity score.\n:::\n:::js\n```typescript\nconst retriever = vectorStore.asRetriever({\n  searchType: \"mmr\",\n  searchKwargs: {\n    fetchK: 1,\n  },\n});\n\nawait retriever.batch([\n  \"When was Nike incorporated?\",\n  \"What was Nike's revenue in 2023?\",\n]);\n```\n```javascript\n[\n    [Document {\n        metadata: {'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125},\n        pageContent: 'direct to consumer operations sell products...',\n    }],\n    [Document {\n        metadata: {'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0},\n  ", "metadata": {"source": "knowledge-base.mdx"}}
{"text": "  searchType: \"mmr\",\n  searchKwargs: {\n    fetchK: 1,\n  },\n});\n\nawait retriever.batch([\n  \"When was Nike incorporated?\",\n  \"What was Nike's revenue in 2023?\",\n]);\n```\n```javascript\n[\n    [Document {\n        metadata: {'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125},\n        pageContent: 'direct to consumer operations sell products...',\n    }],\n    [Document {\n        metadata: {'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0},\n        pageContent: 'Table of Contents...',\n    }],\n]\n```\n:::\n\nRetrievers can easily be incorporated into more complex applications, such as [retrieval-augmented generation (RAG)](/oss/langchain/retrieval) applications that combine a given question with retrieved context into a prompt for a LLM. To learn more about building such an application, check out the [RAG tutorial](/oss/langchain/rag) tutorial.\n\n\n## Next steps\n\nYou've now seen how to build a semantic search engine over a PDF document.\n\nFor more on document loaders:\n\n- [Overview](/oss/langchain/retrieval#document_loaders)\n- [Available integrations](/oss/integrations/document_loaders/)\n\nFor more on embeddings:\n\n- [Overview](/oss/langchain/retrieval#embedding_models/)\n- [Available integrations](/oss/integrations/text_embedding/)\n\nFor more on vector stores:\n\n- [Overview](/oss/langchain/retrieval#vectorstores/)\n- [Available integrations](/oss/integrations/vectorstores/)\n\nFor more on RAG, see:\n\n- [Build a Retrieval Augmented Generation (RAG) App](/oss/langchain/rag/)\n", "metadata": {"source": "knowledge-base.mdx"}}
{"text": "---\ntitle: Install LangChain\nsidebarTitle: Install\n---\n\nTo install the LangChain package:\n\n:::python\n<CodeGroup>\n    ```bash pip\n    pip install -U langchain\n    # Requires Python 3.10+\n    ```\n\n    ```bash uv\n    uv add langchain\n    # Requires Python 3.10+\n    ```\n</CodeGroup>\n:::\n\n:::js\n    <CodeGroup>\n    ```bash npm\n    npm install langchain @langchain/core\n    # Requires Node.js 20+\n    ```\n\n    ```bash pnpm\n    pnpm add langchain @langchain/core\n    # Requires Node.js 20+\n    ```\n\n    ```bash yarn\n    yarn add langchain @langchain/core\n    # Requires Node.js 20+\n    ```\n\n    ```bash bun\n    bun add langchain @langchain/core\n    # Requires Node.js 20+\n    ```\n    </CodeGroup>\n:::\n\nLangChain provides integrations to hundreds of LLMs and thousands of other integrations. These live in independent provider packages.\n\n:::python\n<CodeGroup>\n    ```bash pip\n    # Installing the OpenAI integration\n    pip install -U langchain-openai\n\n    # Installing the Anthropic integration\n    pip install -U langchain-anthropic\n    ```\n    ```bash uv\n    # Installing the OpenAI integration\n    uv add langchain-openai\n\n    # Installing the Anthropic integration\n    uv add langchain-anthropic\n    ```\n</CodeGroup>\n\n:::\n\n:::js\n<CodeGroup>\n    ```bash npm\n    # Installing the OpenAI integration\n    npm install @langchain/openai\n    # Installing the Anthropic integration\n    npm install @langchain/anthropic\n    ```\n\n    ```bash pnpm\n    # Installing the OpenAI integration\n    pnpm install @langchain/openai\n    # Installing the Anthropic integration\n    pnpm install @langchain/anthropic\n    ```\n\n    ```bash yarn\n    # Installing the OpenAI integration\n    yarn add @langchain/openai\n    # Installing the Anthropic integration\n    yarn add @langchain/anthropic\n    ```\n\n    ```bash bun\n    # Installing the OpenAI integration\n    bun add @langchain/openai\n    # Installing the Anthropic integration\n    bun add @langchain/anthropic\n    ```\n</CodeGroup>\n:::\n\n<Tip>\nSee the [Integrations tab](/oss/integrations/providers/overview) for a full list of available integrations.\n</Tip>\n\nNow that you have LangChain installed, you can get started by following the [Quickstart guide](/oss/langchain/quickstart).\n", "metadata": {"source": "install.mdx"}}
{"text": "---\ntitle: Models\n---\n\nimport ChatModelTabsPy from '/snippets/chat-model-tabs.mdx';\nimport ChatModelTabsJS from '/snippets/chat-model-tabs-js.mdx';\n\n[LLMs](https://en.wikipedia.org/wiki/Large_language_model) are powerful AI tools that can interpret and generate text like humans. They're versatile enough to write content, translate languages, summarize, and answer questions without needing specialized training for each task.\n\nIn addition to text generation, many models support:\n\n* <Icon icon=\"hammer\" size={16} /> [Tool calling](#tool-calling) - calling external tools (like databases queries or API calls) and use results in their responses.\n* <Icon icon=\"shapes\" size={16} /> [Structured output](#structured-output) - where the model's response is constrained to follow a defined format.\n* <Icon icon=\"image\" size={16} /> [Multimodality](#multimodal) - process and return data other than text, such as images, audio, and video.\n* <Icon icon=\"brain\" size={16} /> [Reasoning](#reasoning) - models perform multi-step reasoning to arrive at a conclusion.\n\nModels are the reasoning engine of [agents](/oss/langchain/agents). They drive the agent's decision-making process, determining which tools to call, how to interpret results, and when to provide a final answer.\n\nThe quality and capabilities of the model you choose directly impact your agent's baseline reliability and performance. Different models excel at different tasks - some are better at following complex instructions, others at structured reasoning, and some support larger context windows for handling more information.\n\nLangChain's standard model interfaces give you access to many different provider integrations, which makes it easy to experiment with and switch between models to find the best fit for your use case.\n\n<Info>\n    For provider-specific integration information and capabilities, see the provider's [chat model page](/oss/integrations/chat).\n</Info>\n\n## Basic usage\n\nModels can be utilized in two ways:\n\n1. **With agents** - Models can be dynamically specified when creating an [agent](/oss/langchain/agents#model).\n2. **Standalone** - Models can be called directly (outside of the agent loop) for tasks like text generation, classification, or extraction without the need for an agent framework.\n\nThe same model interface works in both contexts, which gives you the flexibility to start simple and scale up to more complex agent-based workflows as needed.\n\n### Initialize a model\n\n:::python\nThe easiest way to get started with a standalone model in LangChain is to use @[`init_chat_model`] to initialize one from a chat model provider of your choice (examples below):\n\n<ChatModelTabsPy />\n```python\nresponse = model.invoke(\"Why do parrots talk?\")\n```\n\nSee @[`init_chat_model`][init_chat_model] for more detail, including information on how to pass model [parameters](#parameters).\n:::\n:::js\nThe easiest way to get started with a standalone model in LangChain is to use `initChatModel` to initialize one from a [chat model provider](/oss/integrations/chat) of your choice (examples below):\n\n<ChatModelTabsJS />\n```typescript\nconst response = await model.invoke(\"Why do parrots talk?\");\n```\nSee @[`", "metadata": {"source": "models.mdx"}}
{"text": " get started with a standalone model in LangChain is to use @[`init_chat_model`] to initialize one from a chat model provider of your choice (examples below):\n\n<ChatModelTabsPy />\n```python\nresponse = model.invoke(\"Why do parrots talk?\")\n```\n\nSee @[`init_chat_model`][init_chat_model] for more detail, including information on how to pass model [parameters](#parameters).\n:::\n:::js\nThe easiest way to get started with a standalone model in LangChain is to use `initChatModel` to initialize one from a [chat model provider](/oss/integrations/chat) of your choice (examples below):\n\n<ChatModelTabsJS />\n```typescript\nconst response = await model.invoke(\"Why do parrots talk?\");\n```\nSee @[`initChatModel`][initChatModel] for more detail, including information on how to pass model [parameters](#parameters).\n:::\n\n### Supported models\n\nLangChain supports all major model providers, including OpenAI, Anthropic, Google, Azure, AWS Bedrock, and more. Each provider offers a variety of models with different capabilities. For a full list of supported models in LangChain, see the [integrations page](/oss/integrations/providers/overview).\n\n### Key methods\n\n<Card title=\"Invoke\" href=\"#invoke\" icon=\"paper-plane\" arrow=\"true\" horizontal>\n    The model takes messages as input and outputs messages after generating a complete response.\n</Card>\n<Card title=\"Stream\" href=\"#stream\" icon=\"tower-broadcast\" arrow=\"true\" horizontal>\n    Invoke the model, but stream the output as it is generated in real-time.\n</Card>\n<Card title=\"Batch\" href=\"#batch\" icon=\"grip\" arrow=\"true\" horizontal>\n    Send multiple requests to a model in a batch for more efficient processing.\n</Card>\n\n<Info>\n    In addition to chat models, LangChain provides support for other adjacent technologies, such as embedding models and vector stores. See the [integrations page](/oss/integrations/providers/overview) for details.\n</Info>\n\n## Parameters\n\nA chat model takes parameters that can be used to configure its behavior. The full set of supported parameters varies by model and provider, but standard ones include:\n\n<ParamField body=\"model\" type=\"string\" required>\n   The name or identifier of the specific model you want to use with a provider. You can also specify both the model and its provider in a single argument using the '{model_provider}:{model}' format, for example, 'openai:o1'.\n</ParamField>\n\n:::python\n<ParamField body=\"api_key\" type=\"string\">\n    The key required for authenticating with the model's provider. This is usually issued when you sign up for access to the model. Often accessed by setting an <Tooltip tip=\"A variable whose value is set outside the program, typically through functionality built into the operating system or microservice.\">environment variable</Tooltip>.\n</ParamField>\n:::\n:::js\n<ParamField body=\"apiKey\" type=\"string\">\n    The key required for authenticating with the model's provider. This is usually issued when you sign up for access to the model. Often accessed by setting an <Tooltip tip=\"A variable whose value is set outside the program, typically through functionality built into the operating system or", "metadata": {"source": "models.mdx"}}
{"text": " '{model_provider}:{model}' format, for example, 'openai:o1'.\n</ParamField>\n\n:::python\n<ParamField body=\"api_key\" type=\"string\">\n    The key required for authenticating with the model's provider. This is usually issued when you sign up for access to the model. Often accessed by setting an <Tooltip tip=\"A variable whose value is set outside the program, typically through functionality built into the operating system or microservice.\">environment variable</Tooltip>.\n</ParamField>\n:::\n:::js\n<ParamField body=\"apiKey\" type=\"string\">\n    The key required for authenticating with the model's provider. This is usually issued when you sign up for access to the model. Often accessed by setting an <Tooltip tip=\"A variable whose value is set outside the program, typically through functionality built into the operating system or microservice.\">environment variable</Tooltip>.\n</ParamField>\n:::\n\n<ParamField body=\"temperature\" type=\"number\">\n    Controls the randomness of the model's output. A higher number makes responses more creative; lower ones make them more deterministic.\n</ParamField>\n\n:::python\n<ParamField body=\"max_tokens\" type=\"number\">\n    Limits the total number of <Tooltip tip=\"The basic unit that a model reads and generates. Providers may define them differently, but in general, they can represent a whole or part of word.\">tokens</Tooltip> in the response, effectively controlling how long the output can be.\n</ParamField>\n:::\n:::js\n<ParamField body=\"maxTokens\" type=\"number\">\n    Limits the total number of <Tooltip tip=\"The basic unit that a model reads and generates. Providers may define them differently, but in general, they can represent a whole or part of word.\">tokens</Tooltip> in the response, effectively controlling how long the output can be.\n</ParamField>\n:::\n\n<ParamField body=\"timeout\" type=\"number\">\n    The maximum time (in seconds) to wait for a response from the model before canceling the request.\n</ParamField>\n\n:::python\n<ParamField body=\"max_retries\" type=\"number\">\n    The maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits.\n</ParamField>\n:::\n:::js\n<ParamField body=\"maxRetries\" type=\"number\">\n    The maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits.\n</ParamField>\n:::\n\n:::python\nUsing @[`init_chat_model`], pass these parameters as inline <Tooltip tip=\"Arbitrary keyword arguments\" cta=\"Learn more\" href=\"https://www.w3schools.com/python/python_args_kwargs.asp\">`**kwargs`</Tooltip>:\n\n```python Initialize using model parameters\nmodel = init_chat_model(\n    \"claude-sonnet-4-5-20250929\",\n    # Kwargs passed to the model:\n    temperature=0.7,\n    timeout=30,\n    max_tokens=1000,\n)\n```\n:::\n:::js\nUsing `initChatModel`, pass these parameters as inline parameters:\n\n```typesc", "metadata": {"source": "models.mdx"}}
{"text": " timeouts or rate limits.\n</ParamField>\n:::\n\n:::python\nUsing @[`init_chat_model`], pass these parameters as inline <Tooltip tip=\"Arbitrary keyword arguments\" cta=\"Learn more\" href=\"https://www.w3schools.com/python/python_args_kwargs.asp\">`**kwargs`</Tooltip>:\n\n```python Initialize using model parameters\nmodel = init_chat_model(\n    \"claude-sonnet-4-5-20250929\",\n    # Kwargs passed to the model:\n    temperature=0.7,\n    timeout=30,\n    max_tokens=1000,\n)\n```\n:::\n:::js\nUsing `initChatModel`, pass these parameters as inline parameters:\n\n```typescript Initialize using model parameters\nconst model = await initChatModel(\n    \"claude-sonnet-4-5-20250929\",\n    { temperature: 0.7, timeout: 30, max_tokens: 1000 }\n)\n```\n:::\n\n<Info>\n    Each chat model integration may have additional params used to control provider-specific functionality.\n\n    For example, @[`ChatOpenAI`] has `use_responses_api` to dictate whether to use the OpenAI Responses or Completions API.\n\n    To find all the parameters supported by a given chat model, head to the [chat model integrations](/oss/integrations/chat) page.\n</Info>\n\n---\n\n## Invocation\n\nA chat model must be invoked to generate an output. There are three primary invocation methods, each suited to different use cases.\n\n### Invoke\n\nThe most straightforward way to call a model is to use @[`invoke()`][BaseChatModel.invoke] with a single message or a list of messages.\n\n:::python\n```python Single message\nresponse = model.invoke(\"Why do parrots have colorful feathers?\")\nprint(response)\n```\n:::\n\n:::js\n```typescript Single message\nconst response = await model.invoke(\"Why do parrots have colorful feathers?\");\nconsole.log(response);\n```\n:::\n\nA list of messages can be provided to a chat model to represent conversation history. Each message has a role that models use to indicate who sent the message in the conversation.\n\nSee the [messages](/oss/langchain/messages) guide for more detail on roles, types, and content.\n\n:::python\n```python Dictionary format\nconversation = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant that translates English to French.\"},\n    {\"role\": \"user\", \"content\": \"Translate: I love programming.\"},\n    {\"role\": \"assistant\", \"content\": \"J'adore la programmation.\"},\n    {\"role\": \"user\", \"content\": \"Translate: I love building applications.\"}\n]\n\nresponse = model.invoke(conversation)\nprint(response)  # AIMessage(\"J'adore cr\u00e9er des applications.\")\n```\n```python Message objects\nfrom langchain.messages import HumanMessage, AIMessage, SystemMessage\n\nconversation = [\n    SystemMessage(\"You are a helpful assistant that translates English to French.\"),\n    HumanMessage", "metadata": {"source": "models.mdx"}}
{"text": "```python Dictionary format\nconversation = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant that translates English to French.\"},\n    {\"role\": \"user\", \"content\": \"Translate: I love programming.\"},\n    {\"role\": \"assistant\", \"content\": \"J'adore la programmation.\"},\n    {\"role\": \"user\", \"content\": \"Translate: I love building applications.\"}\n]\n\nresponse = model.invoke(conversation)\nprint(response)  # AIMessage(\"J'adore cr\u00e9er des applications.\")\n```\n```python Message objects\nfrom langchain.messages import HumanMessage, AIMessage, SystemMessage\n\nconversation = [\n    SystemMessage(\"You are a helpful assistant that translates English to French.\"),\n    HumanMessage(\"Translate: I love programming.\"),\n    AIMessage(\"J'adore la programmation.\"),\n    HumanMessage(\"Translate: I love building applications.\")\n]\n\nresponse = model.invoke(conversation)\nprint(response)  # AIMessage(\"J'adore cr\u00e9er des applications.\")\n```\n:::\n\n:::js\n```typescript Object format\nconst conversation = [\n  { role: \"system\", content: \"You are a helpful assistant that translates English to French.\" },\n  { role: \"user\", content: \"Translate: I love programming.\" },\n  { role: \"assistant\", content: \"J'adore la programmation.\" },\n  { role: \"user\", content: \"Translate: I love building applications.\" },\n];\n\nconst response = await model.invoke(conversation);\nconsole.log(response);  // AIMessage(\"J'adore cr\u00e9er des applications.\")\n```\n```typescript Message objects\nimport { HumanMessage, AIMessage, SystemMessage } from \"langchain\";\n\nconst conversation = [\n  new SystemMessage(\"You are a helpful assistant that translates English to French.\"),\n  new HumanMessage(\"Translate: I love programming.\"),\n  new AIMessage(\"J'adore la programmation.\"),\n  new HumanMessage(\"Translate: I love building applications.\"),\n];\n\nconst response = await model.invoke(conversation);\nconsole.log(response);  // AIMessage(\"J'adore cr\u00e9er des applications.\")\n```\n:::\n\n<Info>\n    If the return type of your invocation is a string, ensure that you are using a chat model as opposed to a LLM. Legacy, text-completion LLMs return strings directly. LangChain chat models are prefixed with \"Chat\", e.g., @[`ChatOpenAI`](/oss/integrations/chat/openai).\n</Info>\n\n### Stream\n\nMost models can stream their output content while it is being generated. By displaying output progressively, streaming significantly improves user experience, particularly for longer responses.\n\nCalling @[`stream()`][BaseChatModel.stream] returns an <Tooltip tip=\"An object that progressively provides access to each item of a collection, in order.\">iterator</Tooltip> that yields output chunks as they are produced. You can use a loop to process each chunk in real-time:\n\n:::python\n<CodeGroup>\n    ```python Basic text streaming\n    for chunk in model.stream(\"Why do parrots have colorful feathers?\"):\n   ", "metadata": {"source": "models.mdx"}}
{"text": " LLM. Legacy, text-completion LLMs return strings directly. LangChain chat models are prefixed with \"Chat\", e.g., @[`ChatOpenAI`](/oss/integrations/chat/openai).\n</Info>\n\n### Stream\n\nMost models can stream their output content while it is being generated. By displaying output progressively, streaming significantly improves user experience, particularly for longer responses.\n\nCalling @[`stream()`][BaseChatModel.stream] returns an <Tooltip tip=\"An object that progressively provides access to each item of a collection, in order.\">iterator</Tooltip> that yields output chunks as they are produced. You can use a loop to process each chunk in real-time:\n\n:::python\n<CodeGroup>\n    ```python Basic text streaming\n    for chunk in model.stream(\"Why do parrots have colorful feathers?\"):\n        print(chunk.text, end=\"|\", flush=True)\n    ```\n\n    ```python Stream tool calls, reasoning, and other content\n    for chunk in model.stream(\"What color is the sky?\"):\n        for block in chunk.content_blocks:\n            if block[\"type\"] == \"reasoning\" and (reasoning := block.get(\"reasoning\")):\n                print(f\"Reasoning: {reasoning}\")\n            elif block[\"type\"] == \"tool_call_chunk\":\n                print(f\"Tool call chunk: {block}\")\n            elif block[\"type\"] == \"text\":\n                print(block[\"text\"])\n            else:\n                ...\n    ```\n</CodeGroup>\n:::\n:::js\n<CodeGroup>\n    ```typescript Basic text streaming\n    const stream = await model.stream(\"Why do parrots have colorful feathers?\");\n    for await (const chunk of stream) {\n      console.log(chunk.text)\n    }\n    ```\n\n    ```typescript Stream tool calls, reasoning, and other content\n    const stream = await model.stream(\"What color is the sky?\");\n    for await (const chunk of stream) {\n      for (const block of chunk.contentBlocks) {\n        if (block.type === \"reasoning\") {\n          console.log(`Reasoning: ${block.reasoning}`);\n        } else if (block.type === \"tool_call_chunk\") {\n          console.log(`Tool call chunk: ${block}`);\n        } else if (block.type === \"text\") {\n          console.log(block.text);\n        } else {\n          ...\n        }\n    ", "metadata": {"source": "models.mdx"}}
{"text": "?\");\n    for await (const chunk of stream) {\n      for (const block of chunk.contentBlocks) {\n        if (block.type === \"reasoning\") {\n          console.log(`Reasoning: ${block.reasoning}`);\n        } else if (block.type === \"tool_call_chunk\") {\n          console.log(`Tool call chunk: ${block}`);\n        } else if (block.type === \"text\") {\n          console.log(block.text);\n        } else {\n          ...\n        }\n      }\n    }\n    ```\n</CodeGroup>\n:::\n\nAs opposed to [`invoke()`](#invoke), which returns a single @[`AIMessage`][AIMessage] after the model has finished generating its full response, `stream()` returns multiple @[`AIMessageChunk`][AIMessageChunk] objects, each containing a portion of the output text. Importantly, each chunk in a stream is designed to be gathered into a full message via summation:\n\n:::python\n```python Construct an AIMessage\nfull = None  # None | AIMessageChunk\nfor chunk in model.stream(\"What color is the sky?\"):\n    full = chunk if full is None else full + chunk\n    print(full.text)\n\n# The\n# The sky\n# The sky is\n# The sky is typically\n# The sky is typically blue\n# ...\n\nprint(full.content_blocks)\n# [{\"type\": \"text\", \"text\": \"The sky is typically blue...\"}]\n```\n:::\n\n:::js\n```typescript Construct AIMessage\nlet full: AIMessageChunk | null = null;\nfor await (const chunk of stream) {\n  full = full ? full.concat(chunk) : chunk;\n  console.log(full.text);\n}\n\n// The\n// The sky\n// The sky is\n// The sky is typically\n// The sky is typically blue\n// ...\n\nconsole.log(full.contentBlocks);\n// [{\"type\": \"text\", \"text\": \"The sky is typically blue...\"}]\n```\n:::\n\nThe resulting message can be treated the same as a message that was generated with [`invoke()`](#invoke) \u2013 for example, it can be aggregated into a message history and passed back to the model as conversational context.\n\n<Warning>\n    Streaming only works if all steps in the program know how to process a stream of chunks. For instance, an application that isn't streaming-capable would be one that needs to store the entire output in memory before it can be processed.\n</Warning>\n\n<Accordion title=\"Advanced streaming topics\">\n    <Accordion title=\"Streaming events\">\n        :::python\n        LangChain chat models can also stream semantic events using `astream_events()`.\n\n        This simplifies filtering based on event types and other metadata, and will aggregate the full message in the background. See below for an example.\n\n       ", "metadata": {"source": "models.mdx"}}
{"text": " [`invoke()`](#invoke) \u2013 for example, it can be aggregated into a message history and passed back to the model as conversational context.\n\n<Warning>\n    Streaming only works if all steps in the program know how to process a stream of chunks. For instance, an application that isn't streaming-capable would be one that needs to store the entire output in memory before it can be processed.\n</Warning>\n\n<Accordion title=\"Advanced streaming topics\">\n    <Accordion title=\"Streaming events\">\n        :::python\n        LangChain chat models can also stream semantic events using `astream_events()`.\n\n        This simplifies filtering based on event types and other metadata, and will aggregate the full message in the background. See below for an example.\n\n        ```python\n        async for event in model.astream_events(\"Hello\"):\n\n            if event[\"event\"] == \"on_chat_model_start\":\n                print(f\"Input: {event['data']['input']}\")\n\n            elif event[\"event\"] == \"on_chat_model_stream\":\n                print(f\"Token: {event['data']['chunk'].text}\")\n\n            elif event[\"event\"] == \"on_chat_model_end\":\n                print(f\"Full message: {event['data']['output'].text}\")\n\n            else:\n                pass\n        ```\n        ```txt\n        Input: Hello\n        Token: Hi\n        Token:  there\n        Token: !\n        Token:  How\n        Token:  can\n        Token:  I\n        ...\n        Full message: Hi there! How can I help today?\n        ```\n\n        <Tip>\n            See the @[`astream_events()`][BaseChatModel.astream_events] reference for event types and other details.\n        </Tip>\n        :::\n\n        :::js\n        LangChain chat models can also stream semantic events using\n        [`streamEvents()`][BaseChatModel.streamEvents].\n\n        This simplifies filtering based on event types and other metadata, and will aggregate the full message in the background. See below for an example.\n\n        ```typescript\n        const stream = await model.streamEvents(\"Hello\");\n        for await (const event of stream) {\n     ", "metadata": {"source": "models.mdx"}}
{"text": "         See the @[`astream_events()`][BaseChatModel.astream_events] reference for event types and other details.\n        </Tip>\n        :::\n\n        :::js\n        LangChain chat models can also stream semantic events using\n        [`streamEvents()`][BaseChatModel.streamEvents].\n\n        This simplifies filtering based on event types and other metadata, and will aggregate the full message in the background. See below for an example.\n\n        ```typescript\n        const stream = await model.streamEvents(\"Hello\");\n        for await (const event of stream) {\n            if (event.event === \"on_chat_model_start\") {\n                console.log(`Input: ${event.data.input}`);\n            }\n            if (event.event === \"on_chat_model_stream\") {\n                console.log(`Token: ${event.data.chunk.text}`);\n            }\n            if (event.event === \"on_chat_model_end\") {\n                console.log(`Full message: ${event.data.output.text}`);\n            }\n        }\n        ```\n        ```txt\n        Input: Hello\n        Token: Hi\n        Token:  there\n        Token: !\n        Token:  How\n        Token:  can\n        Token:  I\n        ...\n        Full message: Hi there! How can I help today?\n        ```\n\n        See the @[`streamEvents()`][BaseChatModel.streamEvents] reference for event types and other details.\n        :::\n    </Accordion>\n    <Accordion title='\"Auto-streaming\" chat models'>\n        LangChain simplifies streaming from chat models by automatically enabling streaming mode in certain cases, even when you're not explicitly calling the streaming methods. This is particularly useful when you use the non-streaming invoke method but still want to stream the entire application, including intermediate results from the chat model.\n\n        In [LangGraph agents](/oss/langchain/agents), for example, you can call `model.invoke()` within nodes, but LangChain will automatically delegate to streaming if running in a streaming mode.\n\n        #### How it works\n\n        When you `invoke()` a chat model, LangChain will automatically switch to an internal streaming", "metadata": {"source": "models.mdx"}}
{"text": " details.\n        :::\n    </Accordion>\n    <Accordion title='\"Auto-streaming\" chat models'>\n        LangChain simplifies streaming from chat models by automatically enabling streaming mode in certain cases, even when you're not explicitly calling the streaming methods. This is particularly useful when you use the non-streaming invoke method but still want to stream the entire application, including intermediate results from the chat model.\n\n        In [LangGraph agents](/oss/langchain/agents), for example, you can call `model.invoke()` within nodes, but LangChain will automatically delegate to streaming if running in a streaming mode.\n\n        #### How it works\n\n        When you `invoke()` a chat model, LangChain will automatically switch to an internal streaming mode if it detects that you are trying to stream the overall application. The result of the invocation will be the same as far as the code that was using invoke is concerned; however, while the chat model is being streamed, LangChain will take care of invoking @[`on_llm_new_token`] events in LangChain's callback system.\n\n        :::python\n        Callback events allow LangGraph `stream()` and `astream_events()` to surface the chat model's output in real-time.\n        :::\n        :::js\n        Callback events allow LangGraph `stream()` and `streamEvents()` to surface the chat model's output in real-time.\n        :::\n    </Accordion>\n</Accordion>\n\n### Batch\n\nBatching a collection of independent requests to a model can significantly improve performance and reduce costs, as the processing can be done in parallel:\n\n:::python\n```python Batch\nresponses = model.batch([\n    \"Why do parrots have colorful feathers?\",\n    \"How do airplanes fly?\",\n    \"What is quantum computing?\"\n])\nfor response in responses:\n    print(response)\n```\n\n<Note>\n    This section describes a chat model method @[`batch()`][BaseChatModel.batch], which parallelizes model calls client-side.\n\n    It is **distinct** from batch APIs supported by inference providers, such as [OpenAI](https://platform.openai.com/docs/guides/batch) or [Anthropic](https://platform.claude.com/docs/en/build-with-claude/batch-processing#message-batches-api).\n</Note>\n\nBy default, @[`batch()`][BaseChatModel.batch] will only return the final output for the entire batch. If you want to receive the output for each individual input as it finishes generating, you can stream results with @[`batch_as_completed()`][BaseChatModel.batch_as_completed]:\n\n```python Yield batch responses upon completion\nfor response in model.batch_as_completed([\n    \"Why do parrots have colorful feathers?\",\n    \"How do airplanes fly?\",\n    \"What is quantum computing?\"\n]):\n    print(response)\n```\n<Note>\n    When using @[`batch_as_completed()`][BaseChatModel.", "metadata": {"source": "models.mdx"}}
{"text": ".com/docs/en/build-with-claude/batch-processing#message-batches-api).\n</Note>\n\nBy default, @[`batch()`][BaseChatModel.batch] will only return the final output for the entire batch. If you want to receive the output for each individual input as it finishes generating, you can stream results with @[`batch_as_completed()`][BaseChatModel.batch_as_completed]:\n\n```python Yield batch responses upon completion\nfor response in model.batch_as_completed([\n    \"Why do parrots have colorful feathers?\",\n    \"How do airplanes fly?\",\n    \"What is quantum computing?\"\n]):\n    print(response)\n```\n<Note>\n    When using @[`batch_as_completed()`][BaseChatModel.batch_as_completed], results may arrive out of order. Each includes the input index for matching to reconstruct the original order as needed.\n</Note>\n\n<Tip>\n    When processing a large number of inputs using @[`batch()`][BaseChatModel.batch] or @[`batch_as_completed()`][BaseChatModel.batch_as_completed], you may want to control the maximum number of parallel calls. This can be done by setting the @[`max_concurrency`][RunnableConfig(max_concurrency)] attribute in the @[`RunnableConfig`] dictionary.\n\n    ```python Batch with max concurrency\n    model.batch(\n        list_of_inputs,\n        config={\n            'max_concurrency': 5,  # Limit to 5 parallel calls\n        }\n    )\n    ```\n\n    See the @[`RunnableConfig`] reference for a full list of supported attributes.\n</Tip>\n\nFor more details on batching, see the @[reference][BaseChatModel.batch].\n:::\n\n:::js\n```typescript Batch\nconst responses = await model.batch([\n  \"Why do parrots have colorful feathers?\",\n  \"How do airplanes fly?\",\n  \"What is quantum computing?\",\n  \"Why do parrots have colorful feathers?\",\n  \"How do airplanes fly?\",\n  \"What is quantum computing?\",\n]);\nfor (const response of responses) {\n  console.log(response);\n}\n```\n\n<Tip>\n    When processing a large number of inputs using `batch()`, you may want to control the maximum number of parallel calls. This can be done by setting the `maxConcurrency` attribute in the @[`RunnableConfig`] dictionary.\n\n    ```typescript Batch with max concurrency\n    model.batch(\n      listOfInputs,\n      {\n        maxConcurrency: 5,  // Limit to 5 parallel calls\n      }\n    )\n    ```\n\n    See the @[`RunnableConfig`] reference for a full list of supported attributes.\n</Tip>\n\nFor more details on batching, see the @[reference][BaseChatModel.batch].\n:::\n\n---\n\n## Tool calling\n\nModels can request to call tools that perform tasks such as fetching data", "metadata": {"source": "models.mdx"}}
{"text": " using `batch()`, you may want to control the maximum number of parallel calls. This can be done by setting the `maxConcurrency` attribute in the @[`RunnableConfig`] dictionary.\n\n    ```typescript Batch with max concurrency\n    model.batch(\n      listOfInputs,\n      {\n        maxConcurrency: 5,  // Limit to 5 parallel calls\n      }\n    )\n    ```\n\n    See the @[`RunnableConfig`] reference for a full list of supported attributes.\n</Tip>\n\nFor more details on batching, see the @[reference][BaseChatModel.batch].\n:::\n\n---\n\n## Tool calling\n\nModels can request to call tools that perform tasks such as fetching data from a database, searching the web, or running code. Tools are pairings of:\n\n1. A schema, including the name of the tool, a description, and/or argument definitions (often a JSON schema)\n2. A function or <Tooltip tip=\"A method that can suspend execution and resume at a later time\">coroutine</Tooltip> to execute.\n\n<Note>\n    You may hear the term \"function calling\". We use this interchangeably with \"tool calling\".\n</Note>\n\nHere's the basic tool calling flow between a user and a model:\n\n:::python\n```mermaid\nsequenceDiagram\n    participant U as User\n    participant M as Model\n    participant T as Tools\n\n    U->>M: \"What's the weather in SF and NYC?\"\n    M->>M: Analyze request & decide tools needed\n\n    par Parallel Tool Calls\n        M->>T: get_weather(\"San Francisco\")\n        M->>T: get_weather(\"New York\")\n    end\n\n    par Tool Execution\n        T-->>M: SF weather data\n        T-->>M: NYC weather data\n    end\n\n    M->>M: Process results & generate response\n    M->>U: \"SF: 72\u00b0F sunny, NYC: 68\u00b0F cloudy\"\n```\n:::\n\n:::js\n```mermaid\nsequenceDiagram\n    participant U as User\n    participant M as Model\n    participant T as Tools\n\n    U->>M: \"What's the weather in SF and NYC?\"\n    M->>M: Analyze request & decide tools needed\n\n    par Parallel Tool Calls\n        M->>T: getWeather(\"San Francisco\")\n        M->>T: getWeather(\"New York\")\n    end\n\n    par Tool Execution\n        T-->>M: SF weather data\n        T-->>M: NYC weather data\n    end\n\n    M->>M: Process results & generate response\n    M->>U: \"SF: 72\u00b0F sunny, NYC: 68\u00b0F cloudy\"\n```\n:::\n\n:::python\nTo make tools that you have defined available for use by a model, you must bind them using @[`bind_tools`][BaseChatModel.bind_tools]. In subsequent invocations, the", "metadata": {"source": "models.mdx"}}
{"text": "   M->>M: Analyze request & decide tools needed\n\n    par Parallel Tool Calls\n        M->>T: getWeather(\"San Francisco\")\n        M->>T: getWeather(\"New York\")\n    end\n\n    par Tool Execution\n        T-->>M: SF weather data\n        T-->>M: NYC weather data\n    end\n\n    M->>M: Process results & generate response\n    M->>U: \"SF: 72\u00b0F sunny, NYC: 68\u00b0F cloudy\"\n```\n:::\n\n:::python\nTo make tools that you have defined available for use by a model, you must bind them using @[`bind_tools`][BaseChatModel.bind_tools]. In subsequent invocations, the model can choose to call any of the bound tools as needed.\n:::\n\n:::js\nTo make tools that you have defined available for use by a model, you must bind them using @[`bindTools`][BaseChatModel.bindTools]. In subsequent invocations, the model can choose to call any of the bound tools as needed.\n:::\n\nSome model providers offer <Tooltip tip=\"Tools that are executed server-side, such as web search and code interpreters\">built-in tools</Tooltip> that can be enabled via model or invocation parameters (e.g. [`ChatOpenAI`](/oss/integrations/chat/openai), [`ChatAnthropic`](/oss/integrations/chat/anthropic)). Check the respective [provider reference](/oss/integrations/providers/overview) for details.\n\n<Tip>\n    See the [tools guide](/oss/langchain/tools) for details and other options for creating tools.\n</Tip>\n\n:::python\n```python Binding user tools\nfrom langchain.tools import tool\n\n@tool\ndef get_weather(location: str) -> str:\n    \"\"\"Get the weather at a location.\"\"\"\n    return f\"It's sunny in {location}.\"\n\n\nmodel_with_tools = model.bind_tools([get_weather])  # [!code highlight]\n\nresponse = model_with_tools.invoke(\"What's the weather like in Boston?\")\nfor tool_call in response.tool_calls:\n    # View tool calls made by the model\n    print(f\"Tool: {tool_call['name']}\")\n    print(f\"Args: {tool_call['args']}\")\n```\n:::\n\n:::js\n```typescript Binding user tools\nimport { tool } from \"langchain\";\nimport * as z from \"zod\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst getWeather = tool(\n  (input) => `It's sunny in ${input.location}.`,\n  {\n    name: \"get_weather\",\n    description: \"Get the weather at a location.\",\n    schema: z.object({\n      location: z.string().describe(\"The location to get the weather for\"),\n    }),\n  },\n);\n\nconst model = new ChatOpenAI({ model: \"gpt-4.1\" });\nconst modelWithTools = model.bindTools([getWeather]);  // [!code highlight]\n\nconst response = await modelWithTools", "metadata": {"source": "models.mdx"}}
{"text": "\n:::\n\n:::js\n```typescript Binding user tools\nimport { tool } from \"langchain\";\nimport * as z from \"zod\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst getWeather = tool(\n  (input) => `It's sunny in ${input.location}.`,\n  {\n    name: \"get_weather\",\n    description: \"Get the weather at a location.\",\n    schema: z.object({\n      location: z.string().describe(\"The location to get the weather for\"),\n    }),\n  },\n);\n\nconst model = new ChatOpenAI({ model: \"gpt-4.1\" });\nconst modelWithTools = model.bindTools([getWeather]);  // [!code highlight]\n\nconst response = await modelWithTools.invoke(\"What's the weather like in Boston?\");\nconst toolCalls = response.tool_calls || [];\nfor (const tool_call of toolCalls) {\n  // View tool calls made by the model\n  console.log(`Tool: ${tool_call.name}`);\n  console.log(`Args: ${tool_call.args}`);\n}\n```\n:::\n\nWhen binding user-defined tools, the model's response includes a **request** to execute a tool. When using a model separately from an [agent](/oss/langchain/agents), it is up to you to execute the requested tool and return the result back to the model for use in subsequent reasoning. When using an [agent](/oss/langchain/agents), the agent loop will handle the tool execution loop for you.\n\nBelow, we show some common ways you can use tool calling.\n\n<AccordionGroup>\n    <Accordion title=\"Tool execution loop\" icon=\"arrow-rotate-right\">\n        When a model returns tool calls, you need to execute the tools and pass the results back to the model. This creates a conversation loop where the model can use tool results to generate its final response. LangChain includes [agent](/oss/langchain/agents) abstractions that handle this orchestration for you.\n\n        Here's a simple example of how to do this:\n\n        :::python\n\n        ```python Tool execution loop\n        # Bind (potentially multiple) tools to the model\n        model_with_tools = model.bind_tools([get_weather])\n\n        # Step 1: Model generates tool calls\n        messages = [{\"role\": \"user\", \"content\": \"What's the weather in Boston?\"}]\n        ai_msg = model_with_tools.invoke(messages)\n        messages.append(ai_msg)\n\n        # Step 2: Execute tools and collect results\n        for tool_call in ai_msg.tool_calls:\n            # Execute the tool with the generated arguments\n            tool_result = get_weather.invoke(tool_call)\n            messages.append(tool_result)\n\n        # Step 3: Pass results back to model", "metadata": {"source": "models.mdx"}}
{"text": ": Model generates tool calls\n        messages = [{\"role\": \"user\", \"content\": \"What's the weather in Boston?\"}]\n        ai_msg = model_with_tools.invoke(messages)\n        messages.append(ai_msg)\n\n        # Step 2: Execute tools and collect results\n        for tool_call in ai_msg.tool_calls:\n            # Execute the tool with the generated arguments\n            tool_result = get_weather.invoke(tool_call)\n            messages.append(tool_result)\n\n        # Step 3: Pass results back to model for final response\n        final_response = model_with_tools.invoke(messages)\n        print(final_response.text)\n        # \"The current weather in Boston is 72\u00b0F and sunny.\"\n        ```\n\n        :::\n        :::js\n\n        ```typescript Tool execution loop\n        // Bind (potentially multiple) tools to the model\n        const modelWithTools = model.bindTools([get_weather])\n\n        // Step 1: Model generates tool calls\n        const messages = [{\"role\": \"user\", \"content\": \"What's the weather in Boston?\"}]\n        const ai_msg = await modelWithTools.invoke(messages)\n        messages.push(ai_msg)\n\n        // Step 2: Execute tools and collect results\n        for (const tool_call of ai_msg.tool_calls) {\n            // Execute the tool with the generated arguments\n            const tool_result = await get_weather.invoke(tool_call)\n            messages.push(tool_result)\n        }\n\n        // Step 3: Pass results back to model for final response\n        const final_response = await modelWithTools.invoke(messages)\n        console.log(final_response.text)\n        // \"The current weather in Boston is 72\u00b0F and sunny.\"\n        ```\n\n        :::\n\n        Each @[`ToolMessage`] returned by the tool includes a `tool_call_id` that matches the original tool call, helping the model correlate results with requests.\n    </Accordion>\n    <Accordion title=\"Forcing tool calls\" icon=\"asterisk\">\n        By default, the model has the freedom to choose which bound tool to use based on the user's input. However, you might want to force choosing a tool, ensuring the model uses either a particular tool or **any** tool from a given list:\n\n   ", "metadata": {"source": "models.mdx"}}
{"text": "(messages)\n        console.log(final_response.text)\n        // \"The current weather in Boston is 72\u00b0F and sunny.\"\n        ```\n\n        :::\n\n        Each @[`ToolMessage`] returned by the tool includes a `tool_call_id` that matches the original tool call, helping the model correlate results with requests.\n    </Accordion>\n    <Accordion title=\"Forcing tool calls\" icon=\"asterisk\">\n        By default, the model has the freedom to choose which bound tool to use based on the user's input. However, you might want to force choosing a tool, ensuring the model uses either a particular tool or **any** tool from a given list:\n\n        :::python\n\n        <CodeGroup>\n            ```python Force use of any tool\n            model_with_tools = model.bind_tools([tool_1], tool_choice=\"any\")\n            ```\n            ```python Force use of specific tools\n            model_with_tools = model.bind_tools([tool_1], tool_choice=\"tool_1\")\n            ```\n        </CodeGroup>\n\n        :::\n        :::js\n\n        <CodeGroup>\n            ```typescript Force use of any tool\n            const modelWithTools = model.bindTools([tool_1], { toolChoice: \"any\" })\n            ```\n            ```typescript Force use of specific tools\n            const modelWithTools = model.bindTools([tool_1], { toolChoice: \"tool_1\" })\n            ```\n        </CodeGroup>\n        :::\n    </Accordion>\n    <Accordion title=\"Parallel tool calls\" icon=\"layer-group\">\n        Many models support calling multiple tools in parallel when appropriate. This allows the model to gather information from different sources simultaneously.\n\n        :::python\n\n        ```python Parallel tool calls\n        model_with_tools = model.bind_tools([get_weather])\n\n        response = model_with_tools.invoke(\n            \"What's the weather in Boston and Tokyo?\"\n        )\n\n\n        # The model may generate multiple tool calls\n        print(response.tool_calls)\n        # [\n        #   {'name': 'get_weather', 'args': {'location': 'Boston'},", "metadata": {"source": "models.mdx"}}
{"text": "     Many models support calling multiple tools in parallel when appropriate. This allows the model to gather information from different sources simultaneously.\n\n        :::python\n\n        ```python Parallel tool calls\n        model_with_tools = model.bind_tools([get_weather])\n\n        response = model_with_tools.invoke(\n            \"What's the weather in Boston and Tokyo?\"\n        )\n\n\n        # The model may generate multiple tool calls\n        print(response.tool_calls)\n        # [\n        #   {'name': 'get_weather', 'args': {'location': 'Boston'}, 'id': 'call_1'},\n        #   {'name': 'get_weather', 'args': {'location': 'Tokyo'}, 'id': 'call_2'},\n        # ]\n\n\n        # Execute all tools (can be done in parallel with async)\n        results = []\n        for tool_call in response.tool_calls:\n            if tool_call['name'] == 'get_weather':\n                result = get_weather.invoke(tool_call)\n            ...\n            results.append(result)\n        ```\n\n        :::\n        :::js\n\n        ```typescript Parallel tool calls\n        const modelWithTools = model.bind_tools([get_weather])\n\n        const response = await modelWithTools.invoke(\n            \"What's the weather in Boston and Tokyo?\"\n        )\n\n\n        // The model may generate multiple tool calls\n        console.log(response.tool_calls)\n        // [\n        //   { name: 'get_weather', args: { location: 'Boston' }, id: 'call_1' },\n        //   { name: 'get_time', args: { location: 'Tokyo' }, id: 'call_2' }\n        // ]\n\n\n        // Execute all tools (can be done in parallel with async)\n        const results = []\n        for (const tool_call of response.tool_calls || []) {\n            if (tool_call.name === 'get_weather') {\n                const result = await get_weather.invoke(tool_call)\n                results.push(result)\n            }\n  ", "metadata": {"source": "models.mdx"}}
{"text": "        //   { name: 'get_time', args: { location: 'Tokyo' }, id: 'call_2' }\n        // ]\n\n\n        // Execute all tools (can be done in parallel with async)\n        const results = []\n        for (const tool_call of response.tool_calls || []) {\n            if (tool_call.name === 'get_weather') {\n                const result = await get_weather.invoke(tool_call)\n                results.push(result)\n            }\n        }\n        ```\n\n        :::\n\n        The model intelligently determines when parallel execution is appropriate based on the independence of the requested operations.\n\n        <Tip>\n        Most models supporting tool calling enable parallel tool calls by default. Some (including [OpenAI](/oss/integrations/chat/openai) and [Anthropic](/oss/integrations/chat/anthropic)) allow you to disable this feature. To do this, set `parallel_tool_calls=False`:\n        ```python\n        model.bind_tools([get_weather], parallel_tool_calls=False)\n        ```\n        </Tip>\n    </Accordion>\n    <Accordion title=\"Streaming tool calls\" icon=\"rss\">\n        When streaming responses, tool calls are progressively built through @[`ToolCallChunk`]. This allows you to see tool calls as they're being generated rather than waiting for the complete response.\n\n        :::python\n\n        ```python Streaming tool calls\n        for chunk in model_with_tools.stream(\n            \"What's the weather in Boston and Tokyo?\"\n        ):\n            # Tool call chunks arrive progressively\n            for tool_chunk in chunk.tool_call_chunks:\n                if name := tool_chunk.get(\"name\"):\n                    print(f\"Tool: {name}\")\n                if id_ := tool_chunk.get(\"id\"):\n                    print(f\"ID: {id_}\")\n                if args := tool_chunk.get(\"args\"):\n                    print(f\"Args: {args}\")\n\n        # Output:\n        # Tool: get_", "metadata": {"source": "models.mdx"}}
{"text": "              if name := tool_chunk.get(\"name\"):\n                    print(f\"Tool: {name}\")\n                if id_ := tool_chunk.get(\"id\"):\n                    print(f\"ID: {id_}\")\n                if args := tool_chunk.get(\"args\"):\n                    print(f\"Args: {args}\")\n\n        # Output:\n        # Tool: get_weather\n        # ID: call_SvMlU1TVIZugrFLckFE2ceRE\n        # Args: {\"lo\n        # Args: catio\n        # Args: n\": \"B\n        # Args: osto\n        # Args: n\"}\n        # Tool: get_weather\n        # ID: call_QMZdy6qInx13oWKE7KhuhOLR\n        # Args: {\"lo\n        # Args: catio\n        # Args: n\": \"T\n        # Args: okyo\n        # Args: \"}\n        ```\n\n        You can accumulate chunks to build complete tool calls:\n\n        ```python Accumulate tool calls\n        gathered = None\n        for chunk in model_with_tools.stream(\"What's the weather in Boston?\"):\n            gathered = chunk if gathered is None else gathered + chunk\n            print(gathered.tool_calls)\n        ```\n\n        :::\n        :::js\n\n        ```typescript Streaming tool calls\n        const stream = await modelWithTools.stream(\n            \"What's the weather in Boston and Tokyo?\"\n        )\n        for await (const chunk of stream) {\n            // Tool call chunks arrive progressively\n            if (chunk.tool_call_chunks) {\n                for (const tool_chunk of chunk.tool_call_chunks) {\n                console.log(`Tool: ${tool_chunk.get('name', '')}`)\n                console.log(`Args: ${tool_ch", "metadata": {"source": "models.mdx"}}
{"text": "  const stream = await modelWithTools.stream(\n            \"What's the weather in Boston and Tokyo?\"\n        )\n        for await (const chunk of stream) {\n            // Tool call chunks arrive progressively\n            if (chunk.tool_call_chunks) {\n                for (const tool_chunk of chunk.tool_call_chunks) {\n                console.log(`Tool: ${tool_chunk.get('name', '')}`)\n                console.log(`Args: ${tool_chunk.get('args', '')}`)\n                }\n            }\n        }\n\n        // Output:\n        // Tool: get_weather\n        // Args:\n        // Tool:\n        // Args: {\"loc\n        // Tool:\n        // Args: ation\": \"BOS\"}\n        // Tool: get_time\n        // Args:\n        // Tool:\n        // Args: {\"timezone\": \"Tokyo\"}\n        ```\n\n        You can accumulate chunks to build complete tool calls:\n\n        ```typescript Accumulate tool calls\n        let full: AIMessageChunk | null = null\n        const stream = await modelWithTools.stream(\"What's the weather in Boston?\")\n        for await (const chunk of stream) {\n            full = full ? full.concat(chunk) : chunk\n            console.log(full.contentBlocks)\n        }\n        ```\n\n        :::\n    </Accordion>\n</AccordionGroup>\n\n---\n\n## Structured output\n\nModels can be requested to provide their response in a format matching a given schema. This is useful for ensuring the output can be easily parsed and used in subsequent processing. LangChain supports multiple schema types and methods for enforcing structured output.\n\n<Tip>\n    To learn about structured output, see [Structured output](/oss/langchain/structured-output).\n</Tip>\n\n:::python\n<Tabs>\n    <Tab title=\"Pydantic\">\n        [Pydantic models](https://docs.pydantic.dev/latest/concepts/models/#basic-model-usage) provide the richest feature set with field validation, descriptions, and nested structures.\n\n        ```python\n        from pydantic import BaseModel, Field\n\n    ", "metadata": {"source": "models.mdx"}}
{"text": "\n---\n\n## Structured output\n\nModels can be requested to provide their response in a format matching a given schema. This is useful for ensuring the output can be easily parsed and used in subsequent processing. LangChain supports multiple schema types and methods for enforcing structured output.\n\n<Tip>\n    To learn about structured output, see [Structured output](/oss/langchain/structured-output).\n</Tip>\n\n:::python\n<Tabs>\n    <Tab title=\"Pydantic\">\n        [Pydantic models](https://docs.pydantic.dev/latest/concepts/models/#basic-model-usage) provide the richest feature set with field validation, descriptions, and nested structures.\n\n        ```python\n        from pydantic import BaseModel, Field\n\n        class Movie(BaseModel):\n            \"\"\"A movie with details.\"\"\"\n            title: str = Field(..., description=\"The title of the movie\")\n            year: int = Field(..., description=\"The year the movie was released\")\n            director: str = Field(..., description=\"The director of the movie\")\n            rating: float = Field(..., description=\"The movie's rating out of 10\")\n\n        model_with_structure = model.with_structured_output(Movie)\n        response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n        print(response)  # Movie(title=\"Inception\", year=2010, director=\"Christopher Nolan\", rating=8.8)\n        ```\n    </Tab>\n    <Tab title=\"TypedDict\">\n        Python's `TypedDict` provides a simpler alternative to Pydantic models, ideal when you don't need runtime validation.\n\n        ```python\n        from typing_extensions import TypedDict, Annotated\n\n        class MovieDict(TypedDict):\n            \"\"\"A movie with details.\"\"\"\n            title: Annotated[str, ..., \"The title of the movie\"]\n            year: Annotated[int, ..., \"The year the movie was released\"]\n            director: Annotated[str, ..., \"The director of the movie\"]\n            rating: Annotated[float, ..., \"The movie's rating out of 10\"]\n\n        model_with_structure = model.with_structured_output(MovieDict)\n        response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n        print(response)  # {'title': 'Inception', 'year': 2010, 'director': 'Christopher Nolan', 'rating': 8.8}\n        ```\n    </Tab", "metadata": {"source": "models.mdx"}}
{"text": "          year: Annotated[int, ..., \"The year the movie was released\"]\n            director: Annotated[str, ..., \"The director of the movie\"]\n            rating: Annotated[float, ..., \"The movie's rating out of 10\"]\n\n        model_with_structure = model.with_structured_output(MovieDict)\n        response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n        print(response)  # {'title': 'Inception', 'year': 2010, 'director': 'Christopher Nolan', 'rating': 8.8}\n        ```\n    </Tab>\n    <Tab title=\"JSON Schema\">\n        Provide a [JSON Schema](https://json-schema.org/understanding-json-schema/about) for maximum control and interoperability.\n\n        ```python\n        import json\n\n        json_schema = {\n            \"title\": \"Movie\",\n            \"description\": \"A movie with details\",\n            \"type\": \"object\",\n            \"properties\": {\n                \"title\": {\n                    \"type\": \"string\",\n                    \"description\": \"The title of the movie\"\n                },\n                \"year\": {\n                    \"type\": \"integer\",\n                    \"description\": \"The year the movie was released\"\n                },\n                \"director\": {\n                    \"type\": \"string\",\n                    \"description\": \"The director of the movie\"\n                },\n                \"rating\": {\n                    \"type\": \"number\",\n                    \"description\": \"The movie's rating out of 10\"\n                }\n            },\n            \"required\": [\"title\", \"year\", \"director\", \"rating\"]\n        }\n\n        model_with_structure", "metadata": {"source": "models.mdx"}}
{"text": "                \"description\": \"The director of the movie\"\n                },\n                \"rating\": {\n                    \"type\": \"number\",\n                    \"description\": \"The movie's rating out of 10\"\n                }\n            },\n            \"required\": [\"title\", \"year\", \"director\", \"rating\"]\n        }\n\n        model_with_structure = model.with_structured_output(\n            json_schema,\n            method=\"json_schema\",\n        )\n        response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n        print(response)  # {'title': 'Inception', 'year': 2010, ...}\n        ```\n    </Tab>\n</Tabs>\n:::\n\n:::js\n<Tabs>\n    <Tab title=\"Zod\">\n        A [zod schema](https://zod.dev/) is the preferred method of defining an output schema. Note that when a zod schema is provided, the model output will also be validated against the schema using zod's parse methods.\n\n        ```typescript\n        import * as z from \"zod\";\n\n        const Movie = z.object({\n          title: z.string().describe(\"The title of the movie\"),\n          year: z.number().describe(\"The year the movie was released\"),\n          director: z.string().describe(\"The director of the movie\"),\n          rating: z.number().describe(\"The movie's rating out of 10\"),\n        });\n\n        const modelWithStructure = model.withStructuredOutput(Movie);\n\n        const response = await modelWithStructure.invoke(\"Provide details about the movie Inception\");\n        console.log(response);\n        // {\n        //   title: \"Inception\",\n        //   year: 2010,\n        //   director: \"Christopher Nolan\",\n        //   rating: 8.8,\n        // }\n        ```\n    </Tab>\n    <Tab title=\"JSON Schema\">\n        For maximum control or interoperability, you can provide a raw JSON Schema.\n\n        ```typescript\n        const json", "metadata": {"source": "models.mdx"}}
{"text": "     const response = await modelWithStructure.invoke(\"Provide details about the movie Inception\");\n        console.log(response);\n        // {\n        //   title: \"Inception\",\n        //   year: 2010,\n        //   director: \"Christopher Nolan\",\n        //   rating: 8.8,\n        // }\n        ```\n    </Tab>\n    <Tab title=\"JSON Schema\">\n        For maximum control or interoperability, you can provide a raw JSON Schema.\n\n        ```typescript\n        const jsonSchema = {\n          \"title\": \"Movie\",\n          \"description\": \"A movie with details\",\n          \"type\": \"object\",\n          \"properties\": {\n            \"title\": {\n              \"type\": \"string\",\n              \"description\": \"The title of the movie\",\n            },\n            \"year\": {\n              \"type\": \"integer\",\n              \"description\": \"The year the movie was released\",\n            },\n            \"director\": {\n              \"type\": \"string\",\n              \"description\": \"The director of the movie\",\n            },\n            \"rating\": {\n              \"type\": \"number\",\n              \"description\": \"The movie's rating out of 10\",\n            },\n          },\n          \"required\": [\"title\", \"year\", \"director\", \"rating\"],\n        }\n\n        const modelWithStructure = model.withStructuredOutput(\n          jsonSchema,\n          { method: \"jsonSchema\" },\n        )\n\n        const response = await modelWithStructure.invoke(\"Provide details about the movie Inception\")\n        console.log(response)  // {'title': 'Inception', 'year': 2010, ...}\n        ```\n    </Tab>\n</Tabs>\n:::\n\n:::python\n<Note>\n    **Key considerations for structured output**\n\n    - **Method parameter**: Some providers support different methods for structured output:\n    ", "metadata": {"source": "models.mdx"}}
{"text": "\n        }\n\n        const modelWithStructure = model.withStructuredOutput(\n          jsonSchema,\n          { method: \"jsonSchema\" },\n        )\n\n        const response = await modelWithStructure.invoke(\"Provide details about the movie Inception\")\n        console.log(response)  // {'title': 'Inception', 'year': 2010, ...}\n        ```\n    </Tab>\n</Tabs>\n:::\n\n:::python\n<Note>\n    **Key considerations for structured output**\n\n    - **Method parameter**: Some providers support different methods for structured output:\n        - `'json_schema'`: Uses dedicated structured output features offered by the provider.\n        - `'function_calling'`: Derives structured output by forcing a [tool call](#tool-calling) that follows the given schema.\n        - `'json_mode'`: A precursor to `'json_schema'` offered by some providers. Generates valid JSON, but the schema must be described in the prompt.\n    - **Include raw**: Set `include_raw=True` to get both the parsed output and the raw AI message.\n    - **Validation**: Pydantic models provide automatic validation. `TypedDict` and JSON Schema require manual validation.\n\n    See your [provider's integration page](/oss/integrations/providers/overview) for supported methods and configuration options.\n</Note>\n:::\n\n:::js\n<Note>\n    **Key considerations for structured output:**\n\n    - **Method parameter**: Some providers support different methods (`'jsonSchema'`, `'functionCalling'`, `'jsonMode'`)\n    - **Include raw**: Use @[`includeRaw: true`][BaseChatModel.with_structured_output(include_raw)] to get both the parsed output and the raw @[`AIMessage`]\n    - **Validation**: Zod models provide automatic validation, while JSON Schema requires manual validation\n\n    See your [provider's integration page](/oss/integrations/providers/overview) for supported methods and configuration options.\n</Note>\n:::\n\n<Accordion title=\"Example: Message output alongside parsed structure\">\n\nIt can be useful to return the raw @[`AIMessage`] object alongside the parsed representation to access response metadata such as [token counts](#token-usage). To do this, set @[`include_raw=True`][BaseChatModel.with_structured_output(include_raw)] when calling @[`with_structured_output`][BaseChatModel.with_structured_output]:\n\n    :::python\n    ```python\n    from pydantic import BaseModel, Field\n\n    class Movie(BaseModel):\n        \"\"\"A movie with details.\"\"\"\n        title: str = Field(..., description=\"The title of the movie\")\n        year: int = Field(..., description=\"The year the movie was released\")\n     ", "metadata": {"source": "models.mdx"}}
{"text": "\">\n\nIt can be useful to return the raw @[`AIMessage`] object alongside the parsed representation to access response metadata such as [token counts](#token-usage). To do this, set @[`include_raw=True`][BaseChatModel.with_structured_output(include_raw)] when calling @[`with_structured_output`][BaseChatModel.with_structured_output]:\n\n    :::python\n    ```python\n    from pydantic import BaseModel, Field\n\n    class Movie(BaseModel):\n        \"\"\"A movie with details.\"\"\"\n        title: str = Field(..., description=\"The title of the movie\")\n        year: int = Field(..., description=\"The year the movie was released\")\n        director: str = Field(..., description=\"The director of the movie\")\n        rating: float = Field(..., description=\"The movie's rating out of 10\")\n\n    model_with_structure = model.with_structured_output(Movie, include_raw=True)  # [!code highlight]\n    response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n    response\n    # {\n    #     \"raw\": AIMessage(...),\n    #     \"parsed\": Movie(title=..., year=..., ...),\n    #     \"parsing_error\": None,\n    # }\n    ```\n    :::\n\n    :::js\n    ```typescript\n    import * as z from \"zod\";\n\n    const Movie = z.object({\n      title: z.string().describe(\"The title of the movie\"),\n      year: z.number().describe(\"The year the movie was released\"),\n      director: z.string().describe(\"The director of the movie\"),\n      rating: z.number().describe(\"The movie's rating out of 10\"),\n      title: z.string().describe(\"The title of the movie\"),\n      year: z.number().describe(\"The year the movie was released\"),\n      director: z.string().describe(\"The director of the movie\"),  // [!code highlight]\n      rating: z.number().describe(\"The movie's rating out of 10\"),\n    });\n\n    const modelWithStructure = model.withStructuredOutput(Movie, { includeRaw: true });\n\n    const response = await modelWithStructure.invoke(\"Provide details about the movie Inception\");\n    console.log(response);\n    // {\n    //   raw: AIMessage { ... },\n    //   parsed: { title: \"Inception\", ... }\n    // }\n    ```\n    :::\n</Accordion>\n<Accordion title=\"Example: Nested structures\">\n    Schemas can be nested:\n    :::python\n    <CodeGroup>\n        ```python Pydantic BaseModel\n  ", "metadata": {"source": "models.mdx"}}
{"text": " z.number().describe(\"The movie's rating out of 10\"),\n    });\n\n    const modelWithStructure = model.withStructuredOutput(Movie, { includeRaw: true });\n\n    const response = await modelWithStructure.invoke(\"Provide details about the movie Inception\");\n    console.log(response);\n    // {\n    //   raw: AIMessage { ... },\n    //   parsed: { title: \"Inception\", ... }\n    // }\n    ```\n    :::\n</Accordion>\n<Accordion title=\"Example: Nested structures\">\n    Schemas can be nested:\n    :::python\n    <CodeGroup>\n        ```python Pydantic BaseModel\n        from pydantic import BaseModel, Field\n\n        class Actor(BaseModel):\n            name: str\n            role: str\n\n        class MovieDetails(BaseModel):\n            title: str\n            year: int\n            cast: list[Actor]\n            genres: list[str]\n            budget: float | None = Field(None, description=\"Budget in millions USD\")\n\n        model_with_structure = model.with_structured_output(MovieDetails)\n        ```\n\n        ```python TypedDict\n        from typing_extensions import Annotated, TypedDict\n\n        class Actor(TypedDict):\n            name: str\n            role: str\n\n        class MovieDetails(TypedDict):\n            title: str\n            year: int\n            cast: list[Actor]\n            genres: list[str]\n            budget: Annotated[float | None, ..., \"Budget in millions USD\"]\n\n        model_with_structure = model.with_structured_output(MovieDetails)\n        ```\n    </CodeGroup>\n    :::\n\n    :::js\n    ```typescript\n    import * as z from \"zod\";\n\n    const Actor = z.object({\n      name: str\n      role: z.string(),\n    });\n\n    const MovieDetails = z.object({\n      title: z.string(),\n      year: z.number(),\n      cast: z.array(Actor),\n      genres: z.array(z.string()),\n      budget: z.number().null", "metadata": {"source": "models.mdx"}}
{"text": " millions USD\"]\n\n        model_with_structure = model.with_structured_output(MovieDetails)\n        ```\n    </CodeGroup>\n    :::\n\n    :::js\n    ```typescript\n    import * as z from \"zod\";\n\n    const Actor = z.object({\n      name: str\n      role: z.string(),\n    });\n\n    const MovieDetails = z.object({\n      title: z.string(),\n      year: z.number(),\n      cast: z.array(Actor),\n      genres: z.array(z.string()),\n      budget: z.number().nullable().describe(\"Budget in millions USD\"),\n    });\n\n    const modelWithStructure = model.withStructuredOutput(MovieDetails);\n    ```\n    :::\n</Accordion>\n\n---\n\n## Advanced topics\n\n### Model profiles\n\n<Info>\n    Model profiles require `langchain>=1.1`.\n</Info>\n\n:::python\nLangChain chat models can expose a dictionary of supported features and capabilities through a `.profile` attribute:\n\n```python\nmodel.profile\n# {\n#   \"max_input_tokens\": 400000,\n#   \"image_inputs\": True,\n#   \"reasoning_output\": True,\n#   \"tool_calling\": True,\n#   ...\n# }\n```\n\nRefer to the full set of fields in the [API reference](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.BaseChatModel.profile).\n\nMuch of the model profile data is powered by the [models.dev](https://github.com/sst/models.dev) project, an open source initiative that provides model capability data. These data are augmented with additional fields for purposes of use with LangChain. These augmentations are kept aligned with the upstream project as it evolves.\n\nModel profile data allow applications to work around model capabilities dynamically. For example:\n\n1. [Summarization middleware](/oss/langchain/middleware/built-in#summarization) can trigger summarization based on a model's context window size.\n2. [Structured output](/oss/langchain/structured-output) strategies in `create_agent` can be inferred automatically (e.g., by checking support for native structured output features).\n3. Model inputs can be gated based on supported [modalities](#multimodal) and maximum input tokens.\n\n<Accordion title=\"Updating or overwriting profile data\">\n    Model profile data can be changed if it is missing, stale, or incorrect.\n\n    **Option 1 (quick fix)**\n\n    You can instantiate a chat model with any valid profile:\n\n    ```python\n    custom_profile = {\n        \"max_input_tokens\": 100_000,\n        \"tool_calling\": True,\n        \"structured_output\": True,\n        # ...\n    }\n    model = init_", "metadata": {"source": "models.mdx"}}
{"text": " be inferred automatically (e.g., by checking support for native structured output features).\n3. Model inputs can be gated based on supported [modalities](#multimodal) and maximum input tokens.\n\n<Accordion title=\"Updating or overwriting profile data\">\n    Model profile data can be changed if it is missing, stale, or incorrect.\n\n    **Option 1 (quick fix)**\n\n    You can instantiate a chat model with any valid profile:\n\n    ```python\n    custom_profile = {\n        \"max_input_tokens\": 100_000,\n        \"tool_calling\": True,\n        \"structured_output\": True,\n        # ...\n    }\n    model = init_chat_model(\"...\", profile=custom_profile)\n    ```\n\n    The `profile` is also a regular `dict` and can be updated in place. If the model instance is shared, consider using `model_copy` to avoid mutating shared state.\n\n    ```python\n    new_profile = model.profile | {\"key\": \"value\"}\n    model.model_copy(update={\"profile\": new_profile})\n    ```\n\n    **Option 2 (fix data upstream)**\n\n    The primary source for the data is the [models.dev](https://models.dev/) project. This data is merged with additional fields and overrides in LangChain [integration packages](/oss/integrations/providers/overview) and are shipped with those packages.\n\n    Model profile data can be updated through the following process:\n\n    1. (If needed) update the source data at [models.dev](https://models.dev/) through a pull request to its [repository on GitHub](https://github.com/sst/models.dev).\n    2. (If needed) update additional fields and overrides in `langchain_<package>/data/profile_augmentations.toml` through a pull request to the LangChain [integration package](/oss/integrations/providers/overview)`.\n    3. Use the [`langchain-model-profiles`](https://pypi.org/project/langchain-model-profiles/) CLI tool to pull the latest data from [models.dev](https://models.dev/), merge in the augmentations and update the profile data:\n\n    ```bash\n    pip install langchain-model-profiles\n    ```\n\n    ```bash\n    langchain-profiles refresh --provider <provider> --data-dir <data_dir>\n    ```\n\n    This command:\n    - Downloads the latest data for `<provider>` from models.dev\n    - Merges augmentations from `profile_augmentations.toml` in `<data_dir>`\n    - Writes merged profiles to `profiles.py` in `<data_dir>`\n\n    For example: from [`libs/partners/anthropic`](https://github.com/langchain-ai/langchain/tree/master/libs/partners/anthropic) in the [LangChain monorepo](https://github.com/langchain-ai/langchain):\n\n ", "metadata": {"source": "models.mdx"}}
{"text": "-profiles\n    ```\n\n    ```bash\n    langchain-profiles refresh --provider <provider> --data-dir <data_dir>\n    ```\n\n    This command:\n    - Downloads the latest data for `<provider>` from models.dev\n    - Merges augmentations from `profile_augmentations.toml` in `<data_dir>`\n    - Writes merged profiles to `profiles.py` in `<data_dir>`\n\n    For example: from [`libs/partners/anthropic`](https://github.com/langchain-ai/langchain/tree/master/libs/partners/anthropic) in the [LangChain monorepo](https://github.com/langchain-ai/langchain):\n\n    ```bash\n    uv run --with langchain-model-profiles --provider anthropic --data-dir langchain_anthropic/data\n    ```\n</Accordion>\n\n:::\n\n:::js\nLangChain chat models can expose a dictionary of supported features and capabilities through a `.profile` property:\n\n```typescript\nmodel.profile;\n// {\n//   maxInputTokens: 400000,\n//   imageInputs: true,\n//   reasoningOutput: true,\n//   toolCalling: true,\n//   ...\n// }\n```\n\nRefer to the full set of fields in the [API reference](https://reference.langchain.com/javascript/interfaces/_langchain_core.language_models_profile.ModelProfile.html).\n\nMuch of the model profile data is powered by the [models.dev](https://github.com/sst/models.dev) project, an open source initiative that provides model capability data. This data is augmented with additional fields for purposes of use with LangChain. These augmentations are kept aligned with the upstream project as it evolves.\n\nModel profile data allow applications to work around model capabilities dynamically. For example:\n\n1. [Summarization middleware](/oss/langchain/middleware/built-in#summarization) can trigger summarization based on a model's context window size.\n2. [Structured output](/oss/langchain/structured-output) strategies in `createAgent` can be inferred automatically (e.g., by checking support for native structured output features).\n3. Model inputs can be gated based on supported [modalities](#multimodal) and maximum input tokens.\n\n<Accordion title=\"Modify profile data\">\n    Model profile data can be changed if it is missing, stale, or incorrect.\n\n    **Option 1 (quick fix)**\n\n    You can instantiate a chat model with any valid profile:\n\n    ```typescript\n    const customProfile = {\n    maxInputTokens: 100_000,\n    toolCalling: true,\n    structuredOutput: true,\n    // ...\n    };\n    const model = initChatModel(\"...\", { profile: customProfile });\n    ```\n\n    **Option 2 (fix data upstream)**\n\n    The primary source for the data is the [models.dev](https://models.dev/) project. These data are merged with additional fields and overrides in LangChain [integration packages](/oss/integrations/providers/", "metadata": {"source": "models.mdx"}}
{"text": " profile data\">\n    Model profile data can be changed if it is missing, stale, or incorrect.\n\n    **Option 1 (quick fix)**\n\n    You can instantiate a chat model with any valid profile:\n\n    ```typescript\n    const customProfile = {\n    maxInputTokens: 100_000,\n    toolCalling: true,\n    structuredOutput: true,\n    // ...\n    };\n    const model = initChatModel(\"...\", { profile: customProfile });\n    ```\n\n    **Option 2 (fix data upstream)**\n\n    The primary source for the data is the [models.dev](https://models.dev/) project. These data are merged with additional fields and overrides in LangChain [integration packages](/oss/integrations/providers/overview) and are shipped with those packages.\n\n    Model profile data can be updated through the following process:\n\n    1. (If needed) update the source data at [models.dev](https://models.dev/) through a pull request to its [repository on GitHub](https://github.com/sst/models.dev).\n    2. (If needed) update additional fields and overrides in `langchain-<package>/profiles.toml` through a pull request to the LangChain [integration package](/oss/integrations/providers/overview).\n</Accordion>\n\n:::\n\n<Warning>\n    Model profiles are a beta feature. The format of a profile is subject to change.\n</Warning>\n\n### Multimodal\n\nCertain models can process and return non-textual data such as images, audio, and video. You can pass non-textual data to a model by providing [content blocks](/oss/langchain/messages#message-content).\n\n<Tip>\n    All LangChain chat models with underlying multimodal capabilities support:\n\n    1. Data in the cross-provider standard format (see [our messages guide](/oss/langchain/messages))\n    2. OpenAI [chat completions](https://platform.openai.com/docs/api-reference/chat) format\n    3. Any format that is native to that specific provider (e.g., Anthropic models accept Anthropic native format)\n</Tip>\n\nSee the [multimodal section](/oss/langchain/messages#multimodal) of the messages guide for details.\n\n<Tooltip tip=\"Not all LLMs are made equally!\" cta=\"See reference\" href=\"https://models.dev/\">Some models</Tooltip> can return multimodal data as part of their response. If invoked to do so, the resulting @[`AIMessage`] will have content blocks with multimodal types.\n\n:::python\n```python Multimodal output\nresponse = model.invoke(\"Create a picture of a cat\")\nprint(response.content_blocks)\n# [\n#     {\"type\": \"text\", \"text\": \"Here's a picture of a cat\"},\n#     {\"type\": \"image\", \"base64\": \"...\", \"mime_type\": \"image/jpeg\"},\n# ]\n```\n:::\n\n:::js\n```typescript Multimodal output\nconst response = await model.invoke(\"Create a picture of a cat\");\nconsole.log(response.contentBlocks", "metadata": {"source": "models.mdx"}}
{"text": " reference\" href=\"https://models.dev/\">Some models</Tooltip> can return multimodal data as part of their response. If invoked to do so, the resulting @[`AIMessage`] will have content blocks with multimodal types.\n\n:::python\n```python Multimodal output\nresponse = model.invoke(\"Create a picture of a cat\")\nprint(response.content_blocks)\n# [\n#     {\"type\": \"text\", \"text\": \"Here's a picture of a cat\"},\n#     {\"type\": \"image\", \"base64\": \"...\", \"mime_type\": \"image/jpeg\"},\n# ]\n```\n:::\n\n:::js\n```typescript Multimodal output\nconst response = await model.invoke(\"Create a picture of a cat\");\nconsole.log(response.contentBlocks);\n// [\n//   { type: \"text\", text: \"Here's a picture of a cat\" },\n//   { type: \"image\", data: \"...\", mimeType: \"image/jpeg\" },\n// ]\n```\n:::\n\nSee the [integrations page](/oss/integrations/providers/overview) for details on specific providers.\n\n### Reasoning\n\nMany models are capable of performing multi-step reasoning to arrive at a conclusion. This involves breaking down complex problems into smaller, more manageable steps.\n\n**If supported by the underlying model,** you can surface this reasoning process to better understand how the model arrived at its final answer.\n\n:::python\n<CodeGroup>\n    ```python Stream reasoning output\n    for chunk in model.stream(\"Why do parrots have colorful feathers?\"):\n        reasoning_steps = [r for r in chunk.content_blocks if r[\"type\"] == \"reasoning\"]\n        print(reasoning_steps if reasoning_steps else chunk.text)\n    ```\n\n    ```python Complete reasoning output\n    response = model.invoke(\"Why do parrots have colorful feathers?\")\n    reasoning_steps = [b for b in response.content_blocks if b[\"type\"] == \"reasoning\"]\n    print(\" \".join(step[\"reasoning\"] for step in reasoning_steps))\n    ```\n</CodeGroup>\n:::\n\n:::js\n<CodeGroup>\n    ```typescript Stream reasoning output\n    const stream = model.stream(\"Why do parrots have colorful feathers?\");\n    for await (const chunk of stream) {\n        const reasoningSteps = chunk.contentBlocks.filter(b => b.type === \"reasoning\");\n        console.log(reasoningSteps.length > 0 ? reasoningSteps : chunk.text);\n    }\n    ```\n\n    ```typescript Complete reasoning output\n    const response = await model.invoke(\"Why do parrots have colorful feathers?\");\n    const reasoningSteps = response.contentBlocks.filter(b => b.type === \"reasoning\");\n    console.log(reasoningSteps.map(step => step.reasoning).join(\" \"));\n    ```\n</CodeGroup>\n:::\n\nDepending on the model, you can sometimes specify the level of effort it should put into reasoning. Similarly, you can request that the model turn", "metadata": {"source": "models.mdx"}}
{"text": " {\n        const reasoningSteps = chunk.contentBlocks.filter(b => b.type === \"reasoning\");\n        console.log(reasoningSteps.length > 0 ? reasoningSteps : chunk.text);\n    }\n    ```\n\n    ```typescript Complete reasoning output\n    const response = await model.invoke(\"Why do parrots have colorful feathers?\");\n    const reasoningSteps = response.contentBlocks.filter(b => b.type === \"reasoning\");\n    console.log(reasoningSteps.map(step => step.reasoning).join(\" \"));\n    ```\n</CodeGroup>\n:::\n\nDepending on the model, you can sometimes specify the level of effort it should put into reasoning. Similarly, you can request that the model turn off reasoning entirely. This may take the form of categorical \"tiers\" of reasoning (e.g., `'low'` or `'high'`) or integer token budgets.\n\nFor details, see the [integrations page](/oss/integrations/providers/overview) or [reference](https://reference.langchain.com/python/integrations/) for your respective chat model.\n\n\n### Local models\n\nLangChain supports running models locally on your own hardware. This is useful for scenarios where either data privacy is critical, you want to invoke a custom model, or when you want to avoid the costs incurred when using a cloud-based model.\n\n[Ollama](/oss/integrations/chat/ollama) is one of the easiest ways to run chat and embedding models locally.\n\n{/* TODO: whenever we have a better integrations directory, x-ref to that page with a local query filter */}\n\n### Prompt caching\n\nMany providers offer prompt caching features to reduce latency and cost on repeat processing of the same tokens. These features can be **implicit** or **explicit**:\n\n- **Implicit prompt caching:** providers will automatically pass on cost savings if a request hits a cache. Examples: [OpenAI](/oss/integrations/chat/openai) and [Gemini](/oss/integrations/chat/google_generative_ai).\n- **Explicit caching:** providers allow you to manually indicate cache points for greater control or to guarantee cost savings. Examples:\n    - @[`ChatOpenAI`] (via `prompt_cache_key`)\n    - Anthropic's [`AnthropicPromptCachingMiddleware`](/oss/integrations/chat/anthropic#prompt-caching)\n    - [Gemini](https://reference.langchain.com/python/integrations/langchain_google_genai/).\n    - [AWS Bedrock](/oss/integrations/chat/bedrock#prompt-caching)\n\n<Warning>\n    Prompt caching is often only engaged above a minimum input token threshold. See [provider pages](/oss/integrations/chat) for details.\n</Warning>\n\nCache usage will be reflected in the [usage metadata](/oss/langchain/messages#token-usage) of the model response.\n\n### Server-side tool use\n\nSome providers support server-side [tool-calling](#tool-calling) loops: models can interact with web search, code interpreters, and other tools and analyze the results in a single conversational turn.\n\nIf a model invokes a tool server-side, the content", "metadata": {"source": "models.mdx"}}
{"text": "  - [Gemini](https://reference.langchain.com/python/integrations/langchain_google_genai/).\n    - [AWS Bedrock](/oss/integrations/chat/bedrock#prompt-caching)\n\n<Warning>\n    Prompt caching is often only engaged above a minimum input token threshold. See [provider pages](/oss/integrations/chat) for details.\n</Warning>\n\nCache usage will be reflected in the [usage metadata](/oss/langchain/messages#token-usage) of the model response.\n\n### Server-side tool use\n\nSome providers support server-side [tool-calling](#tool-calling) loops: models can interact with web search, code interpreters, and other tools and analyze the results in a single conversational turn.\n\nIf a model invokes a tool server-side, the content of the response message will include content representing the invocation and result of the tool. Accessing the [content blocks](/oss/langchain/messages#standard-content-blocks) of the response will return the server-side tool calls and results in a provider-agnostic format:\n\n:::python\n```python Invoke with server-side tool use\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"gpt-4.1-mini\")\n\ntool = {\"type\": \"web_search\"}\nmodel_with_tools = model.bind_tools([tool])\n\nresponse = model_with_tools.invoke(\"What was a positive news story from today?\")\nprint(response.content_blocks)\n```\n```python Result expandable\n[\n    {\n        \"type\": \"server_tool_call\",\n        \"name\": \"web_search\",\n        \"args\": {\n            \"query\": \"positive news stories today\",\n            \"type\": \"search\"\n        },\n        \"id\": \"ws_abc123\"\n    },\n    {\n        \"type\": \"server_tool_result\",\n        \"tool_call_id\": \"ws_abc123\",\n        \"status\": \"success\"\n    },\n    {\n        \"type\": \"text\",\n        \"text\": \"Here are some positive news stories from today...\",\n        \"annotations\": [\n            {\n                \"end_index\": 410,\n                \"start_index\": 337,\n                \"title\": \"article title\",\n                \"type\": \"citation\",\n                \"url\": \"...\"\n            }\n        ]\n    }\n]\n```\n:::\n:::js\n```typescript\nimport { initChatModel } from \"langchain\";", "metadata": {"source": "models.mdx"}}
{"text": " from today...\",\n        \"annotations\": [\n            {\n                \"end_index\": 410,\n                \"start_index\": 337,\n                \"title\": \"article title\",\n                \"type\": \"citation\",\n                \"url\": \"...\"\n            }\n        ]\n    }\n]\n```\n:::\n:::js\n```typescript\nimport { initChatModel } from \"langchain\";\n\nconst model = await initChatModel(\"gpt-4.1-mini\");\nconst modelWithTools = model.bindTools([{ type: \"web_search\" }])\n\nconst message = await modelWithTools.invoke(\"What was a positive news story from today?\");\nconsole.log(message.contentBlocks);\n```\n:::\nThis represents a single conversational turn; there are no associated [ToolMessage](/oss/langchain/messages#tool-message) objects that need to be passed in as in client-side [tool-calling](#tool-calling).\n\nSee the [integration page](/oss/integrations/chat) for your given provider for available tools and usage details.\n\n:::python\n### Rate limiting\n\nMany chat model providers impose a limit on the number of invocations that can be made in a given time period. If you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.\n\nTo help manage rate limits, chat model integrations accept a `rate_limiter` parameter that can be provided during initialization to control the rate at which requests are made.\n\n<Accordion title=\"Initialize and use a rate limiter\" icon=\"gauge-high\">\n    LangChain in comes with (an optional) built-in @[`InMemoryRateLimiter`]. This limiter is thread safe and can be shared by multiple threads in the same process.\n\n    ```python Define a rate limiter\n    from langchain_core.rate_limiters import InMemoryRateLimiter\n\n    rate_limiter = InMemoryRateLimiter(\n        requests_per_second=0.1,  # 1 request every 10s\n        check_every_n_seconds=0.1,  # Check every 100ms whether allowed to make a request\n        max_bucket_size=10,  # Controls the maximum burst size.\n    )\n\n    model = init_chat_model(\n        model=\"gpt-5\",\n        model_provider=\"openai\",\n        rate_limiter=rate_limiter  # [!code highlight]\n    )\n    ```\n\n    <Warning>\n        The provided rate limiter can only limit the number of requests per unit time. It will not help if you need to also limit based on the size of the requests.\n    </Warning>\n</Acc", "metadata": {"source": "models.mdx"}}
{"text": "s\n        check_every_n_seconds=0.1,  # Check every 100ms whether allowed to make a request\n        max_bucket_size=10,  # Controls the maximum burst size.\n    )\n\n    model = init_chat_model(\n        model=\"gpt-5\",\n        model_provider=\"openai\",\n        rate_limiter=rate_limiter  # [!code highlight]\n    )\n    ```\n\n    <Warning>\n        The provided rate limiter can only limit the number of requests per unit time. It will not help if you need to also limit based on the size of the requests.\n    </Warning>\n</Accordion>\n:::\n\n### Base URL or proxy\n\nFor many chat model integrations, you can configure the base URL for API requests, which allows you to use model providers that have OpenAI-compatible APIs or to use a proxy server.\n\n<Accordion title=\"Base URL\" icon=\"link\">\n    :::python\n    Many model providers offer OpenAI-compatible APIs (e.g., [Together AI](https://www.together.ai/), [vLLM](https://github.com/vllm-project/vllm)). You can use @[`init_chat_model`] with these providers by specifying the appropriate `base_url` parameter:\n\n    ```python\n    model = init_chat_model(\n        model=\"MODEL_NAME\",\n        model_provider=\"openai\",\n        base_url=\"BASE_URL\",\n        api_key=\"YOUR_API_KEY\",\n    )\n    ```\n    :::\n\n    :::js\n    Many model providers offer OpenAI-compatible APIs (e.g., [Together AI](https://www.together.ai/), [vLLM](https://github.com/vllm-project/vllm)). You can use `initChatModel` with these providers by specifying the appropriate `base_url` parameter:\n\n    ```python\n    model = initChatModel(\n        \"MODEL_NAME\",\n        {\n            modelProvider: \"openai\",\n            baseUrl: \"BASE_URL\",\n            apiKey: \"YOUR_API_KEY\",\n        }\n    )\n    ```\n    :::\n\n    <Note>\n        When using direct chat model class instantiation, the parameter name may vary by provider. Check the respective [reference](/oss/integrations/providers/overview) for details.\n    </Note>\n</Accordion>\n\n:::python\n<Accordion title=\"Proxy configuration\" icon=\"shield\">\n    For deployments requiring HTTP proxies, some model integrations support proxy configuration:\n\n    ```python\n    from langchain_openai import ChatOpenAI\n\n    model = ChatOpenAI", "metadata": {"source": "models.mdx"}}
{"text": "          baseUrl: \"BASE_URL\",\n            apiKey: \"YOUR_API_KEY\",\n        }\n    )\n    ```\n    :::\n\n    <Note>\n        When using direct chat model class instantiation, the parameter name may vary by provider. Check the respective [reference](/oss/integrations/providers/overview) for details.\n    </Note>\n</Accordion>\n\n:::python\n<Accordion title=\"Proxy configuration\" icon=\"shield\">\n    For deployments requiring HTTP proxies, some model integrations support proxy configuration:\n\n    ```python\n    from langchain_openai import ChatOpenAI\n\n    model = ChatOpenAI(\n        model=\"gpt-4.1\",\n        openai_proxy=\"http://proxy.example.com:8080\"\n    )\n    ```\n\n<Note>\n    Proxy support varies by integration. Check the specific model provider's [reference](/oss/integrations/providers/overview) for proxy configuration options.\n</Note>\n\n</Accordion>\n:::\n\n\n### Log probabilities\n\nCertain models can be configured to return token-level log probabilities representing the likelihood of a given token by setting the `logprobs` parameter when initializing the model:\n\n:::python\n```python\nmodel = init_chat_model(\n    model=\"gpt-4.1\",\n    model_provider=\"openai\"\n).bind(logprobs=True)\n\nresponse = model.invoke(\"Why do parrots talk?\")\nprint(response.response_metadata[\"logprobs\"])\n```\n:::\n\n:::js\n```typescript\nconst model = new ChatOpenAI({\n    model: \"gpt-4.1\",\n    logprobs: true,\n});\n\nconst responseMessage = await model.invoke(\"Why do parrots talk?\");\n\nresponseMessage.response_metadata.logprobs.content.slice(0, 5);\n```\n:::\n\n### Token usage\n\nA number of model providers return token usage information as part of the invocation response. When available, this information will be included on the @[`AIMessage`] objects produced by the corresponding model. For more details, see the [messages](/oss/langchain/messages) guide.\n\n<Note>\n    Some provider APIs, notably OpenAI and Azure OpenAI chat completions, require users opt-in to receiving token usage data in streaming contexts. See the [streaming usage metadata](/oss/integrations/chat/openai#streaming-usage-metadata) section of the integration guide for details.\n</Note>\n\n:::python\nYou can track aggregate token counts across models in an application using either a callback or context manager, as shown below:\n\n<Tabs>\n    <Tab title=\"Callback handler\">\n        ```python\n        from langchain.chat_models import init_chat_model\n        from langchain_core.callbacks import UsageMetadataCallbackHandler\n\n        model_1 = init_chat_model(model=\"gpt-4.1-", "metadata": {"source": "models.mdx"}}
{"text": "Note>\n    Some provider APIs, notably OpenAI and Azure OpenAI chat completions, require users opt-in to receiving token usage data in streaming contexts. See the [streaming usage metadata](/oss/integrations/chat/openai#streaming-usage-metadata) section of the integration guide for details.\n</Note>\n\n:::python\nYou can track aggregate token counts across models in an application using either a callback or context manager, as shown below:\n\n<Tabs>\n    <Tab title=\"Callback handler\">\n        ```python\n        from langchain.chat_models import init_chat_model\n        from langchain_core.callbacks import UsageMetadataCallbackHandler\n\n        model_1 = init_chat_model(model=\"gpt-4.1-mini\")\n        model_2 = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n\n        callback = UsageMetadataCallbackHandler()\n        result_1 = model_1.invoke(\"Hello\", config={\"callbacks\": [callback]})\n        result_2 = model_2.invoke(\"Hello\", config={\"callbacks\": [callback]})\n        print(callback.usage_metadata)\n        ```\n        ```python\n        {\n            'gpt-4.1-mini-2025-04-14': {\n                'input_tokens': 8,\n                'output_tokens': 10,\n                'total_tokens': 18,\n                'input_token_details': {'audio': 0, 'cache_read': 0},\n                'output_token_details': {'audio': 0, 'reasoning': 0}\n            },\n            'claude-haiku-4-5-20251001': {\n                'input_tokens': 8,\n                'output_tokens': 21,\n                'total_tokens': 29,\n                'input_token_details': {'cache_read': 0, 'cache_creation': 0}\n            }\n        }\n        ```\n    </Tab>\n    <Tab title=\"Context manager\">\n        ```python\n        from langchain.chat_models import init_chat_model\n        from langchain_core.callbacks import get_usage_metadata_callback\n\n   ", "metadata": {"source": "models.mdx"}}
{"text": " 8,\n                'output_tokens': 21,\n                'total_tokens': 29,\n                'input_token_details': {'cache_read': 0, 'cache_creation': 0}\n            }\n        }\n        ```\n    </Tab>\n    <Tab title=\"Context manager\">\n        ```python\n        from langchain.chat_models import init_chat_model\n        from langchain_core.callbacks import get_usage_metadata_callback\n\n        model_1 = init_chat_model(model=\"gpt-4.1-mini\")\n        model_2 = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n\n        with get_usage_metadata_callback() as cb:\n            model_1.invoke(\"Hello\")\n            model_2.invoke(\"Hello\")\n            print(cb.usage_metadata)\n        ```\n        ```python\n        {\n            'gpt-4.1-mini-2025-04-14': {\n                'input_tokens': 8,\n                'output_tokens': 10,\n                'total_tokens': 18,\n                'input_token_details': {'audio': 0, 'cache_read': 0},\n                'output_token_details': {'audio': 0, 'reasoning': 0}\n            },\n            'claude-haiku-4-5-20251001': {\n                'input_tokens': 8,\n                'output_tokens': 21,\n                'total_tokens': 29,\n                'input_token_details': {'cache_read': 0, 'cache_creation': 0}\n            }\n        }\n        ```\n    </Tab>\n</Tabs>\n:::\n\n### Invocation config\n\n:::python\nWhen invoking a model, you can pass additional configuration through the `config` parameter using a @[`RunnableConfig`] dictionary. This provides run-time control over execution behavior,", "metadata": {"source": "models.mdx"}}
{"text": "      'input_tokens': 8,\n                'output_tokens': 21,\n                'total_tokens': 29,\n                'input_token_details': {'cache_read': 0, 'cache_creation': 0}\n            }\n        }\n        ```\n    </Tab>\n</Tabs>\n:::\n\n### Invocation config\n\n:::python\nWhen invoking a model, you can pass additional configuration through the `config` parameter using a @[`RunnableConfig`] dictionary. This provides run-time control over execution behavior, callbacks, and metadata tracking.\n:::\n\n:::js\nWhen invoking a model, you can pass additional configuration through the `config` parameter using a @[`RunnableConfig`] object. This provides run-time control over execution behavior, callbacks, and metadata tracking.\n:::\n\nCommon configuration options include:\n\n:::python\n```python Invocation with config\nresponse = model.invoke(\n    \"Tell me a joke\",\n    config={\n        \"run_name\": \"joke_generation\",      # Custom name for this run\n        \"tags\": [\"humor\", \"demo\"],          # Tags for categorization\n        \"metadata\": {\"user_id\": \"123\"},     # Custom metadata\n        \"callbacks\": [my_callback_handler], # Callback handlers\n    }\n)\n```\n:::\n\n:::js\n```typescript Invocation with config\nconst response = await model.invoke(\n    \"Tell me a joke\",\n    {\n        runName: \"joke_generation\",      // Custom name for this run\n        tags: [\"humor\", \"demo\"],          // Tags for categorization\n        metadata: {\"user_id\": \"123\"},     // Custom metadata\n        callbacks: [my_callback_handler], // Callback handlers\n    }\n)\n```\n:::\n\nThese configuration values are particularly useful when:\n- Debugging with [LangSmith](https://docs.langchain.com/langsmith/home) tracing\n- Implementing custom logging or monitoring\n- Controlling resource usage in production\n- Tracking invocations across complex pipelines\n\n:::python\n<Accordion title=\"Key configuration attributes\">\n    <ParamField body=\"run_name\" type=\"string\">\n        Identifies this specific invocation in logs and traces. Not inherited by sub-calls.\n    </ParamField>\n\n    <ParamField body=\"tags\" type=\"string[]\">\n        Labels inherited by all sub-calls for filtering and organization in debugging tools.\n    </ParamField>\n\n    <ParamField body=\"metadata\" type=\"object\">\n        Custom key-value pairs for tracking additional context,", "metadata": {"source": "models.mdx"}}
{"text": " when:\n- Debugging with [LangSmith](https://docs.langchain.com/langsmith/home) tracing\n- Implementing custom logging or monitoring\n- Controlling resource usage in production\n- Tracking invocations across complex pipelines\n\n:::python\n<Accordion title=\"Key configuration attributes\">\n    <ParamField body=\"run_name\" type=\"string\">\n        Identifies this specific invocation in logs and traces. Not inherited by sub-calls.\n    </ParamField>\n\n    <ParamField body=\"tags\" type=\"string[]\">\n        Labels inherited by all sub-calls for filtering and organization in debugging tools.\n    </ParamField>\n\n    <ParamField body=\"metadata\" type=\"object\">\n        Custom key-value pairs for tracking additional context, inherited by all sub-calls.\n    </ParamField>\n\n    <ParamField body=\"max_concurrency\" type=\"number\">\n        Controls the maximum number of parallel calls when using @[`batch()`][BaseChatModel.batch] or @[`batch_as_completed()`][BaseChatModel.batch_as_completed].\n    </ParamField>\n\n    <ParamField body=\"callbacks\" type=\"array\">\n        Handlers for monitoring and responding to events during execution.\n    </ParamField>\n\n    <ParamField body=\"recursion_limit\" type=\"number\">\n        Maximum recursion depth for chains to prevent infinite loops in complex pipelines.\n    </ParamField>\n</Accordion>\n:::\n\n:::js\n<Accordion title=\"Key configuration attributes\">\n    <ParamField body=\"runName\" type=\"string\">\n        Identifies this specific invocation in logs and traces. Not inherited by sub-calls.\n    </ParamField>\n\n    <ParamField body=\"tags\" type=\"string[]\">\n        Labels inherited by all sub-calls for filtering and organization in debugging tools.\n    </ParamField>\n\n    <ParamField body=\"metadata\" type=\"object\">\n        Custom key-value pairs for tracking additional context, inherited by all sub-calls.\n    </ParamField>\n\n    <ParamField body=\"maxConcurrency\" type=\"number\">\n        Controls the maximum number of parallel calls when using `batch()`.\n    </ParamField>\n\n    <ParamField body=\"callbacks\" type=\"CallbackHandler[]\">\n        Handlers for monitoring and responding to events during execution.\n    </ParamField>\n\n    <ParamField body=\"recursion_limit\" type=\"number\">\n        Maximum recursion depth for chains to prevent infinite loops in complex pipelines.\n    </ParamField>\n</Accordion>\n:::\n\n<Tip>\n    See full @[`RunnableConfig`] reference for all supported attributes.\n</Tip>\n\n:::python\n### Configurable models\n\nYou can also create a runtime-configurable model by specifying @[`configurable_fields`][BaseChatModel.configurable_fields]. If you don't specify a model value, then `'model'` and `'model_", "metadata": {"source": "models.mdx"}}
{"text": "   </ParamField>\n\n    <ParamField body=\"callbacks\" type=\"CallbackHandler[]\">\n        Handlers for monitoring and responding to events during execution.\n    </ParamField>\n\n    <ParamField body=\"recursion_limit\" type=\"number\">\n        Maximum recursion depth for chains to prevent infinite loops in complex pipelines.\n    </ParamField>\n</Accordion>\n:::\n\n<Tip>\n    See full @[`RunnableConfig`] reference for all supported attributes.\n</Tip>\n\n:::python\n### Configurable models\n\nYou can also create a runtime-configurable model by specifying @[`configurable_fields`][BaseChatModel.configurable_fields]. If you don't specify a model value, then `'model'` and `'model_provider'` will be configurable by default.\n\n```python\nfrom langchain.chat_models import init_chat_model\n\nconfigurable_model = init_chat_model(temperature=0)\n\nconfigurable_model.invoke(\n    \"what's your name\",\n    config={\"configurable\": {\"model\": \"gpt-5-nano\"}},  # Run with GPT-5-Nano\n)\nconfigurable_model.invoke(\n    \"what's your name\",\n    config={\"configurable\": {\"model\": \"claude-sonnet-4-5-20250929\"}},  # Run with Claude\n)\n```\n\n<Accordion title=\"Configurable model with default values\">\n    We can create a configurable model with default model values, specify which parameters are configurable, and add prefixes to configurable params:\n\n    ```python\n    first_model = init_chat_model(\n            model=\"gpt-4.1-mini\",\n            temperature=0,\n            configurable_fields=(\"model\", \"model_provider\", \"temperature\", \"max_tokens\"),\n            config_prefix=\"first\",  # Useful when you have a chain with multiple models\n    )\n\n    first_model.invoke(\"what's your name\")\n    ```\n\n    ```python\n    first_model.invoke(\n        \"what's your name\",\n        config={\n            \"configurable\": {\n                \"first_model\": \"claude-sonnet-4-5-20250929\",\n                \"first_temperature\": 0.5,\n                \"first_max_tokens\": 100,\n            }\n        },\n    )\n    ```\n\n    See the @[`init_chat_model`] reference for more details on `configurable_fields` and `config_prefix`.\n</Accordion>\n\n<Accordion title=\"Using a configurable model declaratively\">\n", "metadata": {"source": "models.mdx"}}
{"text": " config={\n            \"configurable\": {\n                \"first_model\": \"claude-sonnet-4-5-20250929\",\n                \"first_temperature\": 0.5,\n                \"first_max_tokens\": 100,\n            }\n        },\n    )\n    ```\n\n    See the @[`init_chat_model`] reference for more details on `configurable_fields` and `config_prefix`.\n</Accordion>\n\n<Accordion title=\"Using a configurable model declaratively\">\n    We can call declarative operations like `bind_tools`, `with_structured_output`, `with_configurable`, etc. on a configurable model and chain a configurable model in the same way that we would a regularly instantiated chat model object.\n\n    ```python\n    from pydantic import BaseModel, Field\n\n\n    class GetWeather(BaseModel):\n        \"\"\"Get the current weather in a given location\"\"\"\n\n            location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n\n    class GetPopulation(BaseModel):\n        \"\"\"Get the current population in a given location\"\"\"\n\n            location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n\n    model = init_chat_model(temperature=0)\n    model_with_tools = model.bind_tools([GetWeather, GetPopulation])\n\n    model_with_tools.invoke(\n        \"what's bigger in 2024 LA or NYC\", config={\"configurable\": {\"model\": \"gpt-4.1-mini\"}}\n    ).tool_calls\n    ```\n    ```\n    [\n        {\n            'name': 'GetPopulation',\n            'args': {'location': 'Los Angeles, CA'},\n            'id': 'call_Ga9m8FAArIyEjItHmztPYA22',\n            'type': 'tool_call'\n        },\n        {\n            'name': 'GetPopulation',\n            'args': {'location': 'New York, NY'},\n            'id': 'call_jh2dEvBaAHRaw5JUDthOs7rt',\n            'type': 'tool_call'\n        }\n    ]\n    ```\n    ```python\n    model_with_tools.invoke(\n     ", "metadata": {"source": "models.mdx"}}
{"text": "m8FAArIyEjItHmztPYA22',\n            'type': 'tool_call'\n        },\n        {\n            'name': 'GetPopulation',\n            'args': {'location': 'New York, NY'},\n            'id': 'call_jh2dEvBaAHRaw5JUDthOs7rt',\n            'type': 'tool_call'\n        }\n    ]\n    ```\n    ```python\n    model_with_tools.invoke(\n        \"what's bigger in 2024 LA or NYC\",\n        config={\"configurable\": {\"model\": \"claude-sonnet-4-5-20250929\"}},\n    ).tool_calls\n    ```\n    ```\n    [\n        {\n            'name': 'GetPopulation',\n            'args': {'location': 'Los Angeles, CA'},\n            'id': 'toolu_01JMufPf4F4t2zLj7miFeqXp',\n            'type': 'tool_call'\n        },\n        {\n            'name': 'GetPopulation',\n            'args': {'location': 'New York City, NY'},\n            'id': 'toolu_01RQBHcE8kEEbYTuuS8WqY1u',\n            'type': 'tool_call'\n        }\n    ]\n    ```\n</Accordion>\n:::\n", "metadata": {"source": "models.mdx"}}
{"text": "---\ntitle: Component architecture\n---\n\nLangChain's power comes from how its components work together to create sophisticated AI applications. This page provides diagrams showcasing the relationships between different components.\n\n## Core component ecosystem\n\nThe diagram below shows how LangChain's major components connect to form complete AI applications:\n\n```mermaid\ngraph TD\n    %% Input processing\n    subgraph \"\ud83d\udce5 Input processing\"\n        A[Text input] --> B[Document loaders]\n        B --> C[Text splitters]\n        C --> D[Documents]\n    end\n\n    %% Embedding & storage\n    subgraph \"\ud83d\udd22 Embedding & storage\"\n        D --> E[Embedding models]\n        E --> F[Vectors]\n        F --> G[(Vector stores)]\n    end\n\n    %% Retrieval\n    subgraph \"\ud83d\udd0d Retrieval\"\n        H[User Query] --> I[Embedding models]\n        I --> J[Query vector]\n        J --> K[Retrievers]\n        K --> G\n        G --> L[Relevant context]\n    end\n\n    %% Generation\n    subgraph \"\ud83e\udd16 Generation\"\n        M[Chat models] --> N[Tools]\n        N --> O[Tool results]\n        O --> M\n        L --> M\n        M --> P[AI response]\n    end\n\n    %% Orchestration\n    subgraph \"\ud83c\udfaf Orchestration\"\n        Q[Agents] --> M\n        Q --> N\n        Q --> K\n        Q --> R[Memory]\n    end\n```\n\n### How components connect\n\nEach component layer builds on the previous ones:\n\n1. **Input processing** \u2013 Transform raw data into structured documents\n2. **Embedding & storage** \u2013 Convert text into searchable vector representations\n3. **Retrieval** \u2013 Find relevant information based on user queries\n4. **Generation** \u2013 Use AI models to create responses, optionally with tools\n5. **Orchestration** \u2013 Coordinate everything through agents and memory systems\n\n## Component categories\n\nLangChain organizes components into these main categories:\n\n| Category | Purpose | Key Components | Use Cases |\n|----------|---------|---------------|-----------|\n| **[Models](/oss/langchain/models)** | AI reasoning and generation | Chat models, LLMs, Embedding models | Text generation, reasoning, semantic understanding |\n| **[Tools](/oss/langchain/tools)** | External capabilities | APIs, databases, etc. | Web search, data access, computations |\n| **[Agents](/oss/langchain/agents)** | Orchestration and reasoning | ReAct agents, tool calling agents | Nondeterministic workflows, decision making |\n| **[Memory](/oss/langchain/short-term-memory)** | Context preservation | Message history, custom state | Conversations, stateful interactions |", "metadata": {"source": "component-architecture.mdx"}}
{"text": "inate everything through agents and memory systems\n\n## Component categories\n\nLangChain organizes components into these main categories:\n\n| Category | Purpose | Key Components | Use Cases |\n|----------|---------|---------------|-----------|\n| **[Models](/oss/langchain/models)** | AI reasoning and generation | Chat models, LLMs, Embedding models | Text generation, reasoning, semantic understanding |\n| **[Tools](/oss/langchain/tools)** | External capabilities | APIs, databases, etc. | Web search, data access, computations |\n| **[Agents](/oss/langchain/agents)** | Orchestration and reasoning | ReAct agents, tool calling agents | Nondeterministic workflows, decision making |\n| **[Memory](/oss/langchain/short-term-memory)** | Context preservation | Message history, custom state | Conversations, stateful interactions |\n| **[Retrievers](/oss/integrations/retrievers)** | Information access | Vector retrievers, web retrievers | RAG, knowledge base search |\n| **[Document processing](/oss/integrations/document_loaders)** | Data ingestion | Loaders, splitters, transformers | PDF processing, web scraping |\n| **[Vector Stores](/oss/integrations/vectorstores)** | Semantic search | Chroma, Pinecone, FAISS | Similarity search, embeddings storage |\n\n## Common patterns\n\n### RAG (Retrieval-Augmented generation)\n```mermaid\ngraph LR\n    A[User question] --> B[Retriever]\n    B --> C[Relevant docs]\n    C --> D[Chat model]\n    A --> D\n    D --> E[Informed response]\n```\n\n### Agent with tools\n```mermaid\ngraph LR\n    A[User request] --> B[Agent]\n    B --> C{Need tool?}\n    C -->|Yes| D[Call tool]\n    D --> E[Tool result]\n    E --> B\n    C -->|No| F[Final answer]\n```\n\n### Multi-agent system\n```mermaid\ngraph LR\n    A[Complex Task] --> B[Supervisor agent]\n    B --> C[Specialist agent 1]\n    B --> D[Specialist agent 2]\n    C --> E[Results]\n    D --> E\n    E --> B\n    B --> F[Coordinated response]\n```\n\n## Learn more\n\nNow that you understand how components relate to each other, explore specific areas:\n\n- [Building your first RAG system](/oss/langchain/knowledge-base)\n- [Creating agents](/oss/langchain/agents)\n- [Working with tools](/oss/langchain/tools)\n- [Setting up memory](/oss/langchain/short-term-memory)\n- [Browse integrations](/oss/integrations/providers/overview)\n", "metadata": {"source": "component-architecture.mdx"}}
{"text": "---\ntitle: Runtime\n---\n\n## Overview\n\n:::python\nLangChain's @[`create_agent`] runs on LangGraph's runtime under the hood.\n:::\n:::js\nLangChain's `createAgent` runs on LangGraph's runtime under the hood.\n:::\nLangGraph exposes a @[`Runtime`] object with the following information:\n\n1. **Context**: static information like user id, db connections, or other dependencies for an agent invocation\n2. **Store**: a @[BaseStore] instance used for [long-term memory](/oss/langchain/long-term-memory)\n3. **Stream writer**: an object used for streaming information via the `\"custom\"` stream mode\n\n:::python\n<Tip>\nRuntime context provides **dependency injection** for your tools and middleware. Instead of hardcoding values or using global state, you can inject runtime dependencies (like database connections, user IDs, or configuration) when invoking your agent. This makes your tools more testable, reusable, and flexible.\n</Tip>\n:::\n\n:::js\n<Tip>\nThe runtime context is how you thread data through your agent. Rather than storing things in global state, you can attach values \u2014 like a database connection, user session, or configuration \u2014 to the context and access them inside tools and middleware. This keeps things stateless, testable, and reusable.\n</Tip>\n:::\n\nYou can access the runtime information within [tools](#inside-tools) and [middleware](#inside-middleware).\n\n## Access\n\n:::python\nWhen creating an agent with @[`create_agent`], you can specify a `context_schema` to define the structure of the `context` stored in the agent @[`Runtime`].\n:::\n:::js\nWhen creating an agent with `createAgent`, you can specify a `contextSchema` to define the structure of the `context` stored in the agent @[`Runtime`].\n:::\n\nWhen invoking the agent, pass the `context` argument with the relevant configuration for the run:\n\n:::python\n```python\nfrom dataclasses import dataclass\n\nfrom langchain.agents import create_agent\n\n\n@dataclass\nclass Context:\n    user_name: str\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[...],\n    context_schema=Context  # [!code highlight]\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my name?\"}]},\n    context=Context(user_name=\"John Smith\")  # [!code highlight]\n)\n```\n:::\n:::js\n```ts\nimport * as z from \"zod\";\nimport { createAgent } from \"langchain\";\n\nconst contextSchema = z.object({ // [!code highlight]\n  userName: z.string(), // [!code highlight]\n}); // [!code highlight]\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [\n    /* ... */\n  ],\n  contextSchema, // [!code highlight]\n});\n\nconst result = await agent.invoke(\n  { messages: [{ role: \"user\", content: \"What's my name?\" }] },\n  { context: { userName: \"John Smith\" } } // [!code", "metadata": {"source": "runtime.mdx"}}
{"text": "\n    context=Context(user_name=\"John Smith\")  # [!code highlight]\n)\n```\n:::\n:::js\n```ts\nimport * as z from \"zod\";\nimport { createAgent } from \"langchain\";\n\nconst contextSchema = z.object({ // [!code highlight]\n  userName: z.string(), // [!code highlight]\n}); // [!code highlight]\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [\n    /* ... */\n  ],\n  contextSchema, // [!code highlight]\n});\n\nconst result = await agent.invoke(\n  { messages: [{ role: \"user\", content: \"What's my name?\" }] },\n  { context: { userName: \"John Smith\" } } // [!code highlight]\n);\n```\n:::\n\n### Inside tools\n\nYou can access the runtime information inside tools to:\n\n* Access the context\n* Read or write long-term memory\n* Write to the [custom stream](/oss/langchain/streaming#custom-updates) (ex, tool progress / updates)\n\n:::python\nUse the `ToolRuntime` parameter to access the @[`Runtime`] object inside a tool.\n\n```python\nfrom dataclasses import dataclass\nfrom langchain.tools import tool, ToolRuntime  # [!code highlight]\n\n@dataclass\nclass Context:\n    user_id: str\n\n@tool\ndef fetch_user_email_preferences(runtime: ToolRuntime[Context]) -> str:  # [!code highlight]\n    \"\"\"Fetch the user's email preferences from the store.\"\"\"\n    user_id = runtime.context.user_id  # [!code highlight]\n\n    preferences: str = \"The user prefers you to write a brief and polite email.\"\n    if runtime.store:  # [!code highlight]\n        if memory := runtime.store.get((\"users\",), user_id):  # [!code highlight]\n            preferences = memory.value[\"preferences\"]\n\n    return preferences\n```\n:::\n:::js\nUse the `runtime` parameter to access the @[`Runtime`] object inside a tool.\n\n```ts\nimport * as z from \"zod\";\nimport { tool } from \"langchain\";\nimport { type ToolRuntime } from \"@langchain/core/tools\"; // [!code highlight]\n\nconst contextSchema = z.object({\n  userName: z.string(),\n});\n\nconst fetchUserEmailPreferences = tool(\n  async (_, runtime: ToolRuntime<any, typeof contextSchema>) => { // [!code highlight]\n    const userName = runtime.context?.userName; // [!code highlight]\n    if (!userName) {\n      throw new Error(\"userName is required\");\n    }\n\n    let preferences = \"The user prefers you to write a brief and polite email.\";\n    if (runtime.store) { // [!code highlight]\n      const memory = await runtime.store?.get([\"users\"], userName); // [!code highlight]\n      if (memory) {\n        preferences = memory.value", "metadata": {"source": "runtime.mdx"}}
{"text": "const contextSchema = z.object({\n  userName: z.string(),\n});\n\nconst fetchUserEmailPreferences = tool(\n  async (_, runtime: ToolRuntime<any, typeof contextSchema>) => { // [!code highlight]\n    const userName = runtime.context?.userName; // [!code highlight]\n    if (!userName) {\n      throw new Error(\"userName is required\");\n    }\n\n    let preferences = \"The user prefers you to write a brief and polite email.\";\n    if (runtime.store) { // [!code highlight]\n      const memory = await runtime.store?.get([\"users\"], userName); // [!code highlight]\n      if (memory) {\n        preferences = memory.value.preferences;\n      }\n    }\n    return preferences;\n  },\n  {\n    name: \"fetch_user_email_preferences\",\n    description: \"Fetch the user's email preferences.\",\n    schema: z.object({}),\n  }\n);\n```\n:::\n\n### Inside middleware\n\nYou can access runtime information in middleware to create dynamic prompts, modify messages, or control agent behavior based on user context.\n\n:::python\nUse the `Runtime` parameter to access the @[`Runtime`] object inside [node-style hooks](/oss/langchain/middleware/custom#node-style-hooks).  For [wrap-style hooks](/oss/langchain/middleware/custom#wrap-style-hooks), the `Runtime` object is available inside the @[`ModelRequest`] parameter.\n\n```python\nfrom dataclasses import dataclass\n\nfrom langchain.messages import AnyMessage\nfrom langchain.agents import create_agent, AgentState\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest, before_model, after_model\nfrom langgraph.runtime import Runtime\n\n\n@dataclass\nclass Context:\n    user_name: str\n\n# Dynamic prompts\n@dynamic_prompt\ndef dynamic_system_prompt(request: ModelRequest) -> str:\n    user_name = request.runtime.context.user_name  # [!code highlight]\n    system_prompt = f\"You are a helpful assistant. Address the user as {user_name}.\"\n    return system_prompt\n\n# Before model hook\n@before_model\ndef log_before_model(state: AgentState, runtime: Runtime[Context]) -> dict | None:  # [!code highlight]\n    print(f\"Processing request for user: {runtime.context.user_name}\")  # [!code highlight]\n    return None\n\n# After model hook\n@after_model\ndef log_after_model(state: AgentState, runtime: Runtime[Context]) -> dict | None:  # [!code highlight]\n    print(f\"Completed request for user: {runtime.context.user_name}\")  # [!code highlight]\n    return None\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[...],\n    middleware=[dynamic_system_prompt, log_before_model, log_after", "metadata": {"source": "runtime.mdx"}}
{"text": "\ndef log_before_model(state: AgentState, runtime: Runtime[Context]) -> dict | None:  # [!code highlight]\n    print(f\"Processing request for user: {runtime.context.user_name}\")  # [!code highlight]\n    return None\n\n# After model hook\n@after_model\ndef log_after_model(state: AgentState, runtime: Runtime[Context]) -> dict | None:  # [!code highlight]\n    print(f\"Completed request for user: {runtime.context.user_name}\")  # [!code highlight]\n    return None\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[...],\n    middleware=[dynamic_system_prompt, log_before_model, log_after_model],  # [!code highlight]\n    context_schema=Context\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my name?\"}]},\n    context=Context(user_name=\"John Smith\")\n)\n```\n:::\n:::js\nUse the `runtime` parameter to access the @[`Runtime`] object inside middleware.\n\n```ts\nimport * as z from \"zod\";\nimport { createAgent, createMiddleware, SystemMessage } from \"langchain\";\n\nconst contextSchema = z.object({\n  userName: z.string(),\n});\n\n// Dynamic prompt middleware\nconst dynamicPromptMiddleware = createMiddleware({\n  name: \"DynamicPrompt\",\n  contextSchema,\n  beforeModel: (state, runtime) => { // [!code highlight]\n    const userName = runtime.context?.userName; // [!code highlight]\n    if (!userName) {\n      throw new Error(\"userName is required\");\n    }\n\n    const systemMsg = `You are a helpful assistant. Address the user as ${userName}.`;\n    return {\n      messages: [new SystemMessage(systemMsg), ...state.messages],\n    };\n  },\n});\n\n// Logging middleware\nconst loggingMiddleware = createMiddleware({\n  name: \"Logging\",\n  contextSchema,\n  beforeModel: (state, runtime) => {  // [!code highlight]\n    console.log(`Processing request for user: ${runtime.context?.userName}`);  // [!code highlight]\n    return;\n  },\n  afterModel: (state, runtime) => {  // [!code highlight]\n    console.log(`Completed request for user: ${runtime.context?.userName}`);  // [!code highlight]\n    return;\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [\n    /* ... */\n  ],\n  middleware: [dynamicPromptMiddleware, loggingMiddleware],  // [!code highlight]\n  contextSchema,\n});\n\nconst result = await agent.invoke(\n  { messages: [{ role: \"user\", content: \"What's my name?\" }] },\n  { context: { userName: \"John Smith\" } }\n);\n\n```", "metadata": {"source": "runtime.mdx"}}
{"text": ");  // [!code highlight]\n    return;\n  },\n  afterModel: (state, runtime) => {  // [!code highlight]\n    console.log(`Completed request for user: ${runtime.context?.userName}`);  // [!code highlight]\n    return;\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [\n    /* ... */\n  ],\n  middleware: [dynamicPromptMiddleware, loggingMiddleware],  // [!code highlight]\n  contextSchema,\n});\n\nconst result = await agent.invoke(\n  { messages: [{ role: \"user\", content: \"What's my name?\" }] },\n  { context: { userName: \"John Smith\" } }\n);\n\n```\n:::\n", "metadata": {"source": "runtime.mdx"}}
{"text": "---\ntitle: Long-term memory\n---\n\n## Overview\n\nLangChain agents use [LangGraph persistence](/oss/langgraph/persistence#memory-store) to enable long-term memory. This is a more advanced topic and requires knowledge of LangGraph to use.\n\n\n## Memory storage\n\nLangGraph stores long-term memories as JSON documents in a [store](/oss/langgraph/persistence#memory-store).\n\nEach memory is organized under a custom `namespace` (similar to a folder) and a distinct `key` (like a file name). Namespaces often include user or org IDs or other labels that makes it easier to organize information.\n\nThis structure enables hierarchical organization of memories. Cross-namespace searching is then supported through content filters.\n\n:::python\n```python\nfrom langgraph.store.memory import InMemoryStore\n\n\ndef embed(texts: list[str]) -> list[list[float]]:\n    # Replace with an actual embedding function or LangChain embeddings object\n    return [[1.0, 2.0] * len(texts)]\n\n\n# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use.\nstore = InMemoryStore(index={\"embed\": embed, \"dims\": 2}) # [!code highlight]\nuser_id = \"my-user\"\napplication_context = \"chitchat\"\nnamespace = (user_id, application_context) # [!code highlight]\nstore.put( # [!code highlight]\n    namespace,\n    \"a-memory\",\n    {\n        \"rules\": [\n            \"User likes short, direct language\",\n            \"User only speaks English & python\",\n        ],\n        \"my-key\": \"my-value\",\n    },\n)\n# get the \"memory\" by ID\nitem = store.get(namespace, \"a-memory\") # [!code highlight]\n# search for \"memories\" within this namespace, filtering on content equivalence, sorted by vector similarity\nitems = store.search( # [!code highlight]\n    namespace, filter={\"my-key\": \"my-value\"}, query=\"language preferences\"\n)\n```\n:::\n\n:::js\n```typescript\nimport { InMemoryStore } from \"@langchain/langgraph\";\n\nconst embed = (texts: string[]): number[][] => {\n    // Replace with an actual embedding function or LangChain embeddings object\n    return texts.map(() => [1.0, 2.0]);\n};\n\n// InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use.\nconst store = new InMemoryStore({ index: { embed, dims: 2 } }); // [!code highlight]\nconst userId = \"my-user\";\nconst applicationContext = \"chitchat\";\nconst namespace = [userId, applicationContext]; // [!code highlight]\n\nawait store.put( // [!code highlight]\n    namespace,\n    \"a-memory\",\n    {\n        rules: [\n            \"User likes short, direct language\",\n        ", "metadata": {"source": "long-term-memory.mdx"}}
{"text": "  // Replace with an actual embedding function or LangChain embeddings object\n    return texts.map(() => [1.0, 2.0]);\n};\n\n// InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use.\nconst store = new InMemoryStore({ index: { embed, dims: 2 } }); // [!code highlight]\nconst userId = \"my-user\";\nconst applicationContext = \"chitchat\";\nconst namespace = [userId, applicationContext]; // [!code highlight]\n\nawait store.put( // [!code highlight]\n    namespace,\n    \"a-memory\",\n    {\n        rules: [\n            \"User likes short, direct language\",\n            \"User only speaks English & TypeScript\",\n        ],\n        \"my-key\": \"my-value\",\n    }\n);\n\n// get the \"memory\" by ID\nconst item = await store.get(namespace, \"a-memory\"); // [!code highlight]\n\n// search for \"memories\" within this namespace, filtering on content equivalence, sorted by vector similarity\nconst items = await store.search( // [!code highlight]\n    namespace,\n    {\n        filter: { \"my-key\": \"my-value\" },\n        query: \"language preferences\"\n    }\n);\n```\n:::\n\nFor more information about the memory store, see the [Persistence](/oss/langgraph/persistence#memory-store) guide.\n\n## Read long-term memory in tools\n\n:::python\n```python A tool the agent can use to look up user information\nfrom dataclasses import dataclass\n\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool, ToolRuntime\nfrom langgraph.store.memory import InMemoryStore\n\n\n@dataclass\nclass Context:\n    user_id: str\n\n# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production.\nstore = InMemoryStore() # [!code highlight]\n\n# Write sample data to the store using the put method\nstore.put( # [!code highlight]\n    (\"users\",),  # Namespace to group related data together (users namespace for user data)\n    \"user_123\",  # Key within the namespace (user ID as key)\n    {\n        \"name\": \"John Smith\",\n        \"language\": \"English\",\n    }  # Data to store for the given user\n)\n\n@tool\ndef get_user_info(runtime: ToolRuntime[Context]) -> str:\n    \"\"\"Look up user info.\"\"\"\n    # Access the store - same as that provided to `create_agent`\n    store = runtime.store # [!code highlight]\n    user_id = runtime.context.user_id\n    # Retrieve data from store - returns StoreValue object with value and metadata\n    user_info = store.get((\"users\",), user_id) # [!code highlight]\n ", "metadata": {"source": "long-term-memory.mdx"}}
{"text": "\n    \"user_123\",  # Key within the namespace (user ID as key)\n    {\n        \"name\": \"John Smith\",\n        \"language\": \"English\",\n    }  # Data to store for the given user\n)\n\n@tool\ndef get_user_info(runtime: ToolRuntime[Context]) -> str:\n    \"\"\"Look up user info.\"\"\"\n    # Access the store - same as that provided to `create_agent`\n    store = runtime.store # [!code highlight]\n    user_id = runtime.context.user_id\n    # Retrieve data from store - returns StoreValue object with value and metadata\n    user_info = store.get((\"users\",), user_id) # [!code highlight]\n    return str(user_info.value) if user_info else \"Unknown user\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[get_user_info],\n    # Pass store to agent - enables agent to access store when running tools\n    store=store, # [!code highlight]\n    context_schema=Context\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"look up user information\"}]},\n    context=Context(user_id=\"user_123\") # [!code highlight]\n)\n```\n\n:::\n\n:::js\n```typescript A tool the agent can use to look up user information\nimport * as z from \"zod\";\nimport { createAgent, tool, type ToolRuntime } from \"langchain\";\nimport { InMemoryStore } from \"@langchain/langgraph\";\n\n// InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production.\nconst store = new InMemoryStore(); // [!code highlight]\nconst contextSchema = z.object({\n  userId: z.string(),\n});\n\n// Write sample data to the store using the put method\nawait store.put( // [!code highlight]\n  [\"users\"], // Namespace to group related data together (users namespace for user data)\n  \"user_123\", // Key within the namespace (user ID as key)\n  {\n    name: \"John Smith\",\n    language: \"English\",\n  } // Data to store for the given user\n);\n\nconst getUserInfo = tool(\n  // Look up user info.\n  async (_, runtime: ToolRuntime<unknown, z.infer<typeof contextSchema>>) => {\n    // Access the store - same as that provided to `createAgent`\n    const userId = runtime.context.userId;\n    if (!userId) {\n      throw new Error(\"userId is required\");\n    }\n    // Retrieve data from store - returns StoreValue object with value and metadata\n    const userInfo = await runtime.store.get([\"users\"], userId);\n    return userInfo?.value ? JSON.stringify(userInfo.value) : \"Unknown user\";\n  },\n  {\n    name: \"getUserInfo\",\n    description: \"Look up user info by userId", "metadata": {"source": "long-term-memory.mdx"}}
{"text": " getUserInfo = tool(\n  // Look up user info.\n  async (_, runtime: ToolRuntime<unknown, z.infer<typeof contextSchema>>) => {\n    // Access the store - same as that provided to `createAgent`\n    const userId = runtime.context.userId;\n    if (!userId) {\n      throw new Error(\"userId is required\");\n    }\n    // Retrieve data from store - returns StoreValue object with value and metadata\n    const userInfo = await runtime.store.get([\"users\"], userId);\n    return userInfo?.value ? JSON.stringify(userInfo.value) : \"Unknown user\";\n  },\n  {\n    name: \"getUserInfo\",\n    description: \"Look up user info by userId from the store.\",\n    schema: z.object({}),\n  }\n);\n\nconst agent = createAgent({\n  model: \"gpt-4.1-mini\",\n  tools: [getUserInfo],\n  contextSchema,\n  // Pass store to agent - enables agent to access store when running tools\n  store, // [!code highlight]\n});\n\n// Run the agent\nconst result = await agent.invoke(\n  { messages: [{ role: \"user\", content: \"look up user information\" }] },\n  { context: { userId: \"user_123\" } } // [!code highlight]\n);\n\nconsole.log(result.messages.at(-1)?.content);\n\n/**\n * Outputs:\n * User Information:\n * - Name: John Smith\n * - Language: English\n */\n```\n\n:::\n\n<a id=\"write-long-term\"></a>\n## Write long-term memory from tools\n\n:::python\n```python Example of a tool that updates user information\nfrom dataclasses import dataclass\nfrom typing_extensions import TypedDict\n\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool, ToolRuntime\nfrom langgraph.store.memory import InMemoryStore\n\n\n# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production.\nstore = InMemoryStore() # [!code highlight]\n\n@dataclass\nclass Context:\n    user_id: str\n\n# TypedDict defines the structure of user information for the LLM\nclass UserInfo(TypedDict):\n    name: str\n\n# Tool that allows agent to update user information (useful for chat applications)\n@tool\ndef save_user_info(user_info: UserInfo, runtime: ToolRuntime[Context]) -> str:\n    \"\"\"Save user info.\"\"\"\n    # Access the store - same as that provided to `create_agent`\n    store = runtime.store # [!code highlight]\n    user_id = runtime.context.user_id # [!code highlight]\n    # Store data in the store (namespace, key, data)\n    store.put((\"users\",), user_id, user_info) # [!code highlight]\n    return \"Successfully saved user info.\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[save_user_info],\n    store=", "metadata": {"source": "long-term-memory.mdx"}}
{"text": " chat applications)\n@tool\ndef save_user_info(user_info: UserInfo, runtime: ToolRuntime[Context]) -> str:\n    \"\"\"Save user info.\"\"\"\n    # Access the store - same as that provided to `create_agent`\n    store = runtime.store # [!code highlight]\n    user_id = runtime.context.user_id # [!code highlight]\n    # Store data in the store (namespace, key, data)\n    store.put((\"users\",), user_id, user_info) # [!code highlight]\n    return \"Successfully saved user info.\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[save_user_info],\n    store=store, # [!code highlight]\n    context_schema=Context\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"My name is John Smith\"}]},\n    # user_id passed in context to identify whose information is being updated\n    context=Context(user_id=\"user_123\") # [!code highlight]\n)\n\n# You can access the store directly to get the value\nstore.get((\"users\",), \"user_123\").value\n```\n\n:::\n\n:::js\n```typescript Example of a tool that updates user information\nimport * as z from \"zod\";\nimport { tool, createAgent, type ToolRuntime } from \"langchain\";\nimport { InMemoryStore } from \"@langchain/langgraph\";\n\n// InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production.\nconst store = new InMemoryStore(); // [!code highlight]\n\nconst contextSchema = z.object({\n  userId: z.string(),\n});\n\n// Schema defines the structure of user information for the LLM\nconst UserInfo = z.object({\n  name: z.string(),\n});\n\n// Tool that allows agent to update user information (useful for chat applications)\nconst saveUserInfo = tool(\n  async (\n    userInfo: z.infer<typeof UserInfo>,\n    runtime: ToolRuntime<unknown, z.infer<typeof contextSchema>>\n  ) => {\n    const userId = runtime.context.userId;\n    if (!userId) {\n      throw new Error(\"userId is required\");\n    }\n    // Store data in the store (namespace, key, data)\n    await runtime.store.put([\"users\"], userId, userInfo);\n    return \"Successfully saved user info.\";\n  },\n  {\n    name: \"save_user_info\",\n    description: \"Save user info\",\n    schema: UserInfo,\n  }\n);\n\nconst agent = createAgent({\n  model: \"gpt-4.1-mini\",\n  tools: [saveUserInfo],\n  contextSchema,\n  store, // [!code highlight]\n});\n\n// Run the agent\nawait agent.invoke(\n  { messages: [{ role: \"user\", content: \"My name is John Smith\" }] },\n  // userId passed in context to", "metadata": {"source": "long-term-memory.mdx"}}
{"text": "userId is required\");\n    }\n    // Store data in the store (namespace, key, data)\n    await runtime.store.put([\"users\"], userId, userInfo);\n    return \"Successfully saved user info.\";\n  },\n  {\n    name: \"save_user_info\",\n    description: \"Save user info\",\n    schema: UserInfo,\n  }\n);\n\nconst agent = createAgent({\n  model: \"gpt-4.1-mini\",\n  tools: [saveUserInfo],\n  contextSchema,\n  store, // [!code highlight]\n});\n\n// Run the agent\nawait agent.invoke(\n  { messages: [{ role: \"user\", content: \"My name is John Smith\" }] },\n  // userId passed in context to identify whose information is being updated\n  { context: { userId: \"user_123\" } } // [!code highlight]\n);\n\n// You can access the store directly to get the value\nconst result = await store.get([\"users\"], \"user_123\");\nconsole.log(result?.value); // Output: { name: \"John Smith\" }\n\n```\n\n:::\n", "metadata": {"source": "long-term-memory.mdx"}}
{"text": "---\ntitle: Messages\n---\n\n{/* TODO: section on metadata types (response and usage) */}\n\nMessages are the fundamental unit of context for models in LangChain. They represent the input and output of models, carrying both the content and metadata needed to represent the state of a conversation when interacting with an LLM.\n\nMessages are objects that contain:\n\n* <Icon icon=\"user\" size={16} /> [**Role**](#message-types) - Identifies the message type (e.g. `system`, `user`)\n* <Icon icon=\"folder-closed\" size={16} /> [**Content**](#message-content) - Represents the actual content of the message (like text, images, audio, documents, etc.)\n* <Icon icon=\"tag\" size={16} /> [**Metadata**](#message-metadata) - Optional fields such as response information, message IDs, and token usage\n\nLangChain provides a standard message type that works across all model providers, ensuring consistent behavior regardless of the model being called.\n\n## Basic usage\n\nThe simplest way to use messages is to create message objects and pass them to a model when [invoking](/oss/langchain/models#invocation).\n\n:::python\n```python\nfrom langchain.chat_models import init_chat_model\nfrom langchain.messages import HumanMessage, AIMessage, SystemMessage\n\nmodel = init_chat_model(\"gpt-5-nano\")\n\nsystem_msg = SystemMessage(\"You are a helpful assistant.\")\nhuman_msg = HumanMessage(\"Hello, how are you?\")\n\n# Use with chat models\nmessages = [system_msg, human_msg]\nresponse = model.invoke(messages)  # Returns AIMessage\n```\n:::\n\n:::js\n```typescript\nimport { initChatModel, HumanMessage, SystemMessage } from \"langchain\";\n\nconst model = await initChatModel(\"gpt-5-nano\");\n\nconst systemMsg = new SystemMessage(\"You are a helpful assistant.\");\nconst humanMsg = new HumanMessage(\"Hello, how are you?\");\n\nconst messages = [systemMsg, humanMsg];\nconst response = await model.invoke(messages);  // Returns AIMessage\n```\n:::\n\n### Text prompts\n\nText prompts are strings - ideal for straightforward generation tasks where you don't need to retain conversation history.\n\n:::python\n```python\nresponse = model.invoke(\"Write a haiku about spring\")\n```\n:::\n\n:::js\n```typescript\nconst response = await model.invoke(\"Write a haiku about spring\");\n```\n:::\n\n**Use text prompts when:**\n* You have a single, standalone request\n* You don't need conversation history\n* You want minimal code complexity\n\n### Message prompts\n\nAlternatively, you can pass in a list of messages to the model by providing a list of message objects.\n\n:::python\n```python\nfrom langchain.messages import SystemMessage, HumanMessage, AIMessage\n\nmessages = [\n    SystemMessage(\"You are a poetry expert\"),\n    HumanMessage(\"Write a haiku about spring\"),\n    AIMessage(\"Cherry blossoms bloom...\")\n]\nresponse = model.invoke(messages)\n```\n:::\n\n:::js\n```typescript\nimport { SystemMessage, HumanMessage, AIMessage } from \"langchain\";\n\nconst messages = [\n  new System", "metadata": {"source": "messages.mdx"}}
{"text": " spring\");\n```\n:::\n\n**Use text prompts when:**\n* You have a single, standalone request\n* You don't need conversation history\n* You want minimal code complexity\n\n### Message prompts\n\nAlternatively, you can pass in a list of messages to the model by providing a list of message objects.\n\n:::python\n```python\nfrom langchain.messages import SystemMessage, HumanMessage, AIMessage\n\nmessages = [\n    SystemMessage(\"You are a poetry expert\"),\n    HumanMessage(\"Write a haiku about spring\"),\n    AIMessage(\"Cherry blossoms bloom...\")\n]\nresponse = model.invoke(messages)\n```\n:::\n\n:::js\n```typescript\nimport { SystemMessage, HumanMessage, AIMessage } from \"langchain\";\n\nconst messages = [\n  new SystemMessage(\"You are a poetry expert\"),\n  new HumanMessage(\"Write a haiku about spring\"),\n  new AIMessage(\"Cherry blossoms bloom...\"),\n];\nconst response = await model.invoke(messages);\n```\n:::\n\n**Use message prompts when:**\n* Managing multi-turn conversations\n* Working with multimodal content (images, audio, files)\n* Including system instructions\n\n### Dictionary format\n\nYou can also specify messages directly in OpenAI chat completions format.\n\n:::python\n```python\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a poetry expert\"},\n    {\"role\": \"user\", \"content\": \"Write a haiku about spring\"},\n    {\"role\": \"assistant\", \"content\": \"Cherry blossoms bloom...\"}\n]\nresponse = model.invoke(messages)\n```\n:::\n\n:::js\n```typescript\nconst messages = [\n  { role: \"system\", content: \"You are a poetry expert\" },\n  { role: \"user\", content: \"Write a haiku about spring\" },\n  { role: \"assistant\", content: \"Cherry blossoms bloom...\" },\n];\nconst response = await model.invoke(messages);\n```\n:::\n\n## Message types\n\n- <Icon icon=\"gear\" size={16} /> [System message](#system-message) - Tells the model how to behave and provide context for interactions\n- <Icon icon=\"user\" size={16} /> [Human message](#human-message) - Represents user input and interactions with the model\n- <Icon icon=\"robot\" size={16} /> [AI message](#ai-message) - Responses generated by the model, including text content, tool calls, and metadata\n- <Icon icon=\"wrench\" size={16} /> [Tool message](#tool-message) - Represents the outputs of [tool calls](/oss/langchain/models#tool-calling)\n\n### System message\n\nA @[`SystemMessage`] represent an initial set of instructions that primes the model's behavior. You can use a system message to set the tone, define the model's role, and establish guidelines for responses.\n\n:::python\n```python Basic instructions\nsystem_msg = SystemMessage(\"You are a helpful coding assistant.\")\n\nmessages = [\n    system_msg,\n    HumanMessage(\"How do I create a REST API?\")\n]\nresponse = model.invoke(messages)\n```\n:::\n\n:::js\n```typescript Basic instructions\nimport { System", "metadata": {"source": "messages.mdx"}}
{"text": " the model, including text content, tool calls, and metadata\n- <Icon icon=\"wrench\" size={16} /> [Tool message](#tool-message) - Represents the outputs of [tool calls](/oss/langchain/models#tool-calling)\n\n### System message\n\nA @[`SystemMessage`] represent an initial set of instructions that primes the model's behavior. You can use a system message to set the tone, define the model's role, and establish guidelines for responses.\n\n:::python\n```python Basic instructions\nsystem_msg = SystemMessage(\"You are a helpful coding assistant.\")\n\nmessages = [\n    system_msg,\n    HumanMessage(\"How do I create a REST API?\")\n]\nresponse = model.invoke(messages)\n```\n:::\n\n:::js\n```typescript Basic instructions\nimport { SystemMessage, HumanMessage, AIMessage } from \"langchain\";\n\nconst systemMsg = new SystemMessage(\"You are a helpful coding assistant.\");\n\nconst messages = [\n  systemMsg,\n  new HumanMessage(\"How do I create a REST API?\"),\n];\nconst response = await model.invoke(messages);\n```\n:::\n\n:::python\n```python Detailed persona\nfrom langchain.messages import SystemMessage, HumanMessage\n\nsystem_msg = SystemMessage(\"\"\"\nYou are a senior Python developer with expertise in web frameworks.\nAlways provide code examples and explain your reasoning.\nBe concise but thorough in your explanations.\n\"\"\")\n\nmessages = [\n    system_msg,\n    HumanMessage(\"How do I create a REST API?\")\n]\nresponse = model.invoke(messages)\n```\n:::\n\n:::js\n```typescript Detailed persona\nimport { SystemMessage, HumanMessage } from \"langchain\";\n\nconst systemMsg = new SystemMessage(`\nYou are a senior TypeScript developer with expertise in web frameworks.\nAlways provide code examples and explain your reasoning.\nBe concise but thorough in your explanations.\n`);\n\nconst messages = [\n  systemMsg,\n  new HumanMessage(\"How do I create a REST API?\"),\n];\nconst response = await model.invoke(messages);\n```\n:::\n\n---\n\n### Human message\n\nA @[`HumanMessage`] represents user input and interactions. They can contain text, images, audio, files, and any other amount of multimodal [content](#message-content).\n\n#### Text content\n\n:::python\n<CodeGroup>\n    ```python Message object\n    response = model.invoke([\n      HumanMessage(\"What is machine learning?\")\n    ])\n    ```\n\n    ```python String shortcut\n    # Using a string is a shortcut for a single HumanMessage\n    response = model.invoke(\"What is machine learning?\")\n    ```\n</CodeGroup>\n:::\n\n:::js\n```typescript Message object\nconst response = await model.invoke([\n  new HumanMessage(\"What is machine learning?\"),\n]);\n```\n\n```typescript String shortcut\nconst response = await model.invoke(\"What is machine learning?\");\n```\n:::\n\n#### Message metadata\n\n:::python\n```python Add metadata\nhuman_msg = HumanMessage(\n    content=\"Hello!\",\n    name=\"alice\",  # Optional: identify different users", "metadata": {"source": "messages.mdx"}}
{"text": "invoke([\n      HumanMessage(\"What is machine learning?\")\n    ])\n    ```\n\n    ```python String shortcut\n    # Using a string is a shortcut for a single HumanMessage\n    response = model.invoke(\"What is machine learning?\")\n    ```\n</CodeGroup>\n:::\n\n:::js\n```typescript Message object\nconst response = await model.invoke([\n  new HumanMessage(\"What is machine learning?\"),\n]);\n```\n\n```typescript String shortcut\nconst response = await model.invoke(\"What is machine learning?\");\n```\n:::\n\n#### Message metadata\n\n:::python\n```python Add metadata\nhuman_msg = HumanMessage(\n    content=\"Hello!\",\n    name=\"alice\",  # Optional: identify different users\n    id=\"msg_123\",  # Optional: unique identifier for tracing\n)\n```\n:::\n\n:::js\n```typescript Add metadata\nconst humanMsg = new HumanMessage({\n  content: \"Hello!\",\n  name: \"alice\",\n  id: \"msg_123\",\n});\n```\n:::\n\n<Note>\n    The `name` field behavior varies by provider \u2013 some use it for user identification, others ignore it. To check, refer to the model provider's [reference](https://reference.langchain.com/python/integrations/).\n</Note>\n\n---\n\n### AI message\n\nAn @[`AIMessage`] represents the output of a model invocation. They can include multimodal data, tool calls, and provider-specific metadata that you can later access.\n\n:::python\n```python\nresponse = model.invoke(\"Explain AI\")\nprint(type(response))  # <class 'langchain.messages.AIMessage'>\n```\n:::\n\n:::js\n```typescript\nconst response = await model.invoke(\"Explain AI\");\nconsole.log(typeof response);  // AIMessage\n```\n:::\n\n@[`AIMessage`] objects are returned by the model when calling it, which contains all of the associated metadata in the response.\n\nProviders weigh/contextualize types of messages differently, which means it is sometimes helpful to manually create a new @[`AIMessage`] object and insert it into the message history as if it came from the model.\n\n:::python\n```python\nfrom langchain.messages import AIMessage, SystemMessage, HumanMessage\n\n# Create an AI message manually (e.g., for conversation history)\nai_msg = AIMessage(\"I'd be happy to help you with that question!\")\n\n# Add to conversation history\nmessages = [\n    SystemMessage(\"You are a helpful assistant\"),\n    HumanMessage(\"Can you help me?\"),\n    ai_msg,  # Insert as if it came from the model\n    HumanMessage(\"Great! What's 2+2?\")\n]\n\nresponse = model.invoke(messages)\n```\n:::\n\n:::js\n```typescript\nimport { AIMessage, SystemMessage, HumanMessage } from \"langchain\";\n\nconst aiMsg = new AIMessage(\"I'd be happy to help you with that question!\");\n\nconst messages = [\n  new SystemMessage(\"You are a helpful assistant\"),\n  new HumanMessage(\"Can you help me?\"", "metadata": {"source": "messages.mdx"}}
{"text": "., for conversation history)\nai_msg = AIMessage(\"I'd be happy to help you with that question!\")\n\n# Add to conversation history\nmessages = [\n    SystemMessage(\"You are a helpful assistant\"),\n    HumanMessage(\"Can you help me?\"),\n    ai_msg,  # Insert as if it came from the model\n    HumanMessage(\"Great! What's 2+2?\")\n]\n\nresponse = model.invoke(messages)\n```\n:::\n\n:::js\n```typescript\nimport { AIMessage, SystemMessage, HumanMessage } from \"langchain\";\n\nconst aiMsg = new AIMessage(\"I'd be happy to help you with that question!\");\n\nconst messages = [\n  new SystemMessage(\"You are a helpful assistant\"),\n  new HumanMessage(\"Can you help me?\"),\n  aiMsg,  // Insert as if it came from the model\n  new HumanMessage(\"Great! What's 2+2?\")\n]\n\nconst response = await model.invoke(messages);\n```\n:::\n\n<Accordion title=\"Attributes\">\n    :::python\n    <ParamField path=\"text\" type=\"string\">\n        The text content of the message.\n    </ParamField>\n    <ParamField path=\"content\" type=\"string | dict[]\">\n        The raw content of the message.\n    </ParamField>\n    <ParamField path=\"content_blocks\" type=\"ContentBlock[]\">\n        The standardized [content blocks](#message-content) of the message.\n    </ParamField>\n    <ParamField path=\"tool_calls\" type=\"dict[] | None\">\n        The tool calls made by the model.\n\n        Empty if no tools are called.\n    </ParamField>\n    <ParamField path=\"id\" type=\"string\">\n        A unique identifier for the message (either automatically generated by LangChain or returned in the provider response)\n    </ParamField>\n    <ParamField path=\"usage_metadata\" type=\"dict | None\">\n        The usage metadata of the message, which can contain token counts when available.\n    </ParamField>\n    <ParamField path=\"response_metadata\" type=\"ResponseMetadata | None\">\n        The response metadata of the message.\n    </ParamField>\n    :::\n\n    :::js\n    <ParamField path=\"text\" type=\"string\">\n        The text content of the message.\n    </ParamField>\n    <ParamField path=\"content\" type=\"string | ContentBlock[]\">\n        The raw content of the message.\n    </ParamField>\n    <ParamField path=\"content_blocks\" type=\"ContentBlock.Standard[]\">\n        The standardized content blocks of the message. (See [content](#message-content))\n    </ParamField>\n    <ParamField path=\"tool_calls\" type=\"ToolCall[] | None\">\n        The tool calls made by the model.\n\n        Empty if", "metadata": {"source": "messages.mdx"}}
{"text": " </ParamField>\n    :::\n\n    :::js\n    <ParamField path=\"text\" type=\"string\">\n        The text content of the message.\n    </ParamField>\n    <ParamField path=\"content\" type=\"string | ContentBlock[]\">\n        The raw content of the message.\n    </ParamField>\n    <ParamField path=\"content_blocks\" type=\"ContentBlock.Standard[]\">\n        The standardized content blocks of the message. (See [content](#message-content))\n    </ParamField>\n    <ParamField path=\"tool_calls\" type=\"ToolCall[] | None\">\n        The tool calls made by the model.\n\n        Empty if no tools are called.\n    </ParamField>\n    <ParamField path=\"id\" type=\"string\">\n        A unique identifier for the message (either automatically generated by LangChain or returned in the provider response)\n    </ParamField>\n    <ParamField path=\"usage_metadata\" type=\"UsageMetadata | None\">\n        The usage metadata of the message, which can contain token counts when available. See @[`UsageMetadata`].\n    </ParamField>\n    <ParamField path=\"response_metadata\" type=\"ResponseMetadata | None\">\n        The response metadata of the message.\n    </ParamField>\n    :::\n</Accordion>\n\n#### Tool calls\n\nWhen models make [tool calls](/oss/langchain/models#tool-calling), they're included in the @[`AIMessage`]:\n\n:::python\n```python\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"gpt-5-nano\")\n\ndef get_weather(location: str) -> str:\n    \"\"\"Get the weather at a location.\"\"\"\n    ...\n\nmodel_with_tools = model.bind_tools([get_weather])\nresponse = model_with_tools.invoke(\"What's the weather in Paris?\")\n\nfor tool_call in response.tool_calls:\n    print(f\"Tool: {tool_call['name']}\")\n    print(f\"Args: {tool_call['args']}\")\n    print(f\"ID: {tool_call['id']}\")\n```\n:::\n\n:::js\n```typescript\nconst modelWithTools = model.bindTools([getWeather]);\nconst response = await modelWithTools.invoke(\"What's the weather in Paris?\");\n\nfor (const toolCall of response.tool_calls) {\n  console.log(`Tool: ${toolCall.name}`);\n  console.log(`Args: ${toolCall.args}`);\n  console.log(`ID: ${toolCall.id}`);\n}\n```\n:::\n\nOther structured data, such as reasoning or citations, can also appear in message [content](/oss/langchain/messages#message-content).\n\n#### Token usage\n\nAn @[`AIMessage`] can hold token counts and other usage metadata in its @[`usage_metadata`][UsageMetadata] field:\n", "metadata": {"source": "messages.mdx"}}
{"text": "\")\n```\n:::\n\n:::js\n```typescript\nconst modelWithTools = model.bindTools([getWeather]);\nconst response = await modelWithTools.invoke(\"What's the weather in Paris?\");\n\nfor (const toolCall of response.tool_calls) {\n  console.log(`Tool: ${toolCall.name}`);\n  console.log(`Args: ${toolCall.args}`);\n  console.log(`ID: ${toolCall.id}`);\n}\n```\n:::\n\nOther structured data, such as reasoning or citations, can also appear in message [content](/oss/langchain/messages#message-content).\n\n#### Token usage\n\nAn @[`AIMessage`] can hold token counts and other usage metadata in its @[`usage_metadata`][UsageMetadata] field:\n\n:::python\n```python\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"gpt-5-nano\")\n\nresponse = model.invoke(\"Hello!\")\nresponse.usage_metadata\n```\n\n```\n{'input_tokens': 8,\n 'output_tokens': 304,\n 'total_tokens': 312,\n 'input_token_details': {'audio': 0, 'cache_read': 0},\n 'output_token_details': {'audio': 0, 'reasoning': 256}}\n```\n:::\n:::js\n```typescript\nimport { initChatModel } from \"langchain\";\n\nconst model = await initChatModel(\"gpt-5-nano\");\n\nconst response = await model.invoke(\"Hello!\");\nconsole.log(response.usage_metadata);\n```\n\n```json\n{\n  \"output_tokens\": 304,\n  \"input_tokens\": 8,\n  \"total_tokens\": 312,\n  \"input_token_details\": {\n    \"cache_read\": 0\n  },\n  \"output_token_details\": {\n    \"reasoning\": 256\n  }\n}\n```\n:::\n\nSee @[`UsageMetadata`] for details.\n\n#### Streaming and chunks\n\nDuring streaming, you'll receive @[`AIMessageChunk`] objects that can be combined into a full message object:\n\n:::python\n```python\nchunks = []\nfull_message = None\nfor chunk in model.stream(\"Hi\"):\n    chunks.append(chunk)\n    print(chunk.text)\n    full_message = chunk if full_message is None else full_message + chunk\n```\n:::\n\n:::js\n<CodeGroup>\n```typescript\nimport { AIMessageChunk } from \"langchain\";\n\nlet finalChunk: AIMessageChunk | undefined;\nfor (const chunk of chunks) {\n  finalChunk = finalChunk ? finalChunk.concat(chunk) : chunk;\n}\n```\n</CodeGroup>\n:::\n\n<Note>\nLearn more:\n- [Streaming tokens from chat models](/oss/langchain/models#stream)\n- [Streaming tokens and/or steps from agents](/oss/langchain/streaming)\n</Note>\n\n---\n\n### Tool message\n\nFor models that support [tool calling](/oss/langchain/", "metadata": {"source": "messages.mdx"}}
{"text": "unk.text)\n    full_message = chunk if full_message is None else full_message + chunk\n```\n:::\n\n:::js\n<CodeGroup>\n```typescript\nimport { AIMessageChunk } from \"langchain\";\n\nlet finalChunk: AIMessageChunk | undefined;\nfor (const chunk of chunks) {\n  finalChunk = finalChunk ? finalChunk.concat(chunk) : chunk;\n}\n```\n</CodeGroup>\n:::\n\n<Note>\nLearn more:\n- [Streaming tokens from chat models](/oss/langchain/models#stream)\n- [Streaming tokens and/or steps from agents](/oss/langchain/streaming)\n</Note>\n\n---\n\n### Tool message\n\nFor models that support [tool calling](/oss/langchain/models#tool-calling), AI messages can contain tool calls. Tool messages are used to pass the results of a single tool execution back to the model.\n\n[Tools](/oss/langchain/tools) can generate @[`ToolMessage`] objects directly. Below, we show a simple example. Read more in the [tools guide](/oss/langchain/tools).\n\n:::python\n```python\nfrom langchain.messages import AIMessage\nfrom langchain.messages import ToolMessage\n\n# After a model makes a tool call\n# (Here, we demonstrate manually creating the messages for brevity)\nai_message = AIMessage(\n    content=[],\n    tool_calls=[{\n        \"name\": \"get_weather\",\n        \"args\": {\"location\": \"San Francisco\"},\n        \"id\": \"call_123\"\n    }]\n)\n\n# Execute tool and create result message\nweather_result = \"Sunny, 72\u00b0F\"\ntool_message = ToolMessage(\n    content=weather_result,\n    tool_call_id=\"call_123\"  # Must match the call ID\n)\n\n# Continue conversation\nmessages = [\n    HumanMessage(\"What's the weather in San Francisco?\"),\n    ai_message,  # Model's tool call\n    tool_message,  # Tool execution result\n]\nresponse = model.invoke(messages)  # Model processes the result\n```\n:::\n\n:::js\n```typescript\nimport { AIMessage, ToolMessage } from \"langchain\";\n\nconst aiMessage = new AIMessage({\n  content: [],\n  tool_calls: [{\n    name: \"get_weather\",\n    args: { location: \"San Francisco\" },\n    id: \"call_123\"\n  }]\n});\n\nconst toolMessage = new ToolMessage({\n  content: \"Sunny, 72\u00b0F\",\n  tool_call_id: \"call_123\"\n});\n\nconst messages = [\n  new HumanMessage(\"What's the weather in San Francisco?\"),\n  aiMessage,  // Model's tool call\n  toolMessage,  // Tool execution result\n];\n\nconst response = await model.invoke(messages);  // Model processes the result\n```\n:::\n\n<Accordion title=\"Attributes\">\n    <ParamField path=\"content\" type=\"string\" required>\n        The string", "metadata": {"source": "messages.mdx"}}
{"text": "  content: [],\n  tool_calls: [{\n    name: \"get_weather\",\n    args: { location: \"San Francisco\" },\n    id: \"call_123\"\n  }]\n});\n\nconst toolMessage = new ToolMessage({\n  content: \"Sunny, 72\u00b0F\",\n  tool_call_id: \"call_123\"\n});\n\nconst messages = [\n  new HumanMessage(\"What's the weather in San Francisco?\"),\n  aiMessage,  // Model's tool call\n  toolMessage,  // Tool execution result\n];\n\nconst response = await model.invoke(messages);  // Model processes the result\n```\n:::\n\n<Accordion title=\"Attributes\">\n    <ParamField path=\"content\" type=\"string\" required>\n        The stringified output of the tool call.\n    </ParamField>\n    <ParamField path=\"tool_call_id\" type=\"string\" required>\n        The ID of the tool call that this message is responding to. Must match the ID of the tool call in the @[`AIMessage`].\n    </ParamField>\n    <ParamField path=\"name\" type=\"string\" required>\n        The name of the tool that was called.\n    </ParamField>\n    <ParamField path=\"artifact\" type=\"dict\">\n        Additional data not sent to the model but can be accessed programmatically.\n    </ParamField>\n</Accordion>\n\n<Note>\n    The `artifact` field stores supplementary data that won't be sent to the model but can be accessed programmatically. This is useful for storing raw results, debugging information, or data for downstream processing without cluttering the model's context.\n\n    <Accordion title=\"Example: Using artifact for retrieval metadata\">\n        For example, a [retrieval](/oss/langchain/retrieval) tool could retrieve a passage from a document for reference by a model. Where message `content` contains text that the model will reference, an `artifact` can contain document identifiers or other metadata that an application can use (e.g., to render a page). See example below:\n\n        :::python\n\n        ```python\n        from langchain.messages import ToolMessage\n\n        # Sent to model\n        message_content = \"It was the best of times, it was the worst of times.\"\n\n        # Artifact available downstream\n        artifact = {\"document_id\": \"doc_123\", \"page\": 0}\n\n        tool_message = ToolMessage(\n            content=message_content,\n            tool_call_id=\"call_123\",\n            name=\"search_books\",\n            artifact=artifact,\n        )\n        ```\n        :::\n        :::js\n        ```typescript\n    ", "metadata": {"source": "messages.mdx"}}
{"text": " \"It was the best of times, it was the worst of times.\"\n\n        # Artifact available downstream\n        artifact = {\"document_id\": \"doc_123\", \"page\": 0}\n\n        tool_message = ToolMessage(\n            content=message_content,\n            tool_call_id=\"call_123\",\n            name=\"search_books\",\n            artifact=artifact,\n        )\n        ```\n        :::\n        :::js\n        ```typescript\n        import { ToolMessage } from \"langchain\";\n\n        // Artifact available downstream\n        const artifact = { document_id: \"doc_123\", page: 0 };\n\n        const toolMessage = new ToolMessage({\n          content: \"It was the best of times, it was the worst of times.\",\n          tool_call_id: \"call_123\",\n          name: \"search_books\",\n          artifact\n        });\n        ```\n        :::\n\n        See the [RAG tutorial](/oss/langchain/rag) for an end-to-end example of building retrieval [agents](/oss/langchain/agents) with LangChain.\n    </Accordion>\n</Note>\n\n---\n\n## Message content\n\nYou can think of a message's content as the payload of data that gets sent to the model. Messages have a `content` attribute that is loosely-typed, supporting strings and lists of untyped objects (e.g., dictionaries). This allows support for provider-native structures directly in LangChain chat models, such as [multimodal](#multimodal) content and other data.\n\nSeparately, LangChain provides dedicated content types for text, reasoning, citations, multi-modal data, server-side tool calls, and other message content. See [content blocks](#standard-content-blocks) below.\n\nLangChain chat models accept message content in the `content` attribute.\n\nThis may contain either:\n\n1. A string\n2. A list of content blocks in a provider-native format\n3. A list of [LangChain's standard content blocks](#standard-content-blocks)\n\nSee below for an example using [multimodal](#multimodal) inputs:\n\n:::python\n```python\nfrom langchain.messages import HumanMessage\n\n# String content\nhuman_message = HumanMessage(\"Hello, how are you?\")\n\n# Provider-native format (e.g., OpenAI)\nhuman_message = HumanMessage(content=[\n    {\"type\": \"text\", \"text\": \"Hello, how are you?\"},\n    {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://example.com/image.jpg\"}}\n])\n\n# List of standard content blocks\nhuman_message = HumanMessage(", "metadata": {"source": "messages.mdx"}}
{"text": ":\n\n1. A string\n2. A list of content blocks in a provider-native format\n3. A list of [LangChain's standard content blocks](#standard-content-blocks)\n\nSee below for an example using [multimodal](#multimodal) inputs:\n\n:::python\n```python\nfrom langchain.messages import HumanMessage\n\n# String content\nhuman_message = HumanMessage(\"Hello, how are you?\")\n\n# Provider-native format (e.g., OpenAI)\nhuman_message = HumanMessage(content=[\n    {\"type\": \"text\", \"text\": \"Hello, how are you?\"},\n    {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://example.com/image.jpg\"}}\n])\n\n# List of standard content blocks\nhuman_message = HumanMessage(content_blocks=[\n    {\"type\": \"text\", \"text\": \"Hello, how are you?\"},\n    {\"type\": \"image\", \"url\": \"https://example.com/image.jpg\"},\n])\n```\n\n<Tip>\n    Specifying `content_blocks` when initializing a message will still populate message\n    `content`, but provides a type-safe interface for doing so.\n</Tip>\n:::\n\n:::js\n```typescript\nimport { HumanMessage } from \"langchain\";\n\n// String content\nconst humanMessage = new HumanMessage(\"Hello, how are you?\");\n\n// Provider-native format (e.g., OpenAI)\nconst humanMessage = new HumanMessage({\n  content: [\n    { type: \"text\", text: \"Hello, how are you?\" },\n    {\n      type: \"image_url\",\n      image_url: { url: \"https://example.com/image.jpg\" },\n    },\n  ],\n});\n\n// List of standard content blocks\nconst humanMessage = new HumanMessage({\n  contentBlocks: [\n    { type: \"text\", text: \"Hello, how are you?\" },\n    { type: \"image\", url: \"https://example.com/image.jpg\" },\n  ],\n});\n```\n:::\n\n### Standard content blocks\n\nLangChain provides a standard representation for message content that works across providers.\n\n:::python\nMessage objects implement a `content_blocks` property that will lazily parse the `content` attribute into a standard, type-safe representation. For example, messages generated from [`ChatAnthropic`](/oss/integrations/chat/anthropic) or [`ChatOpenAI`](/oss/integrations/chat/openai) will include `thinking` or `reasoning` blocks in the format of the respective provider, but can be lazily parsed into a consistent [`ReasoningContentBlock`](#content-block-reference) representation:\n\n<Tabs>\n<Tab title=\"Anthropic\">\n```python\nfrom langchain.messages import AIMessage\n\nmessage = AIMessage(\n    content=[\n        {\"type\": \"thinking\", \"thinking\": \"...\", \"signature\": \"WaUjzkyp...\"},\n        {\"type\": \"text\", \"text\": \"...\"},\n    ],\n    response_metadata={\"model_provider\": \"anthropic\"}\n)\nmessage.content", "metadata": {"source": "messages.mdx"}}
{"text": "oss/integrations/chat/anthropic) or [`ChatOpenAI`](/oss/integrations/chat/openai) will include `thinking` or `reasoning` blocks in the format of the respective provider, but can be lazily parsed into a consistent [`ReasoningContentBlock`](#content-block-reference) representation:\n\n<Tabs>\n<Tab title=\"Anthropic\">\n```python\nfrom langchain.messages import AIMessage\n\nmessage = AIMessage(\n    content=[\n        {\"type\": \"thinking\", \"thinking\": \"...\", \"signature\": \"WaUjzkyp...\"},\n        {\"type\": \"text\", \"text\": \"...\"},\n    ],\n    response_metadata={\"model_provider\": \"anthropic\"}\n)\nmessage.content_blocks\n```\n```\n[{'type': 'reasoning',\n  'reasoning': '...',\n  'extras': {'signature': 'WaUjzkyp...'}},\n {'type': 'text', 'text': '...'}]\n```\n</Tab>\n\n<Tab title=\"OpenAI\">\n```python\nfrom langchain.messages import AIMessage\n\nmessage = AIMessage(\n    content=[\n        {\n            \"type\": \"reasoning\",\n            \"id\": \"rs_abc123\",\n            \"summary\": [\n                {\"type\": \"summary_text\", \"text\": \"summary 1\"},\n                {\"type\": \"summary_text\", \"text\": \"summary 2\"},\n            ],\n        },\n        {\"type\": \"text\", \"text\": \"...\", \"id\": \"msg_abc123\"},\n    ],\n    response_metadata={\"model_provider\": \"openai\"}\n)\nmessage.content_blocks\n```\n```\n[{'type': 'reasoning', 'id': 'rs_abc123', 'reasoning': 'summary 1'},\n {'type': 'reasoning', 'id': 'rs_abc123', 'reasoning': 'summary 2'},\n {'type': 'text', 'text': '...', 'id': 'msg_abc123'}]\n```\n</Tab>\n</Tabs>\n:::\n\n:::js\nMessage objects implement a `contentBlocks` property that will lazily parse the `content` attribute into a standard, type-safe representation. For example, messages generated from [`ChatAnthropic`](/oss/integrations/chat/anthropic) or [`ChatOpenAI`](/oss/integrations/chat/openai) will include `thinking` or `reasoning` blocks in the format of the respective provider, but can be lazily parsed into a consistent [`ReasoningContentBlock`](#content-block-reference) representation:\n\n<Tabs>\n<Tab title=\"Anthropic\">\n```typescript\nimport { AIMessage } from \"@langchain/core/messages\";\n\nconst message = new AIMessage({\n  content: [\n   ", "metadata": {"source": "messages.mdx"}}
{"text": "abc123'}]\n```\n</Tab>\n</Tabs>\n:::\n\n:::js\nMessage objects implement a `contentBlocks` property that will lazily parse the `content` attribute into a standard, type-safe representation. For example, messages generated from [`ChatAnthropic`](/oss/integrations/chat/anthropic) or [`ChatOpenAI`](/oss/integrations/chat/openai) will include `thinking` or `reasoning` blocks in the format of the respective provider, but can be lazily parsed into a consistent [`ReasoningContentBlock`](#content-block-reference) representation:\n\n<Tabs>\n<Tab title=\"Anthropic\">\n```typescript\nimport { AIMessage } from \"@langchain/core/messages\";\n\nconst message = new AIMessage({\n  content: [\n    {\n      \"type\": \"thinking\",\n      \"thinking\": \"...\",\n      \"signature\": \"WaUjzkyp...\",\n    },\n    {\n      \"type\":\"text\",\n      \"text\": \"...\",\n      \"id\": \"msg_abc123\",\n    },\n  ],\n  response_metadata: { model_provider: \"anthropic\" },\n});\n\nconsole.log(message.contentBlocks);\n```\n</Tab>\n\n<Tab title=\"OpenAI\">\n```typescript\nimport { AIMessage } from \"@langchain/core/messages\";\n\nconst message = new AIMessage({\n  content: [\n    {\n      \"type\": \"reasoning\",\n      \"id\": \"rs_abc123\",\n      \"summary\": [\n        {\"type\": \"summary_text\", \"text\": \"summary 1\"},\n        {\"type\": \"summary_text\", \"text\": \"summary 2\"},\n      ],\n    },\n    {\"type\": \"text\", \"text\": \"...\"},\n  ],\n  response_metadata: { model_provider: \"openai\" },\n});\n\nconsole.log(message.contentBlocks);\n```\n</Tab>\n</Tabs>\n:::\n\nSee the [integrations guides](/oss/integrations/providers/overview) to get started with the\ninference provider of your choice.\n\n<Note>\n    **Serializing standard content**\n\n    If an application outside of LangChain needs access to the standard content block\n    representation, you can opt-in to storing content blocks in message content.\n\n    :::python\n    To do this, you can set the `LC_OUTPUT_VERSION` environment variable to `v1`. Or,\n    initialize any chat model with `output_version=\"v1\"`:\n\n    ```python\n    from langchain.chat_models import init_chat_model\n\n    model = init_chat_model(\"gpt-5-nano\", output_version=\"v1\")\n    ```\n    :::\n    :::js\n    To do this, you can set the `LC_OUTPUT_VERSION` environment variable to `v1`. Or,\n    initialize any chat model with", "metadata": {"source": "messages.mdx"}}
{"text": "   If an application outside of LangChain needs access to the standard content block\n    representation, you can opt-in to storing content blocks in message content.\n\n    :::python\n    To do this, you can set the `LC_OUTPUT_VERSION` environment variable to `v1`. Or,\n    initialize any chat model with `output_version=\"v1\"`:\n\n    ```python\n    from langchain.chat_models import init_chat_model\n\n    model = init_chat_model(\"gpt-5-nano\", output_version=\"v1\")\n    ```\n    :::\n    :::js\n    To do this, you can set the `LC_OUTPUT_VERSION` environment variable to `v1`. Or,\n    initialize any chat model with `outputVersion: \"v1\"`:\n\n    ```typescript\n    import { initChatModel } from \"langchain\";\n\n    const model = await initChatModel(\n      \"gpt-5-nano\",\n      { outputVersion: \"v1\" }\n    );\n    ```\n    :::\n</Note>\n\n### Multimodal\n\n**Multimodality** refers to the ability to work with data that comes in different\nforms, such as text, audio, images, and video. LangChain includes standard types\nfor these data that can be used across providers.\n\n[Chat models](/oss/langchain/models) can accept multimodal data as input and generate\nit as output. Below we show short examples of input messages featuring multimodal data.\n\n<Note>\nExtra keys can be included top-level in the content block or nested in `\"extras\": {\"key\": value}`.\n\n[OpenAI](/oss/integrations/chat/openai#pdfs) and [AWS Bedrock Converse](/oss/integrations/chat/bedrock),\nfor example, require a filename for PDFs. See the [provider page](/oss/integrations/providers/overview)\nfor your chosen model for specifics.\n</Note>\n\n:::python\n<CodeGroup>\n    ```python Image input\n    # From URL\n    message = {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Describe the content of this image.\"},\n            {\"type\": \"image\", \"url\": \"https://example.com/path/to/image.jpg\"},\n        ]\n    }\n\n    # From base64 data\n    message = {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Describe the content of this image.\"},\n            {\n                \"type\": \"image\",\n                \"base64\": \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQy", "metadata": {"source": "messages.mdx"}}
{"text": "},\n            {\"type\": \"image\", \"url\": \"https://example.com/path/to/image.jpg\"},\n        ]\n    }\n\n    # From base64 data\n    message = {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Describe the content of this image.\"},\n            {\n                \"type\": \"image\",\n                \"base64\": \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\",\n                \"mime_type\": \"image/jpeg\",\n            },\n        ]\n    }\n\n    # From provider-managed File ID\n    message = {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Describe the content of this image.\"},\n            {\"type\": \"image\", \"file_id\": \"file-abc123\"},\n        ]\n    }\n    ```\n\n    ```python PDF document input\n    # From URL\n    message = {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Describe the content of this document.\"},\n            {\"type\": \"file\", \"url\": \"https://example.com/path/to/document.pdf\"},\n        ]\n    }\n\n    # From base64 data\n    message = {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Describe the content of this document.\"},\n            {\n                \"type\": \"file\",\n                \"base64\": \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\",\n                \"mime_type\": \"application/pdf\",\n            },\n        ]\n    }\n\n    # From provider-managed File ID\n    message = {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Describe the content of", "metadata": {"source": "messages.mdx"}}
{"text": "    {\n                \"type\": \"file\",\n                \"base64\": \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\",\n                \"mime_type\": \"application/pdf\",\n            },\n        ]\n    }\n\n    # From provider-managed File ID\n    message = {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Describe the content of this document.\"},\n            {\"type\": \"file\", \"file_id\": \"file-abc123\"},\n        ]\n    }\n    ```\n\n    ```python Audio input\n    # From base64 data\n    message = {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Describe the content of this audio.\"},\n            {\n                \"type\": \"audio\",\n                \"base64\": \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\",\n                \"mime_type\": \"audio/wav\",\n            },\n        ]\n    }\n\n    # From provider-managed File ID\n    message = {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Describe the content of this audio.\"},\n            {\"type\": \"audio\", \"file_id\": \"file-abc123\"},\n        ]\n    }\n    ```\n\n    ```python Video input\n    # From base64 data\n    message = {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Describe the content of this video.\"},\n            {\n                \"type\": \"video\",\n                \"base64\": \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\",\n                \"mime_type\": \"video/mp4\",\n        ", "metadata": {"source": "messages.mdx"}}
{"text": "  ```python Video input\n    # From base64 data\n    message = {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Describe the content of this video.\"},\n            {\n                \"type\": \"video\",\n                \"base64\": \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\",\n                \"mime_type\": \"video/mp4\",\n            },\n        ]\n    }\n\n    # From provider-managed File ID\n    message = {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Describe the content of this video.\"},\n            {\"type\": \"video\", \"file_id\": \"file-abc123\"},\n        ]\n    }\n    ```\n</CodeGroup>\n:::\n\n:::js\n<CodeGroup>\n    ```typescript Image input\n    // From URL\n    const message = new HumanMessage({\n      content: [\n        { type: \"text\", text: \"Describe the content of this image.\" },\n        {\n          type: \"image\",\n          source_type: \"url\",\n          url: \"https://example.com/path/to/image.jpg\"\n        },\n      ],\n    });\n\n    // From base64 data\n    const message = new HumanMessage({\n      content: [\n        { type: \"text\", text: \"Describe the content of this image.\" },\n        {\n          type: \"image\",\n          source_type: \"base64\",\n          data: \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\",\n        },\n      ],\n    });\n\n    // From provider-managed File ID\n    const message = new HumanMessage({\n      content: [\n        { type: \"text\", text: \"Describe the content of this image.\" },\n        { type: \"image\", source_type: \"id\", id: \"file-abc123\" },\n      ],\n    });\n    ```\n\n    ```typescript PDF document input\n    // From URL\n   ", "metadata": {"source": "messages.mdx"}}
{"text": "   source_type: \"base64\",\n          data: \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\",\n        },\n      ],\n    });\n\n    // From provider-managed File ID\n    const message = new HumanMessage({\n      content: [\n        { type: \"text\", text: \"Describe the content of this image.\" },\n        { type: \"image\", source_type: \"id\", id: \"file-abc123\" },\n      ],\n    });\n    ```\n\n    ```typescript PDF document input\n    // From URL\n    const message = new HumanMessage({\n      content: [\n        { type: \"text\", text: \"Describe the content of this document.\" },\n        { type: \"file\", source_type: \"url\", url: \"https://example.com/path/to/document.pdf\", mime_type: \"application/pdf\" },\n      ],\n    });\n\n    // From base64 data\n    const message = new HumanMessage({\n      content: [\n        { type: \"text\", text: \"Describe the content of this document.\" },\n        {\n          type: \"file\",\n          source_type: \"base64\",\n          data: \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\",\n          mime_type: \"application/pdf\",\n        },\n      ],\n    });\n\n    // From provider-managed File ID\n    const message = new HumanMessage({\n      content: [\n        { type: \"text\", text: \"Describe the content of this document.\" },\n        { type: \"file\", source_type: \"id\", id: \"file-abc123\" },\n      ],\n    });\n    ```\n\n    ```typescript Audio input\n    // From base64 data\n    const message = new HumanMessage({\n      content: [\n        { type: \"text\", text: \"Describe the content of this audio.\" },\n        {\n          type: \"audio\",\n          source_type: \"base64\",\n          data: \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\",\n        },\n      ],\n    });\n\n    // From provider-managed File ID\n    const message = new HumanMessage({\n      content: [\n        { type: \"", "metadata": {"source": "messages.mdx"}}
{"text": "   // From base64 data\n    const message = new HumanMessage({\n      content: [\n        { type: \"text\", text: \"Describe the content of this audio.\" },\n        {\n          type: \"audio\",\n          source_type: \"base64\",\n          data: \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\",\n        },\n      ],\n    });\n\n    // From provider-managed File ID\n    const message = new HumanMessage({\n      content: [\n        { type: \"text\", text: \"Describe the content of this audio.\" },\n        { type: \"audio\", source_type: \"id\", id: \"file-abc123\" },\n      ],\n    });\n    ```\n\n    ```typescript Video input\n    // From base64 data\n    const message = new HumanMessage({\n      content: [\n        { type: \"text\", text: \"Describe the content of this video.\" },\n        {\n          type: \"video\",\n          source_type: \"base64\",\n          data: \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\",\n        },\n      ],\n    });\n\n    // From provider-managed File ID\n    const message = new HumanMessage({\n      content: [\n        { type: \"text\", text: \"Describe the content of this video.\" },\n        { type: \"video\", source_type: \"id\", id: \"file-abc123\" },\n      ],\n    });\n    ```\n</CodeGroup>\n:::\n\n\n<Warning>\n    Not all models support all file types. Check the model provider's [reference](https://reference.langchain.com/python/integrations/) for supported formats and size limits.\n</Warning>\n\n### Content block reference\n\n:::python\nContent blocks are represented (either when creating a message or accessing the `content_blocks` property) as a list of typed dictionaries. Each item in the list must adhere to one of the following block types:\n\n<AccordionGroup>\n    <Accordion title=\"Core\" icon=\"cube\">\n        <AccordionGroup>\n            <Accordion title=\"TextContentBlock\" icon=\"text\">\n                **Purpose:** Standard text output\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"text\"`\n      ", "metadata": {"source": "messages.mdx"}}
{"text": " reference\n\n:::python\nContent blocks are represented (either when creating a message or accessing the `content_blocks` property) as a list of typed dictionaries. Each item in the list must adhere to one of the following block types:\n\n<AccordionGroup>\n    <Accordion title=\"Core\" icon=\"cube\">\n        <AccordionGroup>\n            <Accordion title=\"TextContentBlock\" icon=\"text\">\n                **Purpose:** Standard text output\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"text\"`\n                </ParamField>\n\n                <ParamField body=\"text\" type=\"string\" required>\n                    The text content\n                </ParamField>\n\n                <ParamField body=\"annotations\" type=\"object[]\">\n                    List of annotations for the text\n                </ParamField>\n\n                <ParamField body=\"extras\" type=\"object\">\n                    Additional provider-specific data\n                </ParamField>\n\n                **Example:**\n                ```python\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Hello world\",\n                    \"annotations\": []\n                }\n                ```\n            </Accordion>\n            <Accordion title=\"ReasoningContentBlock\" icon=\"brain\">\n                **Purpose:** Model reasoning steps\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"reasoning\"`\n                </ParamField>\n\n                <ParamField body=\"reasoning\" type=\"string\">\n                    The reasoning content\n  ", "metadata": {"source": "messages.mdx"}}
{"text": " ```\n            </Accordion>\n            <Accordion title=\"ReasoningContentBlock\" icon=\"brain\">\n                **Purpose:** Model reasoning steps\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"reasoning\"`\n                </ParamField>\n\n                <ParamField body=\"reasoning\" type=\"string\">\n                    The reasoning content\n                </ParamField>\n\n                <ParamField body=\"extras\" type=\"object\">\n                    Additional provider-specific data\n                </ParamField>\n\n                **Example:**\n                ```python\n                {\n                    \"type\": \"reasoning\",\n                    \"reasoning\": \"The user is asking about...\",\n                    \"extras\": {\"signature\": \"abc123\"},\n                }\n                ```\n            </Accordion>\n        </AccordionGroup>\n    </Accordion>\n\n    <Accordion title=\"Multimodal\" icon=\"images\">\n        <AccordionGroup>\n            <Accordion title=\"ImageContentBlock\" icon=\"image\">\n                **Purpose:** Image data\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"image\"`\n                </ParamField>\n\n                <ParamField body=\"url\" type=\"string\">\n                    URL pointing to the image location.\n                </ParamField>\n\n                <ParamField body=\"base64\" type=\"string\">\n                    Base64-encoded image data.\n", "metadata": {"source": "messages.mdx"}}
{"text": "           <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"image\"`\n                </ParamField>\n\n                <ParamField body=\"url\" type=\"string\">\n                    URL pointing to the image location.\n                </ParamField>\n\n                <ParamField body=\"base64\" type=\"string\">\n                    Base64-encoded image data.\n                </ParamField>\n\n                <ParamField body=\"id\" type=\"string\">\n                    Unique identifier for this content block (either generated by the provider or by LangChain).\n                </ParamField>\n\n                <ParamField body=\"mime_type\" type=\"string\">\n                    Image [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#image) (e.g., `image/jpeg`, `image/png`). Required for base64 data.\n                </ParamField>\n            </Accordion>\n            <Accordion title=\"AudioContentBlock\" icon=\"volume-high\">\n                **Purpose:** Audio data\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"audio\"`\n                </ParamField>\n\n                <ParamField body=\"url\" type=\"string\">\n                    URL pointing to the audio location.\n                </ParamField>\n\n                <ParamField body=\"base64\" type=\"string\">\n                    Base64-encoded audio data.\n                </ParamField>\n\n                <ParamField body=\"id\" type=\"string\">\n                    Unique identifier for this content block (either generated by the provider or by LangChain).\n               ", "metadata": {"source": "messages.mdx"}}
{"text": "                  URL pointing to the audio location.\n                </ParamField>\n\n                <ParamField body=\"base64\" type=\"string\">\n                    Base64-encoded audio data.\n                </ParamField>\n\n                <ParamField body=\"id\" type=\"string\">\n                    Unique identifier for this content block (either generated by the provider or by LangChain).\n                </ParamField>\n\n                <ParamField body=\"mime_type\" type=\"string\">\n                    Audio [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#audio) (e.g., `audio/mpeg`, `audio/wav`). Required for base64 data.\n                </ParamField>\n            </Accordion>\n            <Accordion title=\"VideoContentBlock\" icon=\"video\">\n                **Purpose:** Video data\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"video\"`\n                </ParamField>\n\n                <ParamField body=\"url\" type=\"string\">\n                    URL pointing to the video location.\n                </ParamField>\n\n                <ParamField body=\"base64\" type=\"string\">\n                    Base64-encoded video data.\n                </ParamField>\n\n                <ParamField body=\"id\" type=\"string\">\n                    Unique identifier for this content block (either generated by the provider or by LangChain).\n                </ParamField>\n\n                <ParamField body=\"mime_type\" type=\"string\">\n                    Video [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#video) (e.g., `video/mp4`, `video/webm`", "metadata": {"source": "messages.mdx"}}
{"text": "               </ParamField>\n\n                <ParamField body=\"id\" type=\"string\">\n                    Unique identifier for this content block (either generated by the provider or by LangChain).\n                </ParamField>\n\n                <ParamField body=\"mime_type\" type=\"string\">\n                    Video [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#video) (e.g., `video/mp4`, `video/webm`). Required for base64 data.\n                </ParamField>\n            </Accordion>\n            <Accordion title=\"FileContentBlock\" icon=\"file\">\n                **Purpose:** Generic files (PDF, etc)\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"file\"`\n                </ParamField>\n\n                <ParamField body=\"url\" type=\"string\">\n                    URL pointing to the file location.\n                </ParamField>\n\n                <ParamField body=\"base64\" type=\"string\">\n                    Base64-encoded file data.\n                </ParamField>\n\n                <ParamField body=\"id\" type=\"string\">\n                    Unique identifier for this content block (either generated by the provider or by LangChain).\n                </ParamField>\n\n                <ParamField body=\"mime_type\" type=\"string\">\n                    File [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml) (e.g., `application/pdf`). Required for base64 data.\n                </ParamField>\n            </Accordion>\n            <Accordion title=\"PlainTextContentBlock\" icon=\"align-left\">\n                **Purpose:** Document text (`.txt`, `.md", "metadata": {"source": "messages.mdx"}}
{"text": "    </ParamField>\n\n                <ParamField body=\"mime_type\" type=\"string\">\n                    File [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml) (e.g., `application/pdf`). Required for base64 data.\n                </ParamField>\n            </Accordion>\n            <Accordion title=\"PlainTextContentBlock\" icon=\"align-left\">\n                **Purpose:** Document text (`.txt`, `.md`)\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"text-plain\"`\n                </ParamField>\n\n                <ParamField body=\"text\" type=\"string\">\n                    The text content\n                </ParamField>\n\n                <ParamField body=\"mime_type\" type=\"string\">\n                    [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml) of the text (e.g., `text/plain`, `text/markdown`)\n                </ParamField>\n            </Accordion>\n        </AccordionGroup>\n    </Accordion>\n\n    <Accordion title=\"Tool Calling\" icon=\"wrench\">\n        <AccordionGroup>\n            <Accordion title=\"ToolCall\" icon=\"function\">\n                **Purpose:** Function calls\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"tool_call\"`\n                </ParamField>\n\n                <ParamField body=\"name\" type=\"string\" required>\n                    Name of the tool to call\n                </ParamField>\n\n                <ParamField body=\"args\" type=\"object\" required>\n                    Arguments to pass to the tool\n    ", "metadata": {"source": "messages.mdx"}}
{"text": "    <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"tool_call\"`\n                </ParamField>\n\n                <ParamField body=\"name\" type=\"string\" required>\n                    Name of the tool to call\n                </ParamField>\n\n                <ParamField body=\"args\" type=\"object\" required>\n                    Arguments to pass to the tool\n                </ParamField>\n\n                <ParamField body=\"id\" type=\"string\" required>\n                    Unique identifier for this tool call\n                </ParamField>\n\n                **Example:**\n                ```python\n                {\n                    \"type\": \"tool_call\",\n                    \"name\": \"search\",\n                    \"args\": {\"query\": \"weather\"},\n                    \"id\": \"call_123\"\n                }\n                ```\n            </Accordion>\n            <Accordion title=\"ToolCallChunk\" icon=\"puzzle-piece\">\n                **Purpose:** Streaming tool call fragments\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"tool_call_chunk\"`\n                </ParamField>\n\n                <ParamField body=\"name\" type=\"string\">\n                    Name of the tool being called\n                </ParamField>\n\n                <ParamField body=\"args\" type=\"string\">\n                    Partial tool arguments (may be incomplete JSON)\n                </ParamField>\n\n      ", "metadata": {"source": "messages.mdx"}}
{"text": "              Always `\"tool_call_chunk\"`\n                </ParamField>\n\n                <ParamField body=\"name\" type=\"string\">\n                    Name of the tool being called\n                </ParamField>\n\n                <ParamField body=\"args\" type=\"string\">\n                    Partial tool arguments (may be incomplete JSON)\n                </ParamField>\n\n                <ParamField body=\"id\" type=\"string\">\n                    Tool call identifier\n                </ParamField>\n\n                <ParamField body=\"index\" type=\"number | string\">\n                    Position of this chunk in the stream\n                </ParamField>\n            </Accordion>\n            <Accordion title=\"InvalidToolCall\" icon=\"triangle-exclamation\">\n                **Purpose:** Malformed calls, intended to catch JSON parsing errors.\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"invalid_tool_call\"`\n                </ParamField>\n\n                <ParamField body=\"name\" type=\"string\">\n                    Name of the tool that failed to be called\n                </ParamField>\n\n                <ParamField body=\"args\" type=\"object\">\n                    Arguments to pass to the tool\n                </ParamField>\n\n                <ParamField body=\"error\" type=\"string\">\n                    Description of what went wrong\n                </ParamField>\n            </Accordion>\n        </AccordionGroup>\n    </Accordion>\n\n    <Accordion title=\"Server-Side Tool Execution\" icon=\"server\">\n        <AccordionGroup>\n     ", "metadata": {"source": "messages.mdx"}}
{"text": "\">\n                    Arguments to pass to the tool\n                </ParamField>\n\n                <ParamField body=\"error\" type=\"string\">\n                    Description of what went wrong\n                </ParamField>\n            </Accordion>\n        </AccordionGroup>\n    </Accordion>\n\n    <Accordion title=\"Server-Side Tool Execution\" icon=\"server\">\n        <AccordionGroup>\n            <Accordion title=\"ServerToolCall\" icon=\"wrench\">\n                **Purpose:** Tool call that is executed server-side.\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"server_tool_call\"`\n                </ParamField>\n\n                <ParamField body=\"id\" type=\"string\" required>\n                    An identifier associated with the tool call.\n                </ParamField>\n\n                <ParamField body=\"name\" type=\"string\" required>\n                    The name of the tool to be called.\n                </ParamField>\n\n                <ParamField body=\"args\" type=\"string\" required>\n                    Partial tool arguments (may be incomplete JSON)\n                </ParamField>\n            </Accordion>\n            <Accordion title=\"ServerToolCallChunk\" icon=\"puzzle-piece\">\n                **Purpose:** Streaming server-side tool call fragments\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"server_tool_call_chunk\"`\n                </ParamField>\n\n                <ParamField body=\"id\" type=\"string\">\n                    An identifier associated with the tool call.\n                </ParamField>\n\n   ", "metadata": {"source": "messages.mdx"}}
{"text": "CallChunk\" icon=\"puzzle-piece\">\n                **Purpose:** Streaming server-side tool call fragments\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"server_tool_call_chunk\"`\n                </ParamField>\n\n                <ParamField body=\"id\" type=\"string\">\n                    An identifier associated with the tool call.\n                </ParamField>\n\n                <ParamField body=\"name\" type=\"string\">\n                    Name of the tool being called\n                </ParamField>\n\n                <ParamField body=\"args\" type=\"string\">\n                    Partial tool arguments (may be incomplete JSON)\n                </ParamField>\n\n                <ParamField body=\"index\" type=\"number | string\">\n                    Position of this chunk in the stream\n                </ParamField>\n            </Accordion>\n            <Accordion title=\"ServerToolResult\" icon=\"box-open\">\n                **Purpose:** Search results\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"server_tool_result\"`\n                </ParamField>\n\n                <ParamField body=\"tool_call_id\" type=\"string\" required>\n                    Identifier of the corresponding server tool call.\n                </ParamField>\n\n                <ParamField body=\"id\" type=\"string\">\n                    Identifier associated with the server tool result.\n                </ParamField>\n\n                <ParamField body=\"status\" type=\"string\" required>\n                    Execution status of the server-side tool. `\"success\"` or `\"error\"`.\n    ", "metadata": {"source": "messages.mdx"}}
{"text": ">\n                    Identifier of the corresponding server tool call.\n                </ParamField>\n\n                <ParamField body=\"id\" type=\"string\">\n                    Identifier associated with the server tool result.\n                </ParamField>\n\n                <ParamField body=\"status\" type=\"string\" required>\n                    Execution status of the server-side tool. `\"success\"` or `\"error\"`.\n                </ParamField>\n\n                <ParamField body=\"output\">\n                    Output of the executed tool.\n                </ParamField>\n            </Accordion>\n        </AccordionGroup>\n    </Accordion>\n\n    <Accordion title=\"Provider-Specific Blocks\" icon=\"plug\">\n        <Accordion title=\"NonStandardContentBlock\" icon=\"asterisk\">\n            **Purpose:** Provider-specific escape hatch\n\n            <ParamField body=\"type\" type=\"string\" required>\n                Always `\"non_standard\"`\n            </ParamField>\n\n            <ParamField body=\"value\" type=\"object\" required>\n                Provider-specific data structure\n            </ParamField>\n\n            **Usage:** For experimental or provider-unique features\n        </Accordion>\n\n        Additional provider-specific content types may be found within the [reference documentation](/oss/integrations/providers/overview) of each model provider.\n    </Accordion>\n</AccordionGroup>\n:::\n\n:::js\nContent blocks are represented (either when creating a message or accessing the `contentBlocks` field) as a list of typed objects. Each item in the list must adhere to one of the following block types:\n\n<AccordionGroup>\n    <Accordion title=\"Core\" icon=\"cube\">\n        <AccordionGroup>\n            <Accordion title=\"ContentBlock.Text\" icon=\"text\">\n                **Purpose:** Standard text output\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"text", "metadata": {"source": "messages.mdx"}}
{"text": ">\n</AccordionGroup>\n:::\n\n:::js\nContent blocks are represented (either when creating a message or accessing the `contentBlocks` field) as a list of typed objects. Each item in the list must adhere to one of the following block types:\n\n<AccordionGroup>\n    <Accordion title=\"Core\" icon=\"cube\">\n        <AccordionGroup>\n            <Accordion title=\"ContentBlock.Text\" icon=\"text\">\n                **Purpose:** Standard text output\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"text\"`\n                </ParamField>\n\n                <ParamField body=\"text\" type=\"string\" required>\n                    The text content\n                </ParamField>\n\n                <ParamField body=\"annotations\" type=\"Citation[]\">\n                    List of annotations for the text\n                </ParamField>\n\n                **Example:**\n                ```typescript\n                {\n                    type: \"text\",\n                    text: \"Hello world\",\n                    annotations: []\n                }\n                ```\n            </Accordion>\n            <Accordion title=\"ContentBlock.Reasoning\" icon=\"brain\">\n                **Purpose:** Model reasoning steps\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"reasoning\"`\n                </ParamField>\n\n                <ParamField body=\"reasoning\" type=\"string\" required>\n                    The reasoning content\n                </ParamField>\n\n                **Example:**\n                ```typescript\n   ", "metadata": {"source": "messages.mdx"}}
{"text": "pose:** Model reasoning steps\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"reasoning\"`\n                </ParamField>\n\n                <ParamField body=\"reasoning\" type=\"string\" required>\n                    The reasoning content\n                </ParamField>\n\n                **Example:**\n                ```typescript\n                {\n                    type: \"reasoning\",\n                    reasoning: \"The user is asking about...\"\n                }\n                ```\n            </Accordion>\n        </AccordionGroup>\n    </Accordion>\n\n    <Accordion title=\"Multimodal\" icon=\"images\">\n        <AccordionGroup>\n            <Accordion title=\"ContentBlock.Multimodal.Image\" icon=\"image\">\n                **Purpose:** Image data\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"image\"`\n                </ParamField>\n\n                <ParamField body=\"url\" type=\"string\">\n                    URL pointing to the image location.\n                </ParamField>\n\n                <ParamField body=\"data\" type=\"string\">\n                    Base64-encoded image data.\n                </ParamField>\n\n                <ParamField body=\"fileId\" type=\"string\">\n                    Reference to the image in an external file storage system (e.g., OpenAI or Anthropic's Files API).\n                </ParamField>\n\n                <ParamField body=\"mimeType\" type=\"string\">\n                    Image [MIME", "metadata": {"source": "messages.mdx"}}
{"text": "=\"data\" type=\"string\">\n                    Base64-encoded image data.\n                </ParamField>\n\n                <ParamField body=\"fileId\" type=\"string\">\n                    Reference to the image in an external file storage system (e.g., OpenAI or Anthropic's Files API).\n                </ParamField>\n\n                <ParamField body=\"mimeType\" type=\"string\">\n                    Image [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#image) (e.g., `image/jpeg`, `image/png`). Required for base64 data.\n                </ParamField>\n            </Accordion>\n            <Accordion title=\"ContentBlock.Multimodal.Audio\" icon=\"volume-high\">\n                **Purpose:** Audio data\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"audio\"`\n                </ParamField>\n\n                <ParamField body=\"url\" type=\"string\">\n                    URL pointing to the audio location.\n                </ParamField>\n\n                <ParamField body=\"data\" type=\"string\">\n                    Base64-encoded audio data.\n                </ParamField>\n\n                <ParamField body=\"fileId\" type=\"string\">\n                    Reference to the audio file in an external file storage system (e.g., OpenAI or Anthropic's Files API).\n                </ParamField>\n\n                <ParamField body=\"mimeType\" type=\"string\">\n                    Audio [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#audio) (e.g., `audio/mpeg`, `audio/wav`). Required for base64 data.\n                </ParamField>\n            </Accordion>", "metadata": {"source": "messages.mdx"}}
{"text": "                 Reference to the audio file in an external file storage system (e.g., OpenAI or Anthropic's Files API).\n                </ParamField>\n\n                <ParamField body=\"mimeType\" type=\"string\">\n                    Audio [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#audio) (e.g., `audio/mpeg`, `audio/wav`). Required for base64 data.\n                </ParamField>\n            </Accordion>\n            <Accordion title=\"ContentBlock.Multimodal.Video\" icon=\"video\">\n                **Purpose:** Video data\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"video\"`\n                </ParamField>\n\n                <ParamField body=\"url\" type=\"string\">\n                    URL pointing to the video location.\n                </ParamField>\n\n                <ParamField body=\"data\" type=\"string\">\n                    Base64-encoded video data.\n                </ParamField>\n\n                <ParamField body=\"fileId\" type=\"string\">\n                    Reference to the video file in an external file storage system (e.g., OpenAI or Anthropic's Files API).\n                </ParamField>\n\n                <ParamField body=\"mimeType\" type=\"string\">\n                    Video [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#video) (e.g., `video/mp4`, `video/webm`). Required for base64 data.\n                </ParamField>\n            </Accordion>\n            <Accordion title=\"ContentBlock.Multimodal.File\" icon=\"file\">\n                **Purpose:** Generic files (PDF, etc)\n\n                <ParamField body=\"type\" type=\"string\"", "metadata": {"source": "messages.mdx"}}
{"text": "\">\n                    Video [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#video) (e.g., `video/mp4`, `video/webm`). Required for base64 data.\n                </ParamField>\n            </Accordion>\n            <Accordion title=\"ContentBlock.Multimodal.File\" icon=\"file\">\n                **Purpose:** Generic files (PDF, etc)\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"file\"`\n                </ParamField>\n\n                <ParamField body=\"url\" type=\"string\">\n                    URL pointing to the file location.\n                </ParamField>\n\n                <ParamField body=\"data\" type=\"string\">\n                    Base64-encoded file data.\n                </ParamField>\n\n                <ParamField body=\"fileId\" type=\"string\">\n                    Reference to the file in an external file storage system (e.g., OpenAI or Anthropic's Files API).\n                </ParamField>\n\n                <ParamField body=\"mimeType\" type=\"string\">\n                    File [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml) (e.g., `application/pdf`). Required for base64 data.\n                </ParamField>\n            </Accordion>\n            <Accordion title=\"ContentBlock.Multimodal.PlainText\" icon=\"align-left\">\n                **Purpose:** Document text (`.txt`, `.md`)\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"text-plain\"`\n                </ParamField>\n\n                <ParamField body=\"text\" type=\"string\" required>\n    ", "metadata": {"source": "messages.mdx"}}
{"text": " </ParamField>\n            </Accordion>\n            <Accordion title=\"ContentBlock.Multimodal.PlainText\" icon=\"align-left\">\n                **Purpose:** Document text (`.txt`, `.md`)\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"text-plain\"`\n                </ParamField>\n\n                <ParamField body=\"text\" type=\"string\" required>\n                    The text content\n                </ParamField>\n\n                <ParamField body=\"title\" type=\"string\">\n                    Title of the text content\n                </ParamField>\n\n                <ParamField body=\"mimeType\" type=\"string\">\n                    [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml) of the text (e.g., `text/plain`, `text/markdown`)\n                </ParamField>\n            </Accordion>\n        </AccordionGroup>\n    </Accordion>\n\n    <Accordion title=\"Tool Calling\" icon=\"wrench\">\n        <AccordionGroup>\n            <Accordion title=\"ContentBlock.Tools.ToolCall\" icon=\"function\">\n                **Purpose:** Function calls\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"tool_call\"`\n                </ParamField>\n\n                <ParamField body=\"name\" type=\"string\" required>\n                    Name of the tool to call\n                </ParamField>\n\n                <ParamField body=\"args\" type=\"object\" required>\n                    Arguments to pass to the tool\n                </ParamField>\n\n                <ParamField body", "metadata": {"source": "messages.mdx"}}
{"text": "  Always `\"tool_call\"`\n                </ParamField>\n\n                <ParamField body=\"name\" type=\"string\" required>\n                    Name of the tool to call\n                </ParamField>\n\n                <ParamField body=\"args\" type=\"object\" required>\n                    Arguments to pass to the tool\n                </ParamField>\n\n                <ParamField body=\"id\" type=\"string\" required>\n                    Unique identifier for this tool call\n                </ParamField>\n\n                **Example:**\n                ```typescript\n                {\n                    type: \"tool_call\",\n                    name: \"search\",\n                    args: { query: \"weather\" },\n                    id: \"call_123\"\n                }\n                ```\n            </Accordion>\n            <Accordion title=\"ContentBlock.Tools.ToolCallChunk\" icon=\"puzzle-piece\">\n                **Purpose:** Streaming tool fragments\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"tool_call_chunk\"`\n                </ParamField>\n\n                <ParamField body=\"name\" type=\"string\">\n                    Name of the tool being called\n                </ParamField>\n\n                <ParamField body=\"args\" type=\"string\">\n                    Partial tool arguments (may be incomplete JSON)\n                </ParamField>\n\n                <ParamField body=\"id\" type=\"string\">\n           ", "metadata": {"source": "messages.mdx"}}
{"text": "         </ParamField>\n\n                <ParamField body=\"name\" type=\"string\">\n                    Name of the tool being called\n                </ParamField>\n\n                <ParamField body=\"args\" type=\"string\">\n                    Partial tool arguments (may be incomplete JSON)\n                </ParamField>\n\n                <ParamField body=\"id\" type=\"string\">\n                    Tool call identifier\n                </ParamField>\n\n                <ParamField body=\"index\" type=\"number | string\" required>\n                    Position of this chunk in the stream\n                </ParamField>\n            </Accordion>\n            <Accordion title=\"ContentBlock.Tools.InvalidToolCall\" icon=\"triangle-exclamation\">\n                **Purpose:** Malformed calls\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"invalid_tool_call\"`\n                </ParamField>\n\n                <ParamField body=\"name\" type=\"string\">\n                    Name of the tool that failed to be called\n                </ParamField>\n\n                <ParamField body=\"args\" type=\"string\">\n                    Raw arguments that failed to parse\n                </ParamField>\n\n                <ParamField body=\"error\" type=\"string\" required>\n                    Description of what went wrong\n                </ParamField>\n\n                **Common errors:** Invalid JSON, missing required fields\n            </Accordion>\n        </AccordionGroup>\n    </Accordion>\n\n    <Accordion title=\"Server-Side Tool Execution\" icon=\"server\">\n        <AccordionGroup>\n          ", "metadata": {"source": "messages.mdx"}}
{"text": "           </ParamField>\n\n                <ParamField body=\"error\" type=\"string\" required>\n                    Description of what went wrong\n                </ParamField>\n\n                **Common errors:** Invalid JSON, missing required fields\n            </Accordion>\n        </AccordionGroup>\n    </Accordion>\n\n    <Accordion title=\"Server-Side Tool Execution\" icon=\"server\">\n        <AccordionGroup>\n            <Accordion title=\"ContentBlock.Tools.ServerToolCall\" icon=\"wrench\">\n                **Purpose:** Tool call that is executed server-side.\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"server_tool_call\"`\n                </ParamField>\n\n                <ParamField body=\"id\" type=\"string\" required>\n                    An identifier associated with the tool call.\n                </ParamField>\n\n                <ParamField body=\"name\" type=\"string\" required>\n                    The name of the tool to be called.\n                </ParamField>\n\n                <ParamField body=\"args\" type=\"string\" required>\n                    Partial tool arguments (may be incomplete JSON)\n                </ParamField>\n            </Accordion>\n            <Accordion title=\"ContentBlock.Tools.ServerToolCallChunk\" icon=\"puzzle-piece\">\n                **Purpose:** Streaming server-side tool call fragments\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"server_tool_call_chunk\"`\n                </ParamField>\n\n                <ParamField body=\"id\" type=\"string\">\n                    An identifier associated with the tool call.\n                </ParamField", "metadata": {"source": "messages.mdx"}}
{"text": ".Tools.ServerToolCallChunk\" icon=\"puzzle-piece\">\n                **Purpose:** Streaming server-side tool call fragments\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"server_tool_call_chunk\"`\n                </ParamField>\n\n                <ParamField body=\"id\" type=\"string\">\n                    An identifier associated with the tool call.\n                </ParamField>\n\n                <ParamField body=\"name\" type=\"string\">\n                    Name of the tool being called\n                </ParamField>\n\n                <ParamField body=\"args\" type=\"string\">\n                    Partial tool arguments (may be incomplete JSON)\n                </ParamField>\n\n                <ParamField body=\"index\" type=\"number | string\">\n                    Position of this chunk in the stream\n                </ParamField>\n            </Accordion>\n            <Accordion title=\"ContentBlock.Tools.ServerToolResult\" icon=\"box-open\">\n                **Purpose:** Search results\n\n                <ParamField body=\"type\" type=\"string\" required>\n                    Always `\"server_tool_result\"`\n                </ParamField>\n\n                <ParamField body=\"tool_call_id\" type=\"string\" required>\n                    Identifier of the corresponding server tool call.\n                </ParamField>\n\n                <ParamField body=\"id\" type=\"string\">\n                    Identifier associated with the server tool result.\n                </ParamField>\n\n                <ParamField body=\"status\" type=\"string\" required>\n                    Execution status of the server-side tool. `\"success\"` or", "metadata": {"source": "messages.mdx"}}
{"text": "_call_id\" type=\"string\" required>\n                    Identifier of the corresponding server tool call.\n                </ParamField>\n\n                <ParamField body=\"id\" type=\"string\">\n                    Identifier associated with the server tool result.\n                </ParamField>\n\n                <ParamField body=\"status\" type=\"string\" required>\n                    Execution status of the server-side tool. `\"success\"` or `\"error\"`.\n                </ParamField>\n\n                <ParamField body=\"output\">\n                    Output of the executed tool.\n                </ParamField>\n            </Accordion>\n        </AccordionGroup>\n    </Accordion>\n    <Accordion title=\"Provider-Specific Blocks\" icon=\"plug\">\n        <Accordion title=\"ContentBlock.NonStandard\" icon=\"asterisk\">\n            **Purpose:** Provider-specific escape hatch\n\n            <ParamField body=\"type\" type=\"string\" required>\n                Always `\"non_standard\"`\n            </ParamField>\n\n            <ParamField body=\"value\" type=\"object\" required>\n                Provider-specific data structure\n            </ParamField>\n\n            **Usage:** For experimental or provider-unique features\n        </Accordion>\n\n        Additional provider-specific content types may be found within the [reference documentation](/oss/integrations/providers/overview) of each model provider.\n    </Accordion>\n</AccordionGroup>\n\nEach of these content blocks mentioned above are indvidually addressable as types when importing the @[`ContentBlock`] type.\n\n```typescript\nimport { ContentBlock } from \"langchain\";\n\n// Text block\nconst textBlock: ContentBlock.Text = {\n    type: \"text\",\n    text: \"Hello world\",\n}\n\n// Image block\nconst imageBlock: ContentBlock.Multimodal.Image = {\n    type: \"image\",\n    url: \"https://example.com/image.png\",\n    mimeType: \"image/png\",\n}\n```\n:::\n\n<Tip>\n    View the canonical type definitions in the @[API reference][langchain.messages].\n</", "metadata": {"source": "messages.mdx"}}
{"text": " model provider.\n    </Accordion>\n</AccordionGroup>\n\nEach of these content blocks mentioned above are indvidually addressable as types when importing the @[`ContentBlock`] type.\n\n```typescript\nimport { ContentBlock } from \"langchain\";\n\n// Text block\nconst textBlock: ContentBlock.Text = {\n    type: \"text\",\n    text: \"Hello world\",\n}\n\n// Image block\nconst imageBlock: ContentBlock.Multimodal.Image = {\n    type: \"image\",\n    url: \"https://example.com/image.png\",\n    mimeType: \"image/png\",\n}\n```\n:::\n\n<Tip>\n    View the canonical type definitions in the @[API reference][langchain.messages].\n</Tip>\n\n<Info>\n    Content blocks were introduced as a new property on messages in LangChain v1 to standardize content formats across providers while maintaining backward compatibility with existing code.\n\n    Content blocks are not a replacement for the @[`content`][BaseMessage(content)] property, but rather a new property that can be used to access the content of a message in a standardized format.\n</Info>\n\n## Use with chat models\n\n[Chat models](/oss/langchain/models) accept a sequence of message objects as input and return an @[`AIMessage`] as output. Interactions are often stateless, so that a simple conversational loop involves invoking a model with a growing list of messages.\n\nRefer to the below guides to learn more:\n\n- Built-in features for [persisting and managing conversation histories](/oss/langchain/short-term-memory)\n- Strategies for managing context windows, including [trimming and summarizing messages](/oss/langchain/short-term-memory#common-patterns)\n", "metadata": {"source": "messages.mdx"}}
{"text": "---\ntitle: Agent Chat UI\n---\n\nimport agent_chat_ui from '/snippets/oss/agent-chat-ui.mdx';\n\n<agent_chat_ui />\n\n### Connect to your agent\n\nAgent Chat UI can connect to both [local](/oss/langchain/studio#setup-local-agent-server) and [deployed agents](/oss/langchain/deploy).\n\nAfter starting Agent Chat UI, you'll need to configure it to connect to your agent:\n\n1. **Graph ID**: Enter your graph name (find this under `graphs` in your `langgraph.json` file)\n2. **Deployment URL**: Your Agent server's endpoint (e.g., `http://localhost:2024` for local development, or your deployed agent's URL)\n3. **LangSmith API key (optional)**: Add your LangSmith API key (not required if you're using a local Agent server)\n\nOnce configured, Agent Chat UI will automatically fetch and display any interrupted threads from your agent.\n\n<Tip>\n  Agent Chat UI has out-of-the-box support for rendering tool calls and tool result messages. To customize what messages are shown, see [Hiding Messages in the Chat](https://github.com/langchain-ai/agent-chat-ui?tab=readme-ov-file#hiding-messages-in-the-chat).\n</Tip>\n", "metadata": {"source": "ui.mdx"}}
{"text": "---\ntitle: Human-in-the-loop\n---\n\nThe Human-in-the-Loop (HITL) [middleware](/oss/langchain/middleware/built-in#human-in-the-loop) lets you add human oversight to agent tool calls.\nWhen a model proposes an action that might require review \u2014 for example, writing to a file or executing SQL \u2014 the middleware can pause execution and wait for a decision.\n\nIt does this by checking each tool call against a configurable policy. If intervention is needed, the middleware issues an @[interrupt] that halts execution. The graph state is saved using LangGraph's [persistence layer](/oss/langgraph/persistence), so execution can pause safely and resume later.\n\nA human decision then determines what happens next: the action can be approved as-is (`approve`), modified before running (`edit`), or rejected with feedback (`reject`).\n\n## Interrupt decision types\n\nThe [middleware](/oss/langchain/middleware/built-in#human-in-the-loop) defines three built-in ways a human can respond to an interrupt:\n\n| Decision Type | Description                                                               | Example Use Case                                    |\n|---------------|---------------------------------------------------------------------------|-----------------------------------------------------|\n| \u2705 `approve`   | The action is approved as-is and executed without changes.                | Send an email draft exactly as written              |\n| \u270f\ufe0f `edit`     | The tool call is executed with modifications.                             | Change the recipient before sending an email        |\n| \u274c `reject`    | The tool call is rejected, with an explanation added to the conversation. | Reject an email draft and explain how to rewrite it |\n\nThe available decision types for each tool depend on the policy you configure in `interrupt_on`.\nWhen multiple tool calls are paused at the same time, each action requires a separate decision.\nDecisions must be provided in the same order as the actions appear in the interrupt request.\n\n<Tip>\nWhen **editing** tool arguments, make changes conservatively. Significant modifications to the original arguments may cause the model to re-evaluate its approach and potentially execute the tool multiple times or take unexpected actions.\n</Tip>\n\n\n## Configuring interrupts\n\nTo use HITL, add the [middleware](/oss/langchain/middleware/built-in#human-in-the-loop) to the agent's `middleware` list when creating the agent.\n\nYou configure it with a mapping of tool actions to the decision types that are allowed for each action. The middleware will interrupt execution when a tool call matches an action in the mapping.\n\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware # [!code highlight]\nfrom langgraph.checkpoint.memory import InMemorySaver", "metadata": {"source": "human-in-the-loop.mdx"}}
{"text": ".\n\n<Tip>\nWhen **editing** tool arguments, make changes conservatively. Significant modifications to the original arguments may cause the model to re-evaluate its approach and potentially execute the tool multiple times or take unexpected actions.\n</Tip>\n\n\n## Configuring interrupts\n\nTo use HITL, add the [middleware](/oss/langchain/middleware/built-in#human-in-the-loop) to the agent's `middleware` list when creating the agent.\n\nYou configure it with a mapping of tool actions to the decision types that are allowed for each action. The middleware will interrupt execution when a tool call matches an action in the mapping.\n\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware # [!code highlight]\nfrom langgraph.checkpoint.memory import InMemorySaver # [!code highlight]\n\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[write_file_tool, execute_sql_tool, read_data_tool],\n    middleware=[\n        HumanInTheLoopMiddleware( # [!code highlight]\n            interrupt_on={\n                \"write_file\": True,  # All decisions (approve, edit, reject) allowed\n                \"execute_sql\": {\"allowed_decisions\": [\"approve\", \"reject\"]},  # No editing allowed\n                # Safe operation, no approval needed\n                \"read_data\": False,\n            },\n            # Prefix for interrupt messages - combined with tool name and args to form the full message\n            # e.g., \"Tool execution pending approval: execute_sql with query='DELETE FROM...'\"\n            # Individual tools can override this by specifying a \"description\" in their interrupt config\n            description_prefix=\"Tool execution pending approval\",\n        ),\n    ],\n    # Human-in-the-loop requires checkpointing to handle interrupts.\n    # In production, use a persistent checkpointer like AsyncPostgresSaver.\n    checkpointer=InMemorySaver(),  # [!code highlight]\n)\n```\n:::\n\n:::js\n```ts\nimport { createAgent, humanInTheLoopMiddleware } from \"langchain\"; // [!code highlight]\nimport { MemorySaver } from \"@langchain/langgraph\"; // [!code highlight]\n\nconst agent = createAgent({\n    model: \"gpt-4.1\",\n    tools: [writeFileTool, executeSQLTool, readDataTool],\n    middleware: [\n        humanInTheLoopMiddleware({\n            interruptOn: {\n                write_file: true, // All decisions (approve, edit, reject) allowed\n     ", "metadata": {"source": "human-in-the-loop.mdx"}}
{"text": "    checkpointer=InMemorySaver(),  # [!code highlight]\n)\n```\n:::\n\n:::js\n```ts\nimport { createAgent, humanInTheLoopMiddleware } from \"langchain\"; // [!code highlight]\nimport { MemorySaver } from \"@langchain/langgraph\"; // [!code highlight]\n\nconst agent = createAgent({\n    model: \"gpt-4.1\",\n    tools: [writeFileTool, executeSQLTool, readDataTool],\n    middleware: [\n        humanInTheLoopMiddleware({\n            interruptOn: {\n                write_file: true, // All decisions (approve, edit, reject) allowed\n                execute_sql: {\n                    allowedDecisions: [\"approve\", \"reject\"],\n                    // No editing allowed\n                    description: \"\ud83d\udea8 SQL execution requires DBA approval\",\n                },\n                // Safe operation, no approval needed\n                read_data: false,\n            },\n            // Prefix for interrupt messages - combined with tool name and args to form the full message\n            // e.g., \"Tool execution pending approval: execute_sql with query='DELETE FROM...'\"\n            // Individual tools can override this by specifying a \"description\" in their interrupt config\n            descriptionPrefix: \"Tool execution pending approval\",\n        }),\n    ],\n    // Human-in-the-loop requires checkpointing to handle interrupts.\n    // In production, use a persistent checkpointer like AsyncPostgresSaver.\n    checkpointer: new MemorySaver(), // [!code highlight]\n});\n```\n:::\n\n\n<Info>\n    You must configure a checkpointer to persist the graph state across interrupts.\n    In production, use a persistent checkpointer like @[`AsyncPostgresSaver`]. For testing or prototyping, use @[`InMemorySaver`].\n\n    When invoking the agent, pass a `config` that includes the **thread ID** to associate execution with a conversation thread.\n    See the [LangGraph interrupts documentation](/oss/langgraph/interrupts) for details.\n</Info>\n\n<Accordion title=\"Configuration options\">\n\n:::python\n<ParamField body=\"interrupt_on\" type=\"dict\" required>\n    Mapping of tool names to approval configs. Values can be `True` (interrupt with default config), `False` (auto-approve), or an `InterruptOnConfig` object.\n</ParamField>\n\n<ParamField body=\"description_prefix\" type=\"string\" default=\"Tool execution requires", "metadata": {"source": "human-in-the-loop.mdx"}}
{"text": "   In production, use a persistent checkpointer like @[`AsyncPostgresSaver`]. For testing or prototyping, use @[`InMemorySaver`].\n\n    When invoking the agent, pass a `config` that includes the **thread ID** to associate execution with a conversation thread.\n    See the [LangGraph interrupts documentation](/oss/langgraph/interrupts) for details.\n</Info>\n\n<Accordion title=\"Configuration options\">\n\n:::python\n<ParamField body=\"interrupt_on\" type=\"dict\" required>\n    Mapping of tool names to approval configs. Values can be `True` (interrupt with default config), `False` (auto-approve), or an `InterruptOnConfig` object.\n</ParamField>\n\n<ParamField body=\"description_prefix\" type=\"string\" default=\"Tool execution requires approval\">\n    Prefix for action request descriptions\n</ParamField>\n\n**`InterruptOnConfig` options:**\n\n<ParamField body=\"allowed_decisions\" type=\"list[string]\">\n    List of allowed decisions: `'approve'`, `'edit'`, or `'reject'`\n</ParamField>\n\n<ParamField body=\"description\" type=\"string | callable\">\n    Static string or callable function for custom description\n</ParamField>\n:::\n\n:::js\n<ParamField body=\"interruptOn\" type=\"object\" required>\n    Mapping of tool names to approval configs\n</ParamField>\n\n**Tool approval config options:**\n\n<ParamField body=\"allowAccept\" type=\"boolean\" default=\"false\">\n    Whether approval is allowed\n</ParamField>\n\n<ParamField body=\"allowEdit\" type=\"boolean\" default=\"false\">\n    Whether editing is allowed\n</ParamField>\n\n<ParamField body=\"allowRespond\" type=\"boolean\" default=\"false\">\n    Whether responding/rejection is allowed\n</ParamField>\n:::\n\n</Accordion>\n\n## Responding to interrupts\n\nWhen you invoke the agent, it runs until it either completes or an interrupt is raised. An interrupt is triggered when a tool call matches the policy you configured in `interrupt_on`. In that case, the invocation result will include an `__interrupt__` field with the actions that require review. You can then present those actions to a reviewer and resume execution once decisions are provided.\n\n\n:::python\n```python\nfrom langgraph.types import Command\n\n# Human-in-the-loop leverages LangGraph's persistence layer.\n# You must provide a thread ID to associate the execution with a conversation thread,\n# so the conversation can be paused and resumed (as is needed for human review).\nconfig = {\"configurable\": {\"thread_id\": \"some_id\"}} # [!code highlight]\n# Run the graph until the interrupt is hit.\nresult = agent.invoke(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"Delete old records from the database\",\n            }\n        ]\n    },\n    config=config # [!code highlight]\n)\n\n#", "metadata": {"source": "human-in-the-loop.mdx"}}
{"text": " You must provide a thread ID to associate the execution with a conversation thread,\n# so the conversation can be paused and resumed (as is needed for human review).\nconfig = {\"configurable\": {\"thread_id\": \"some_id\"}} # [!code highlight]\n# Run the graph until the interrupt is hit.\nresult = agent.invoke(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"Delete old records from the database\",\n            }\n        ]\n    },\n    config=config # [!code highlight]\n)\n\n# The interrupt contains the full HITL request with action_requests and review_configs\nprint(result['__interrupt__'])\n# > [\n# >    Interrupt(\n# >       value={\n# >          'action_requests': [\n# >             {\n# >                'name': 'execute_sql',\n# >                'arguments': {'query': 'DELETE FROM records WHERE created_at < NOW() - INTERVAL \\'30 days\\';'},\n# >                'description': 'Tool execution pending approval\\n\\nTool: execute_sql\\nArgs: {...}'\n# >             }\n# >          ],\n# >          'review_configs': [\n# >             {\n# >                'action_name': 'execute_sql',\n# >                'allowed_decisions': ['approve', 'reject']\n# >             }\n# >          ]\n# >       }\n# >    )\n# > ]\n\n\n# Resume with approval decision\nagent.invoke(\n    Command( # [!code highlight]\n        resume={\"decisions\": [{\"type\": \"approve\"}]}  # or \"reject\" [!code highlight]\n    ), # [!code highlight]\n    config=config # Same thread ID to resume the paused conversation\n)\n```\n:::\n\n:::js\n```typescript\nimport { HumanMessage } from \"@langchain/core/messages\";\nimport { Command } from \"@langchain/langgraph\";\n\n// You must provide a thread ID to associate the execution with a conversation thread,\n// so the conversation can be paused and resumed (as is needed for human review).\nconst config = { configurable: { thread_id: \"some_id\" } }; // [!code highlight]\n\n// Run the graph until the interrupt is hit.\nconst result = await agent.invoke(\n ", "metadata": {"source": "human-in-the-loop.mdx"}}
{"text": " [!code highlight]\n        resume={\"decisions\": [{\"type\": \"approve\"}]}  # or \"reject\" [!code highlight]\n    ), # [!code highlight]\n    config=config # Same thread ID to resume the paused conversation\n)\n```\n:::\n\n:::js\n```typescript\nimport { HumanMessage } from \"@langchain/core/messages\";\nimport { Command } from \"@langchain/langgraph\";\n\n// You must provide a thread ID to associate the execution with a conversation thread,\n// so the conversation can be paused and resumed (as is needed for human review).\nconst config = { configurable: { thread_id: \"some_id\" } }; // [!code highlight]\n\n// Run the graph until the interrupt is hit.\nconst result = await agent.invoke(\n    {\n        messages: [new HumanMessage(\"Delete old records from the database\")],\n    },\n    config // [!code highlight]\n);\n\n\n// The interrupt contains the full HITL request with action_requests and review_configs\nconsole.log(result.__interrupt__);\n// > [\n// >    Interrupt(\n// >       value: {\n// >          action_requests: [\n// >             {\n// >                name: 'execute_sql',\n// >                arguments: { query: 'DELETE FROM records WHERE created_at < NOW() - INTERVAL \\'30 days\\';' },\n// >                description: 'Tool execution pending approval\\n\\nTool: execute_sql\\nArgs: {...}'\n// >             }\n// >          ],\n// >          review_configs: [\n// >             {\n// >                action_name: 'execute_sql',\n// >                allowed_decisions: ['approve', 'reject']\n// >             }\n// >          ]\n// >       }\n// >    )\n// > ]\n\n// Resume with approval decision\nawait agent.invoke(\n    new Command({ // [!code highlight]\n        resume: { decisions: [{ type: \"approve\" }] }, // or \"reject\" [!code highlight]\n    }), // [!code highlight]\n    config // Same thread ID to resume the paused conversation\n);\n```\n:::\n\n### Decision types\n\n<Tabs>\n<Tab title=\"\u2705 approve\">\nUse `approve` to approve the tool call as-is and execute it without changes.\n\n:::python\n```python\nagent.invoke(\n    Command(\n        # Decisions are provided as a list, one per action under review.", "metadata": {"source": "human-in-the-loop.mdx"}}
{"text": "// >       }\n// >    )\n// > ]\n\n// Resume with approval decision\nawait agent.invoke(\n    new Command({ // [!code highlight]\n        resume: { decisions: [{ type: \"approve\" }] }, // or \"reject\" [!code highlight]\n    }), // [!code highlight]\n    config // Same thread ID to resume the paused conversation\n);\n```\n:::\n\n### Decision types\n\n<Tabs>\n<Tab title=\"\u2705 approve\">\nUse `approve` to approve the tool call as-is and execute it without changes.\n\n:::python\n```python\nagent.invoke(\n    Command(\n        # Decisions are provided as a list, one per action under review.\n        # The order of decisions must match the order of actions\n        # listed in the `__interrupt__` request.\n        resume={\n            \"decisions\": [\n                {\n                    \"type\": \"approve\",\n                }\n            ]\n        }\n    ),\n    config=config  # Same thread ID to resume the paused conversation\n)\n```\n:::\n\n:::js\n```typescript\nawait agent.invoke(\n    new Command({\n        // Decisions are provided as a list, one per action under review.\n        // The order of decisions must match the order of actions\n        // listed in the `__interrupt__` request.\n        resume: {\n            decisions: [\n                {\n                    type: \"approve\",\n                }\n            ]\n        }\n    }),\n    config  // Same thread ID to resume the paused conversation\n);\n```\n:::\n</Tab>\n<Tab title=\"\u270f\ufe0f edit\">\n    Use `edit` to modify the tool call before execution.\n    Provide the edited action with the new tool name and arguments.\n\n    :::python\n    ```python\n    agent.invoke(\n        Command(\n            # Decisions are provided as a list, one per action under review.\n            # The order of decisions must match the order of actions\n            # listed in the `__interrupt__` request.\n            resume={\n                \"decisions\": [\n         ", "metadata": {"source": "human-in-the-loop.mdx"}}
{"text": "\n:::\n</Tab>\n<Tab title=\"\u270f\ufe0f edit\">\n    Use `edit` to modify the tool call before execution.\n    Provide the edited action with the new tool name and arguments.\n\n    :::python\n    ```python\n    agent.invoke(\n        Command(\n            # Decisions are provided as a list, one per action under review.\n            # The order of decisions must match the order of actions\n            # listed in the `__interrupt__` request.\n            resume={\n                \"decisions\": [\n                    {\n                        \"type\": \"edit\",\n                        # Edited action with tool name and args\n                        \"edited_action\": {\n                            # Tool name to call.\n                            # Will usually be the same as the original action.\n                            \"name\": \"new_tool_name\",\n                            # Arguments to pass to the tool.\n                            \"args\": {\"key1\": \"new_value\", \"key2\": \"original_value\"},\n                        }\n                    }\n                ]\n            }\n        ),\n        config=config  # Same thread ID to resume the paused conversation\n    )\n    ```\n    :::\n\n    :::js\n    ```typescript\n    await agent.invoke(\n        new Command({\n            // Decisions are provided as a list, one per action under review.\n            // The order of decisions must match the order of actions\n            // listed in the `__interrupt__` request.\n            resume: {\n                decisions: [\n                    {\n           ", "metadata": {"source": "human-in-the-loop.mdx"}}
{"text": " ID to resume the paused conversation\n    )\n    ```\n    :::\n\n    :::js\n    ```typescript\n    await agent.invoke(\n        new Command({\n            // Decisions are provided as a list, one per action under review.\n            // The order of decisions must match the order of actions\n            // listed in the `__interrupt__` request.\n            resume: {\n                decisions: [\n                    {\n                        type: \"edit\",\n                        // Edited action with tool name and args\n                        editedAction: {\n                            // Tool name to call.\n                            // Will usually be the same as the original action.\n                            name: \"new_tool_name\",\n                            // Arguments to pass to the tool.\n                            args: { key1: \"new_value\", key2: \"original_value\" },\n                        }\n                    }\n                ]\n            }\n        }),\n        config  // Same thread ID to resume the paused conversation\n    );\n    ```\n    :::\n\n    <Tip>\n        When **editing** tool arguments, make changes conservatively. Significant modifications to the original arguments may cause the model to re-evaluate its approach and potentially execute the tool multiple times or take unexpected actions.\n    </Tip>\n\n</Tab>\n\n<Tab title=\"\u274c reject\">\n    Use `reject` to reject the tool call and provide feedback instead of execution.\n\n    :::python\n    ```python\n    agent.invoke(\n        Command(\n            # Decisions are provided as a list, one per action under review.\n            # The order of decisions must match the order of actions\n            # listed in the `__interrupt__` request.\n", "metadata": {"source": "human-in-the-loop.mdx"}}
{"text": ">\n        When **editing** tool arguments, make changes conservatively. Significant modifications to the original arguments may cause the model to re-evaluate its approach and potentially execute the tool multiple times or take unexpected actions.\n    </Tip>\n\n</Tab>\n\n<Tab title=\"\u274c reject\">\n    Use `reject` to reject the tool call and provide feedback instead of execution.\n\n    :::python\n    ```python\n    agent.invoke(\n        Command(\n            # Decisions are provided as a list, one per action under review.\n            # The order of decisions must match the order of actions\n            # listed in the `__interrupt__` request.\n            resume={\n                \"decisions\": [\n                    {\n                        \"type\": \"reject\",\n                        # An explanation about why the action was rejected\n                        \"message\": \"No, this is wrong because ..., instead do this ...\",\n                    }\n                ]\n            }\n        ),\n        config=config  # Same thread ID to resume the paused conversation\n    )\n    ```\n    :::\n\n    :::js\n    ```typescript\n    await agent.invoke(\n        new Command({\n            // Decisions are provided as a list, one per action under review.\n            // The order of decisions must match the order of actions\n            // listed in the `__interrupt__` request.\n            resume: {\n                decisions: [\n                    {\n                        type: \"reject\",\n                        // An explanation about why the action was rejected\n                        message: \"No, this is wrong because ..., instead do this ...\",\n                    }\n                ]\n            }\n        }),\n        config  // Same thread", "metadata": {"source": "human-in-the-loop.mdx"}}
{"text": ": [\n                    {\n                        type: \"reject\",\n                        // An explanation about why the action was rejected\n                        message: \"No, this is wrong because ..., instead do this ...\",\n                    }\n                ]\n            }\n        }),\n        config  // Same thread ID to resume the paused conversation\n    );\n    ```\n    :::\n\nThe `message` is added to the conversation as feedback to help the agent understand why the action was rejected and what it should do instead.\n\n---\n\n### Multiple decisions\n\nWhen multiple actions are under review, provide a decision for each action in the same order as they appear in the interrupt:\n\n:::python\n```python\n{\n    \"decisions\": [\n        {\"type\": \"approve\"},\n        {\n            \"type\": \"edit\",\n            \"edited_action\": {\n                \"name\": \"tool_name\",\n                \"args\": {\"param\": \"new_value\"}\n            }\n        },\n        {\n            \"type\": \"reject\",\n            \"message\": \"This action is not allowed\"\n        }\n    ]\n}\n```\n:::\n\n:::js\n```typescript\n{\n    decisions: [\n        { type: \"approve\" },\n        {\n            type: \"edit\",\n            editedAction: {\n                name: \"tool_name\",\n                args: { param: \"new_value\" }\n            }\n        },\n        {\n            type: \"reject\",\n            message: \"This action is not allowed\"\n        }\n    ]\n}\n```\n:::\n\n</Tab>\n</Tabs>\n\n## Streaming with human-in-the-loop\n\nYou can use `stream()` instead of `invoke()` to get real-time updates while the agent runs and handles interrupts. Use `stream_mode=['updates', 'messages']` to stream", "metadata": {"source": "human-in-the-loop.mdx"}}
{"text": "       name: \"tool_name\",\n                args: { param: \"new_value\" }\n            }\n        },\n        {\n            type: \"reject\",\n            message: \"This action is not allowed\"\n        }\n    ]\n}\n```\n:::\n\n</Tab>\n</Tabs>\n\n## Streaming with human-in-the-loop\n\nYou can use `stream()` instead of `invoke()` to get real-time updates while the agent runs and handles interrupts. Use `stream_mode=['updates', 'messages']` to stream both agent progress and LLM tokens.\n\n:::python\n```python\nfrom langgraph.types import Command\n\nconfig = {\"configurable\": {\"thread_id\": \"some_id\"}}\n\n# Stream agent progress and LLM tokens until interrupt\nfor mode, chunk in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Delete old records from the database\"}]},\n    config=config,\n    stream_mode=[\"updates\", \"messages\"],  # [!code highlight]\n):\n    if mode == \"messages\":\n        # LLM token\n        token, metadata = chunk\n        if token.content:\n            print(token.content, end=\"\", flush=True)\n    elif mode == \"updates\":\n        # Check for interrupt\n        if \"__interrupt__\" in chunk:\n            print(f\"\\n\\nInterrupt: {chunk['__interrupt__']}\")\n\n# Resume with streaming after human decision\nfor mode, chunk in agent.stream(\n    Command(resume={\"decisions\": [{\"type\": \"approve\"}]}),\n    config=config,\n    stream_mode=[\"updates\", \"messages\"],\n):\n    if mode == \"messages\":\n        token, metadata = chunk\n        if token.content:\n            print(token.content, end=\"\", flush=True)\n```\n:::\n\n:::js\n```typescript\nimport { Command } from \"@langchain/langgraph\";\n\nconst config = { configurable: { thread_id: \"some_id\" } };\n\n// Stream agent progress and LLM tokens until interrupt\nfor await (const [mode, chunk] of await agent.stream(\n    { messages: [{ role: \"user\", content: \"Delete old records from the database\" }] },\n    { ...config, streamMode: [\"updates\", \"messages\"] }  // [!code highlight]\n)) {\n    if (mode === \"messages\") {\n        // LLM token\n        const [token, metadata] = chunk;\n        if (token.content)", "metadata": {"source": "human-in-the-loop.mdx"}}
{"text": ", end=\"\", flush=True)\n```\n:::\n\n:::js\n```typescript\nimport { Command } from \"@langchain/langgraph\";\n\nconst config = { configurable: { thread_id: \"some_id\" } };\n\n// Stream agent progress and LLM tokens until interrupt\nfor await (const [mode, chunk] of await agent.stream(\n    { messages: [{ role: \"user\", content: \"Delete old records from the database\" }] },\n    { ...config, streamMode: [\"updates\", \"messages\"] }  // [!code highlight]\n)) {\n    if (mode === \"messages\") {\n        // LLM token\n        const [token, metadata] = chunk;\n        if (token.content) {\n            process.stdout.write(token.content);\n        }\n    } else if (mode === \"updates\") {\n        // Check for interrupt\n        if (\"__interrupt__\" in chunk) {\n            console.log(`\\n\\nInterrupt: ${JSON.stringify(chunk.__interrupt__)}`);\n        }\n    }\n}\n\n// Resume with streaming after human decision\nfor await (const [mode, chunk] of await agent.stream(\n    new Command({ resume: { decisions: [{ type: \"approve\" }] } }),\n    { ...config, streamMode: [\"updates\", \"messages\"] }\n)) {\n    if (mode === \"messages\") {\n        const [token, metadata] = chunk;\n        if (token.content) {\n            process.stdout.write(token.content);\n        }\n    }\n}\n```\n:::\n\nSee the [Streaming](/oss/langchain/streaming) guide for more details on stream modes.\n\n## Execution lifecycle\n\nThe middleware defines an `after_model` hook that runs after the model generates a response but before any tool calls are executed:\n\n1. The agent invokes the model to generate a response.\n2. The middleware inspects the response for tool calls.\n3. If any calls require human input, the middleware builds a `HITLRequest` with `action_requests` and `review_configs` and calls @[interrupt].\n4. The agent waits for human decisions.\n5. Based on the `HITLResponse` decisions, the middleware executes approved or edited calls, synthesizes @[ToolMessage]'s for rejected calls, and resumes execution.\n\n\n\n## Custom HITL logic\n\nFor more specialized workflows, you can build custom HITL logic directly using the @[interrupt] primitive and [middleware](/oss/langchain/middleware) abstraction.\n\nReview the [execution lifecycle](#execution-lifecycle) above to understand how to integrate interrupts into the agent's operation.\n\n", "metadata": {"source": "human-in-the-loop.mdx"}}
{"text": "---\ntitle: Evaluate agent performance\n---\n\nTo evaluate your agent's performance you can use `LangSmith` [evaluations](https://docs.langchain.com/langsmith/evaluation). You would need to first define an evaluator function to judge the results from an agent, such as final outputs or trajectory. Depending on your evaluation technique, this may or may not involve a reference output:\n\n:::python\n```python\ndef evaluator(*, outputs: dict, reference_outputs: dict):\n    # compare agent outputs against reference outputs\n    output_messages = outputs[\"messages\"]\n    reference_messages = reference_outputs[\"messages\"]\n    score = compare_messages(output_messages, reference_messages)\n    return {\"key\": \"evaluator_score\", \"score\": score}\n```\n:::\n\n:::js\n```typescript\ntype EvaluatorParams = {\n    outputs: Record<string, any>;\n    referenceOutputs: Record<string, any>;\n};\n\nfunction evaluator({ outputs, referenceOutputs }: EvaluatorParams) {\n    // compare agent outputs against reference outputs\n    const outputMessages = outputs.messages;\n    const referenceMessages = referenceOutputs.messages;\n    const score = compareMessages(outputMessages, referenceMessages);\n    return { key: \"evaluator_score\", score: score };\n}\n```\n:::\n\nTo get started, you can use prebuilt evaluators from `AgentEvals` package:\n\n:::python\n<CodeGroup>\n```bash pip\npip install -U agentevals\n```\n\n```bash uv\nuv add agentevals\n```\n</CodeGroup>\n:::\n\n:::js\n```bash\nnpm install agentevals\n```\n:::\n\n## Create evaluator\n\nA common way to evaluate agent performance is by comparing its trajectory (the order in which it calls its tools) against a reference trajectory:\n\n:::python\n```python\nimport json\nfrom agentevals.trajectory.match import create_trajectory_match_evaluator  # [!code highlight]\n\noutputs = [\n    {\n        \"role\": \"assistant\",\n        \"tool_calls\": [\n            {\n                \"function\": {\n                    \"name\": \"get_weather\",\n                    \"arguments\": json.dumps({\"city\": \"san francisco\"}),\n                }\n            },\n            {\n                \"function\": {\n                    \"name\": \"get_directions\",\n                    \"arguments\": json.dumps({\"destination\": \"", "metadata": {"source": "evals.mdx"}}
{"text": "     \"function\": {\n                    \"name\": \"get_weather\",\n                    \"arguments\": json.dumps({\"city\": \"san francisco\"}),\n                }\n            },\n            {\n                \"function\": {\n                    \"name\": \"get_directions\",\n                    \"arguments\": json.dumps({\"destination\": \"presidio\"}),\n                }\n            }\n        ],\n    }\n]\nreference_outputs = [\n    {\n        \"role\": \"assistant\",\n        \"tool_calls\": [\n            {\n                \"function\": {\n                    \"name\": \"get_weather\",\n                    \"arguments\": json.dumps({\"city\": \"san francisco\"}),\n                }\n            },\n        ],\n    }\n]\n\n# Create the evaluator\nevaluator = create_trajectory_match_evaluator(\n    trajectory_match_mode=\"superset\",    # [!code highlight]\n)\n\n# Run the evaluator\nresult = evaluator(\n    outputs=outputs, reference_outputs=reference_outputs\n)\n```\n:::\n\n:::js\n```typescript\nimport { createTrajectoryMatchEvaluator } from \"agentevals/trajectory/match\";\n\nconst outputs = [\n    {\n        role: \"assistant\",\n        tool_calls: [\n        {\n            function: {\n            name: \"get_weather\",\n            arguments: JSON.stringify({ city: \"san francisco\" }),\n            },\n        },\n        {\n            function: {\n            name: \"get_directions\",\n            arguments: JSON.stringify({ destination: \"presidio\" }),\n            },\n        },\n        ],", "metadata": {"source": "evals.mdx"}}
{"text": "calls: [\n        {\n            function: {\n            name: \"get_weather\",\n            arguments: JSON.stringify({ city: \"san francisco\" }),\n            },\n        },\n        {\n            function: {\n            name: \"get_directions\",\n            arguments: JSON.stringify({ destination: \"presidio\" }),\n            },\n        },\n        ],\n    },\n];\n\nconst referenceOutputs = [\n    {\n        role: \"assistant\",\n        tool_calls: [\n        {\n            function: {\n            name: \"get_weather\",\n            arguments: JSON.stringify({ city: \"san francisco\" }),\n            },\n        },\n        ],\n    },\n];\n\n// Create the evaluator\nconst evaluator = createTrajectoryMatchEvaluator({\n  // Specify how the trajectories will be compared. `superset` will accept output trajectory as valid if it's a superset of the reference one. Other options include: strict, unordered and subset\n  trajectoryMatchMode: \"superset\", // [!code highlight]\n});\n\n// Run the evaluator\nconst result = evaluator({\n    outputs: outputs,\n    referenceOutputs: referenceOutputs,\n});\n```\n:::\n\n1. Specify how the trajectories will be compared. `superset` will accept output trajectory as valid if it's a superset of the reference one. Other options include: [strict](https://github.com/langchain-ai/agentevals?tab=readme-ov-file#strict-match), [unordered](https://github.com/langchain-ai/agentevals?tab=readme-ov-file#unordered-match) and [subset](https://github.com/langchain-ai/agentevals?tab=readme-ov-file#subset-and-superset-match)\n\nAs a next step, learn more about how to [customize trajectory match evaluator](https://github.com/langchain-ai/agentevals?tab=readme-ov-file#agent-trajectory-match).\n\n### LLM-as-a-judge\n\nYou can use LLM-as-a-judge evaluator that uses an LLM to compare the trajectory against the reference outputs and output a score:\n\n:::python\n```python\nimport json\nfrom agentevals.trajectory.llm import (\n    create_trajectory_llm_as_judge,  # [!code highlight]\n    TRAJECTORY_ACCURACY_PROM", "metadata": {"source": "evals.mdx"}}
{"text": "https://github.com/langchain-ai/agentevals?tab=readme-ov-file#subset-and-superset-match)\n\nAs a next step, learn more about how to [customize trajectory match evaluator](https://github.com/langchain-ai/agentevals?tab=readme-ov-file#agent-trajectory-match).\n\n### LLM-as-a-judge\n\nYou can use LLM-as-a-judge evaluator that uses an LLM to compare the trajectory against the reference outputs and output a score:\n\n:::python\n```python\nimport json\nfrom agentevals.trajectory.llm import (\n    create_trajectory_llm_as_judge,  # [!code highlight]\n    TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE\n)\n\nevaluator = create_trajectory_llm_as_judge(\n    prompt=TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE,\n    model=\"openai:o3-mini\"\n)\n```\n:::\n\n:::js\n```typescript\nimport {\n    createTrajectoryLlmAsJudge,\n    TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE,\n} from \"agentevals/trajectory/llm\";\n\nconst evaluator = createTrajectoryLlmAsJudge({\n    prompt: TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE,\n    model: \"openai:o3-mini\",\n});\n```\n:::\n\n## Run evaluator\n\nTo run an evaluator, you will first need to create a [LangSmith dataset](https://docs.langchain.com/langsmith/evaluation#datasets). To use the prebuilt AgentEvals evaluators, you will need a dataset with the following schema:\n\n* **input**: `{\"messages\": [...]}` input messages to call the agent with.\n* **output**: `{\"messages\": [...]}` expected message history in the agent output. For trajectory evaluation, you can choose to keep only assistant messages.\n\n:::python\n```python\nfrom langsmith import Client\nfrom langchain.agents import create_agent\nfrom agentevals.trajectory.match import create_trajectory_match_evaluator\n\n\nclient = Client()\nagent = create_agent(...)\nevaluator = create_trajectory_match_evaluator(...)\n\nexperiment_results = client.evaluate(\n    lambda inputs: agent.invoke(inputs),\n    # replace with your dataset name\n    data=\"<Name of your dataset>\",\n    evaluators=[evaluator]\n)\n```\n:::\n\n:::js\n```typescript\nimport { Client } from \"langsmith\";\nimport { createAgent } from \"langchain\";\nimport { createTrajectoryMatchEvaluator } from \"agentevals/trajectory/match\";\n\nconst client = new Client();\nconst agent = createAgent({...});\nconst evaluator = createTrajectoryMatchEvaluator({...});\n\nconst experimentResults = await client.evaluate(\n    (inputs) => agent.invoke", "metadata": {"source": "evals.mdx"}}
{"text": "evaluator = create_trajectory_match_evaluator(...)\n\nexperiment_results = client.evaluate(\n    lambda inputs: agent.invoke(inputs),\n    # replace with your dataset name\n    data=\"<Name of your dataset>\",\n    evaluators=[evaluator]\n)\n```\n:::\n\n:::js\n```typescript\nimport { Client } from \"langsmith\";\nimport { createAgent } from \"langchain\";\nimport { createTrajectoryMatchEvaluator } from \"agentevals/trajectory/match\";\n\nconst client = new Client();\nconst agent = createAgent({...});\nconst evaluator = createTrajectoryMatchEvaluator({...});\n\nconst experimentResults = await client.evaluate(\n    (inputs) => agent.invoke(inputs),\n    // replace with your dataset name\n    { data: \"<Name of your dataset>\" },\n    { evaluators: [evaluator] }\n);\n```\n:::\n", "metadata": {"source": "evals.mdx"}}
{"text": "---\ntitle: Quickstart\n---\n\nThis quickstart takes you from a simple setup to a fully functional AI agent in just a few minutes.\n\n<Tip>\n    **LangChain Docs MCP server**\n\n    If you're using an AI coding assistant or IDE (e.g. Claude Code or Cursor), you should install the [LangChain Docs MCP server](/use-these-docs) to get the most out of it. This ensures your agent has access to up-to-date LangChain documentation and examples.\n</Tip>\n\n## Requirements\n\nFor these examples, you will need to:\n\n* [Install](/oss/langchain/install) the LangChain package\n* Set up a [Claude (Anthropic)](https://www.anthropic.com/) account and get an API key\n* Set the `ANTHROPIC_API_KEY` environment variable in your terminal\n\nAlthough these examples use Claude, you can use [any supported model](/oss/integrations/providers/overview) by changing the model name in the code and setting up the appropriate API key.\n\n## Build a basic agent\n\nStart by creating a simple agent that can answer questions and call tools. The agent will use Claude Sonnet 4.5 as its language model, a basic weather function as a tool, and a simple prompt to guide its behavior.\n\n:::python\n```python\nfrom langchain.agents import create_agent\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[get_weather],\n    system_prompt=\"You are a helpful assistant\",\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n)\n```\n:::\n\n:::js\n```ts\nimport { createAgent, tool } from \"langchain\";\nimport * as z from \"zod\";\n\nconst getWeather = tool(\n  (input) => `It's always sunny in ${input.city}!`,\n  {\n    name: \"get_weather\",\n    description: \"Get the weather for a given city\",\n    schema: z.object({\n      city: z.string().describe(\"The city to get the weather for\"),\n    }),\n  }\n);\n\nconst agent = createAgent({\n  model: \"claude-sonnet-4-5-20250929\",\n  tools: [getWeather],\n});\n\nconsole.log(\n  await agent.invoke({\n    messages: [{ role: \"user\", content: \"What's the weather in Tokyo?\" }],\n  })\n);\n```\n:::\n\n<Tip>\n    To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langchain).\n</Tip>\n\n## Build a real-world agent\n\nNext, build a practical weather forecasting agent that demonstrates key production concepts:\n\n1. **Detailed system prompts** for better agent behavior\n2. **Create tools** that integrate with external data\n3. **Model configuration** for consistent responses\n4. **", "metadata": {"source": "quickstart.mdx"}}
{"text": "\n  }\n);\n\nconst agent = createAgent({\n  model: \"claude-sonnet-4-5-20250929\",\n  tools: [getWeather],\n});\n\nconsole.log(\n  await agent.invoke({\n    messages: [{ role: \"user\", content: \"What's the weather in Tokyo?\" }],\n  })\n);\n```\n:::\n\n<Tip>\n    To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langchain).\n</Tip>\n\n## Build a real-world agent\n\nNext, build a practical weather forecasting agent that demonstrates key production concepts:\n\n1. **Detailed system prompts** for better agent behavior\n2. **Create tools** that integrate with external data\n3. **Model configuration** for consistent responses\n4. **Structured output** for predictable results\n5. **Conversational memory** for chat-like interactions\n6. **Create and run the agent** to test the fully functional agent\n\nLet's walk through each step:\n\n<Steps>\n    <Step title=\"Define the system prompt\">\n        The system prompt defines your agent\u2019s role and behavior. Keep it specific and actionable:\n\n        :::python\n        ```python wrap\n        SYSTEM_PROMPT = \"\"\"You are an expert weather forecaster, who speaks in puns.\n\n        You have access to two tools:\n\n        - get_weather_for_location: use this to get the weather for a specific location\n        - get_user_location: use this to get the user's location\n\n        If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.\"\"\"\n        ```\n        :::\n\n        :::js\n        ```ts\n        const systemPrompt = `You are an expert weather forecaster, who speaks in puns.\n\n        You have access to two tools:\n\n        - get_weather_for_location: use this to get the weather for a specific location\n        - get_user_location: use this to get the user's location\n\n        If a user asks you for the weather, make sure you know the location. If you can't tell from the question that they mean wherever they are, use the get_user_location tool to find their location.`;\n        ```\n        :::\n    </Step>\n    <Step title=\"Create tools\">\n        :::python\n        [Tools](/oss/langchain/tools) let a model interact with external systems by calling functions you define.\n        Tools can depend on [runtime context](/oss/langchain/runtime) and also interact with [agent memory](/oss/langchain/short-term-memory).\n\n        Notice below how the `get_user_location` tool uses runtime context", "metadata": {"source": "quickstart.mdx"}}
{"text": "       If a user asks you for the weather, make sure you know the location. If you can't tell from the question that they mean wherever they are, use the get_user_location tool to find their location.`;\n        ```\n        :::\n    </Step>\n    <Step title=\"Create tools\">\n        :::python\n        [Tools](/oss/langchain/tools) let a model interact with external systems by calling functions you define.\n        Tools can depend on [runtime context](/oss/langchain/runtime) and also interact with [agent memory](/oss/langchain/short-term-memory).\n\n        Notice below how the `get_user_location` tool uses runtime context:\n\n        ```python\n        from dataclasses import dataclass\n        from langchain.tools import tool, ToolRuntime\n\n        @tool\n        def get_weather_for_location(city: str) -> str:\n            \"\"\"Get weather for a given city.\"\"\"\n            return f\"It's always sunny in {city}!\"\n\n        @dataclass\n        class Context:\n            \"\"\"Custom runtime context schema.\"\"\"\n            user_id: str\n\n        @tool\n        def get_user_location(runtime: ToolRuntime[Context]) -> str:\n            \"\"\"Retrieve user information based on user ID.\"\"\"\n            user_id = runtime.context.user_id\n            return \"Florida\" if user_id == \"1\" else \"SF\"\n        ```\n\n        <Tip>\n            Tools should be well-documented: their name, description, and argument names become part of the model's prompt.\n            LangChain's @[`@tool` decorator][@tool] adds metadata and enables runtime injection with the `ToolRuntime` parameter.\n        </Tip>\n        :::\n\n        :::js\n        [Tools](/oss/langchain/tools) are functions your agent can call. Oftentimes tools will want to connect to external systems, and will rely on runtime configuration to do so. Notice here how the `getUserLocation` tool does exactly that:\n\n        ```ts\n        import { tool, type ToolRuntime } from \"langchain\";\n        import * as z from \"zod\";\n\n        const getWeather = tool(\n          (input) => `It's always sunny in ${input.city}!`,\n          {\n            name: \"", "metadata": {"source": "quickstart.mdx"}}
{"text": ">\n        :::\n\n        :::js\n        [Tools](/oss/langchain/tools) are functions your agent can call. Oftentimes tools will want to connect to external systems, and will rely on runtime configuration to do so. Notice here how the `getUserLocation` tool does exactly that:\n\n        ```ts\n        import { tool, type ToolRuntime } from \"langchain\";\n        import * as z from \"zod\";\n\n        const getWeather = tool(\n          (input) => `It's always sunny in ${input.city}!`,\n          {\n            name: \"get_weather_for_location\",\n            description: \"Get the weather for a given city\",\n            schema: z.object({\n              city: z.string().describe(\"The city to get the weather for\"),\n            }),\n          }\n        );\n\n        type AgentRuntime = ToolRuntime<unknown, { user_id: string }>;\n\n        const getUserLocation = tool(\n          (_, config: AgentRuntime) => {\n            const { user_id } = config.context;\n            return user_id === \"1\" ? \"Florida\" : \"SF\";\n          },\n          {\n            name: \"get_user_location\",\n            description: \"Retrieve user information based on user ID\",\n          }\n        );\n        ```\n\n        <Note>\n            [Zod](https://zod.dev/) is a library for validating and parsing pre-defined schemas. You can use it to define the input schema for your tools to make sure the agent only calls the tool with the correct arguments.\n\n            Alternatively, you can define the `schema` property as a [JSON schema](https://json-schema.org/overview/what-is-jsonschema) object. Keep in mind that JSON schemas **won't** be validated at runtime.\n\n            <Accordion title=\"Example: Using JSON schema for tool input\">\n                ```ts\n                const getWeather = tool(\n                  ({ city }) => `It's always sunny in ${city}!`,\n                  {\n               ", "metadata": {"source": "quickstart.mdx"}}
{"text": "         Alternatively, you can define the `schema` property as a [JSON schema](https://json-schema.org/overview/what-is-jsonschema) object. Keep in mind that JSON schemas **won't** be validated at runtime.\n\n            <Accordion title=\"Example: Using JSON schema for tool input\">\n                ```ts\n                const getWeather = tool(\n                  ({ city }) => `It's always sunny in ${city}!`,\n                  {\n                    name: \"get_weather_for_location\",\n                    description: \"Get the weather for a given city\",\n                    schema: {\n                      type: \"object\",\n                      properties: {\n                        city: {\n                          type: \"string\",\n                          description: \"The city to get the weather for\"\n                        }\n                      },\n                      required: [\"city\"]\n                    },\n                  }\n                );\n            ```\n            </Accordion>\n        </Note>\n        :::\n    </Step>\n    <Step title=\"Configure your model\">\n        Set up your [language model](/oss/langchain/models) with the right parameters for your use case:\n\n        :::python\n\n        ```python\n        from langchain.chat_models import init_chat_model\n\n        model = init_chat_model(\n            \"claude-sonnet-4-5-20250929\",\n            temperature=0.5,\n            timeout=10,\n            max_tokens=1000\n      ", "metadata": {"source": "quickstart.mdx"}}
{"text": "\n    </Step>\n    <Step title=\"Configure your model\">\n        Set up your [language model](/oss/langchain/models) with the right parameters for your use case:\n\n        :::python\n\n        ```python\n        from langchain.chat_models import init_chat_model\n\n        model = init_chat_model(\n            \"claude-sonnet-4-5-20250929\",\n            temperature=0.5,\n            timeout=10,\n            max_tokens=1000\n        )\n        ```\n        :::\n\n        :::js\n\n        ```ts\n        import { initChatModel } from \"langchain\";\n\n        const model = await initChatModel(\n          \"claude-sonnet-4-5-20250929\",\n          { temperature: 0.5, timeout: 10, maxTokens: 1000 }\n        );\n        ```\n        :::\n\n        Depending on the model and provider chosen, initialization parameters may vary; refer to their reference pages for details.\n    </Step>\n    <Step title=\"Define response format\">\n        :::python\n        Optionally, define a structured response format if you need the agent responses to match\n        a specific schema.\n\n        ```python\n        from dataclasses import dataclass\n\n        # We use a dataclass here, but Pydantic models are also supported.\n        @dataclass\n        class ResponseFormat:\n            \"\"\"Response schema for the agent.\"\"\"\n            # A punny response (always required)\n            punny_response: str\n            # Any interesting information about the weather if available\n            weather_conditions: str | None = None\n        ```\n        :::\n\n        :::js\n        Optionally, define a structured response format if you need the agent responses to match\n        a specific schema.\n\n        ```ts\n        const responseFormat = z.object({\n          punny_response: z.string(),\n          weather_conditions: z.string().optional(),\n        });\n        ```\n        :::\n", "metadata": {"source": "quickstart.mdx"}}
{"text": "      # Any interesting information about the weather if available\n            weather_conditions: str | None = None\n        ```\n        :::\n\n        :::js\n        Optionally, define a structured response format if you need the agent responses to match\n        a specific schema.\n\n        ```ts\n        const responseFormat = z.object({\n          punny_response: z.string(),\n          weather_conditions: z.string().optional(),\n        });\n        ```\n        :::\n    </Step>\n    <Step title=\"Add memory\">\n        Add [memory](/oss/langchain/short-term-memory) to your agent to maintain state across interactions. This allows\n        the agent to remember previous conversations and context.\n\n        :::python\n        ```python\n        from langgraph.checkpoint.memory import InMemorySaver\n\n        checkpointer = InMemorySaver()\n        ```\n        :::\n\n        :::js\n        ```ts\n        import { MemorySaver } from \"@langchain/langgraph\";\n\n        const checkpointer = new MemorySaver();\n        ```\n        :::\n\n        <Info>\n            In production, use a persistent checkpointer that saves message history to a database.\n            See [Add and manage memory](/oss/langgraph/add-memory#manage-short-term-memory) for more details.\n        </Info>\n    </Step>\n    <Step title=\"Create and run the agent\">\n        Now assemble your agent with all the components and run it!\n\n        :::python\n\n        ```python\n        from langchain.agents.structured_output import ToolStrategy\n\n        agent = create_agent(\n            model=model,\n            system_prompt=SYSTEM_PROMPT,\n            tools=[get_user_location, get_weather_for_location],\n            context_schema=Context,\n            response_format=ToolStrategy(ResponseFormat),\n            checkpointer=checkpointer\n        )\n\n        # `thread_id` is a unique identifier for a given conversation.\n        config = {\"configurable\": {\"thread_id\": \"1", "metadata": {"source": "quickstart.mdx"}}
{"text": "_output import ToolStrategy\n\n        agent = create_agent(\n            model=model,\n            system_prompt=SYSTEM_PROMPT,\n            tools=[get_user_location, get_weather_for_location],\n            context_schema=Context,\n            response_format=ToolStrategy(ResponseFormat),\n            checkpointer=checkpointer\n        )\n\n        # `thread_id` is a unique identifier for a given conversation.\n        config = {\"configurable\": {\"thread_id\": \"1\"}}\n\n        response = agent.invoke(\n            {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather outside?\"}]},\n            config=config,\n            context=Context(user_id=\"1\")\n        )\n\n        print(response['structured_response'])\n        # ResponseFormat(\n        #     punny_response=\"Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!\",\n        #     weather_conditions=\"It's always sunny in Florida!\"\n        # )\n\n\n        # Note that we can continue the conversation using the same `thread_id`.\n        response = agent.invoke(\n            {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]},\n            config=config,\n            context=Context(user_id=\"1\")\n        )\n\n        print(response['structured_response'])\n        # ResponseFormat(\n        #     punny_response=\"You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!\",\n        #     weather_conditions=None\n        # )\n        ```\n        :::\n        :::js\n        ```ts\n        import { createAgent } from \"langchain\";\n\n        const agent = createAgent({\n    ", "metadata": {"source": "quickstart.mdx"}}
{"text": "   # ResponseFormat(\n        #     punny_response=\"You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!\",\n        #     weather_conditions=None\n        # )\n        ```\n        :::\n        :::js\n        ```ts\n        import { createAgent } from \"langchain\";\n\n        const agent = createAgent({\n          model: \"claude-sonnet-4-5-20250929\",\n          systemPrompt: systemPrompt,\n          tools: [getUserLocation, getWeather],\n          responseFormat,\n          checkpointer,\n        });\n\n        // `thread_id` is a unique identifier for a given conversation.\n        const config = {\n          configurable: { thread_id: \"1\" },\n          context: { user_id: \"1\" },\n        };\n\n        const response = await agent.invoke(\n          { messages: [{ role: \"user\", content: \"what is the weather outside?\" }] },\n          config\n        );\n        console.log(response.structuredResponse);\n        // {\n        //   punny_response: \"Florida is still having a 'sun-derful' day ...\",\n        //   weather_conditions: \"It's always sunny in Florida!\"\n        // }\n\n        // Note that we can continue the conversation using the same `thread_id`.\n        const thankYouResponse = await agent.invoke(\n          { messages: [{ role: \"user\", content: \"thank you!\" }] },\n          config\n        );\n        console.log(thankYouResponse.structuredResponse);\n        // {\n        //   punny_response: \"You're 'thund-erfully' welcome! ...\",\n        //   weather_conditions: undefined\n        // }\n        ```\n        :::\n    </Step>\n</Steps>\n\n<Expandable title=\"Full example code\">\n:::python\n```python\nfrom dataclasses import dataclass\n\nfrom langchain.agents import create_agent\nfrom langchain.", "metadata": {"source": "quickstart.mdx"}}
{"text": " [{ role: \"user\", content: \"thank you!\" }] },\n          config\n        );\n        console.log(thankYouResponse.structuredResponse);\n        // {\n        //   punny_response: \"You're 'thund-erfully' welcome! ...\",\n        //   weather_conditions: undefined\n        // }\n        ```\n        :::\n    </Step>\n</Steps>\n\n<Expandable title=\"Full example code\">\n:::python\n```python\nfrom dataclasses import dataclass\n\nfrom langchain.agents import create_agent\nfrom langchain.chat_models import init_chat_model\nfrom langchain.tools import tool, ToolRuntime\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langchain.agents.structured_output import ToolStrategy\n\n\n# Define system prompt\nSYSTEM_PROMPT = \"\"\"You are an expert weather forecaster, who speaks in puns.\n\nYou have access to two tools:\n\n- get_weather_for_location: use this to get the weather for a specific location\n- get_user_location: use this to get the user's location\n\nIf a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.\"\"\"\n\n# Define context schema\n@dataclass\nclass Context:\n    \"\"\"Custom runtime context schema.\"\"\"\n    user_id: str\n\n# Define tools\n@tool\ndef get_weather_for_location(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    return f\"It's always sunny in {city}!\"\n\n@tool\ndef get_user_location(runtime: ToolRuntime[Context]) -> str:\n    \"\"\"Retrieve user information based on user ID.\"\"\"\n    user_id = runtime.context.user_id\n    return \"Florida\" if user_id == \"1\" else \"SF\"\n\n# Configure model\nmodel = init_chat_model(\n    \"claude-sonnet-4-5-20250929\",\n    temperature=0\n)\n\n# Define response format\n@dataclass\nclass ResponseFormat:\n    \"\"\"Response schema for the agent.\"\"\"\n    # A punny response (always required)\n    punny_response: str\n    # Any interesting information about the weather if available\n    weather_conditions: str | None = None\n\n# Set up memory\ncheckpointer = InMemorySaver()\n\n# Create agent\nagent = create_agent(\n    model=model,\n    system_prompt=SYSTEM_PROMPT,\n    tools=[get_user_location, get_weather_for_location],\n    context_schema=Context,\n    response_format=ToolStrategy(ResponseFormat),\n    checkpointer=checkpointer\n)\n\n# Run agent\n# `thread_id` is a unique identifier for a given conversation.\nconfig = {\"configurable\": {\"thread_id\": \"", "metadata": {"source": "quickstart.mdx"}}
{"text": " \"\"\"Response schema for the agent.\"\"\"\n    # A punny response (always required)\n    punny_response: str\n    # Any interesting information about the weather if available\n    weather_conditions: str | None = None\n\n# Set up memory\ncheckpointer = InMemorySaver()\n\n# Create agent\nagent = create_agent(\n    model=model,\n    system_prompt=SYSTEM_PROMPT,\n    tools=[get_user_location, get_weather_for_location],\n    context_schema=Context,\n    response_format=ToolStrategy(ResponseFormat),\n    checkpointer=checkpointer\n)\n\n# Run agent\n# `thread_id` is a unique identifier for a given conversation.\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\nresponse = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather outside?\"}]},\n    config=config,\n    context=Context(user_id=\"1\")\n)\n\nprint(response['structured_response'])\n# ResponseFormat(\n#     punny_response=\"Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!\",\n#     weather_conditions=\"It's always sunny in Florida!\"\n# )\n\n\n# Note that we can continue the conversation using the same `thread_id`.\nresponse = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]},\n    config=config,\n    context=Context(user_id=\"1\")\n)\n\nprint(response['structured_response'])\n# ResponseFormat(\n#     punny_response=\"You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!\",\n#     weather_conditions=None\n# )\n```\n:::\n\n\n:::js\n```ts\nimport { createAgent, tool, initChatModel, type ToolRuntime } from \"langchain\";\nimport { MemorySaver } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\n// Define system prompt\nconst systemPrompt = `You are an expert weather forecaster, who speaks in puns.\n\nYou have access to two tools:\n\n- get_weather_for_location: use this to get the weather for a specific location\n- get_user_location: use this to get the user's location\n\nIf a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.`;\n\n// Define tools\nconst getWeather = tool(\n  ({ city }) => `It's always sunny in ${city}!`,\n  {\n    name: \"get_weather_for_", "metadata": {"source": "quickstart.mdx"}}
{"text": " ToolRuntime } from \"langchain\";\nimport { MemorySaver } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\n// Define system prompt\nconst systemPrompt = `You are an expert weather forecaster, who speaks in puns.\n\nYou have access to two tools:\n\n- get_weather_for_location: use this to get the weather for a specific location\n- get_user_location: use this to get the user's location\n\nIf a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.`;\n\n// Define tools\nconst getWeather = tool(\n  ({ city }) => `It's always sunny in ${city}!`,\n  {\n    name: \"get_weather_for_location\",\n    description: \"Get the weather for a given city\",\n    schema: z.object({\n      city: z.string(),\n    }),\n  }\n);\n\ntype AgentRuntime = ToolRuntime<unknown, { user_id: string }>;\n\nconst getUserLocation = tool(\n  (_, config: AgentRuntime) => {\n    const { user_id } = config.context;\n    return user_id === \"1\" ? \"Florida\" : \"SF\";\n  },\n  {\n    name: \"get_user_location\",\n    description: \"Retrieve user information based on user ID\",\n    schema: z.object({}),\n  }\n);\n\n// Configure model\nconst model = await initChatModel(\n  \"claude-sonnet-4-5-20250929\",\n  { temperature: 0 }\n);\n\n// Define response format\nconst responseFormat = z.object({\n  punny_response: z.string(),\n  weather_conditions: z.string().optional(),\n});\n\n// Set up memory\nconst checkpointer = new MemorySaver();\n\n// Create agent\nconst agent = createAgent({\n  model,\n  systemPrompt,\n  responseFormat,\n  checkpointer,\n  tools: [getUserLocation, getWeather],\n});\n\n// Run agent\n// `thread_id` is a unique identifier for a given conversation.\nconst config = {\n  configurable: { thread_id: \"1\" },\n  context: { user_id: \"1\" },\n};\n\nconst response = await agent.invoke(\n  { messages: [{ role: \"user\", content: \"what is the weather outside?\" }] },\n  config\n);\nconsole.log(response.structuredResponse);\n// {\n//   punny_response: \"Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!\",\n//   weather_conditions: \"It's always sunny in Florida!\"\n// }\n\n// Note that we can continue the conversation using the same `thread_id`.\nconst thankYouResponse = await agent.invoke(\n  { messages: [{ role: \"user\", content: \"thank you!\" }] },\n  config\n);\nconsole.log(thankYouResponse", "metadata": {"source": "quickstart.mdx"}}
{"text": "\", content: \"what is the weather outside?\" }] },\n  config\n);\nconsole.log(response.structuredResponse);\n// {\n//   punny_response: \"Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!\",\n//   weather_conditions: \"It's always sunny in Florida!\"\n// }\n\n// Note that we can continue the conversation using the same `thread_id`.\nconst thankYouResponse = await agent.invoke(\n  { messages: [{ role: \"user\", content: \"thank you!\" }] },\n  config\n);\nconsole.log(thankYouResponse.structuredResponse);\n// {\n//   punny_response: \"You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!\",\n//   weather_conditions: undefined\n// }\n```\n:::\n</Expandable>\n\n<Tip>\n    To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langchain).\n</Tip>\n\nCongratulations! You now have an AI agent that can:\n\n- **Understand context** and remember conversations\n- **Use multiple tools** intelligently\n- **Provide structured responses** in a consistent format\n- **Handle user-specific information** through context\n- **Maintain conversation state** across interactions\n", "metadata": {"source": "quickstart.mdx"}}
{"text": "---\ntitle: Structured output\n---\n\n:::python\n\nStructured output allows agents to return data in a specific, predictable format. Instead of parsing natural language responses, you get structured data in the form of JSON objects, [Pydantic models](https://docs.pydantic.dev/latest/concepts/models/#basic-model-usage), or dataclasses that your application can use directly.\n\nLangChain's @[`create_agent`] handles structured output automatically. The user sets their desired structured output schema, and when the model generates the structured data, it's captured, validated, and returned in the `'structured_response'` key of the agent's state.\n\n```python\ndef create_agent(\n    ...\n    response_format: Union[\n        ToolStrategy[StructuredResponseT],\n        ProviderStrategy[StructuredResponseT],\n        type[StructuredResponseT],\n        None,\n    ]\n```\n\n## Response format\n\nUse `response_format` to control how the agent returns structured data:\n\n- **`ToolStrategy[StructuredResponseT]`**: Uses tool calling for structured output\n- **`ProviderStrategy[StructuredResponseT]`**: Uses provider-native structured output\n- **`type[StructuredResponseT]`**: Schema type - automatically selects best strategy based on model capabilities\n- **`None`**: Structured output not explicitly requested\n\nWhen a schema type is provided directly, LangChain automatically chooses:\n\n- `ProviderStrategy` if the model and provider chosen supports native structured output (e.g. [OpenAI](/oss/integrations/providers/openai), [Anthropic (Claude)](/oss/integrations/providers/anthropic), or [xAI (Grok)](/oss/integrations/providers/xai)).\n- `ToolStrategy` for all other models.\n\n<Note>\n    Support for native structured output features is read dynamically from the model's [profile data](/oss/langchain/models#model-profiles) if using `langchain>=1.1`. If data are not available, use another condition or specify manually:\n    ```python\n    custom_profile = {\n        \"structured_output\": True,\n        # ...\n    }\n    model = init_chat_model(\"...\", profile=custom_profile)\n    ```\n    If tools are specified, the model must support simultaneous use of tools and structured output.\n</Note>\n\nThe structured response is returned in the `structured_response` key of the agent's final state.\n:::\n:::js\nStructured output allows agents to return data in a specific, predictable format. Instead of parsing natural language responses, you get typed structured data.\n\nLangChain's prebuilt ReAct agent `createAgent` handles structured output automatically. The user sets their desired structured output schema, and when the model generates the structured data, it's captured, validated, and returned in the `structuredResponse` key of the agent's state.\n\n```ts\ntype ResponseFormat = (\n    | ZodSchema<StructuredResponseT> // a Zod schema\n    | Record<string, unknown> // a JSON Schema\n)\n\nconst agent = createAgent({\n   ", "metadata": {"source": "structured-output.mdx"}}
{"text": " tools are specified, the model must support simultaneous use of tools and structured output.\n</Note>\n\nThe structured response is returned in the `structured_response` key of the agent's final state.\n:::\n:::js\nStructured output allows agents to return data in a specific, predictable format. Instead of parsing natural language responses, you get typed structured data.\n\nLangChain's prebuilt ReAct agent `createAgent` handles structured output automatically. The user sets their desired structured output schema, and when the model generates the structured data, it's captured, validated, and returned in the `structuredResponse` key of the agent's state.\n\n```ts\ntype ResponseFormat = (\n    | ZodSchema<StructuredResponseT> // a Zod schema\n    | Record<string, unknown> // a JSON Schema\n)\n\nconst agent = createAgent({\n    // ...\n    responseFormat: ResponseFormat | ResponseFormat[]\n})\n```\n\n## Response format\n\nControls how the agent returns structured data. You can provide either a Zod object or JSON schema. By default, the agent uses a tool calling strategy, in which the output is created by an additional tool call. Certain models support native structured output, in which case the agent will use that strategy instead.\n\nYou can control the behavior by wrapping `ResponseFormat` in a `toolStrategy` or `providerStrategy` function call:\n\n```ts\nimport { toolStrategy, providerStrategy } from \"langchain\";\n\nconst agent = createAgent({\n    // use a provider strategy if supported by the model\n    responseFormat: providerStrategy(z.object({ ... }))\n    // or enforce a tool strategy\n    responseFormat: toolStrategy(z.object({ ... }))\n})\n```\n\nThe structured response is returned in the `structuredResponse` key of the agent's final state.\n\n<Tip>\n    Support for native structured output features is read dynamically from the model's [profile data](/oss/langchain/models#model-profiles) if using `langchain>=1.1`. If data are not available, use another condition or specify manually:\n    ```typescript\n    const customProfile: ModelProfile = {\n        structuredOutput: true,\n        // ...\n    }\n    const model = await initChatModel(\"...\", { profile: customProfile });\n    ```\n    If tools are specified, the model must support simultaneous use of tools and structured output.\n</Tip>\n:::\n\n## Provider strategy\n\nSome model providers support structured output natively through their APIs (e.g. OpenAI, xAI (Grok), Gemini, Anthropic (Claude)). This is the most reliable method when available.\n\nTo use this strategy, configure a `ProviderStrategy`:\n\n:::python\n```python\nclass ProviderStrategy(Generic[SchemaT]):\n    schema: type[SchemaT]\n    strict: bool | None = None\n```\n<Info>\n    The `strict` param requires `langchain>=1.2`.\n</Info>\n\n<ParamField path=\"schema\" required>\n    The schema defining the structured output format. Supports:\n    - **Pydantic models**: `BaseModel` subclasses with field validation. Returns validated Pydantic instance.\n   ", "metadata": {"source": "structured-output.mdx"}}
{"text": ":\n\n## Provider strategy\n\nSome model providers support structured output natively through their APIs (e.g. OpenAI, xAI (Grok), Gemini, Anthropic (Claude)). This is the most reliable method when available.\n\nTo use this strategy, configure a `ProviderStrategy`:\n\n:::python\n```python\nclass ProviderStrategy(Generic[SchemaT]):\n    schema: type[SchemaT]\n    strict: bool | None = None\n```\n<Info>\n    The `strict` param requires `langchain>=1.2`.\n</Info>\n\n<ParamField path=\"schema\" required>\n    The schema defining the structured output format. Supports:\n    - **Pydantic models**: `BaseModel` subclasses with field validation. Returns validated Pydantic instance.\n    - **Dataclasses**: Python dataclasses with type annotations. Returns dict.\n    - **TypedDict**: Typed dictionary classes. Returns dict.\n    - **JSON Schema**: Dictionary with JSON schema specification. Returns dict.\n</ParamField>\n\n<ParamField path=\"strict\">\n    Optional boolean parameter to enable strict schema adherence. Supported by some providers (e.g., [OpenAI](/oss/integrations/chat/openai) and [xAI](/oss/integrations/chat/xai)). Defaults to `None` (disabled).\n</ParamField>\n\nLangChain automatically uses `ProviderStrategy` when you pass a schema type directly to @[`create_agent.response_format`][create_agent(response_format)] and the model supports native structured output:\n\n<CodeGroup>\n    ```python Pydantic Model\n    from pydantic import BaseModel, Field\n    from langchain.agents import create_agent\n\n\n    class ContactInfo(BaseModel):\n        \"\"\"Contact information for a person.\"\"\"\n        name: str = Field(description=\"The name of the person\")\n        email: str = Field(description=\"The email address of the person\")\n        phone: str = Field(description=\"The phone number of the person\")\n\n    agent = create_agent(\n        model=\"gpt-5\",\n        response_format=ContactInfo  # Auto-selects ProviderStrategy\n    )\n\n    result = agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\n    })\n\n    print(result[\"structured_response\"])\n    # ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')\n    ```\n\n    ```python Dataclass\n    from dataclasses import dataclass\n    from langchain.agents import create_agent\n\n\n    @dataclass\n    class ContactInfo:\n        \"\"\"Contact information for a person.\"\"\"\n        name: str # The name of the person\n        email: str # The email address of the person\n   ", "metadata": {"source": "structured-output.mdx"}}
{"text": "ages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\n    })\n\n    print(result[\"structured_response\"])\n    # ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')\n    ```\n\n    ```python Dataclass\n    from dataclasses import dataclass\n    from langchain.agents import create_agent\n\n\n    @dataclass\n    class ContactInfo:\n        \"\"\"Contact information for a person.\"\"\"\n        name: str # The name of the person\n        email: str # The email address of the person\n        phone: str # The phone number of the person\n\n    agent = create_agent(\n        model=\"gpt-5\",\n        tools=tools,\n        response_format=ContactInfo  # Auto-selects ProviderStrategy\n    )\n\n    result = agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\n    })\n\n    result[\"structured_response\"]\n    # {'name': 'John Doe', 'email': 'john@example.com', 'phone': '(555) 123-4567'}\n    ```\n\n    ```python TypedDict\n    from typing_extensions import TypedDict\n    from langchain.agents import create_agent\n\n\n    class ContactInfo(TypedDict):\n        \"\"\"Contact information for a person.\"\"\"\n        name: str # The name of the person\n        email: str # The email address of the person\n        phone: str # The phone number of the person\n\n    agent = create_agent(\n        model=\"gpt-5\",\n        tools=tools,\n        response_format=ContactInfo  # Auto-selects ProviderStrategy\n    )\n\n    result = agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\n    })\n\n    result[\"structured_response\"]\n    # {'name': 'John Doe', 'email': 'john@example.com', 'phone': '(555) 123-4567'}\n    ```\n\n    ```python JSON Schema\n    from langchain.agents import create_agent\n\n\n    contact_info_schema = {\n        \"type\": \"object\",\n        \"description\": \"Contact information for a person.\",\n        \"properties\": {\n            \"name\": {\"type\": \"string\", \"description\": \"The name", "metadata": {"source": "structured-output.mdx"}}
{"text": "\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\n    })\n\n    result[\"structured_response\"]\n    # {'name': 'John Doe', 'email': 'john@example.com', 'phone': '(555) 123-4567'}\n    ```\n\n    ```python JSON Schema\n    from langchain.agents import create_agent\n\n\n    contact_info_schema = {\n        \"type\": \"object\",\n        \"description\": \"Contact information for a person.\",\n        \"properties\": {\n            \"name\": {\"type\": \"string\", \"description\": \"The name of the person\"},\n            \"email\": {\"type\": \"string\", \"description\": \"The email address of the person\"},\n            \"phone\": {\"type\": \"string\", \"description\": \"The phone number of the person\"}\n        },\n        \"required\": [\"name\", \"email\", \"phone\"]\n    }\n\n    agent = create_agent(\n        model=\"gpt-5\",\n        tools=tools,\n        response_format=ProviderStrategy(contact_info_schema)\n    )\n\n    result = agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\n    })\n\n    result[\"structured_response\"]\n    # {'name': 'John Doe', 'email': 'john@example.com', 'phone': '(555) 123-4567'}\n    ```\n</CodeGroup>\n:::\n:::js\n```ts\nfunction providerStrategy<StructuredResponseT>(\n    schema: ZodSchema<StructuredResponseT> | JsonSchemaFormat\n): ProviderStrategy<StructuredResponseT>\n```\n\n<ParamField path=\"schema\" required>\n    The schema defining the structured output format. Supports:\n    - **Zod Schema**: A zod schema\n    - **JSON Schema**: A JSON schema object\n</ParamField>\n\nLangChain automatically uses `ProviderStrategy` when you pass a schema type directly to `createAgent.responseFormat` and the model supports native structured output:\n\n<CodeGroup>\n    ```ts Zod Schema\n    import * as z from \"zod\";\n    import { createAgent, providerStrategy } from \"langchain\";\n\n    const ContactInfo = z.object({\n        name: z.string().describe(\"The name of the person\"),\n        email: z.string().describe(\"The email address of the person\"),\n        phone: z.string().describe(\"The phone number of the person\"),\n    });\n\n    const agent = createAgent({\n        model: \"gpt", "metadata": {"source": "structured-output.mdx"}}
{"text": " object\n</ParamField>\n\nLangChain automatically uses `ProviderStrategy` when you pass a schema type directly to `createAgent.responseFormat` and the model supports native structured output:\n\n<CodeGroup>\n    ```ts Zod Schema\n    import * as z from \"zod\";\n    import { createAgent, providerStrategy } from \"langchain\";\n\n    const ContactInfo = z.object({\n        name: z.string().describe(\"The name of the person\"),\n        email: z.string().describe(\"The email address of the person\"),\n        phone: z.string().describe(\"The phone number of the person\"),\n    });\n\n    const agent = createAgent({\n        model: \"gpt-5\",\n        tools: [],\n        responseFormat: providerStrategy(ContactInfo)\n    });\n\n    const result = await agent.invoke({\n        messages: [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\n    });\n\n    console.log(result.structuredResponse);\n    // { name: \"John Doe\", email: \"john@example.com\", phone: \"(555) 123-4567\" }\n    ```\n\n    ```ts JSON Schema\n    import { createAgent, providerStrategy } from \"langchain\";\n\n    const contactInfoSchema = {\n        \"type\": \"object\",\n        \"description\": \"Contact information for a person.\",\n        \"properties\": {\n            \"name\": {\"type\": \"string\", \"description\": \"The name of the person\"},\n            \"email\": {\"type\": \"string\", \"description\": \"The email address of the person\"},\n            \"phone\": {\"type\": \"string\", \"description\": \"The phone number of the person\"}\n        },\n        \"required\": [\"name\", \"email\", \"phone\"]\n    }\n\n    const agent = createAgent({\n        model: \"gpt-5\",\n        tools: [],\n        responseFormat: providerStrategy(contactInfoSchema)\n    });\n\n    const result = await agent.invoke({\n        messages: [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\n    });\n\n    console.log(result.structuredResponse);\n    // { name: \"John Doe\", email: \"john@example.com\", phone: \"(555) 123-4567\" }\n    ```\n</CodeGroup>\n:::\n\nProvider-native structured output provides high reliability and strict validation because the model provider enforces the schema. Use it when available.\n\n:::python\n<Note>\n    If the provider natively supports structured output for your model", "metadata": {"source": "structured-output.mdx"}}
{"text": " [],\n        responseFormat: providerStrategy(contactInfoSchema)\n    });\n\n    const result = await agent.invoke({\n        messages: [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\n    });\n\n    console.log(result.structuredResponse);\n    // { name: \"John Doe\", email: \"john@example.com\", phone: \"(555) 123-4567\" }\n    ```\n</CodeGroup>\n:::\n\nProvider-native structured output provides high reliability and strict validation because the model provider enforces the schema. Use it when available.\n\n:::python\n<Note>\n    If the provider natively supports structured output for your model choice, it is functionally equivalent to write `response_format=ProductReview` instead of `response_format=ProviderStrategy(ProductReview)`.\n\n    In either case, if structured output is not supported, the agent will fall back to a tool calling strategy.\n</Note>\n:::\n:::js\n<Note>\n    If the provider natively supports structured output for your model choice, it is functionally equivalent to write `responseFormat: contactInfoSchema` instead of `responseFormat: providerStrategy(contactInfoSchema)`.\n\n    In either case, if structured output is not supported, the agent will fall back to a tool calling strategy.\n</Note>\n:::\n\n## Tool calling strategy\n\nFor models that don't support native structured output, LangChain uses tool calling to achieve the same result. This works with all models that support tool calling (most modern models).\n\nTo use this strategy, configure a `ToolStrategy`:\n\n:::python\n```python\nclass ToolStrategy(Generic[SchemaT]):\n    schema: type[SchemaT]\n    tool_message_content: str | None\n    handle_errors: Union[\n        bool,\n        str,\n        type[Exception],\n        tuple[type[Exception], ...],\n        Callable[[Exception], str],\n    ]\n```\n\n<ParamField path=\"schema\" required>\n    The schema defining the structured output format. Supports:\n    - **Pydantic models**: `BaseModel` subclasses with field validation. Returns validated Pydantic instance.\n    - **Dataclasses**: Python dataclasses with type annotations. Returns dict.\n    - **TypedDict**: Typed dictionary classes. Returns dict.\n    - **JSON Schema**: Dictionary with JSON schema specification. Returns dict.\n    - **Union types**: Multiple schema options. The model will choose the most appropriate schema based on the context.\n</ParamField>\n\n<ParamField path=\"tool_message_content\">\n    Custom content for the tool message returned when structured output is generated.\n    If not provided, defaults to a message showing the structured response data.\n</ParamField>\n\n<ParamField path=\"handle_errors\">\n    Error handling strategy for structured output validation failures. Defaults to `True`.\n\n    - **`True`**: Catch all errors with default error template\n    - **`str`**:", "metadata": {"source": "structured-output.mdx"}}
{"text": "Dataclasses**: Python dataclasses with type annotations. Returns dict.\n    - **TypedDict**: Typed dictionary classes. Returns dict.\n    - **JSON Schema**: Dictionary with JSON schema specification. Returns dict.\n    - **Union types**: Multiple schema options. The model will choose the most appropriate schema based on the context.\n</ParamField>\n\n<ParamField path=\"tool_message_content\">\n    Custom content for the tool message returned when structured output is generated.\n    If not provided, defaults to a message showing the structured response data.\n</ParamField>\n\n<ParamField path=\"handle_errors\">\n    Error handling strategy for structured output validation failures. Defaults to `True`.\n\n    - **`True`**: Catch all errors with default error template\n    - **`str`**: Catch all errors with this custom message\n    - **`type[Exception]`**: Only catch this exception type with default message\n    - **`tuple[type[Exception], ...]`**: Only catch these exception types with default message\n    - **`Callable[[Exception], str]`**: Custom function that returns error message\n    - **`False`**: No retry, let exceptions propagate\n</ParamField>\n\n<CodeGroup>\n    ```python Pydantic Model\n    from pydantic import BaseModel, Field\n    from typing import Literal\n    from langchain.agents import create_agent\n    from langchain.agents.structured_output import ToolStrategy\n\n\n    class ProductReview(BaseModel):\n        \"\"\"Analysis of a product review.\"\"\"\n        rating: int | None = Field(description=\"The rating of the product\", ge=1, le=5)\n        sentiment: Literal[\"positive\", \"negative\"] = Field(description=\"The sentiment of the review\")\n        key_points: list[str] = Field(description=\"The key points of the review. Lowercase, 1-3 words each.\")\n\n    agent = create_agent(\n        model=\"gpt-5\",\n        tools=tools,\n        response_format=ToolStrategy(ProductReview)\n    )\n\n    result = agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": \"Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'\"}]\n    })\n    result[\"structured_response\"]\n    # ProductReview(rating=5, sentiment='positive', key_points=['fast shipping', 'expensive'])\n    ```\n\n    ```python Dataclass\n    from dataclasses import dataclass\n    from typing import Literal\n    from langchain.agents import create_agent\n    from langchain.agents.structured_output import ToolStrategy\n\n\n    @dataclass\n    class ProductReview:\n        \"\"\"Analysis of a product review.\"\"\"\n        rating: int | None  # The rating of the product (1-5)\n        sentiment: Literal[\"positive\",", "metadata": {"source": "structured-output.mdx"}}
{"text": ": 5 out of 5 stars. Fast shipping, but expensive'\"}]\n    })\n    result[\"structured_response\"]\n    # ProductReview(rating=5, sentiment='positive', key_points=['fast shipping', 'expensive'])\n    ```\n\n    ```python Dataclass\n    from dataclasses import dataclass\n    from typing import Literal\n    from langchain.agents import create_agent\n    from langchain.agents.structured_output import ToolStrategy\n\n\n    @dataclass\n    class ProductReview:\n        \"\"\"Analysis of a product review.\"\"\"\n        rating: int | None  # The rating of the product (1-5)\n        sentiment: Literal[\"positive\", \"negative\"]  # The sentiment of the review\n        key_points: list[str]  # The key points of the review\n\n    agent = create_agent(\n        model=\"gpt-5\",\n        tools=tools,\n        response_format=ToolStrategy(ProductReview)\n    )\n\n    result = agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": \"Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'\"}]\n    })\n    result[\"structured_response\"]\n    # {'rating': 5, 'sentiment': 'positive', 'key_points': ['fast shipping', 'expensive']}\n    ```\n\n    ```python TypedDict\n    from typing import Literal\n    from typing_extensions import TypedDict\n    from langchain.agents import create_agent\n    from langchain.agents.structured_output import ToolStrategy\n\n\n    class ProductReview(TypedDict):\n        \"\"\"Analysis of a product review.\"\"\"\n        rating: int | None  # The rating of the product (1-5)\n        sentiment: Literal[\"positive\", \"negative\"]  # The sentiment of the review\n        key_points: list[str]  # The key points of the review\n\n    agent = create_agent(\n        model=\"gpt-5\",\n        tools=tools,\n        response_format=ToolStrategy(ProductReview)\n    )\n\n    result = agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": \"Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'\"}]\n    })\n    result[\"structured_response\"]\n    # {'rating': 5, 'sentiment': 'positive', 'key_points': ['fast shipping', 'expensive']}\n    ```\n\n    ```python JSON Schema\n    from langchain.agents import create_agent\n    from langchain.agents.structured_output import ToolStrategy\n\n\n    product_review_schema = {\n  ", "metadata": {"source": "structured-output.mdx"}}
{"text": "       tools=tools,\n        response_format=ToolStrategy(ProductReview)\n    )\n\n    result = agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": \"Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'\"}]\n    })\n    result[\"structured_response\"]\n    # {'rating': 5, 'sentiment': 'positive', 'key_points': ['fast shipping', 'expensive']}\n    ```\n\n    ```python JSON Schema\n    from langchain.agents import create_agent\n    from langchain.agents.structured_output import ToolStrategy\n\n\n    product_review_schema = {\n        \"type\": \"object\",\n        \"description\": \"Analysis of a product review.\",\n        \"properties\": {\n            \"rating\": {\n                \"type\": [\"integer\", \"null\"],\n                \"description\": \"The rating of the product (1-5)\",\n                \"minimum\": 1,\n                \"maximum\": 5\n            },\n            \"sentiment\": {\n                \"type\": \"string\",\n                \"enum\": [\"positive\", \"negative\"],\n                \"description\": \"The sentiment of the review\"\n            },\n            \"key_points\": {\n                \"type\": \"array\",\n                \"items\": {\"type\": \"string\"},\n                \"description\": \"The key points of the review\"\n            }\n        },\n        \"required\": [\"sentiment\", \"key_points\"]\n    }\n\n    agent = create_agent(\n        model=\"gpt-5\",\n        tools=tools,\n        response_format=ToolStrategy(product_review_schema)\n    )\n\n    result = agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": \"Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'\"}]\n    })\n    result[\"structured_response\"]\n    # {'rating': 5, 'sentiment': 'positive', 'key_points': ['fast shipping', 'expensive']}\n    ```\n\n    ```python Union Types\n    from pydantic import", "metadata": {"source": "structured-output.mdx"}}
{"text": "_points\"]\n    }\n\n    agent = create_agent(\n        model=\"gpt-5\",\n        tools=tools,\n        response_format=ToolStrategy(product_review_schema)\n    )\n\n    result = agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": \"Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'\"}]\n    })\n    result[\"structured_response\"]\n    # {'rating': 5, 'sentiment': 'positive', 'key_points': ['fast shipping', 'expensive']}\n    ```\n\n    ```python Union Types\n    from pydantic import BaseModel, Field\n    from typing import Literal, Union\n    from langchain.agents import create_agent\n    from langchain.agents.structured_output import ToolStrategy\n\n\n    class ProductReview(BaseModel):\n        \"\"\"Analysis of a product review.\"\"\"\n        rating: int | None = Field(description=\"The rating of the product\", ge=1, le=5)\n        sentiment: Literal[\"positive\", \"negative\"] = Field(description=\"The sentiment of the review\")\n        key_points: list[str] = Field(description=\"The key points of the review. Lowercase, 1-3 words each.\")\n\n    class CustomerComplaint(BaseModel):\n        \"\"\"A customer complaint about a product or service.\"\"\"\n        issue_type: Literal[\"product\", \"service\", \"shipping\", \"billing\"] = Field(description=\"The type of issue\")\n        severity: Literal[\"low\", \"medium\", \"high\"] = Field(description=\"The severity of the complaint\")\n        description: str = Field(description=\"Brief description of the complaint\")\n\n    agent = create_agent(\n        model=\"gpt-5\",\n        tools=tools,\n        response_format=ToolStrategy(Union[ProductReview, CustomerComplaint])\n    )\n\n    result = agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": \"Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'\"}]\n    })\n    result[\"structured_response\"]\n    # ProductReview(rating=5, sentiment='positive', key_points=['fast shipping', 'expensive'])\n    ```\n</CodeGroup>\n\n### Custom tool message content\n\nThe `tool_message_content` parameter allows you to customize the message that appears in the conversation history when structured output is generated:\n\n```python\nfrom pydantic import BaseModel, Field\nfrom typing import Literal\nfrom langchain.agents import create_agent\nfrom langchain.agents.structured_output import ToolStrategy\n\n\nclass MeetingAction(BaseModel):\n    \"\"\"Action items extracted from a meeting transcript.\"\"\"\n    task: str = Field(description=\"The specific task to be completed\")\n ", "metadata": {"source": "structured-output.mdx"}}
{"text": ": 'Great product: 5 out of 5 stars. Fast shipping, but expensive'\"}]\n    })\n    result[\"structured_response\"]\n    # ProductReview(rating=5, sentiment='positive', key_points=['fast shipping', 'expensive'])\n    ```\n</CodeGroup>\n\n### Custom tool message content\n\nThe `tool_message_content` parameter allows you to customize the message that appears in the conversation history when structured output is generated:\n\n```python\nfrom pydantic import BaseModel, Field\nfrom typing import Literal\nfrom langchain.agents import create_agent\nfrom langchain.agents.structured_output import ToolStrategy\n\n\nclass MeetingAction(BaseModel):\n    \"\"\"Action items extracted from a meeting transcript.\"\"\"\n    task: str = Field(description=\"The specific task to be completed\")\n    assignee: str = Field(description=\"Person responsible for the task\")\n    priority: Literal[\"low\", \"medium\", \"high\"] = Field(description=\"Priority level\")\n\nagent = create_agent(\n    model=\"gpt-5\",\n    tools=[],\n    response_format=ToolStrategy(\n        schema=MeetingAction,\n        tool_message_content=\"Action item captured and added to meeting notes!\"\n    )\n)\n\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"From our meeting: Sarah needs to update the project timeline as soon as possible\"}]\n})\n```\n\n```\n================================ Human Message =================================\n\nFrom our meeting: Sarah needs to update the project timeline as soon as possible\n================================== Ai Message ==================================\nTool Calls:\n  MeetingAction (call_1)\n Call ID: call_1\n  Args:\n    task: Update the project timeline\n    assignee: Sarah\n    priority: high\n================================= Tool Message =================================\nName: MeetingAction\n\nAction item captured and added to meeting notes!\n```\n\nWithout `tool_message_content`, our final @[`ToolMessage`] would be:\n```\n================================= Tool Message =================================\nName: MeetingAction\n\nReturning structured response: {'task': 'update the project timeline', 'assignee': 'Sarah', 'priority': 'high'}\n```\n:::\n:::js\n```ts\nfunction toolStrategy<StructuredResponseT>(\n    responseFormat:\n        | JsonSchemaFormat\n        | ZodSchema<StructuredResponseT>\n        | (ZodSchema<StructuredResponseT> | JsonSchemaFormat)[]\n    options?: ToolStrategyOptions\n): ToolStrategy<StructuredResponseT>\n```\n\n<ParamField path=\"schema\" required>\n    The schema defining the structured output format. Supports:\n    - **Zod Schema**: A zod schema\n    - **JSON Schema**: A JSON schema object\n</ParamField>\n\n<ParamField path=\"options.toolMessageContent\">\n    Custom content for the tool message returned when structured output is generated.\n    If not provided, defaults to a message showing the structured response data.\n</ParamField>\n\n<ParamField path=\"options.handleError\">\n    Options parameter containing an", "metadata": {"source": "structured-output.mdx"}}
{"text": "   | ZodSchema<StructuredResponseT>\n        | (ZodSchema<StructuredResponseT> | JsonSchemaFormat)[]\n    options?: ToolStrategyOptions\n): ToolStrategy<StructuredResponseT>\n```\n\n<ParamField path=\"schema\" required>\n    The schema defining the structured output format. Supports:\n    - **Zod Schema**: A zod schema\n    - **JSON Schema**: A JSON schema object\n</ParamField>\n\n<ParamField path=\"options.toolMessageContent\">\n    Custom content for the tool message returned when structured output is generated.\n    If not provided, defaults to a message showing the structured response data.\n</ParamField>\n\n<ParamField path=\"options.handleError\">\n    Options parameter containing an optional `handleError` parameter for customizing the error handling strategy.\n\n    - **`true`**: Catch all errors with default error template (default)\n    - **`False`**: No retry, let exceptions propagate\n    - **`(error: ToolStrategyError) => string | Promise<string>`**: retry with the provided message or throw the error\n</ParamField>\n\n<CodeGroup>\n    ```ts Zod Schema\n    import * as z from \"zod\";\n    import { createAgent, toolStrategy } from \"langchain\";\n\n    const ProductReview = z.object({\n        rating: z.number().min(1).max(5).optional(),\n        sentiment: z.enum([\"positive\", \"negative\"]),\n        keyPoints: z.array(z.string()).describe(\"The key points of the review. Lowercase, 1-3 words each.\"),\n    });\n\n    const agent = createAgent({\n        model: \"gpt-5\",\n        tools: [],\n        responseFormat: toolStrategy(ProductReview)\n    })\n\n    const result = await agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": \"Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'\"}]\n    })\n\n    console.log(result.structuredResponse);\n    // { \"rating\": 5, \"sentiment\": \"positive\", \"keyPoints\": [\"fast shipping\", \"expensive\"] }\n    ```\n\n    ```ts JSON Schema\n    import { createAgent, toolStrategy } from \"langchain\";\n\n    const productReviewSchema = {\n        \"type\": \"object\",\n        \"description\": \"Analysis of a product review.\",\n        \"properties\": {\n            \"rating\": {\n                \"type\": [\"integer\", \"null\"],\n                \"description\": \"The rating of the product (1-5)\",\n                \"minimum\": 1,\n       ", "metadata": {"source": "structured-output.mdx"}}
{"text": "keyPoints\": [\"fast shipping\", \"expensive\"] }\n    ```\n\n    ```ts JSON Schema\n    import { createAgent, toolStrategy } from \"langchain\";\n\n    const productReviewSchema = {\n        \"type\": \"object\",\n        \"description\": \"Analysis of a product review.\",\n        \"properties\": {\n            \"rating\": {\n                \"type\": [\"integer\", \"null\"],\n                \"description\": \"The rating of the product (1-5)\",\n                \"minimum\": 1,\n                \"maximum\": 5\n            },\n            \"sentiment\": {\n                \"type\": \"string\",\n                \"enum\": [\"positive\", \"negative\"],\n                \"description\": \"The sentiment of the review\"\n            },\n            \"key_points\": {\n                \"type\": \"array\",\n                \"items\": {\"type\": \"string\"},\n                \"description\": \"The key points of the review\"\n            }\n        },\n        \"required\": [\"sentiment\", \"key_points\"]\n    }\n\n    const agent = createAgent({\n        model: \"gpt-5\",\n        tools: [],\n        responseFormat: toolStrategy(productReviewSchema)\n    });\n\n    const result = await agent.invoke({\n        messages: [{\"role\": \"user\", \"content\": \"Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'\"}]\n    })\n\n    console.log(result.structuredResponse);\n    // { \"rating\": 5, \"sentiment\": \"positive\", \"keyPoints\": [\"fast shipping\", \"expensive\"] }\n    ```\n\n    ```ts Union Types\n    import * as z from \"zod\";\n    import { createAgent, toolStrategy } from \"langchain\";\n\n    const ProductReview = z.object({\n        rating: z.number().min(1).max(5).optional(),\n        sentiment: z.enum([\"positive\", \"negative\"]),\n        keyPoints: z.array(z.string()).describe(\"The key points of the review. Lowercase, 1-3 words each.\"),\n    });\n\n    const CustomerComplaint = z.object({\n   ", "metadata": {"source": "structured-output.mdx"}}
{"text": ".structuredResponse);\n    // { \"rating\": 5, \"sentiment\": \"positive\", \"keyPoints\": [\"fast shipping\", \"expensive\"] }\n    ```\n\n    ```ts Union Types\n    import * as z from \"zod\";\n    import { createAgent, toolStrategy } from \"langchain\";\n\n    const ProductReview = z.object({\n        rating: z.number().min(1).max(5).optional(),\n        sentiment: z.enum([\"positive\", \"negative\"]),\n        keyPoints: z.array(z.string()).describe(\"The key points of the review. Lowercase, 1-3 words each.\"),\n    });\n\n    const CustomerComplaint = z.object({\n        issueType: z.enum([\"product\", \"service\", \"shipping\", \"billing\"]),\n        severity: z.enum([\"low\", \"medium\", \"high\"]),\n        description: z.string().describe(\"Brief description of the complaint\"),\n    });\n\n    const agent = createAgent({\n        model: \"gpt-5\",\n        tools: [],\n        responseFormat: toolStrategy([ProductReview, CustomerComplaint])\n    });\n\n    const result = await agent.invoke({\n        messages: [{\"role\": \"user\", \"content\": \"Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'\"}]\n    })\n\n    console.log(result.structuredResponse);\n    // { \"rating\": 5, \"sentiment\": \"positive\", \"keyPoints\": [\"fast shipping\", \"expensive\"] }\n    ```\n</CodeGroup>\n\n### Custom tool message content\n\nThe `toolMessageContent` parameter allows you to customize the message that appears in the conversation history when structured output is generated:\n\n```ts\nimport * as z from \"zod\";\nimport { createAgent, toolStrategy } from \"langchain\";\n\nconst MeetingAction = z.object({\n    task: z.string().describe(\"The specific task to be completed\"),\n    assignee: z.string().describe(\"Person responsible for the task\"),\n    priority: z.enum([\"low\", \"medium\", \"high\"]).describe(\"Priority level\"),\n});\n\nconst agent = createAgent({\n    model: \"gpt-5\",\n    tools: [],\n    responseFormat: toolStrategy(MeetingAction, {\n        toolMessageContent: \"Action item captured and added to meeting notes!\"\n    })\n});\n\nconst result = await agent.invoke({\n    messages: [{\"role\": \"user\", \"content\": \"From our meeting: Sarah needs to update the project timeline as soon as possible\"}]\n});\n\nconsole.log(result);\n/**\n * {\n *   messages: [\n *     { role: \"user\", content: \"From our meeting: Sarah needs to update the project timeline as soon as possible\" },\n *     { role: \"assistant\", content: \"Action item captured and added to meeting notes!\", tool", "metadata": {"source": "structured-output.mdx"}}
{"text": "\"),\n});\n\nconst agent = createAgent({\n    model: \"gpt-5\",\n    tools: [],\n    responseFormat: toolStrategy(MeetingAction, {\n        toolMessageContent: \"Action item captured and added to meeting notes!\"\n    })\n});\n\nconst result = await agent.invoke({\n    messages: [{\"role\": \"user\", \"content\": \"From our meeting: Sarah needs to update the project timeline as soon as possible\"}]\n});\n\nconsole.log(result);\n/**\n * {\n *   messages: [\n *     { role: \"user\", content: \"From our meeting: Sarah needs to update the project timeline as soon as possible\" },\n *     { role: \"assistant\", content: \"Action item captured and added to meeting notes!\", tool_calls: [ { name: \"MeetingAction\", args: { task: \"update the project timeline\", assignee: \"Sarah\", priority: \"high\" }, id: \"call_456\" } ] },\n *     { role: \"tool\", content: \"Action item captured and added to meeting notes!\", tool_call_id: \"call_456\", name: \"MeetingAction\" }\n *   ],\n *   structuredResponse: { task: \"update the project timeline\", assignee: \"Sarah\", priority: \"high\" }\n * }\n */\n```\n\nWithout `toolMessageContent`, we'd see:\n\n```ts\n# console.log(result);\n/**\n * {\n *   messages: [\n *     ...\n *     { role: \"tool\", content: \"Returning structured response: {'task': 'update the project timeline', 'assignee': 'Sarah', 'priority': 'high'}\", tool_call_id: \"call_456\", name: \"MeetingAction\" }\n *   ],\n *   structuredResponse: { task: \"update the project timeline\", assignee: \"Sarah\", priority: \"high\" }\n * }\n */\n```\n:::\n\n### Error handling\n\nModels can make mistakes when generating structured output via tool calling. LangChain provides intelligent retry mechanisms to handle these errors automatically.\n\n#### Multiple structured outputs error\n\nWhen a model incorrectly calls multiple structured output tools, the agent provides error feedback in a @[`ToolMessage`] and prompts the model to retry:\n\n:::python\n```python\nfrom pydantic import BaseModel, Field\nfrom typing import Union\nfrom langchain.agents import create_agent\nfrom langchain.agents.structured_output import ToolStrategy\n\n\nclass ContactInfo(BaseModel):\n    name: str = Field(description=\"Person's name\")\n    email: str = Field(description=\"Email address\")\n\nclass EventDetails(BaseModel):\n    event_name: str = Field(description=\"Name of the event\")\n    date: str = Field(description=\"Event date\")\n\nagent = create_agent(\n    model=\"gpt-5\",\n    tools=[],\n    response_format=ToolStrategy(Union[ContactInfo, EventDetails])  # Default: handle_errors=True\n)\n\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th\"}]\n})\n```\n", "metadata": {"source": "structured-output.mdx"}}
{"text": "_output import ToolStrategy\n\n\nclass ContactInfo(BaseModel):\n    name: str = Field(description=\"Person's name\")\n    email: str = Field(description=\"Email address\")\n\nclass EventDetails(BaseModel):\n    event_name: str = Field(description=\"Name of the event\")\n    date: str = Field(description=\"Event date\")\n\nagent = create_agent(\n    model=\"gpt-5\",\n    tools=[],\n    response_format=ToolStrategy(Union[ContactInfo, EventDetails])  # Default: handle_errors=True\n)\n\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th\"}]\n})\n```\n\n```\n================================ Human Message =================================\n\nExtract info: John Doe (john@email.com) is organizing Tech Conference on March 15th\nNone\n================================== Ai Message ==================================\nTool Calls:\n  ContactInfo (call_1)\n Call ID: call_1\n  Args:\n    name: John Doe\n    email: john@email.com\n  EventDetails (call_2)\n Call ID: call_2\n  Args:\n    event_name: Tech Conference\n    date: March 15th\n================================= Tool Message =================================\nName: ContactInfo\n\nError: Model incorrectly returned multiple structured responses (ContactInfo, EventDetails) when only one is expected.\n Please fix your mistakes.\n================================= Tool Message =================================\nName: EventDetails\n\nError: Model incorrectly returned multiple structured responses (ContactInfo, EventDetails) when only one is expected.\n Please fix your mistakes.\n================================== Ai Message ==================================\nTool Calls:\n  ContactInfo (call_3)\n Call ID: call_3\n  Args:\n    name: John Doe\n    email: john@email.com\n================================= Tool Message =================================\nName: ContactInfo\n\nReturning structured response: {'name': 'John Doe', 'email': 'john@email.com'}\n```\n:::\n:::js\n```ts\nimport * as z from \"zod\";\nimport { createAgent, toolStrategy } from \"langchain\";\n\nconst ContactInfo = z.object({\n    name: z.string().describe(\"Person's name\"),\n    email: z.string().describe(\"Email address\"),\n});\n\nconst EventDetails = z.object({\n    event_name: z.string().describe(\"Name of the event\"),\n    date: z.string().describe(\"Event date\"),\n});\n\nconst agent = createAgent({\n    model: \"gpt-5\",\n    tools: [],\n    responseFormat: toolStrategy([ContactInfo, EventDetails]),\n});\n\nconst result = await agent.invoke({\n    messages: [\n        {\n        role: \"user\",\n        content:\n            \"Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th\",\n        },\n    ],\n});\n\nconsole.log(result);\n\n/**\n * {\n *   messages: [\n *     { role", "metadata": {"source": "structured-output.mdx"}}
{"text": "(\"Name of the event\"),\n    date: z.string().describe(\"Event date\"),\n});\n\nconst agent = createAgent({\n    model: \"gpt-5\",\n    tools: [],\n    responseFormat: toolStrategy([ContactInfo, EventDetails]),\n});\n\nconst result = await agent.invoke({\n    messages: [\n        {\n        role: \"user\",\n        content:\n            \"Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th\",\n        },\n    ],\n});\n\nconsole.log(result);\n\n/**\n * {\n *   messages: [\n *     { role: \"user\", content: \"Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th\" },\n *     { role: \"assistant\", content: \"\", tool_calls: [ { name: \"ContactInfo\", args: { name: \"John Doe\", email: \"john@email.com\" }, id: \"call_1\" }, { name: \"EventDetails\", args: { event_name: \"Tech Conference\", date: \"March 15th\" }, id: \"call_2\" } ] },\n *     { role: \"tool\", content: \"Error: Model incorrectly returned multiple structured responses (ContactInfo, EventDetails) when only one is expected.\\n Please fix your mistakes.\", tool_call_id: \"call_1\", name: \"ContactInfo\" },\n *     { role: \"tool\", content: \"Error: Model incorrectly returned multiple structured responses (ContactInfo, EventDetails) when only one is expected.\\n Please fix your mistakes.\", tool_call_id: \"call_2\", name: \"EventDetails\" },\n *     { role: \"assistant\", content: \"\", tool_calls: [ { name: \"ContactInfo\", args: { name: \"John Doe\", email: \"john@email.com\" }, id: \"call_3\" } ] },\n *     { role: \"tool\", content: \"Returning structured response: {'name': 'John Doe', 'email': 'john@email.com'}\", tool_call_id: \"call_3\", name: \"ContactInfo\" }\n *   ],\n *   structuredResponse: { name: \"John Doe\", email: \"john@email.com\" }\n * }\n */\n```\n:::\n\n#### Schema validation error\n\nWhen structured output doesn't match the expected schema, the agent provides specific error feedback:\n\n:::python\n```python\nfrom pydantic import BaseModel, Field\nfrom langchain.agents import create_agent\nfrom langchain.agents.structured_output import ToolStrategy\n\n\nclass ProductRating(BaseModel):\n    rating: int | None = Field(description=\"Rating from 1-5\", ge=1, le=5)\n    comment: str = Field(description=\"Review comment\")\n\nagent = create_agent(\n    model=\"gpt-5\",\n    tools=[],\n    response_format=ToolStrategy(ProductRating),  # Default: handle_errors=True\n    system_prompt=\"You are a helpful assistant that parses product reviews. Do not make any field or", "metadata": {"source": "structured-output.mdx"}}
{"text": ":::\n\n#### Schema validation error\n\nWhen structured output doesn't match the expected schema, the agent provides specific error feedback:\n\n:::python\n```python\nfrom pydantic import BaseModel, Field\nfrom langchain.agents import create_agent\nfrom langchain.agents.structured_output import ToolStrategy\n\n\nclass ProductRating(BaseModel):\n    rating: int | None = Field(description=\"Rating from 1-5\", ge=1, le=5)\n    comment: str = Field(description=\"Review comment\")\n\nagent = create_agent(\n    model=\"gpt-5\",\n    tools=[],\n    response_format=ToolStrategy(ProductRating),  # Default: handle_errors=True\n    system_prompt=\"You are a helpful assistant that parses product reviews. Do not make any field or value up.\"\n)\n\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Parse this: Amazing product, 10/10!\"}]\n})\n```\n\n```\n================================ Human Message =================================\n\nParse this: Amazing product, 10/10!\n================================== Ai Message ==================================\nTool Calls:\n  ProductRating (call_1)\n Call ID: call_1\n  Args:\n    rating: 10\n    comment: Amazing product\n================================= Tool Message =================================\nName: ProductRating\n\nError: Failed to parse structured output for tool 'ProductRating': 1 validation error for ProductRating.rating\n  Input should be less than or equal to 5 [type=less_than_equal, input_value=10, input_type=int].\n Please fix your mistakes.\n================================== Ai Message ==================================\nTool Calls:\n  ProductRating (call_2)\n Call ID: call_2\n  Args:\n    rating: 5\n    comment: Amazing product\n================================= Tool Message =================================\nName: ProductRating\n\nReturning structured response: {'rating': 5, 'comment': 'Amazing product'}\n```\n\n#### Error handling strategies\n\nYou can customize how errors are handled using the `handle_errors` parameter:\n\n**Custom error message:**\n```python\nToolStrategy(\n    schema=ProductRating,\n    handle_errors=\"Please provide a valid rating between 1-5 and include a comment.\"\n)\n```\nIf `handle_errors` is a string, the agent will *always* prompt the model to re-try with a fixed tool message:\n```\n================================= Tool Message =================================\nName: ProductRating\n\nPlease provide a valid rating between 1-5 and include a comment.\n```\n\n**Handle specific exceptions only:**\n\n```python\nToolStrategy(\n    schema=ProductRating,\n    handle_errors=ValueError  # Only retry on ValueError, raise others\n)\n```\n\nIf `handle_errors` is an exception type, the agent will only retry (using the default error message) if the exception raised is the specified type. In all other cases, the exception will be raised.\n\n**Handle multiple exception types:**\n```python\nToolStrategy(\n    schema=ProductRating,\n    handle_errors=(ValueError, TypeError)  # Retry on ValueError and TypeError\n)\n```\n\nIf `handle_errors` is a tuple of exceptions, the agent will only retry (using the default error message) if the exception raised is one", "metadata": {"source": "structured-output.mdx"}}
{"text": " 1-5 and include a comment.\n```\n\n**Handle specific exceptions only:**\n\n```python\nToolStrategy(\n    schema=ProductRating,\n    handle_errors=ValueError  # Only retry on ValueError, raise others\n)\n```\n\nIf `handle_errors` is an exception type, the agent will only retry (using the default error message) if the exception raised is the specified type. In all other cases, the exception will be raised.\n\n**Handle multiple exception types:**\n```python\nToolStrategy(\n    schema=ProductRating,\n    handle_errors=(ValueError, TypeError)  # Retry on ValueError and TypeError\n)\n```\n\nIf `handle_errors` is a tuple of exceptions, the agent will only retry (using the default error message) if the exception raised is one of the specified types. In all other cases, the exception will be raised.\n\n**Custom error handler function:**\n\n```python\n\nfrom langchain.agents.structured_output import StructuredOutputValidationError\nfrom langchain.agents.structured_output import MultipleStructuredOutputsError\n\ndef custom_error_handler(error: Exception) -> str:\n    if isinstance(error, StructuredOutputValidationError):\n        return \"There was an issue with the format. Try again.\"\n    elif isinstance(error, MultipleStructuredOutputsError):\n        return \"Multiple structured outputs were returned. Pick the most relevant one.\"\n    else:\n        return f\"Error: {str(error)}\"\n\n\nagent = create_agent(\n    model=\"gpt-5\",\n    tools=[],\n    response_format=ToolStrategy(\n                        schema=Union[ContactInfo, EventDetails],\n                        handle_errors=custom_error_handler\n                    )  # Default: handle_errors=True\n)\n\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th\"}]\n})\n\nfor msg in result['messages']:\n    # If message is actually a ToolMessage object (not a dict), check its class name\n    if type(msg).__name__ == \"ToolMessage\":\n        print(msg.content)\n    # If message is a dictionary or you want a fallback\n    elif isinstance(msg, dict) and msg.get('tool_call_id'):\n        print(msg['content'])\n\n```\n\nOn `StructuredOutputValidationError`:\n```\n================================= Tool Message =================================\nName: ToolStrategy\n\nThere was an issue with the format. Try again.\n```\n\nOn `MultipleStructuredOutputsError`:\n\n```\n================================= Tool Message =================================\nName: ToolStrategy\n\nMultiple structured outputs were returned. Pick the most relevant one.\n```\n\nOn other errors:\n\n```\n=================================", "metadata": {"source": "structured-output.mdx"}}
{"text": "not a dict), check its class name\n    if type(msg).__name__ == \"ToolMessage\":\n        print(msg.content)\n    # If message is a dictionary or you want a fallback\n    elif isinstance(msg, dict) and msg.get('tool_call_id'):\n        print(msg['content'])\n\n```\n\nOn `StructuredOutputValidationError`:\n```\n================================= Tool Message =================================\nName: ToolStrategy\n\nThere was an issue with the format. Try again.\n```\n\nOn `MultipleStructuredOutputsError`:\n\n```\n================================= Tool Message =================================\nName: ToolStrategy\n\nMultiple structured outputs were returned. Pick the most relevant one.\n```\n\nOn other errors:\n\n```\n================================= Tool Message =================================\nName: ToolStrategy\n\nError: <error message>\n```\n\n**No error handling:**\n\n```python\nresponse_format = ToolStrategy(\n    schema=ProductRating,\n    handle_errors=False  # All errors raised\n)\n```\n:::\n:::js\n```ts\nimport * as z from \"zod\";\nimport { createAgent, toolStrategy } from \"langchain\";\n\nconst ProductRating = z.object({\n    rating: z.number().min(1).max(5).describe(\"Rating from 1-5\"),\n    comment: z.string().describe(\"Review comment\"),\n});\n\nconst agent = createAgent({\n    model: \"gpt-5\",\n    tools: [],\n    responseFormat: toolStrategy(ProductRating),\n});\n\nconst result = await agent.invoke({\n    messages: [\n        {\n        role: \"user\",\n        content: \"Parse this: Amazing product, 10/10!\",\n        },\n    ],\n});\n\nconsole.log(result);\n\n/**\n * {\n *   messages: [\n *     { role: \"user\", content: \"Parse this: Amazing product, 10/10!\" },\n *     { role: \"assistant\", content: \"\", tool_calls: [ { name: \"ProductRating\", args: { rating: 10, comment: \"Amazing product\" }, id: \"call_1\" } ] },\n *     { role: \"tool\", content: \"Error: Failed to parse structured output for tool 'ProductRating': 1 validation error for ProductRating\\nrating\\n  Input should be less than or equal to 5 [type=less_than_equal, input_value=10, input_type=int].\\n Please fix your mistakes.\", tool_call_id: \"call_1\", name: \"ProductRating\" },\n *     { role: \"assistant\", content: \"\", tool_calls: [ { name: \"ProductRating\", args: { rating: 5, comment: \"Amazing product\" }, id: \"call_2\" } ] },\n *     { role: \"tool\", content: \"Returning structured response: {'rating': 5, 'comment': 'Amazing product'}\", tool_call_id: \"call_2\", name: \"ProductRating\" }\n *   ],\n *   structuredResponse: { rating", "metadata": {"source": "structured-output.mdx"}}
{"text": ": Failed to parse structured output for tool 'ProductRating': 1 validation error for ProductRating\\nrating\\n  Input should be less than or equal to 5 [type=less_than_equal, input_value=10, input_type=int].\\n Please fix your mistakes.\", tool_call_id: \"call_1\", name: \"ProductRating\" },\n *     { role: \"assistant\", content: \"\", tool_calls: [ { name: \"ProductRating\", args: { rating: 5, comment: \"Amazing product\" }, id: \"call_2\" } ] },\n *     { role: \"tool\", content: \"Returning structured response: {'rating': 5, 'comment': 'Amazing product'}\", tool_call_id: \"call_2\", name: \"ProductRating\" }\n *   ],\n *   structuredResponse: { rating: 5, comment: \"Amazing product\" }\n * }\n */\n```\n\n#### Error handling strategies\n\nYou can customize how errors are handled using the `handleErrors` parameter:\n\n**Custom error message:**\n\n```ts\nconst responseFormat = toolStrategy(ProductRating, {\n    handleError: \"Please provide a valid rating between 1-5 and include a comment.\"\n)\n\n// Error message becomes:\n// { role: \"tool\", content: \"Please provide a valid rating between 1-5 and include a comment.\" }\n```\n\n**Handle specific exceptions only:**\n\n```ts\nimport { ToolInputParsingException } from \"@langchain/core/tools\";\n\nconst responseFormat = toolStrategy(ProductRating, {\n    handleError: (error: ToolStrategyError) => {\n        if (error instanceof ToolInputParsingException) {\n        return \"Please provide a valid rating between 1-5 and include a comment.\";\n        }\n        return error.message;\n    }\n)\n\n// Only validation errors get retried with default message:\n// { role: \"tool\", content: \"Error: Failed to parse structured output for tool 'ProductRating': ...\\n Please fix your mistakes.\" }\n```\n\n**Handle multiple exception types:**\n\n```ts\nconst responseFormat = toolStrategy(ProductRating, {\n    handleError: (error: ToolStrategyError) => {\n        if (error instanceof ToolInputParsingException) {\n        return \"Please provide a valid rating between 1-5 and include a comment.\";\n        }\n        if (error instanceof CustomUserError) {\n        return \"This is a custom user error.\";\n        }\n        return error.message;\n    }\n)\n```\n\n**No error handling:**\n\n```ts\nconst responseFormat = toolStrategy(ProductRating, {\n    handleError: false  // All errors raised\n)\n```\n:::\n", "metadata": {"source": "structured-output.mdx"}}
{"text": "---\ntitle: Context engineering in agents\nsidebarTitle: Context engineering\n---\n\n## Overview\n\nThe hard part of building agents (or any LLM application) is making them reliable enough. While they may work for a prototype, they often fail in real-world use cases.\n\n### Why do agents fail?\n\nWhen agents fail, it's usually because the LLM call inside the agent took the wrong action / didn't do what we expected. LLMs fail for one of two reasons:\n\n1. The underlying LLM is not capable enough\n2. The \"right\" context was not passed to the LLM\n\nMore often than not - it's actually the second reason that causes agents to not be reliable.\n\n**Context engineering** is providing the right information and tools in the right format so the LLM can accomplish a task. This is the number one job of AI Engineers. This lack of \"right\" context is the number one blocker for more reliable agents, and LangChain's agent abstractions are uniquely designed to facilitate context engineering.\n\n<Tip>\nNew to context engineering? Start with the [conceptual overview](/oss/concepts/context) to understand the different types of context and when to use them.\n</Tip>\n\n### The agent loop\n\nA typical agent loop consists of two main steps:\n\n1. **Model call** - calls the LLM with a prompt and available tools, returns either a response or a request to execute tools\n2. **Tool execution** - executes the tools that the LLM requested, returns tool results\n\n<div style={{ display: \"flex\", justifyContent: \"center\" }}>\n  <img\n    src=\"/oss/images/core_agent_loop.png\"\n    alt=\"Core agent loop diagram\"\n    className=\"rounded-lg\"\n  />\n</div>\n\nThis loop continues until the LLM decides to finish.\n\n### What you can control\n\nTo build reliable agents, you need to control what happens at each step of the agent loop, as well as what happens between steps.\n\n| Context Type | What You Control | Transient or Persistent |\n|--------------|------------------|-------------------------|\n| **[Model Context](#model-context)** | What goes into model calls (instructions, message history, tools, response format) | Transient |\n| **[Tool Context](#tool-context)** | What tools can access and produce (reads/writes to state, store, runtime context) | Persistent |\n| **[Life-cycle Context](#life-cycle-context)** | What happens between model and tool calls (summarization, guardrails, logging, etc.) | Persistent |\n\n<CardGroup>\n  <Card title=\"Transient context\" icon=\"bolt\" iconType=\"duotone\">\n    What the LLM sees for a single call. You can modify messages, tools, or prompts without changing what's saved in state.\n  </Card>\n  <Card title=\"Persistent context\" icon=\"database\" iconType=\"duotone\">\n    What gets saved in state across turns. Life-cycle hooks and tool writes modify this permanently.\n  </Card>\n</CardGroup>\n\n### Data sources\n\nThroughout this process, your agent accesses (reads / writes) different sources of data:\n\n| Data Source | Also Known As | Scope | Examples |\n|-------------|---------------|-------|----------|\n| **Runtime Context** | Static configuration | Conversation-scoped | User ID, API keys, database connections, permissions, environment settings |\n| **State** | Short-term memory", "metadata": {"source": "context-engineering.mdx"}}
{"text": ">\n  <Card title=\"Transient context\" icon=\"bolt\" iconType=\"duotone\">\n    What the LLM sees for a single call. You can modify messages, tools, or prompts without changing what's saved in state.\n  </Card>\n  <Card title=\"Persistent context\" icon=\"database\" iconType=\"duotone\">\n    What gets saved in state across turns. Life-cycle hooks and tool writes modify this permanently.\n  </Card>\n</CardGroup>\n\n### Data sources\n\nThroughout this process, your agent accesses (reads / writes) different sources of data:\n\n| Data Source | Also Known As | Scope | Examples |\n|-------------|---------------|-------|----------|\n| **Runtime Context** | Static configuration | Conversation-scoped | User ID, API keys, database connections, permissions, environment settings |\n| **State** | Short-term memory | Conversation-scoped | Current messages, uploaded files, authentication status, tool results |\n| **Store** | Long-term memory | Cross-conversation | User preferences, extracted insights, memories, historical data |\n\n### How it works\n\nLangChain [middleware](/oss/langchain/middleware) is the mechanism under the hood that makes context engineering practical for developers using LangChain.\n\nMiddleware allows you to hook into any step in the agent lifecycle and:\n\n* Update context\n* Jump to a different step in the agent lifecycle\n\nThroughout this guide, you'll see frequent use of the middleware API as a means to the context engineering end.\n\n## Model context\n\nControl what goes into each model call - instructions, available tools, which model to use, and output format. These decisions directly impact reliability and cost.\n\n<CardGroup cols={2}>\n    <Card title=\"System Prompt\" icon=\"message-lines\" href=\"#system-prompt\">\n        Base instructions from the developer to the LLM.\n    </Card>\n    <Card title=\"Messages\" icon=\"comments\" href=\"#messages\">\n        The full list of messages (conversation history) sent to the LLM.\n    </Card>\n    <Card title=\"Tools\" icon=\"wrench\" href=\"#tools\">\n        Utilities the agent has access to to take actions.\n    </Card>\n    <Card title=\"Model\" icon=\"brain-circuit\" href=\"#model\">\n        The actual model (including configuration) to be called.\n    </Card>\n    <Card title=\"Response Format\" icon=\"brackets-curly\" href=\"#response-format\">\n        Schema specification for the model's final response.\n    </Card>\n</CardGroup>\n\nAll of these types of model context can draw from **state** (short-term memory), **store** (long-term memory), or **runtime context** (static configuration).\n\n### System Prompt\n\nThe system prompt sets the LLM's behavior and capabilities. Different users, contexts, or conversation stages need different instructions. Successful agents draw on memories, preferences, and configuration to provide the right instructions for the current state of the conversation.\n\n<Tabs>\n  <Tab title=\"State\">\n    Access message count or conversation context from state:\n\n    :::python\n\n    ```python\n    from langchain.agents import create_agent\n    from langchain.agents.", "metadata": {"source": "context-engineering.mdx"}}
{"text": "brackets-curly\" href=\"#response-format\">\n        Schema specification for the model's final response.\n    </Card>\n</CardGroup>\n\nAll of these types of model context can draw from **state** (short-term memory), **store** (long-term memory), or **runtime context** (static configuration).\n\n### System Prompt\n\nThe system prompt sets the LLM's behavior and capabilities. Different users, contexts, or conversation stages need different instructions. Successful agents draw on memories, preferences, and configuration to provide the right instructions for the current state of the conversation.\n\n<Tabs>\n  <Tab title=\"State\">\n    Access message count or conversation context from state:\n\n    :::python\n\n    ```python\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import dynamic_prompt, ModelRequest\n\n    @dynamic_prompt\n    def state_aware_prompt(request: ModelRequest) -> str:\n        # request.messages is a shortcut for request.state[\"messages\"]\n        message_count = len(request.messages)\n\n        base = \"You are a helpful assistant.\"\n\n        if message_count > 10:\n            base += \"\\nThis is a long conversation - be extra concise.\"\n\n        return base\n\n    agent = create_agent(\n        model=\"gpt-4.1\",\n        tools=[...],\n        middleware=[state_aware_prompt]\n    )\n    ```\n    :::\n\n    :::js\n    ```typescript\n    import { createAgent } from \"langchain\";\n\n    const agent = createAgent({\n      model: \"gpt-4.1\",\n      tools: [...],\n      middleware: [\n        dynamicSystemPromptMiddleware((state) => {\n          // Read from State: check conversation length\n          const messageCount = state.messages.length;\n\n          let base = \"You are a helpful assistant.\";\n\n          if (messageCount > 10) {\n            base += \"\\nThis is a long conversation - be extra concise.\";\n          }\n\n          return base;\n        }),\n      ],\n    });\n    ```\n    :::\n  </Tab>\n\n  <Tab title=\"Store\">\n    Access user preferences from long-term memory:\n\n    :::python\n\n    ```python\n    from dataclasses import dataclass\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import dynamic_prompt, ModelRequest\n    from langgraph.store.memory import InMemoryStore\n\n    @dataclass\n    class Context:\n", "metadata": {"source": "context-engineering.mdx"}}
{"text": "            base += \"\\nThis is a long conversation - be extra concise.\";\n          }\n\n          return base;\n        }),\n      ],\n    });\n    ```\n    :::\n  </Tab>\n\n  <Tab title=\"Store\">\n    Access user preferences from long-term memory:\n\n    :::python\n\n    ```python\n    from dataclasses import dataclass\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import dynamic_prompt, ModelRequest\n    from langgraph.store.memory import InMemoryStore\n\n    @dataclass\n    class Context:\n        user_id: str\n\n    @dynamic_prompt\n    def store_aware_prompt(request: ModelRequest) -> str:\n        user_id = request.runtime.context.user_id\n\n        # Read from Store: get user preferences\n        store = request.runtime.store\n        user_prefs = store.get((\"preferences\",), user_id)\n\n        base = \"You are a helpful assistant.\"\n\n        if user_prefs:\n            style = user_prefs.value.get(\"communication_style\", \"balanced\")\n            base += f\"\\nUser prefers {style} responses.\"\n\n        return base\n\n    agent = create_agent(\n        model=\"gpt-4.1\",\n        tools=[...],\n        middleware=[store_aware_prompt],\n        context_schema=Context,\n        store=InMemoryStore()\n    )\n    ```\n    :::\n\n    :::js\n    ```typescript\n    import * as z from \"zod\";\n    import { createAgent, dynamicSystemPromptMiddleware } from \"langchain\";\n\n    const contextSchema = z.object({\n      userId: z.string(),\n    });\n\n    type Context = z.infer<typeof contextSchema>;\n\n    const agent = createAgent({\n      model: \"gpt-4.1\",\n      tools: [...],\n      contextSchema,\n      middleware: [\n        dynamicSystemPromptMiddleware<Context>(async (state, runtime) => {\n          const userId = runtime.context.userId;\n\n          // Read from Store: get user preferences\n          const store = runtime.store;\n          const userPrefs = await store.get([\"preferences\"], userId);\n\n          let base =", "metadata": {"source": "context-engineering.mdx"}}
{"text": " });\n\n    type Context = z.infer<typeof contextSchema>;\n\n    const agent = createAgent({\n      model: \"gpt-4.1\",\n      tools: [...],\n      contextSchema,\n      middleware: [\n        dynamicSystemPromptMiddleware<Context>(async (state, runtime) => {\n          const userId = runtime.context.userId;\n\n          // Read from Store: get user preferences\n          const store = runtime.store;\n          const userPrefs = await store.get([\"preferences\"], userId);\n\n          let base = \"You are a helpful assistant.\";\n\n          if (userPrefs) {\n            const style = userPrefs.value?.communicationStyle || \"balanced\";\n            base += `\\nUser prefers ${style} responses.`;\n          }\n\n          return base;\n        }),\n      ],\n    });\n    ```\n    :::\n  </Tab>\n\n  <Tab title=\"Runtime Context\">\n    Access user ID or configuration from Runtime Context:\n\n    :::python\n\n    ```python\n    from dataclasses import dataclass\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import dynamic_prompt, ModelRequest\n\n    @dataclass\n    class Context:\n        user_role: str\n        deployment_env: str\n\n    @dynamic_prompt\n    def context_aware_prompt(request: ModelRequest) -> str:\n        # Read from Runtime Context: user role and environment\n        user_role = request.runtime.context.user_role\n        env = request.runtime.context.deployment_env\n\n        base = \"You are a helpful assistant.\"\n\n        if user_role == \"admin\":\n            base += \"\\nYou have admin access. You can perform all operations.\"\n        elif user_role == \"viewer\":\n            base += \"\\nYou have read-only access. Guide users to read operations only.\"\n\n        if env == \"production\":\n            base += \"\\nBe extra careful with any data modifications.\"\n\n        return base\n\n    agent = create_agent(\n        model=\"gpt-4.1\",\n        tools=[...],\n        middleware=[context_aware_prompt],\n        context_schema=Context\n    )\n    ```\n    :::\n\n ", "metadata": {"source": "context-engineering.mdx"}}
{"text": " += \"\\nYou have admin access. You can perform all operations.\"\n        elif user_role == \"viewer\":\n            base += \"\\nYou have read-only access. Guide users to read operations only.\"\n\n        if env == \"production\":\n            base += \"\\nBe extra careful with any data modifications.\"\n\n        return base\n\n    agent = create_agent(\n        model=\"gpt-4.1\",\n        tools=[...],\n        middleware=[context_aware_prompt],\n        context_schema=Context\n    )\n    ```\n    :::\n\n    :::js\n    ```typescript\n    import * as z from \"zod\";\n    import { createAgent, dynamicSystemPromptMiddleware } from \"langchain\";\n\n    const contextSchema = z.object({\n      userRole: z.string(),\n      deploymentEnv: z.string(),\n    });\n\n    type Context = z.infer<typeof contextSchema>;\n\n    const agent = createAgent({\n      model: \"gpt-4.1\",\n      tools: [...],\n      contextSchema,\n      middleware: [\n        dynamicSystemPromptMiddleware<Context>((state, runtime) => {\n          // Read from Runtime Context: user role and environment\n          const userRole = runtime.context.userRole;\n          const env = runtime.context.deploymentEnv;\n\n          let base = \"You are a helpful assistant.\";\n\n          if (userRole === \"admin\") {\n            base += \"\\nYou have admin access. You can perform all operations.\";\n          } else if (userRole === \"viewer\") {\n            base += \"\\nYou have read-only access. Guide users to read operations only.\";\n          }\n\n          if (env === \"production\") {\n            base += \"\\nBe extra careful with any data modifications.\";\n          }\n\n          return base;\n        }),\n      ],\n    });\n    ```\n    :::\n  </Tab>\n\n</Tabs>\n\n### Messages\n\nMessages make up the prompt that is sent to the LLM.\nIt's critical to manage the content of messages to ensure that the LLM has the right information to respond well.\n\n<Tabs>\n  <Tab title=\"State\">\n    Inject uploaded file context from State when relevant to current query:\n\n    :::python\n\n    ```python\n    from langchain.agents import", "metadata": {"source": "context-engineering.mdx"}}
{"text": " (env === \"production\") {\n            base += \"\\nBe extra careful with any data modifications.\";\n          }\n\n          return base;\n        }),\n      ],\n    });\n    ```\n    :::\n  </Tab>\n\n</Tabs>\n\n### Messages\n\nMessages make up the prompt that is sent to the LLM.\nIt's critical to manage the content of messages to ensure that the LLM has the right information to respond well.\n\n<Tabs>\n  <Tab title=\"State\">\n    Inject uploaded file context from State when relevant to current query:\n\n    :::python\n\n    ```python\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from typing import Callable\n\n    @wrap_model_call\n    def inject_file_context(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse]\n    ) -> ModelResponse:\n        \"\"\"Inject context about files user has uploaded this session.\"\"\"\n        # Read from State: get uploaded files metadata\n        uploaded_files = request.state.get(\"uploaded_files\", [])  # [!code highlight]\n\n        if uploaded_files:\n            # Build context about available files\n            file_descriptions = []\n            for file in uploaded_files:\n                file_descriptions.append(\n                    f\"- {file['name']} ({file['type']}): {file['summary']}\"\n                )\n\n            file_context = f\"\"\"Files you have access to in this conversation:\n{chr(10).join(file_descriptions)}\n\nReference these files when answering questions.\"\"\"\n\n            # Inject file context before recent messages\n            messages = [  # [!code highlight]\n                *request.messages,\n                {\"role\": \"user\", \"content\": file_context},\n            ]\n            request = request.override(messages=messages)  # [!code highlight]\n\n        return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4.1\",\n        tools=[...],\n        middleware=[inject_file_context]\n    )\n    ```\n    :::\n\n   ", "metadata": {"source": "context-engineering.mdx"}}
{"text": "    messages = [  # [!code highlight]\n                *request.messages,\n                {\"role\": \"user\", \"content\": file_context},\n            ]\n            request = request.override(messages=messages)  # [!code highlight]\n\n        return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4.1\",\n        tools=[...],\n        middleware=[inject_file_context]\n    )\n    ```\n    :::\n\n    :::js\n    ```typescript\n    import { createMiddleware } from \"langchain\";\n\n    const injectFileContext = createMiddleware({\n      name: \"InjectFileContext\",\n      wrapModelCall: (request, handler) => {\n        // request.state is a shortcut for request.state.messages\n        const uploadedFiles = request.state.uploadedFiles || [];  // [!code highlight]\n\n        if (uploadedFiles.length > 0) {\n          // Build context about available files\n          const fileDescriptions = uploadedFiles.map(file =>\n            `- ${file.name} (${file.type}): ${file.summary}`\n          );\n\n          const fileContext = `Files you have access to in this conversation:\n${fileDescriptions.join(\"\\n\")}\n\nReference these files when answering questions.`;\n\n          // Inject file context before recent messages\n          const messages = [  // [!code highlight]\n            ...request.messages,  // Rest of conversation\n            { role: \"user\", content: fileContext }\n          ];\n          request = request.override({ messages });  // [!code highlight]\n        }\n\n        return handler(request);\n      },\n    });\n\n    const agent = createAgent({\n      model: \"gpt-4.1\",\n      tools: [...],\n      middleware: [injectFileContext],\n    });\n    ```\n    :::\n  </Tab>\n\n  <Tab title=\"Store\">\n    Inject user's email writing style from Store to guide drafting:\n\n    :::python\n\n    ```python\n    from dataclasses import dataclass\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from typing import Callable\n  ", "metadata": {"source": "context-engineering.mdx"}}
{"text": " }\n\n        return handler(request);\n      },\n    });\n\n    const agent = createAgent({\n      model: \"gpt-4.1\",\n      tools: [...],\n      middleware: [injectFileContext],\n    });\n    ```\n    :::\n  </Tab>\n\n  <Tab title=\"Store\">\n    Inject user's email writing style from Store to guide drafting:\n\n    :::python\n\n    ```python\n    from dataclasses import dataclass\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from typing import Callable\n    from langgraph.store.memory import InMemoryStore\n\n    @dataclass\n    class Context:\n        user_id: str\n\n    @wrap_model_call\n    def inject_writing_style(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse]\n    ) -> ModelResponse:\n        \"\"\"Inject user's email writing style from Store.\"\"\"\n        user_id = request.runtime.context.user_id  # [!code highlight]\n\n        # Read from Store: get user's writing style examples\n        store = request.runtime.store  # [!code highlight]\n        writing_style = store.get((\"writing_style\",), user_id)  # [!code highlight]\n\n        if writing_style:\n            style = writing_style.value\n            # Build style guide from stored examples\n            style_context = f\"\"\"Your writing style:\n- Tone: {style.get('tone', 'professional')}\n- Typical greeting: \"{style.get('greeting', 'Hi')}\"\n- Typical sign-off: \"{style.get('sign_off', 'Best')}\"\n- Example email you've written:\n{style.get('example_email', '')}\"\"\"\n\n            # Append at end - models pay more attention to final messages\n            messages = [\n                *request.messages,\n                {\"role\": \"user\", \"content\": style_context}\n            ]\n            request = request.override(messages=messages)  # [!code highlight]\n\n        return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4.1\",\n        tools=[...],\n        middleware=[inject_writing_style],\n        context_schema=Context,\n       ", "metadata": {"source": "context-engineering.mdx"}}
{"text": "         messages = [\n                *request.messages,\n                {\"role\": \"user\", \"content\": style_context}\n            ]\n            request = request.override(messages=messages)  # [!code highlight]\n\n        return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4.1\",\n        tools=[...],\n        middleware=[inject_writing_style],\n        context_schema=Context,\n        store=InMemoryStore()\n    )\n    ```\n    :::\n\n    :::js\n    ```typescript\n    import * as z from \"zod\";\n    import { createMiddleware } from \"langchain\";\n\n    const contextSchema = z.object({\n      userId: z.string(),\n    });\n\n    const injectWritingStyle = createMiddleware({\n      name: \"InjectWritingStyle\",\n      contextSchema,\n      wrapModelCall: async (request, handler) => {\n        const userId = request.runtime.context.userId;  // [!code highlight]\n\n        // Read from Store: get user's writing style examples\n        const store = request.runtime.store;  // [!code highlight]\n        const writingStyle = await store.get([\"writing_style\"], userId);  // [!code highlight]\n\n        if (writingStyle) {\n          const style = writingStyle.value;\n          // Build style guide from stored examples\n          const styleContext = `Your writing style:\n- Tone: ${style.tone || 'professional'}\n- Typical greeting: \"${style.greeting || 'Hi'}\"\n- Typical sign-off: \"${style.signOff || 'Best'}\"\n- Example email you've written:\n${style.exampleEmail || ''}`;\n\n          // Append at end - models pay more attention to final messages\n          const messages = [\n            ...request.messages,\n            { role: \"user\", content: styleContext }\n          ];\n          request = request.override({ messages });  // [!code highlight]\n        }\n\n        return handler(request);\n      },\n    });\n    ```\n    :::\n  </Tab>\n\n  <Tab title=\"Runtime Context\">\n    Inject compliance rules from Runtime Context based on user's jurisdiction:\n\n    :::python\n\n    ```python\n  ", "metadata": {"source": "context-engineering.mdx"}}
{"text": "  // Append at end - models pay more attention to final messages\n          const messages = [\n            ...request.messages,\n            { role: \"user\", content: styleContext }\n          ];\n          request = request.override({ messages });  // [!code highlight]\n        }\n\n        return handler(request);\n      },\n    });\n    ```\n    :::\n  </Tab>\n\n  <Tab title=\"Runtime Context\">\n    Inject compliance rules from Runtime Context based on user's jurisdiction:\n\n    :::python\n\n    ```python\n    from dataclasses import dataclass\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from typing import Callable\n\n    @dataclass\n    class Context:\n        user_jurisdiction: str\n        industry: str\n        compliance_frameworks: list[str]\n\n    @wrap_model_call\n    def inject_compliance_rules(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse]\n    ) -> ModelResponse:\n        \"\"\"Inject compliance constraints from Runtime Context.\"\"\"\n        # Read from Runtime Context: get compliance requirements\n        jurisdiction = request.runtime.context.user_jurisdiction  # [!code highlight]\n        industry = request.runtime.context.industry  # [!code highlight]\n        frameworks = request.runtime.context.compliance_frameworks  # [!code highlight]\n\n        # Build compliance constraints\n        rules = []\n        if \"GDPR\" in frameworks:\n            rules.append(\"- Must obtain explicit consent before processing personal data\")\n            rules.append(\"- Users have right to data deletion\")\n        if \"HIPAA\" in frameworks:\n            rules.append(\"- Cannot share patient health information without authorization\")\n            rules.append(\"- Must use secure, encrypted communication\")\n        if industry == \"finance\":\n            rules.append(\"- Cannot provide financial advice without proper disclaimers\")\n\n        if rules:\n            compliance_context = f\"\"\"Compliance requirements for {jurisdiction}:\n{chr(10).join(rules)}\"\"\"\n\n            # Append at end - models pay more attention to final messages\n            messages = [\n                *request.messages", "metadata": {"source": "context-engineering.mdx"}}
{"text": "           rules.append(\"- Cannot share patient health information without authorization\")\n            rules.append(\"- Must use secure, encrypted communication\")\n        if industry == \"finance\":\n            rules.append(\"- Cannot provide financial advice without proper disclaimers\")\n\n        if rules:\n            compliance_context = f\"\"\"Compliance requirements for {jurisdiction}:\n{chr(10).join(rules)}\"\"\"\n\n            # Append at end - models pay more attention to final messages\n            messages = [\n                *request.messages,\n                {\"role\": \"user\", \"content\": compliance_context}\n            ]\n            request = request.override(messages=messages)  # [!code highlight]\n\n        return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4.1\",\n        tools=[...],\n        middleware=[inject_compliance_rules],\n        context_schema=Context\n    )\n    ```\n    :::\n\n    :::js\n    ```typescript\n    import * as z from \"zod\";\n    import { createMiddleware } from \"langchain\";\n\n    const contextSchema = z.object({\n      userJurisdiction: z.string(),\n      industry: z.string(),\n      complianceFrameworks: z.array(z.string()),\n    });\n\n    type Context = z.infer<typeof contextSchema>;\n\n    const injectComplianceRules = createMiddleware<Context>({\n      name: \"InjectComplianceRules\",\n      contextSchema,\n      wrapModelCall: (request, handler) => {\n        // Read from Runtime Context: get compliance requirements\n        const { userJurisdiction, industry, complianceFrameworks } = request.runtime.context;  // [!code highlight]\n\n        // Build compliance constraints\n        const rules = [];\n        if (complianceFrameworks.includes(\"GDPR\")) {\n          rules.push(\"- Must obtain explicit consent before processing personal data\");\n          rules.push(\"- Users have right to data deletion\");\n        }\n        if (complianceFrameworks.includes(\"HIPAA\")) {\n          rules.push(\"- Cannot share patient health information without authorization\");\n          rules.push(\"- Must use secure, encrypted communication\");\n        }\n        if (industry === \"", "metadata": {"source": "context-engineering.mdx"}}
{"text": " = request.runtime.context;  // [!code highlight]\n\n        // Build compliance constraints\n        const rules = [];\n        if (complianceFrameworks.includes(\"GDPR\")) {\n          rules.push(\"- Must obtain explicit consent before processing personal data\");\n          rules.push(\"- Users have right to data deletion\");\n        }\n        if (complianceFrameworks.includes(\"HIPAA\")) {\n          rules.push(\"- Cannot share patient health information without authorization\");\n          rules.push(\"- Must use secure, encrypted communication\");\n        }\n        if (industry === \"finance\") {\n          rules.push(\"- Cannot provide financial advice without proper disclaimers\");\n        }\n\n        if (rules.length > 0) {\n          const complianceContext = `Compliance requirements for ${userJurisdiction}:\n${rules.join(\"\\n\")}`;\n\n          // Append at end - models pay more attention to final messages\n          const messages = [\n            ...request.messages,\n            { role: \"user\", content: complianceContext }\n          ];\n          request = request.override({ messages });  // [!code highlight]\n        }\n\n        return handler(request);\n      },\n    });\n    ```\n    :::\n  </Tab>\n\n</Tabs>\n\n<Note>\n**Transient vs Persistent Message Updates:**\n\nThe examples above use `wrap_model_call` to make **transient** updates - modifying what messages are sent to the model for a single call without changing what's saved in state.\n\nFor **persistent** updates that modify state (like the summarization example in [Life-cycle Context](#summarization)), use life-cycle hooks like `before_model` or `after_model` to permanently update the conversation history. See the [middleware documentation](/oss/langchain/middleware) for more details.\n</Note>\n\n### Tools\n\nTools let the model interact with databases, APIs, and external systems. How you define and select tools directly impacts whether the model can complete tasks effectively.\n\n#### Defining tools\n\nEach tool needs a clear name, description, argument names, and argument descriptions. These aren't just metadata\u2014they guide the model's reasoning about when and how to use the tool.\n\n:::python\n```python\nfrom langchain.tools import tool\n\n@tool(parse_docstring=True)\ndef search_orders(\n    user_id: str,\n    status: str,\n    limit: int = 10\n) -> str:\n    \"\"\"Search for user orders by status.\n\n    Use this when the user asks about order history or wants to check\n    order status. Always filter by the provided status.\n\n    Args:\n   ", "metadata": {"source": "context-engineering.mdx"}}
{"text": "### Tools\n\nTools let the model interact with databases, APIs, and external systems. How you define and select tools directly impacts whether the model can complete tasks effectively.\n\n#### Defining tools\n\nEach tool needs a clear name, description, argument names, and argument descriptions. These aren't just metadata\u2014they guide the model's reasoning about when and how to use the tool.\n\n:::python\n```python\nfrom langchain.tools import tool\n\n@tool(parse_docstring=True)\ndef search_orders(\n    user_id: str,\n    status: str,\n    limit: int = 10\n) -> str:\n    \"\"\"Search for user orders by status.\n\n    Use this when the user asks about order history or wants to check\n    order status. Always filter by the provided status.\n\n    Args:\n        user_id: Unique identifier for the user\n        status: Order status: 'pending', 'shipped', or 'delivered'\n        limit: Maximum number of results to return\n    \"\"\"\n    # Implementation here\n    pass\n```\n:::\n\n:::js\n```typescript\nimport { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n\nconst searchOrders = tool(\n  async ({ userId, status, limit }) => {\n    // Implementation here\n  },\n  {\n    name: \"search_orders\",\n    description: `Search for user orders by status.\n\n    Use this when the user asks about order history or wants to check\n    order status. Always filter by the provided status.`,\n    schema: z.object({\n      userId: z.string().describe(\"Unique identifier for the user\"),\n      status: z.enum([\"pending\", \"shipped\", \"delivered\"]).describe(\"Order status to filter by\"),\n      limit: z.number().default(10).describe(\"Maximum number of results to return\"),\n    }),\n  }\n);\n```\n:::\n\n#### Selecting tools\n\nNot every tool is appropriate for every situation. Too many tools may overwhelm the model (overload context) and increase errors; too few limit capabilities. Dynamic tool selection adapts the available toolset based on authentication state, user permissions, feature flags, or conversation stage.\n\n<Tabs>\n  <Tab title=\"State\">\n    Enable advanced tools only after certain conversation milestones:\n\n    :::python\n\n    ```python\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from typing import Callable\n\n    @wrap_model_call\n    def state_based_tools(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse]\n    ) -> ModelResponse:\n        \"\"\"Filter tools based on conversation State.\"\"\"\n        # Read from State: check if user has authenticated\n        state = request.state  # [!code highlight]\n        is_authenticated = state.get(\"authenticated\",", "metadata": {"source": "context-engineering.mdx"}}
{"text": " conversation milestones:\n\n    :::python\n\n    ```python\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from typing import Callable\n\n    @wrap_model_call\n    def state_based_tools(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse]\n    ) -> ModelResponse:\n        \"\"\"Filter tools based on conversation State.\"\"\"\n        # Read from State: check if user has authenticated\n        state = request.state  # [!code highlight]\n        is_authenticated = state.get(\"authenticated\", False)  # [!code highlight]\n        message_count = len(state[\"messages\"])\n\n        # Only enable sensitive tools after authentication\n        if not is_authenticated:\n            tools = [t for t in request.tools if t.name.startswith(\"public_\")]\n            request = request.override(tools=tools)  # [!code highlight]\n        elif message_count < 5:\n            # Limit tools early in conversation\n            tools = [t for t in request.tools if t.name != \"advanced_search\"]\n            request = request.override(tools=tools)  # [!code highlight]\n\n        return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4.1\",\n        tools=[public_search, private_search, advanced_search],\n        middleware=[state_based_tools]\n    )\n    ```\n    :::\n\n    :::js\n    ```typescript\n    import { createMiddleware } from \"langchain\";\n\n    const stateBasedTools = createMiddleware({\n      name: \"StateBasedTools\",\n      wrapModelCall: (request, handler) => {\n        // Read from State: check authentication and conversation length\n        const state = request.state;  // [!code highlight]\n        const isAuthenticated = state.authenticated || false;  // [!code highlight]\n        const messageCount = state.messages.length;\n\n        let filteredTools = request.tools;\n\n        // Only enable sensitive tools after authentication\n        if (!isAuthenticated) {\n          filteredTools = request.tools.filter(t => t.name.startsWith(\"public_\"));  // [!code highlight]\n        } else if (messageCount < 5) {\n          filteredTools = request.tools.filter(", "metadata": {"source": "context-engineering.mdx"}}
{"text": " Read from State: check authentication and conversation length\n        const state = request.state;  // [!code highlight]\n        const isAuthenticated = state.authenticated || false;  // [!code highlight]\n        const messageCount = state.messages.length;\n\n        let filteredTools = request.tools;\n\n        // Only enable sensitive tools after authentication\n        if (!isAuthenticated) {\n          filteredTools = request.tools.filter(t => t.name.startsWith(\"public_\"));  // [!code highlight]\n        } else if (messageCount < 5) {\n          filteredTools = request.tools.filter(t => t.name !== \"advanced_search\");  // [!code highlight]\n        }\n\n        return handler({ ...request, tools: filteredTools });  // [!code highlight]\n      },\n    });\n    ```\n    :::\n  </Tab>\n\n  <Tab title=\"Store\">\n    Filter tools based on user preferences or feature flags in Store:\n\n    :::python\n\n    ```python\n    from dataclasses import dataclass\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from typing import Callable\n    from langgraph.store.memory import InMemoryStore\n\n    @dataclass\n    class Context:\n        user_id: str\n\n    @wrap_model_call\n    def store_based_tools(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse]\n    ) -> ModelResponse:\n        \"\"\"Filter tools based on Store preferences.\"\"\"\n        user_id = request.runtime.context.user_id\n\n        # Read from Store: get user's enabled features\n        store = request.runtime.store\n        feature_flags = store.get((\"features\",), user_id)\n\n        if feature_flags:\n            enabled_features = feature_flags.value.get(\"enabled_tools\", [])\n            # Only include tools that are enabled for this user\n            tools = [t for t in request.tools if t.name in enabled_features]\n            request = request.override(tools=tools)\n\n        return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4.1\",\n        tools=[search_tool, analysis_tool, export_tool],\n        middleware=[store_based_tools],\n        context_schema=Context,\n      ", "metadata": {"source": "context-engineering.mdx"}}
{"text": "      enabled_features = feature_flags.value.get(\"enabled_tools\", [])\n            # Only include tools that are enabled for this user\n            tools = [t for t in request.tools if t.name in enabled_features]\n            request = request.override(tools=tools)\n\n        return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4.1\",\n        tools=[search_tool, analysis_tool, export_tool],\n        middleware=[store_based_tools],\n        context_schema=Context,\n        store=InMemoryStore()\n    )\n    ```\n    :::\n\n    :::js\n    ```typescript\n    import * as z from \"zod\";\n    import { createMiddleware } from \"langchain\";\n\n    const contextSchema = z.object({\n      userId: z.string(),\n    });\n\n    const storeBasedTools = createMiddleware({\n      name: \"StoreBasedTools\",\n      contextSchema,\n      wrapModelCall: async (request, handler) => {\n        const userId = request.runtime.context.userId;  // [!code highlight]\n\n        // Read from Store: get user's enabled features\n        const store = request.runtime.store;  // [!code highlight]\n        const featureFlags = await store.get([\"features\"], userId);  // [!code highlight]\n\n        let filteredTools = request.tools;\n\n        if (featureFlags) {\n          const enabledFeatures = featureFlags.value?.enabledTools || [];\n          filteredTools = request.tools.filter(t => enabledFeatures.includes(t.name));  // [!code highlight]\n        }\n\n        return handler({ ...request, tools: filteredTools });  // [!code highlight]\n      },\n    });\n    ```\n    :::\n  </Tab>\n\n  <Tab title=\"Runtime Context\">\n    Filter tools based on user permissions from Runtime Context:\n\n    :::python\n\n    ```python\n    from dataclasses import dataclass\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from typing import Callable\n\n    @dataclass\n    class Context:\n        user_role: str\n\n    @wrap_model_call\n    def context_based_tools(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse]\n    ) -> ModelResponse:\n      ", "metadata": {"source": "context-engineering.mdx"}}
{"text": "  :::\n  </Tab>\n\n  <Tab title=\"Runtime Context\">\n    Filter tools based on user permissions from Runtime Context:\n\n    :::python\n\n    ```python\n    from dataclasses import dataclass\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from typing import Callable\n\n    @dataclass\n    class Context:\n        user_role: str\n\n    @wrap_model_call\n    def context_based_tools(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse]\n    ) -> ModelResponse:\n        \"\"\"Filter tools based on Runtime Context permissions.\"\"\"\n        # Read from Runtime Context: get user role\n        user_role = request.runtime.context.user_role\n\n        if user_role == \"admin\":\n            # Admins get all tools\n            pass\n        elif user_role == \"editor\":\n            # Editors can't delete\n            tools = [t for t in request.tools if t.name != \"delete_data\"]\n            request = request.override(tools=tools)\n        else:\n            # Viewers get read-only tools\n            tools = [t for t in request.tools if t.name.startswith(\"read_\")]\n            request = request.override(tools=tools)\n\n        return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4.1\",\n        tools=[read_data, write_data, delete_data],\n        middleware=[context_based_tools],\n        context_schema=Context\n    )\n    ```\n    :::\n\n    :::js\n    ```typescript\n    import * as z from \"zod\";\n    import { createMiddleware } from \"langchain\";\n\n    const contextSchema = z.object({\n      userRole: z.string(),\n    });\n\n    const contextBasedTools = createMiddleware({\n      name: \"ContextBasedTools\",\n      contextSchema,\n      wrapModelCall: (request, handler) => {\n        // Read from Runtime Context: get user role\n        const userRole = request.runtime.context.userRole;  // [!code highlight]\n\n        let filteredTools = request.tools;\n\n        if (userRole === \"admin\") {\n          //", "metadata": {"source": "context-engineering.mdx"}}
{"text": "    import * as z from \"zod\";\n    import { createMiddleware } from \"langchain\";\n\n    const contextSchema = z.object({\n      userRole: z.string(),\n    });\n\n    const contextBasedTools = createMiddleware({\n      name: \"ContextBasedTools\",\n      contextSchema,\n      wrapModelCall: (request, handler) => {\n        // Read from Runtime Context: get user role\n        const userRole = request.runtime.context.userRole;  // [!code highlight]\n\n        let filteredTools = request.tools;\n\n        if (userRole === \"admin\") {\n          // Admins get all tools\n        } else if (userRole === \"editor\") {\n          filteredTools = request.tools.filter(t => t.name !== \"delete_data\");  // [!code highlight]\n        } else {\n          filteredTools = request.tools.filter(t => t.name.startsWith(\"read_\"));  // [!code highlight]\n        }\n\n        return handler({ ...request, tools: filteredTools });  // [!code highlight]\n      },\n    });\n    ```\n    :::\n  </Tab>\n</Tabs>\n\nSee [Dynamic tools](/oss/langchain/agents#dynamic-tools) for both filtering pre-registered tools and registering tools at runtime (e.g., from MCP servers).\n\n### Model\n\nDifferent models have different strengths, costs, and context windows. Select the right model for the task at hand, which\nmight change during an agent run.\n\n<Tabs>\n  <Tab title=\"State\">\n    Use different models based on conversation length from State:\n\n    :::python\n\n    ```python\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from langchain.chat_models import init_chat_model\n    from typing import Callable\n\n    # Initialize models once outside the middleware\n    large_model = init_chat_model(\"claude-sonnet-4-5-20250929\")\n    standard_model = init_chat_model(\"gpt-4.1\")\n    efficient_model = init_chat_model(\"gpt-4.1-mini\")\n\n    @wrap_model_call\n    def state_based_model(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse]\n    ) -> ModelResponse:\n        \"\"\"Select model based on State conversation length.\"\"\"\n        # request.messages is a shortcut for request.state[\"messages\"]\n        message_count = len(request.messages)  # [!code highlight]\n\n        if message_count > 20:\n ", "metadata": {"source": "context-engineering.mdx"}}
{"text": "4-5-20250929\")\n    standard_model = init_chat_model(\"gpt-4.1\")\n    efficient_model = init_chat_model(\"gpt-4.1-mini\")\n\n    @wrap_model_call\n    def state_based_model(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse]\n    ) -> ModelResponse:\n        \"\"\"Select model based on State conversation length.\"\"\"\n        # request.messages is a shortcut for request.state[\"messages\"]\n        message_count = len(request.messages)  # [!code highlight]\n\n        if message_count > 20:\n            # Long conversation - use model with larger context window\n            model = large_model\n        elif message_count > 10:\n            # Medium conversation\n            model = standard_model\n        else:\n            # Short conversation - use efficient model\n            model = efficient_model\n\n        request = request.override(model=model)  # [!code highlight]\n\n        return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4.1-mini\",\n        tools=[...],\n        middleware=[state_based_model]\n    )\n    ```\n    :::\n\n    :::js\n\n    ```typescript\n    import { createMiddleware, initChatModel } from \"langchain\";\n\n    // Initialize models once outside the middleware\n    const largeModel = initChatModel(\"claude-sonnet-4-5-20250929\");\n    const standardModel = initChatModel(\"gpt-4.1\");\n    const efficientModel = initChatModel(\"gpt-4.1-mini\");\n\n    const stateBasedModel = createMiddleware({\n      name: \"StateBasedModel\",\n      wrapModelCall: (request, handler) => {\n        // request.messages is a shortcut for request.state.messages\n        const messageCount = request.messages.length;  // [!code highlight]\n        let model;\n\n        if (messageCount > 20) {\n          model = largeModel;\n        } else if (messageCount > 10) {\n          model = standardModel;\n        } else {\n          model = efficientModel;\n        }\n\n        return handler({ ...request, model });  // [!code highlight]\n      },\n    });\n ", "metadata": {"source": "context-engineering.mdx"}}
{"text": " handler) => {\n        // request.messages is a shortcut for request.state.messages\n        const messageCount = request.messages.length;  // [!code highlight]\n        let model;\n\n        if (messageCount > 20) {\n          model = largeModel;\n        } else if (messageCount > 10) {\n          model = standardModel;\n        } else {\n          model = efficientModel;\n        }\n\n        return handler({ ...request, model });  // [!code highlight]\n      },\n    });\n    ```\n    :::\n  </Tab>\n\n  <Tab title=\"Store\">\n    Use user's preferred model from Store:\n\n    :::python\n\n    ```python\n    from dataclasses import dataclass\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from langchain.chat_models import init_chat_model\n    from typing import Callable\n    from langgraph.store.memory import InMemoryStore\n\n    @dataclass\n    class Context:\n        user_id: str\n\n    # Initialize available models once\n    MODEL_MAP = {\n        \"gpt-4.1\": init_chat_model(\"gpt-4.1\"),\n        \"gpt-4.1-mini\": init_chat_model(\"gpt-4.1-mini\"),\n        \"claude-sonnet\": init_chat_model(\"claude-sonnet-4-5-20250929\"),\n    }\n\n    @wrap_model_call\n    def store_based_model(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse]\n    ) -> ModelResponse:\n        \"\"\"Select model based on Store preferences.\"\"\"\n        user_id = request.runtime.context.user_id\n\n        # Read from Store: get user's preferred model\n        store = request.runtime.store\n        user_prefs = store.get((\"preferences\",), user_id)\n\n        if user_prefs:\n            preferred_model = user_prefs.value.get(\"preferred_model\")\n            if preferred_model and preferred_model in MODEL_MAP:\n                request = request.override(model=MODEL_MAP[preferred_model])\n\n        return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4.1\",\n  ", "metadata": {"source": "context-engineering.mdx"}}
{"text": " # Read from Store: get user's preferred model\n        store = request.runtime.store\n        user_prefs = store.get((\"preferences\",), user_id)\n\n        if user_prefs:\n            preferred_model = user_prefs.value.get(\"preferred_model\")\n            if preferred_model and preferred_model in MODEL_MAP:\n                request = request.override(model=MODEL_MAP[preferred_model])\n\n        return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4.1\",\n        tools=[...],\n        middleware=[store_based_model],\n        context_schema=Context,\n        store=InMemoryStore()\n    )\n    ```\n    :::\n\n    :::js\n\n    ```typescript\n    import * as z from \"zod\";\n    import { createMiddleware, initChatModel } from \"langchain\";\n\n    const contextSchema = z.object({\n      userId: z.string(),\n    });\n\n    // Initialize available models once\n    const MODEL_MAP = {\n      \"gpt-4.1\": initChatModel(\"gpt-4.1\"),\n      \"gpt-4.1-mini\": initChatModel(\"gpt-4.1-mini\"),\n      \"claude-sonnet\": initChatModel(\"claude-sonnet-4-5-20250929\"),\n    };\n\n    const storeBasedModel = createMiddleware({\n      name: \"StoreBasedModel\",\n      contextSchema,\n      wrapModelCall: async (request, handler) => {\n        const userId = request.runtime.context.userId;  // [!code highlight]\n\n        // Read from Store: get user's preferred model\n        const store = request.runtime.store;  // [!code highlight]\n        const userPrefs = await store.get([\"preferences\"], userId);  // [!code highlight]\n\n        let model = request.model;\n\n        if (userPrefs) {\n          const preferredModel = userPrefs.value?.preferredModel;\n          if (preferredModel && MODEL_MAP[preferredModel]) {\n            model = MODEL_MAP[preferredModel];  // [!code highlight]\n          }\n        }\n\n        return handler({ ...request, model });  // [!code highlight]\n      },\n    });\n    ```\n    :::", "metadata": {"source": "context-engineering.mdx"}}
{"text": "s = await store.get([\"preferences\"], userId);  // [!code highlight]\n\n        let model = request.model;\n\n        if (userPrefs) {\n          const preferredModel = userPrefs.value?.preferredModel;\n          if (preferredModel && MODEL_MAP[preferredModel]) {\n            model = MODEL_MAP[preferredModel];  // [!code highlight]\n          }\n        }\n\n        return handler({ ...request, model });  // [!code highlight]\n      },\n    });\n    ```\n    :::\n  </Tab>\n\n  <Tab title=\"Runtime Context\">\n    Select model based on cost limits or environment from Runtime Context:\n\n    :::python\n\n    ```python\n    from dataclasses import dataclass\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from langchain.chat_models import init_chat_model\n    from typing import Callable\n\n    @dataclass\n    class Context:\n        cost_tier: str\n        environment: str\n\n    # Initialize models once outside the middleware\n    premium_model = init_chat_model(\"claude-sonnet-4-5-20250929\")\n    standard_model = init_chat_model(\"gpt-4.1\")\n    budget_model = init_chat_model(\"gpt-4.1-mini\")\n\n    @wrap_model_call\n    def context_based_model(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse]\n    ) -> ModelResponse:\n        \"\"\"Select model based on Runtime Context.\"\"\"\n        # Read from Runtime Context: cost tier and environment\n        cost_tier = request.runtime.context.cost_tier\n        environment = request.runtime.context.environment\n\n        if environment == \"production\" and cost_tier == \"premium\":\n            # Production premium users get best model\n            model = premium_model\n        elif cost_tier == \"budget\":\n            # Budget tier gets efficient model\n            model = budget_model\n        else:\n            # Standard tier\n            model = standard_model\n\n        request = request.override(model=model)\n\n        return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4.1\",\n       ", "metadata": {"source": "context-engineering.mdx"}}
{"text": "premium\":\n            # Production premium users get best model\n            model = premium_model\n        elif cost_tier == \"budget\":\n            # Budget tier gets efficient model\n            model = budget_model\n        else:\n            # Standard tier\n            model = standard_model\n\n        request = request.override(model=model)\n\n        return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4.1\",\n        tools=[...],\n        middleware=[context_based_model],\n        context_schema=Context\n    )\n    ```\n    :::\n\n    :::js\n\n    ```typescript\n    import * as z from \"zod\";\n    import { createMiddleware, initChatModel } from \"langchain\";\n\n    const contextSchema = z.object({\n      costTier: z.string(),\n      environment: z.string(),\n    });\n\n    // Initialize models once outside the middleware\n    const premiumModel = initChatModel(\"claude-sonnet-4-5-20250929\");\n    const standardModel = initChatModel(\"gpt-4.1\");\n    const budgetModel = initChatModel(\"gpt-4.1-mini\");\n\n    const contextBasedModel = createMiddleware({\n      name: \"ContextBasedModel\",\n      contextSchema,\n      wrapModelCall: (request, handler) => {\n        // Read from Runtime Context: cost tier and environment\n        const costTier = request.runtime.context.costTier;  // [!code highlight]\n        const environment = request.runtime.context.environment;  // [!code highlight]\n\n        let model;\n\n        if (environment === \"production\" && costTier === \"premium\") {\n          model = premiumModel;\n        } else if (costTier === \"budget\") {\n          model = budgetModel;\n        } else {\n          model = standardModel;\n        }\n\n        return handler({ ...request, model });  // [!code highlight]\n      },\n    });\n    ```\n    :::\n  </Tab>\n</Tabs>\n\nSee [Dynamic model](/oss/langchain/agents#dynamic-model) for more examples.\n\n### Response format\n\nStructured output transforms unstructured text into validated, structured data. When extracting specific fields or returning data for downstream systems, free-form text isn't sufficient.\n\n**How it works:** When you provide", "metadata": {"source": "context-engineering.mdx"}}
{"text": "        } else if (costTier === \"budget\") {\n          model = budgetModel;\n        } else {\n          model = standardModel;\n        }\n\n        return handler({ ...request, model });  // [!code highlight]\n      },\n    });\n    ```\n    :::\n  </Tab>\n</Tabs>\n\nSee [Dynamic model](/oss/langchain/agents#dynamic-model) for more examples.\n\n### Response format\n\nStructured output transforms unstructured text into validated, structured data. When extracting specific fields or returning data for downstream systems, free-form text isn't sufficient.\n\n**How it works:** When you provide a schema as the response format, the model's final response is guaranteed to conform to that schema. The agent runs the model / tool calling loop until the model is done calling tools, then the final response is coerced into the provided format.\n\n#### Defining formats\n\nSchema definitions guide the model. Field names, types, and descriptions specify exactly what format the output should adhere to.\n\n:::python\n```python\nfrom pydantic import BaseModel, Field\n\nclass CustomerSupportTicket(BaseModel):\n    \"\"\"Structured ticket information extracted from customer message.\"\"\"\n\n    category: str = Field(\n        description=\"Issue category: 'billing', 'technical', 'account', or 'product'\"\n    )\n    priority: str = Field(\n        description=\"Urgency level: 'low', 'medium', 'high', or 'critical'\"\n    )\n    summary: str = Field(\n        description=\"One-sentence summary of the customer's issue\"\n    )\n    customer_sentiment: str = Field(\n        description=\"Customer's emotional tone: 'frustrated', 'neutral', or 'satisfied'\"\n    )\n```\n:::\n\n:::js\n```typescript\nimport { z } from \"zod\";\n\nconst customerSupportTicket = z.object({\n  category: z.enum([\"billing\", \"technical\", \"account\", \"product\"]).describe(\n    \"Issue category\"\n  ),\n  priority: z.enum([\"low\", \"medium\", \"high\", \"critical\"]).describe(\n    \"Urgency level\"\n  ),\n  summary: z.string().describe(\n    \"One-sentence summary of the customer's issue\"\n  ),\n  customerSentiment: z.enum([\"frustrated\", \"neutral\", \"satisfied\"]).describe(\n    \"Customer's emotional tone\"\n  ),\n}).describe(\"Structured ticket information extracted from customer message\");\n```\n:::\n\n#### Selecting formats\n\nDynamic response format selection adapts schemas based on user preferences, conversation stage, or role\u2014returning simple formats early and detailed formats as complexity increases.\n\n<Tabs>\n  <Tab title=\"State\">\n    Configure structured output based on conversation state:\n\n    :::python\n\n    ```python\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import", "metadata": {"source": "context-engineering.mdx"}}
{"text": "\n  ),\n  summary: z.string().describe(\n    \"One-sentence summary of the customer's issue\"\n  ),\n  customerSentiment: z.enum([\"frustrated\", \"neutral\", \"satisfied\"]).describe(\n    \"Customer's emotional tone\"\n  ),\n}).describe(\"Structured ticket information extracted from customer message\");\n```\n:::\n\n#### Selecting formats\n\nDynamic response format selection adapts schemas based on user preferences, conversation stage, or role\u2014returning simple formats early and detailed formats as complexity increases.\n\n<Tabs>\n  <Tab title=\"State\">\n    Configure structured output based on conversation state:\n\n    :::python\n\n    ```python\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from pydantic import BaseModel, Field\n    from typing import Callable\n\n    class SimpleResponse(BaseModel):\n        \"\"\"Simple response for early conversation.\"\"\"\n        answer: str = Field(description=\"A brief answer\")\n\n    class DetailedResponse(BaseModel):\n        \"\"\"Detailed response for established conversation.\"\"\"\n        answer: str = Field(description=\"A detailed answer\")\n        reasoning: str = Field(description=\"Explanation of reasoning\")\n        confidence: float = Field(description=\"Confidence score 0-1\")\n\n    @wrap_model_call\n    def state_based_output(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse]\n    ) -> ModelResponse:\n        \"\"\"Select output format based on State.\"\"\"\n        # request.messages is a shortcut for request.state[\"messages\"]\n        message_count = len(request.messages)  # [!code highlight]\n\n        if message_count < 3:\n            # Early conversation - use simple format\n            request = request.override(response_format=SimpleResponse)  # [!code highlight]\n        else:\n            # Established conversation - use detailed format\n            request = request.override(response_format=DetailedResponse)  # [!code highlight]\n\n        return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4.1\",\n        tools=[...],\n        middleware=[state_based_output]\n    )\n    ```\n    :::\n\n    :::js\n\n    ```typescript\n    import { createMiddleware } from \"langchain\";\n    import { z } from \"zod\";\n\n    const simpleResponse = z.object({\n      answer: z.string().describe(\"A brief answer\"),\n    });\n\n ", "metadata": {"source": "context-engineering.mdx"}}
{"text": "            request = request.override(response_format=DetailedResponse)  # [!code highlight]\n\n        return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4.1\",\n        tools=[...],\n        middleware=[state_based_output]\n    )\n    ```\n    :::\n\n    :::js\n\n    ```typescript\n    import { createMiddleware } from \"langchain\";\n    import { z } from \"zod\";\n\n    const simpleResponse = z.object({\n      answer: z.string().describe(\"A brief answer\"),\n    });\n\n    const detailedResponse = z.object({\n      answer: z.string().describe(\"A detailed answer\"),\n      reasoning: z.string().describe(\"Explanation of reasoning\"),\n      confidence: z.number().describe(\"Confidence score 0-1\"),\n    });\n\n    const stateBasedOutput = createMiddleware({\n      name: \"StateBasedOutput\",\n      wrapModelCall: (request, handler) => {\n        // request.state is a shortcut for request.state.messages\n        const messageCount = request.messages.length;  // [!code highlight]\n\n        let responseFormat;\n        if (messageCount < 3) {\n          // Early conversation - use simple format\n          responseFormat = simpleResponse; // [!code highlight]\n        } else {\n          // Established conversation - use detailed format\n          responseFormat = detailedResponse; // [!code highlight]\n        }\n\n        return handler({ ...request, responseFormat });\n      },\n    });\n    ```\n    :::\n  </Tab>\n\n  <Tab title=\"Store\">\n    Configure output format based on user preferences in Store:\n\n    :::python\n\n    ```python\n    from dataclasses import dataclass\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from pydantic import BaseModel, Field\n    from typing import Callable\n    from langgraph.store.memory import InMemoryStore\n\n    @dataclass\n    class Context:\n        user_id: str\n\n    class VerboseResponse(BaseModel):\n        \"\"\"Verbose response with details.\"\"\"\n        answer: str = Field(description=\"Detailed answer\")\n        sources: list[str] = Field(description=\"Sources used\")\n\n    class ConciseResponse(BaseModel):\n        \"\"\"Concise response.\"\"\"\n        answer: str =", "metadata": {"source": "context-engineering.mdx"}}
{"text": " create_agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from pydantic import BaseModel, Field\n    from typing import Callable\n    from langgraph.store.memory import InMemoryStore\n\n    @dataclass\n    class Context:\n        user_id: str\n\n    class VerboseResponse(BaseModel):\n        \"\"\"Verbose response with details.\"\"\"\n        answer: str = Field(description=\"Detailed answer\")\n        sources: list[str] = Field(description=\"Sources used\")\n\n    class ConciseResponse(BaseModel):\n        \"\"\"Concise response.\"\"\"\n        answer: str = Field(description=\"Brief answer\")\n\n    @wrap_model_call\n    def store_based_output(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse]\n    ) -> ModelResponse:\n        \"\"\"Select output format based on Store preferences.\"\"\"\n        user_id = request.runtime.context.user_id\n\n        # Read from Store: get user's preferred response style\n        store = request.runtime.store\n        user_prefs = store.get((\"preferences\",), user_id)\n\n        if user_prefs:\n            style = user_prefs.value.get(\"response_style\", \"concise\")\n            if style == \"verbose\":\n                request = request.override(response_format=VerboseResponse)\n            else:\n                request = request.override(response_format=ConciseResponse)\n\n        return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4.1\",\n        tools=[...],\n        middleware=[store_based_output],\n        context_schema=Context,\n        store=InMemoryStore()\n    )\n    ```\n    :::\n\n    :::js\n\n    ```typescript\n    import * as z from \"zod\";\n    import { createMiddleware } from \"langchain\";\n\n    const contextSchema = z.object({\n      userId: z.string(),\n    });\n\n    const verboseResponse = z.object({\n      answer: z.string().describe(\"Detailed answer\"),\n      sources: z.array(z.string()).describe(\"Sources used\"),\n    });\n\n    const conciseResponse = z.object({\n      answer: z.string().describe(\"Brief answer\"),\n    });\n\n   ", "metadata": {"source": "context-engineering.mdx"}}
{"text": "    store=InMemoryStore()\n    )\n    ```\n    :::\n\n    :::js\n\n    ```typescript\n    import * as z from \"zod\";\n    import { createMiddleware } from \"langchain\";\n\n    const contextSchema = z.object({\n      userId: z.string(),\n    });\n\n    const verboseResponse = z.object({\n      answer: z.string().describe(\"Detailed answer\"),\n      sources: z.array(z.string()).describe(\"Sources used\"),\n    });\n\n    const conciseResponse = z.object({\n      answer: z.string().describe(\"Brief answer\"),\n    });\n\n    const storeBasedOutput = createMiddleware({\n      name: \"StoreBasedOutput\",\n      wrapModelCall: async (request, handler) => {\n        const userId = request.runtime.context.userId;  // [!code highlight]\n\n        // Read from Store: get user's preferred response style\n        const store = request.runtime.store;  // [!code highlight]\n        const userPrefs = await store.get([\"preferences\"], userId);  // [!code highlight]\n\n        if (userPrefs) {\n          const style = userPrefs.value?.responseStyle || \"concise\";\n          if (style === \"verbose\") {\n            request.responseFormat = verboseResponse;  // [!code highlight]\n          } else {\n            request.responseFormat = conciseResponse;  // [!code highlight]\n          }\n        }\n\n        return handler(request);\n      },\n    });\n    ```\n    :::\n  </Tab>\n\n  <Tab title=\"Runtime Context\">\n    Configure output format based on Runtime Context like user role or environment:\n\n    :::python\n\n    ```python\n    from dataclasses import dataclass\n    from langchain.agents import create_agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from pydantic import BaseModel, Field\n    from typing import Callable\n\n    @dataclass\n    class Context:\n        user_role: str\n        environment: str\n\n    class AdminResponse(BaseModel):\n        \"\"\"Response with technical details for admins.\"\"\"\n        answer: str = Field(description=\"Answer\")\n        debug_info: dict = Field(description=\"Debug information\")\n        system_status: str = Field(description=\"System status\")\n\n    class UserResponse(BaseModel):\n        \"\"\"Simple response for regular users.\"", "metadata": {"source": "context-engineering.mdx"}}
{"text": "agent\n    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n    from pydantic import BaseModel, Field\n    from typing import Callable\n\n    @dataclass\n    class Context:\n        user_role: str\n        environment: str\n\n    class AdminResponse(BaseModel):\n        \"\"\"Response with technical details for admins.\"\"\"\n        answer: str = Field(description=\"Answer\")\n        debug_info: dict = Field(description=\"Debug information\")\n        system_status: str = Field(description=\"System status\")\n\n    class UserResponse(BaseModel):\n        \"\"\"Simple response for regular users.\"\"\"\n        answer: str = Field(description=\"Answer\")\n\n    @wrap_model_call\n    def context_based_output(\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse]\n    ) -> ModelResponse:\n        \"\"\"Select output format based on Runtime Context.\"\"\"\n        # Read from Runtime Context: user role and environment\n        user_role = request.runtime.context.user_role\n        environment = request.runtime.context.environment\n\n        if user_role == \"admin\" and environment == \"production\":\n            # Admins in production get detailed output\n            request = request.override(response_format=AdminResponse)\n        else:\n            # Regular users get simple output\n            request = request.override(response_format=UserResponse)\n\n        return handler(request)\n\n    agent = create_agent(\n        model=\"gpt-4.1\",\n        tools=[...],\n        middleware=[context_based_output],\n        context_schema=Context\n    )\n    ```\n    :::\n\n    :::js\n\n    ```typescript\n    import * as z from \"zod\";\n    import { createMiddleware } from \"langchain\";\n\n    const contextSchema = z.object({\n      userRole: z.string(),\n      environment: z.string(),\n    });\n\n    const adminResponse = z.object({\n      answer: z.string().describe(\"Answer\"),\n      debugInfo: z.record(z.any()).describe(\"Debug information\"),\n      systemStatus: z.string().describe(\"System status\"),\n    });\n\n    const userResponse = z.object({\n      answer: z.string().describe(\"Answer\"),\n    });\n\n    const contextBasedOutput = createMiddleware({\n      name: \"ContextBasedOutput", "metadata": {"source": "context-engineering.mdx"}}
{"text": " z from \"zod\";\n    import { createMiddleware } from \"langchain\";\n\n    const contextSchema = z.object({\n      userRole: z.string(),\n      environment: z.string(),\n    });\n\n    const adminResponse = z.object({\n      answer: z.string().describe(\"Answer\"),\n      debugInfo: z.record(z.any()).describe(\"Debug information\"),\n      systemStatus: z.string().describe(\"System status\"),\n    });\n\n    const userResponse = z.object({\n      answer: z.string().describe(\"Answer\"),\n    });\n\n    const contextBasedOutput = createMiddleware({\n      name: \"ContextBasedOutput\",\n      wrapModelCall: (request, handler) => {\n        // Read from Runtime Context: user role and environment\n        const userRole = request.runtime.context.userRole;  // [!code highlight]\n        const environment = request.runtime.context.environment;  // [!code highlight]\n\n        let responseFormat;\n        if (userRole === \"admin\" && environment === \"production\") {\n          responseFormat = adminResponse;  // [!code highlight]\n        } else {\n          responseFormat = userResponse;  // [!code highlight]\n        }\n\n        return handler({ ...request, responseFormat });\n      },\n    });\n    ```\n    :::\n  </Tab>\n</Tabs>\n\n## Tool context\n\nTools are special in that they both read and write context.\n\nIn the most basic case, when a tool executes, it receives the LLM's request parameters and returns a tool message back. The tool does its work and produces a result.\n\nTools can also fetch important information for the model that allows it to perform and complete tasks.\n\n### Reads\n\nMost real-world tools need more than just the LLM's parameters. They need user IDs for database queries, API keys for external services, or current session state to make decisions. Tools read from state, store, and runtime context to access this information.\n\n<Tabs>\n  <Tab title=\"State\">\n    Read from State to check current session information:\n\n    :::python\n\n    ```python\n    from langchain.tools import tool, ToolRuntime\n    from langchain.agents import create_agent\n\n    @tool\n    def check_authentication(\n        runtime: ToolRuntime\n    ) -> str:\n        \"\"\"Check if user is authenticated.\"\"\"\n        # Read from State: check current auth status\n        current_state = runtime.state\n        is_authenticated = current_state.get(\"authenticated\", False)\n\n        if is_authenticated:\n            return \"User is authenticated\"\n        else:\n   ", "metadata": {"source": "context-engineering.mdx"}}
{"text": " check current session information:\n\n    :::python\n\n    ```python\n    from langchain.tools import tool, ToolRuntime\n    from langchain.agents import create_agent\n\n    @tool\n    def check_authentication(\n        runtime: ToolRuntime\n    ) -> str:\n        \"\"\"Check if user is authenticated.\"\"\"\n        # Read from State: check current auth status\n        current_state = runtime.state\n        is_authenticated = current_state.get(\"authenticated\", False)\n\n        if is_authenticated:\n            return \"User is authenticated\"\n        else:\n            return \"User is not authenticated\"\n\n    agent = create_agent(\n        model=\"gpt-4.1\",\n        tools=[check_authentication]\n    )\n    ```\n    :::\n\n    :::js\n\n    ```typescript\n    import * as z from \"zod\";\n    import { createAgent, tool, type ToolRuntime } from \"langchain\";\n\n    const checkAuthentication = tool(\n      async (_, runtime: ToolRuntime) => {\n        // Read from State: check current auth status\n        const currentState = runtime.state;\n        const isAuthenticated = currentState.authenticated || false;\n\n        if (isAuthenticated) {\n          return \"User is authenticated\";\n        } else {\n          return \"User is not authenticated\";\n        }\n      },\n      {\n        name: \"check_authentication\",\n        description: \"Check if user is authenticated\",\n        schema: z.object({}),\n      }\n    );\n    ```\n    :::\n  </Tab>\n\n  <Tab title=\"Store\">\n    Read from Store to access persisted user preferences:\n\n    :::python\n\n    ```python\n    from dataclasses import dataclass\n    from langchain.tools import tool, ToolRuntime\n    from langchain.agents import create_agent\n    from langgraph.store.memory import InMemoryStore\n\n    @dataclass\n    class Context:\n        user_id: str\n\n    @tool\n    def get_preference(\n        preference_key: str,\n        runtime: ToolRuntime[Context]\n    ) -> str:\n        \"\"\"Get user preference from Store.\"\"\"\n        user_id = runtime.context.user_id\n\n        # Read from Store: get existing preferences\n        store = runtime.store\n ", "metadata": {"source": "context-engineering.mdx"}}
{"text": "    from dataclasses import dataclass\n    from langchain.tools import tool, ToolRuntime\n    from langchain.agents import create_agent\n    from langgraph.store.memory import InMemoryStore\n\n    @dataclass\n    class Context:\n        user_id: str\n\n    @tool\n    def get_preference(\n        preference_key: str,\n        runtime: ToolRuntime[Context]\n    ) -> str:\n        \"\"\"Get user preference from Store.\"\"\"\n        user_id = runtime.context.user_id\n\n        # Read from Store: get existing preferences\n        store = runtime.store\n        existing_prefs = store.get((\"preferences\",), user_id)\n\n        if existing_prefs:\n            value = existing_prefs.value.get(preference_key)\n            return f\"{preference_key}: {value}\" if value else f\"No preference set for {preference_key}\"\n        else:\n            return \"No preferences found\"\n\n    agent = create_agent(\n        model=\"gpt-4.1\",\n        tools=[get_preference],\n        context_schema=Context,\n        store=InMemoryStore()\n    )\n    ```\n    :::\n\n    :::js\n\n    ```typescript\n    import * as z from \"zod\";\n    import { createAgent, tool, type ToolRuntime } from \"langchain\";\n\n    const contextSchema = z.object({\n      userId: z.string(),\n    });\n\n    const getPreference = tool(\n      async ({ preferenceKey }, runtime: ToolRuntime) => {\n        const userId = runtime.context.userId;\n\n        // Read from Store: get existing preferences\n        const store = runtime.store;\n        const existingPrefs = await store.get([\"preferences\"], userId);\n\n        if (existingPrefs) {\n          const value = existingPrefs.value?.[preferenceKey];\n          return value ? `${preferenceKey}: ${value}` : `No preference set for ${preferenceKey}`;\n        } else {\n          return \"No preferences found\";\n        }\n      },\n      {\n        name: \"get_preference\",\n        description: \"Get user preference from Store\",\n        schema: z.object({\n          preferenceKey: z.string(),\n   ", "metadata": {"source": "context-engineering.mdx"}}
{"text": "preferences\"], userId);\n\n        if (existingPrefs) {\n          const value = existingPrefs.value?.[preferenceKey];\n          return value ? `${preferenceKey}: ${value}` : `No preference set for ${preferenceKey}`;\n        } else {\n          return \"No preferences found\";\n        }\n      },\n      {\n        name: \"get_preference\",\n        description: \"Get user preference from Store\",\n        schema: z.object({\n          preferenceKey: z.string(),\n        }),\n      }\n    );\n    ```\n    :::\n  </Tab>\n\n  <Tab title=\"Runtime Context\">\n    Read from Runtime Context for configuration like API keys and user IDs:\n\n    :::python\n\n    ```python\n    from dataclasses import dataclass\n    from langchain.tools import tool, ToolRuntime\n    from langchain.agents import create_agent\n\n    @dataclass\n    class Context:\n        user_id: str\n        api_key: str\n        db_connection: str\n\n    @tool\n    def fetch_user_data(\n        query: str,\n        runtime: ToolRuntime[Context]\n    ) -> str:\n        \"\"\"Fetch data using Runtime Context configuration.\"\"\"\n        # Read from Runtime Context: get API key and DB connection\n        user_id = runtime.context.user_id\n        api_key = runtime.context.api_key\n        db_connection = runtime.context.db_connection\n\n        # Use configuration to fetch data\n        results = perform_database_query(db_connection, query, api_key)\n\n        return f\"Found {len(results)} results for user {user_id}\"\n\n    agent = create_agent(\n        model=\"gpt-4.1\",\n        tools=[fetch_user_data],\n        context_schema=Context\n    )\n\n    # Invoke with runtime context\n    result = agent.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"Get my data\"}]},\n        context=Context(\n            user_id=\"user_123\",\n            api_key=\"sk-...\",\n            db_connection=\"postgresql://...\"\n        )\n    )\n    ```\n    :::\n\n    :::js\n\n ", "metadata": {"source": "context-engineering.mdx"}}
{"text": "gpt-4.1\",\n        tools=[fetch_user_data],\n        context_schema=Context\n    )\n\n    # Invoke with runtime context\n    result = agent.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"Get my data\"}]},\n        context=Context(\n            user_id=\"user_123\",\n            api_key=\"sk-...\",\n            db_connection=\"postgresql://...\"\n        )\n    )\n    ```\n    :::\n\n    :::js\n\n    ```typescript\n    import * as z from \"zod\";\n    import { tool } from \"@langchain/core/tools\";\n    import { createAgent } from \"langchain\";\n\n    const contextSchema = z.object({\n      userId: z.string(),\n      apiKey: z.string(),\n      dbConnection: z.string(),\n    });\n\n    const fetchUserData = tool(\n      async ({ query }, runtime: ToolRuntime<any, typeof contextSchema>) => {\n        // Read from Runtime Context: get API key and DB connection\n        const { userId, apiKey, dbConnection } = runtime.context;\n\n        // Use configuration to fetch data\n        const results = await performDatabaseQuery(dbConnection, query, apiKey);\n\n        return `Found ${results.length} results for user ${userId}`;\n      },\n      {\n        name: \"fetch_user_data\",\n        description: \"Fetch data using Runtime Context configuration\",\n        schema: z.object({\n          query: z.string(),\n        }),\n      }\n    );\n\n    const agent = createAgent({\n      model: \"gpt-4.1\",\n      tools: [fetchUserData],\n      contextSchema,\n    });\n    ```\n    :::\n  </Tab>\n</Tabs>\n\n### Writes\n\nTool results can be used to help an agent complete a given task. Tools can both return results directly to the model\nand update the memory of the agent to make important context available to future steps.\n\n<Tabs>\n  <Tab title=\"State\">\n    Write to State to track session-specific information using Command:\n\n    :::python\n\n    ```python\n    from langchain.tools import tool, ToolRuntime\n    from langchain.agents import create_agent\n    from langgraph.types import Command\n\n    @tool\n    def authenticate_user(\n        password: str,\n        runtime: ToolRuntime", "metadata": {"source": "context-engineering.mdx"}}
{"text": " contextSchema,\n    });\n    ```\n    :::\n  </Tab>\n</Tabs>\n\n### Writes\n\nTool results can be used to help an agent complete a given task. Tools can both return results directly to the model\nand update the memory of the agent to make important context available to future steps.\n\n<Tabs>\n  <Tab title=\"State\">\n    Write to State to track session-specific information using Command:\n\n    :::python\n\n    ```python\n    from langchain.tools import tool, ToolRuntime\n    from langchain.agents import create_agent\n    from langgraph.types import Command\n\n    @tool\n    def authenticate_user(\n        password: str,\n        runtime: ToolRuntime\n    ) -> Command:\n        \"\"\"Authenticate user and update State.\"\"\"\n        # Perform authentication (simplified)\n        if password == \"correct\":\n            # Write to State: mark as authenticated using Command\n            return Command(\n                update={\"authenticated\": True},\n            )\n        else:\n            return Command(update={\"authenticated\": False})\n\n    agent = create_agent(\n        model=\"gpt-4.1\",\n        tools=[authenticate_user]\n    )\n    ```\n    :::\n\n    :::js\n\n    ```typescript\n    import * as z from \"zod\";\n    import { tool } from \"@langchain/core/tools\";\n    import { createAgent } from \"langchain\";\n    import { Command } from \"@langchain/langgraph\";\n\n    const authenticateUser = tool(\n      async ({ password }) => {\n        // Perform authentication\n        if (password === \"correct\") {\n          // Write to State: mark as authenticated using Command\n          return new Command({\n            update: { authenticated: true },\n          });\n        } else {\n          return new Command({ update: { authenticated: false } });\n        }\n      },\n      {\n        name: \"authenticate_user\",\n        description: \"Authenticate user and update State\",\n        schema: z.object({\n          password: z.string(),\n        }),\n      }\n    );\n    ```\n    :::\n  </Tab>\n\n  <Tab title=\"Store\">\n    Write to Store to persist data across sessions:\n\n    :::python\n\n ", "metadata": {"source": "context-engineering.mdx"}}
{"text": "\n          });\n        } else {\n          return new Command({ update: { authenticated: false } });\n        }\n      },\n      {\n        name: \"authenticate_user\",\n        description: \"Authenticate user and update State\",\n        schema: z.object({\n          password: z.string(),\n        }),\n      }\n    );\n    ```\n    :::\n  </Tab>\n\n  <Tab title=\"Store\">\n    Write to Store to persist data across sessions:\n\n    :::python\n\n    ```python\n    from dataclasses import dataclass\n    from langchain.tools import tool, ToolRuntime\n    from langchain.agents import create_agent\n    from langgraph.store.memory import InMemoryStore\n\n    @dataclass\n    class Context:\n        user_id: str\n\n    @tool\n    def save_preference(\n        preference_key: str,\n        preference_value: str,\n        runtime: ToolRuntime[Context]\n    ) -> str:\n        \"\"\"Save user preference to Store.\"\"\"\n        user_id = runtime.context.user_id\n\n        # Read existing preferences\n        store = runtime.store\n        existing_prefs = store.get((\"preferences\",), user_id)\n\n        # Merge with new preference\n        prefs = existing_prefs.value if existing_prefs else {}\n        prefs[preference_key] = preference_value\n\n        # Write to Store: save updated preferences\n        store.put((\"preferences\",), user_id, prefs)\n\n        return f\"Saved preference: {preference_key} = {preference_value}\"\n\n    agent = create_agent(\n        model=\"gpt-4.1\",\n        tools=[save_preference],\n        context_schema=Context,\n        store=InMemoryStore()\n    )\n    ```\n    :::\n\n    :::js\n\n    ```typescript\n    import * as z from \"zod\";\n    import { createAgent, tool, type ToolRuntime } from \"langchain\";\n\n    const savePreference = tool(\n      async ({ preferenceKey, preferenceValue }, runtime: ToolRuntime<any, typeof contextSchema>) => {\n        const userId = runtime.context.userId;\n\n        // Read existing preferences\n        const store = runtime.store;\n        const existing", "metadata": {"source": "context-engineering.mdx"}}
{"text": "   tools=[save_preference],\n        context_schema=Context,\n        store=InMemoryStore()\n    )\n    ```\n    :::\n\n    :::js\n\n    ```typescript\n    import * as z from \"zod\";\n    import { createAgent, tool, type ToolRuntime } from \"langchain\";\n\n    const savePreference = tool(\n      async ({ preferenceKey, preferenceValue }, runtime: ToolRuntime<any, typeof contextSchema>) => {\n        const userId = runtime.context.userId;\n\n        // Read existing preferences\n        const store = runtime.store;\n        const existingPrefs = await store.get([\"preferences\"], userId);\n\n        // Merge with new preference\n        const prefs = existingPrefs?.value || {};\n        prefs[preferenceKey] = preferenceValue;\n\n        // Write to Store: save updated preferences\n        await store.put([\"preferences\"], userId, prefs);\n\n        return `Saved preference: ${preferenceKey} = ${preferenceValue}`;\n      },\n      {\n        name: \"save_preference\",\n        description: \"Save user preference to Store\",\n        schema: z.object({\n          preferenceKey: z.string(),\n          preferenceValue: z.string(),\n        }),\n      }\n    );\n    ```\n    :::\n  </Tab>\n</Tabs>\n\nSee [Tools](/oss/langchain/tools) for comprehensive examples of accessing state, store, and runtime context in tools.\n\n## Life-cycle context\n\nControl what happens **between** the core agent steps - intercepting data flow to implement cross-cutting concerns like summarization, guardrails, and logging.\n\nAs you've seen in [Model Context](#model-context) and [Tool Context](#tool-context), [middleware](/oss/langchain/middleware) is the mechanism that makes context engineering practical. Middleware allows you to hook into any step in the agent lifecycle and either:\n\n1. **Update context** - Modify state and store to persist changes, update conversation history, or save insights\n2. **Jump in the lifecycle** - Move to different steps in the agent cycle based on context (e.g., skip tool execution if a condition is met, repeat model call with modified context)\n\n<div style={{ display: \"flex\", justifyContent: \"center\" }}>\n  <img\n    src=\"/oss/images/middleware_final.png\"\n    alt=\"Middleware hooks in the agent loop\"\n    className=\"rounded-lg\"\n  />\n</div>\n\n### Example: Summarization\n\nOne of the most common life-cycle patterns is automatically condensing conversation history when it gets too long. Unlike the transient message trimming shown in [Model Context](#messages), summar", "metadata": {"source": "context-engineering.mdx"}}
{"text": " any step in the agent lifecycle and either:\n\n1. **Update context** - Modify state and store to persist changes, update conversation history, or save insights\n2. **Jump in the lifecycle** - Move to different steps in the agent cycle based on context (e.g., skip tool execution if a condition is met, repeat model call with modified context)\n\n<div style={{ display: \"flex\", justifyContent: \"center\" }}>\n  <img\n    src=\"/oss/images/middleware_final.png\"\n    alt=\"Middleware hooks in the agent loop\"\n    className=\"rounded-lg\"\n  />\n</div>\n\n### Example: Summarization\n\nOne of the most common life-cycle patterns is automatically condensing conversation history when it gets too long. Unlike the transient message trimming shown in [Model Context](#messages), summarization **persistently updates state** - permanently replacing old messages with a summary that's saved for all future turns.\n\nLangChain offers built-in middleware for this:\n\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import SummarizationMiddleware\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[...],\n    middleware=[\n        SummarizationMiddleware(\n            model=\"gpt-4.1-mini\",\n            trigger={\"tokens\": 4000},\n            keep={\"messages\": 20},\n        ),\n    ],\n)\n```\n:::\n\n:::js\n```typescript\nimport { createAgent, summarizationMiddleware } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [...],\n  middleware: [\n    summarizationMiddleware({\n      model: \"gpt-4.1-mini\",\n      trigger: { tokens: 4000 },\n      keep: { messages: 20 },\n    }),\n  ],\n});\n```\n:::\n\nWhen the conversation exceeds the token limit, `SummarizationMiddleware` automatically:\n1. Summarizes older messages using a separate LLM call\n2. Replaces them with a summary message in State (permanently)\n3. Keeps recent messages intact for context\n\nThe summarized conversation history is permanently updated - future turns will see the summary instead of the original messages.\n\n<Note>\nFor a complete list of built-in middleware, available hooks, and how to create custom middleware, see the [Middleware documentation](/oss/langchain/middleware).\n</Note>\n\n## Best practices\n\n1. **Start simple** - Begin with static prompts and tools, add dynamics only when needed\n2. **Test incrementally** - Add one context engineering feature at a time\n3. **Monitor performance** - Track model calls, token usage, and latency\n4. **Use built-in middleware** - Leverage [`SummarizationMiddleware`](/oss/langchain/middleware#summarization), [`LLMToolSelectorMiddleware`](/oss/langchain/middleware#llm-tool-selector), etc.\n5. **Document your", "metadata": {"source": "context-engineering.mdx"}}
{"text": "The summarized conversation history is permanently updated - future turns will see the summary instead of the original messages.\n\n<Note>\nFor a complete list of built-in middleware, available hooks, and how to create custom middleware, see the [Middleware documentation](/oss/langchain/middleware).\n</Note>\n\n## Best practices\n\n1. **Start simple** - Begin with static prompts and tools, add dynamics only when needed\n2. **Test incrementally** - Add one context engineering feature at a time\n3. **Monitor performance** - Track model calls, token usage, and latency\n4. **Use built-in middleware** - Leverage [`SummarizationMiddleware`](/oss/langchain/middleware#summarization), [`LLMToolSelectorMiddleware`](/oss/langchain/middleware#llm-tool-selector), etc.\n5. **Document your context strategy** - Make it clear what context is being passed and why\n6. **Understand transient vs persistent**: Model context changes are transient (per-call), while life-cycle context changes persist to state\n\n## Related resources\n\n- [Context conceptual overview](/oss/concepts/context) - Understand context types and when to use them\n- [Middleware](/oss/langchain/middleware) - Complete middleware guide\n- [Tools](/oss/langchain/tools) - Tool creation and context access\n- [Memory](/oss/concepts/memory) - Short-term and long-term memory patterns\n- [Agents](/oss/langchain/agents) - Core agent concepts\n", "metadata": {"source": "context-engineering.mdx"}}
{"text": "---\ntitle: Test\n---\n\nAgentic applications let an LLM decide its own next steps to solve a problem. That flexibility is powerful, but the model's black-box nature makes it hard to predict how a tweak in one part of your agent will affect the rest. To build production-ready agents, thorough testing is essential.\n\nThere are a few approaches to testing your agents:\n:::python\n- [Unit tests](#unit-testing) exercise small, deterministic pieces of your agent in isolation using in-memory fakes so you can assert exact behavior quickly and deterministically.\n:::\n:::js\n- Unit tests exercise small, deterministic pieces of your agent in isolation using in-memory fakes so you can assert exact behavior quickly and deterministically.\n:::\n- [Integration tests](#integration-testing) test the agent using real network calls to confirm that components work together, credentials and schemas line up, and latency is acceptable.\n\nAgentic applications tend to lean more on integration because they chain multiple components together and must deal with flakiness due to the nondeterministic nature of LLMs.\n\n:::python\n\n## Unit testing\n\n### Mocking chat model\n\nFor logic not requiring API calls, you can use an in-memory stub for mocking responses.\n\nLangChain provides [`GenericFakeChatModel`](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.fake_chat_models.GenericFakeChatModel.html) for mocking text responses. It accepts an iterator of responses (AIMessages or strings) and returns one per invocation. It supports both regular and streaming usage.\n\n```python\nfrom langchain_core.language_models.fake_chat_models import GenericFakeChatModel\n\nmodel = GenericFakeChatModel(messages=iter([\n    AIMessage(content=\"\", tool_calls=[ToolCall(name=\"foo\", args={\"bar\": \"baz\"}, id=\"call_1\")]),\n    \"bar\"\n]))\n\nmodel.invoke(\"hello\")\n# AIMessage(content='', ..., tool_calls=[{'name': 'foo', 'args': {'bar': 'baz'}, 'id': 'call_1', 'type': 'tool_call'}])\n```\n\nIf we invoke the model again, it will return the next item in the iterator:\n\n```python\nmodel.invoke(\"hello, again!\")\n# AIMessage(content='bar', ...)\n```\n\n### InMemorySaver checkpointer\n\nTo enable persistence during testing, you can use the @[`InMemorySaver`] checkpointer. This allows you to simulate multiple turns to test state-dependent behavior:\n\n```python\nfrom langgraph.checkpoint.memory import InMemorySaver\n\nagent = create_agent(\n    model,\n    tools=[],\n    checkpointer=InMemorySaver()\n)\n\n# First invocation\nagent.invoke(\n    {\"messages\": [HumanMessage(content=\"I live in Sydney, Australia\")]},\n    config={\"configurable\": {\"thread_id\": \"session-1\"}}\n)\n\n# Second invocation: the first message is persisted (Sydney location), so the model returns GMT+10 time\nagent.invoke(\n    {\"messages\": [HumanMessage(content=\"What's my local time?\")]},\n    config={\"configurable\": {\"thread_id\": \"session-1\"", "metadata": {"source": "test.mdx"}}
{"text": "] checkpointer. This allows you to simulate multiple turns to test state-dependent behavior:\n\n```python\nfrom langgraph.checkpoint.memory import InMemorySaver\n\nagent = create_agent(\n    model,\n    tools=[],\n    checkpointer=InMemorySaver()\n)\n\n# First invocation\nagent.invoke(\n    {\"messages\": [HumanMessage(content=\"I live in Sydney, Australia\")]},\n    config={\"configurable\": {\"thread_id\": \"session-1\"}}\n)\n\n# Second invocation: the first message is persisted (Sydney location), so the model returns GMT+10 time\nagent.invoke(\n    {\"messages\": [HumanMessage(content=\"What's my local time?\")]},\n    config={\"configurable\": {\"thread_id\": \"session-1\"}}\n)\n```\n:::\n\n## Integration testing\n\nMany agent behaviors only emerge when using a real LLM, such as which tool the agent decides to call, how it formats responses, or whether a prompt modification affects the entire execution trajectory. LangChain's [`agentevals`](https://github.com/langchain-ai/agentevals) package provides evaluators specifically designed for testing agent trajectories with live models.\n\nAgentEvals lets you easily evaluate the trajectory of your agent (the exact sequence of messages, including tool calls) by performing a **trajectory match** or by using an **LLM judge**:\n\n<Card title=\"Trajectory match\" icon=\"equals\" arrow=\"true\" href=\"#trajectory-match-evaluator\">\nHard-code a reference trajectory for a given input and validate the run via a step-by-step comparison.\n\nIdeal for testing well-defined workflows where you know the expected behavior. Use when you have specific expectations about which tools should be called and in what order. This approach is deterministic, fast, and cost-effective since it doesn't require additional LLM calls.\n</Card>\n\n<Card title=\"LLM-as-judge\" icon=\"gavel\" arrow=\"true\" href=\"#llm-as-judge-evaluator\">\nUse a LLM to qualitatively validate your agent's execution trajectory. The \"judge\" LLM reviews the agent's decisions against a prompt rubric (which can include a reference trajectory).\n\nMore flexible and can assess nuanced aspects like efficiency and appropriateness, but requires an LLM call and is less deterministic. Use when you want to evaluate the overall quality and reasonableness of the agent's trajectory without strict tool call or ordering requirements.\n</Card>\n\n### Installing AgentEvals\n\n:::python\n```bash\npip install agentevals\n```\n:::\n\n:::js\n```bash\nnpm install agentevals @langchain/core\n```\n:::\n\nOr, clone the [AgentEvals repository](https://github.com/langchain-ai/agentevals) directly.\n\n### Trajectory match evaluator\n\n:::python\nAgentEvals offers the `create_trajectory_match_evaluator` function to match your agent's trajectory against a reference trajectory. There are four modes to choose from:\n:::\n:::js\nAgentEvals offers the `createTrajectoryMatchEvaluator` function to match your agent's trajectory against a reference trajectory. There are four modes to choose from:\n:::\n\n| Mode | Description | Use Case |\n|------|-------------|----------|\n| `st", "metadata": {"source": "test.mdx"}}
{"text": "\n:::python\n```bash\npip install agentevals\n```\n:::\n\n:::js\n```bash\nnpm install agentevals @langchain/core\n```\n:::\n\nOr, clone the [AgentEvals repository](https://github.com/langchain-ai/agentevals) directly.\n\n### Trajectory match evaluator\n\n:::python\nAgentEvals offers the `create_trajectory_match_evaluator` function to match your agent's trajectory against a reference trajectory. There are four modes to choose from:\n:::\n:::js\nAgentEvals offers the `createTrajectoryMatchEvaluator` function to match your agent's trajectory against a reference trajectory. There are four modes to choose from:\n:::\n\n| Mode | Description | Use Case |\n|------|-------------|----------|\n| `strict` | Exact match of messages and tool calls in the same order | Testing specific sequences (e.g., policy lookup before authorization) |\n| `unordered` | Same tool calls allowed in any order | Verifying information retrieval when order doesn't matter |\n| `subset` | Agent calls only tools from reference (no extras) | Ensuring agent doesn't exceed expected scope |\n| `superset` | Agent calls at least the reference tools (extras allowed) | Verifying minimum required actions are taken |\n\n<Accordion title=\"Strict match\">\n\nThe `strict` mode ensures trajectories contain identical messages in the same order with the same tool calls, though it allows for differences in message content. This is useful when you need to enforce a specific sequence of operations, such as requiring a policy lookup before authorizing an action.\n\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool\nfrom langchain.messages import HumanMessage, AIMessage, ToolMessage\nfrom agentevals.trajectory.match import create_trajectory_match_evaluator\n\n\n@tool\ndef get_weather(city: str):\n    \"\"\"Get weather information for a city.\"\"\"\n    return f\"It's 75 degrees and sunny in {city}.\"\n\nagent = create_agent(\"gpt-4.1\", tools=[get_weather])\n\nevaluator = create_trajectory_match_evaluator(  # [!code highlight]\n    trajectory_match_mode=\"strict\",  # [!code highlight]\n)  # [!code highlight]\n\ndef test_weather_tool_called_strict():\n    result = agent.invoke({\n        \"messages\": [HumanMessage(content=\"What's the weather in San Francisco?\")]\n    })\n\n    reference_trajectory = [\n        HumanMessage(content=\"What's the weather in San Francisco?\"),\n        AIMessage(content=\"\", tool_calls=[\n            {\"id\": \"call_1\", \"name\": \"get_weather\", \"args\": {\"city\": \"San Francisco\"}}\n        ]),\n        ToolMessage(content=\"It's 75 degrees and sunny in San Francisco.\", tool_call_id=\"call_1\"),\n        AIMessage(content=\"The weather in San Francisco is 75 degrees and sunny.\"),\n    ]\n\n    evaluation = evaluator(", "metadata": {"source": "test.mdx"}}
{"text": " \"messages\": [HumanMessage(content=\"What's the weather in San Francisco?\")]\n    })\n\n    reference_trajectory = [\n        HumanMessage(content=\"What's the weather in San Francisco?\"),\n        AIMessage(content=\"\", tool_calls=[\n            {\"id\": \"call_1\", \"name\": \"get_weather\", \"args\": {\"city\": \"San Francisco\"}}\n        ]),\n        ToolMessage(content=\"It's 75 degrees and sunny in San Francisco.\", tool_call_id=\"call_1\"),\n        AIMessage(content=\"The weather in San Francisco is 75 degrees and sunny.\"),\n    ]\n\n    evaluation = evaluator(\n        outputs=result[\"messages\"],\n        reference_outputs=reference_trajectory\n    )\n    # {\n    #     'key': 'trajectory_strict_match',\n    #     'score': True,\n    #     'comment': None,\n    # }\n    assert evaluation[\"score\"] is True\n```\n:::\n\n:::js\n```ts\nimport { createAgent, tool, HumanMessage, AIMessage, ToolMessage } from \"langchain\"\nimport { createTrajectoryMatchEvaluator } from \"agentevals\";\nimport * as z from \"zod\";\n\nconst getWeather = tool(\n  async ({ city }: { city: string }) => {\n    return `It's 75 degrees and sunny in ${city}.`;\n  },\n  {\n    name: \"get_weather\",\n    description: \"Get weather information for a city.\",\n    schema: z.object({\n      city: z.string(),\n    }),\n  }\n);\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [getWeather]\n});\n\nconst evaluator = createTrajectoryMatchEvaluator({  // [!code highlight]\n  trajectoryMatchMode: \"strict\",  // [!code highlight]\n});  // [!code highlight]\n\nasync function testWeatherToolCalledStrict() {\n  const result = await agent.invoke({\n    messages: [new HumanMessage(\"What's the weather in San Francisco?\")]\n  });\n\n  const referenceTrajectory = [\n    new HumanMessage(\"What's the weather in San Francisco?\"),\n    new AIMessage({\n      content: \"\",\n      tool_calls: [\n        { id: \"call_1\", name: \"get_weather\", args: { city: \"San Francisco\" } }\n      ]\n    }),\n    new ToolMessage({\n      content: \"It's 75 degrees and sunny in San Francisco.\",\n      tool_call_id: \"call_1\"\n    }),\n    new AIMessage(\"The weather in San Francisco is 75 degrees and sunny.\"),\n  ];\n\n  const evaluation = await evaluator({\n  ", "metadata": {"source": "test.mdx"}}
{"text": " weather in San Francisco?\")]\n  });\n\n  const referenceTrajectory = [\n    new HumanMessage(\"What's the weather in San Francisco?\"),\n    new AIMessage({\n      content: \"\",\n      tool_calls: [\n        { id: \"call_1\", name: \"get_weather\", args: { city: \"San Francisco\" } }\n      ]\n    }),\n    new ToolMessage({\n      content: \"It's 75 degrees and sunny in San Francisco.\",\n      tool_call_id: \"call_1\"\n    }),\n    new AIMessage(\"The weather in San Francisco is 75 degrees and sunny.\"),\n  ];\n\n  const evaluation = await evaluator({\n    outputs: result.messages,\n    referenceOutputs: referenceTrajectory\n  });\n  // {\n  //     'key': 'trajectory_strict_match',\n  //     'score': true,\n  //     'comment': null,\n  // }\n  expect(evaluation.score).toBe(true);\n}\n```\n:::\n\n</Accordion>\n\n<Accordion title=\"Unordered match\">\n\nThe `unordered` mode allows the same tool calls in any order, which is helpful when you want to verify that specific information was retrieved but don't care about the sequence. For example, an agent might need to check both weather and events for a city, but the order doesn't matter.\n\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool\nfrom langchain.messages import HumanMessage, AIMessage, ToolMessage\nfrom agentevals.trajectory.match import create_trajectory_match_evaluator\n\n\n@tool\ndef get_weather(city: str):\n    \"\"\"Get weather information for a city.\"\"\"\n    return f\"It's 75 degrees and sunny in {city}.\"\n\n@tool\ndef get_events(city: str):\n    \"\"\"Get events happening in a city.\"\"\"\n    return f\"Concert at the park in {city} tonight.\"\n\nagent = create_agent(\"gpt-4.1\", tools=[get_weather, get_events])\n\nevaluator = create_trajectory_match_evaluator(  # [!code highlight]\n    trajectory_match_mode=\"unordered\",  # [!code highlight]\n)  # [!code highlight]\n\ndef test_multiple_tools_any_order():\n    result = agent.invoke({\n        \"messages\": [HumanMessage(content=\"What's happening in SF today?\")]\n    })\n\n    # Reference shows tools called in different order than actual execution\n    reference_trajectory = [\n        HumanMessage(content=\"What's happening in SF today?\"),\n        AIMessage(content=\"\", tool_calls=[\n            {\"id\": \"call_1\", \"name\": \"get_events\", \"args\": {\"city\": \"SF\"}},\n            {\"id\": \"call_2\", \"name\": \"get_weather", "metadata": {"source": "test.mdx"}}
{"text": " [!code highlight]\n)  # [!code highlight]\n\ndef test_multiple_tools_any_order():\n    result = agent.invoke({\n        \"messages\": [HumanMessage(content=\"What's happening in SF today?\")]\n    })\n\n    # Reference shows tools called in different order than actual execution\n    reference_trajectory = [\n        HumanMessage(content=\"What's happening in SF today?\"),\n        AIMessage(content=\"\", tool_calls=[\n            {\"id\": \"call_1\", \"name\": \"get_events\", \"args\": {\"city\": \"SF\"}},\n            {\"id\": \"call_2\", \"name\": \"get_weather\", \"args\": {\"city\": \"SF\"}},\n        ]),\n        ToolMessage(content=\"Concert at the park in SF tonight.\", tool_call_id=\"call_1\"),\n        ToolMessage(content=\"It's 75 degrees and sunny in SF.\", tool_call_id=\"call_2\"),\n        AIMessage(content=\"Today in SF: 75 degrees and sunny with a concert at the park tonight.\"),\n    ]\n\n    evaluation = evaluator(\n        outputs=result[\"messages\"],\n        reference_outputs=reference_trajectory,\n    )\n    # {\n    #     'key': 'trajectory_unordered_match',\n    #     'score': True,\n    # }\n    assert evaluation[\"score\"] is True\n```\n:::\n\n:::js\n```ts\nimport { createAgent, tool, HumanMessage, AIMessage, ToolMessage } from \"langchain\"\nimport { createTrajectoryMatchEvaluator } from \"agentevals\";\nimport * as z from \"zod\";\n\nconst getWeather = tool(\n  async ({ city }: { city: string }) => {\n    return `It's 75 degrees and sunny in ${city}.`;\n  },\n  {\n    name: \"get_weather\",\n    description: \"Get weather information for a city.\",\n    schema: z.object({ city: z.string() }),\n  }\n);\n\nconst getEvents = tool(\n  async ({ city }: { city: string }) => {\n    return `Concert at the park in ${city} tonight.`;\n  },\n  {\n    name: \"get_events\",\n    description: \"Get events happening in a city.\",\n    schema: z.object({ city: z.string() }),\n  }\n);\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [getWeather, getEvents]\n});\n\nconst evaluator = createTrajectoryMatchEvaluator({  // [!code highlight]\n  trajectoryMatchMode: \"unordered\",  // [!code highlight]\n});  // [!code highlight]\n\nasync function testMultipleToolsAnyOrder() {\n  const result = await agent.invoke({\n    messages: [new HumanMessage", "metadata": {"source": "test.mdx"}}
{"text": " async ({ city }: { city: string }) => {\n    return `Concert at the park in ${city} tonight.`;\n  },\n  {\n    name: \"get_events\",\n    description: \"Get events happening in a city.\",\n    schema: z.object({ city: z.string() }),\n  }\n);\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [getWeather, getEvents]\n});\n\nconst evaluator = createTrajectoryMatchEvaluator({  // [!code highlight]\n  trajectoryMatchMode: \"unordered\",  // [!code highlight]\n});  // [!code highlight]\n\nasync function testMultipleToolsAnyOrder() {\n  const result = await agent.invoke({\n    messages: [new HumanMessage(\"What's happening in SF today?\")]\n  });\n\n  // Reference shows tools called in different order than actual execution\n  const referenceTrajectory = [\n    new HumanMessage(\"What's happening in SF today?\"),\n    new AIMessage({\n      content: \"\",\n      tool_calls: [\n        { id: \"call_1\", name: \"get_events\", args: { city: \"SF\" } },\n        { id: \"call_2\", name: \"get_weather\", args: { city: \"SF\" } },\n      ]\n    }),\n    new ToolMessage({\n      content: \"Concert at the park in SF tonight.\",\n      tool_call_id: \"call_1\"\n    }),\n    new ToolMessage({\n      content: \"It's 75 degrees and sunny in SF.\",\n      tool_call_id: \"call_2\"\n    }),\n    new AIMessage(\"Today in SF: 75 degrees and sunny with a concert at the park tonight.\"),\n  ];\n\n  const evaluation = await evaluator({\n    outputs: result.messages,\n    referenceOutputs: referenceTrajectory,\n  });\n  // {\n  //     'key': 'trajectory_unordered_match',\n  //     'score': true,\n  // }\n  expect(evaluation.score).toBe(true);\n}\n```\n:::\n\n</Accordion>\n\n<Accordion title=\"Subset and superset match\">\n\nThe `superset` and `subset` modes match partial trajectories. The `superset` mode verifies that the agent called at least the tools in the reference trajectory, allowing additional tool calls. The `subset` mode ensures the agent did not call any tools beyond those in the reference.\n\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool\nfrom langchain.messages import HumanMessage, AIMessage, ToolMessage\nfrom agentevals.trajectory.match import create_trajectory_match_evaluator\n\n\n@tool\ndef get_weather(city: str):\n    \"\"\"Get weather information for a city.\"\"\"\n    return f\"It's 75 degrees and sunny in {city}.\"\n\n@tool\ndef get_detailed_forecast(", "metadata": {"source": "test.mdx"}}
{"text": "=\"Subset and superset match\">\n\nThe `superset` and `subset` modes match partial trajectories. The `superset` mode verifies that the agent called at least the tools in the reference trajectory, allowing additional tool calls. The `subset` mode ensures the agent did not call any tools beyond those in the reference.\n\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool\nfrom langchain.messages import HumanMessage, AIMessage, ToolMessage\nfrom agentevals.trajectory.match import create_trajectory_match_evaluator\n\n\n@tool\ndef get_weather(city: str):\n    \"\"\"Get weather information for a city.\"\"\"\n    return f\"It's 75 degrees and sunny in {city}.\"\n\n@tool\ndef get_detailed_forecast(city: str):\n    \"\"\"Get detailed weather forecast for a city.\"\"\"\n    return f\"Detailed forecast for {city}: sunny all week.\"\n\nagent = create_agent(\"gpt-4.1\", tools=[get_weather, get_detailed_forecast])\n\nevaluator = create_trajectory_match_evaluator(  # [!code highlight]\n    trajectory_match_mode=\"superset\",  # [!code highlight]\n)  # [!code highlight]\n\ndef test_agent_calls_required_tools_plus_extra():\n    result = agent.invoke({\n        \"messages\": [HumanMessage(content=\"What's the weather in Boston?\")]\n    })\n\n    # Reference only requires get_weather, but agent may call additional tools\n    reference_trajectory = [\n        HumanMessage(content=\"What's the weather in Boston?\"),\n        AIMessage(content=\"\", tool_calls=[\n            {\"id\": \"call_1\", \"name\": \"get_weather\", \"args\": {\"city\": \"Boston\"}},\n        ]),\n        ToolMessage(content=\"It's 75 degrees and sunny in Boston.\", tool_call_id=\"call_1\"),\n        AIMessage(content=\"The weather in Boston is 75 degrees and sunny.\"),\n    ]\n\n    evaluation = evaluator(\n        outputs=result[\"messages\"],\n        reference_outputs=reference_trajectory,\n    )\n    # {\n    #     'key': 'trajectory_superset_match',\n    #     'score': True,\n    #     'comment': None,\n    # }\n    assert evaluation[\"score\"] is True\n```\n:::\n\n:::js\n```ts\nimport { createAgent } from \"langchain\"\nimport { tool } from \"@langchain/core/tools\";\nimport { HumanMessage, AIMessage, ToolMessage } from \"@langchain/core/messages\";\nimport { createTrajectoryMatchEvaluator } from \"agentevals\";\nimport * as z from \"zod\";\n\nconst getWeather = tool(\n  async ({ city }: { city: string }) => {\n    return", "metadata": {"source": "test.mdx"}}
{"text": "ory,\n    )\n    # {\n    #     'key': 'trajectory_superset_match',\n    #     'score': True,\n    #     'comment': None,\n    # }\n    assert evaluation[\"score\"] is True\n```\n:::\n\n:::js\n```ts\nimport { createAgent } from \"langchain\"\nimport { tool } from \"@langchain/core/tools\";\nimport { HumanMessage, AIMessage, ToolMessage } from \"@langchain/core/messages\";\nimport { createTrajectoryMatchEvaluator } from \"agentevals\";\nimport * as z from \"zod\";\n\nconst getWeather = tool(\n  async ({ city }: { city: string }) => {\n    return `It's 75 degrees and sunny in ${city}.`;\n  },\n  {\n    name: \"get_weather\",\n    description: \"Get weather information for a city.\",\n    schema: z.object({ city: z.string() }),\n  }\n);\n\nconst getDetailedForecast = tool(\n  async ({ city }: { city: string }) => {\n    return `Detailed forecast for ${city}: sunny all week.`;\n  },\n  {\n    name: \"get_detailed_forecast\",\n    description: \"Get detailed weather forecast for a city.\",\n    schema: z.object({ city: z.string() }),\n  }\n);\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [getWeather, getDetailedForecast]\n});\n\nconst evaluator = createTrajectoryMatchEvaluator({  // [!code highlight]\n  trajectoryMatchMode: \"superset\",  // [!code highlight]\n});  // [!code highlight]\n\nasync function testAgentCallsRequiredToolsPlusExtra() {\n  const result = await agent.invoke({\n    messages: [new HumanMessage(\"What's the weather in Boston?\")]\n  });\n\n  // Reference only requires getWeather, but agent may call additional tools\n  const referenceTrajectory = [\n    new HumanMessage(\"What's the weather in Boston?\"),\n    new AIMessage({\n      content: \"\",\n      tool_calls: [\n        { id: \"call_1\", name: \"get_weather\", args: { city: \"Boston\" } },\n      ]\n    }),\n    new ToolMessage({\n      content: \"It's 75 degrees and sunny in Boston.\",\n      tool_call_id: \"call_1\"\n    }),\n    new AIMessage(\"The weather in Boston is 75 degrees and sunny.\"),\n  ];\n\n  const evaluation = await evaluator({\n    outputs: result.messages,\n    referenceOutputs: referenceTrajectory,\n  });\n  // {\n  //     'key': 'trajectory_superset_match',\n  //     'score': true,\n  //     'comment': null,\n  // }\n  expect(evaluation.score).toBe(true);\n}\n```\n", "metadata": {"source": "test.mdx"}}
{"text": " \"Boston\" } },\n      ]\n    }),\n    new ToolMessage({\n      content: \"It's 75 degrees and sunny in Boston.\",\n      tool_call_id: \"call_1\"\n    }),\n    new AIMessage(\"The weather in Boston is 75 degrees and sunny.\"),\n  ];\n\n  const evaluation = await evaluator({\n    outputs: result.messages,\n    referenceOutputs: referenceTrajectory,\n  });\n  // {\n  //     'key': 'trajectory_superset_match',\n  //     'score': true,\n  //     'comment': null,\n  // }\n  expect(evaluation.score).toBe(true);\n}\n```\n:::\n\n</Accordion>\n\n<Info>\n:::python\nYou can also set the `tool_args_match_mode` property and/or `tool_args_match_overrides` to customize how the evaluator considers equality between tool calls in the actual trajectory vs. the reference. By default, only tool calls with the same arguments to the same tool are considered equal. Visit the [repository](https://github.com/langchain-ai/agentevals?tab=readme-ov-file#tool-args-match-modes) for more details.\n:::\n:::js\nYou can also set the `toolArgsMatchMode` property and/or `toolArgsMatchOverrides` to customize how the evaluator considers equality between tool calls in the actual trajectory vs. the reference. By default, only tool calls with the same arguments to the same tool are considered equal. Visit the [repository](https://github.com/langchain-ai/agentevals?tab=readme-ov-file#tool-args-match-modes) for more details.\n:::\n</Info>\n\n### LLM-as-Judge evaluator\n\n:::python\nYou can also use an LLM to evaluate the agent's execution path with the `create_trajectory_llm_as_judge` function. Unlike the trajectory match evaluators, it doesn't require a reference trajectory, but one can be provided if available.\n:::\n:::js\nYou can also use an LLM to evaluate the agent's execution path with the `createTrajectoryLLMAsJudge` function. Unlike the trajectory match evaluators, it doesn't require a reference trajectory, but one can be provided if available.\n:::\n\n<Accordion title=\"Without reference trajectory\">\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool\nfrom langchain.messages import HumanMessage, AIMessage, ToolMessage\nfrom agentevals.trajectory.llm import create_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT\n\n\n@tool\ndef get_weather(city: str):\n    \"\"\"Get weather information for a city.\"\"\"\n    return f\"It's 75 degrees and sunny in {city}.\"\n\nagent = create_agent(\"gpt-4.1\", tools=[get_weather])\n\nevaluator = create_trajectory_llm_as_judge(  # [!code highlight]\n    model=\"openai:o3-mini\",  # [!code highlight]\n  ", "metadata": {"source": "test.mdx"}}
{"text": " trajectory\">\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool\nfrom langchain.messages import HumanMessage, AIMessage, ToolMessage\nfrom agentevals.trajectory.llm import create_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT\n\n\n@tool\ndef get_weather(city: str):\n    \"\"\"Get weather information for a city.\"\"\"\n    return f\"It's 75 degrees and sunny in {city}.\"\n\nagent = create_agent(\"gpt-4.1\", tools=[get_weather])\n\nevaluator = create_trajectory_llm_as_judge(  # [!code highlight]\n    model=\"openai:o3-mini\",  # [!code highlight]\n    prompt=TRAJECTORY_ACCURACY_PROMPT,  # [!code highlight]\n)  # [!code highlight]\n\ndef test_trajectory_quality():\n    result = agent.invoke({\n        \"messages\": [HumanMessage(content=\"What's the weather in Seattle?\")]\n    })\n\n    evaluation = evaluator(\n        outputs=result[\"messages\"],\n    )\n    # {\n    #     'key': 'trajectory_accuracy',\n    #     'score': True,\n    #     'comment': 'The provided agent trajectory is reasonable...'\n    # }\n    assert evaluation[\"score\"] is True\n```\n:::\n:::js\n```ts\nimport { createAgent } from \"langchain\"\nimport { tool } from \"@langchain/core/tools\";\nimport { HumanMessage, AIMessage, ToolMessage } from \"@langchain/core/messages\";\nimport { createTrajectoryLLMAsJudge, TRAJECTORY_ACCURACY_PROMPT } from \"agentevals\";\nimport * as z from \"zod\";\n\nconst getWeather = tool(\n  async ({ city }: { city: string }) => {\n    return `It's 75 degrees and sunny in ${city}.`;\n  },\n  {\n    name: \"get_weather\",\n    description: \"Get weather information for a city.\",\n    schema: z.object({ city: z.string() }),\n  }\n);\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [getWeather]\n});\n\nconst evaluator = createTrajectoryLLMAsJudge({  // [!code highlight]\n  model: \"openai:o3-mini\",  // [!code highlight]\n  prompt: TRAJECTORY_ACCURACY_PROMPT,  // [!code highlight]\n});  // [!code highlight]\n\nasync function testTrajectoryQuality() {\n  const result = await agent.invoke({\n    messages: [new HumanMessage(\"What's the weather in Seattle?\")]\n  });\n\n  const evaluation = await evaluator({\n    outputs: result.messages,\n  });\n  // {\n  //     'key': 'trajectory_accuracy',\n  //     'score': true,\n  //  ", "metadata": {"source": "test.mdx"}}
{"text": " \"gpt-4.1\",\n  tools: [getWeather]\n});\n\nconst evaluator = createTrajectoryLLMAsJudge({  // [!code highlight]\n  model: \"openai:o3-mini\",  // [!code highlight]\n  prompt: TRAJECTORY_ACCURACY_PROMPT,  // [!code highlight]\n});  // [!code highlight]\n\nasync function testTrajectoryQuality() {\n  const result = await agent.invoke({\n    messages: [new HumanMessage(\"What's the weather in Seattle?\")]\n  });\n\n  const evaluation = await evaluator({\n    outputs: result.messages,\n  });\n  // {\n  //     'key': 'trajectory_accuracy',\n  //     'score': true,\n  //     'comment': 'The provided agent trajectory is reasonable...'\n  // }\n  expect(evaluation.score).toBe(true);\n}\n```\n:::\n</Accordion>\n\n<Accordion title=\"With reference trajectory\">\n\nIf you have a reference trajectory, you can add an extra variable to your prompt and pass in the reference trajectory. Below, we use the prebuilt `TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE` prompt and configure the `reference_outputs` variable:\n\n:::python\n```python\nevaluator = create_trajectory_llm_as_judge(\n    model=\"openai:o3-mini\",\n    prompt=TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE,\n)\nevaluation = evaluator(\n    outputs=result[\"messages\"],\n    reference_outputs=reference_trajectory,\n)\n```\n:::\n\n:::js\n```ts\nimport { TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE } from \"agentevals\";\n\nconst evaluator = createTrajectoryLLMAsJudge({\n  model: \"openai:o3-mini\",\n  prompt: TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE,\n});\n\nconst evaluation = await evaluator({\n  outputs: result.messages,\n  referenceOutputs: referenceTrajectory,\n});\n```\n:::\n\n</Accordion>\n\n<Info>\nFor more configurability over how the LLM evaluates the trajectory, visit the [repository](https://github.com/langchain-ai/agentevals?tab=readme-ov-file#trajectory-llm-as-judge).\n</Info>\n\n:::python\n### Async support\n\nAll `agentevals` evaluators support Python asyncio. For evaluators that use factory functions, async versions are available by adding `async` after `create_` in the function name.\n\n<Accordion title=\"Async judge and evaluator example\">\n\n```python\nfrom agentevals.trajectory.llm import create_async_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT\nfrom agentevals.trajectory.match import create_async_trajectory_match_evaluator\n\nasync_judge = create_async_trajectory_llm", "metadata": {"source": "test.mdx"}}
{"text": "github.com/langchain-ai/agentevals?tab=readme-ov-file#trajectory-llm-as-judge).\n</Info>\n\n:::python\n### Async support\n\nAll `agentevals` evaluators support Python asyncio. For evaluators that use factory functions, async versions are available by adding `async` after `create_` in the function name.\n\n<Accordion title=\"Async judge and evaluator example\">\n\n```python\nfrom agentevals.trajectory.llm import create_async_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT\nfrom agentevals.trajectory.match import create_async_trajectory_match_evaluator\n\nasync_judge = create_async_trajectory_llm_as_judge(\n    model=\"openai:o3-mini\",\n    prompt=TRAJECTORY_ACCURACY_PROMPT,\n)\n\nasync_evaluator = create_async_trajectory_match_evaluator(\n    trajectory_match_mode=\"strict\",\n)\n\nasync def test_async_evaluation():\n    result = await agent.ainvoke({\n        \"messages\": [HumanMessage(content=\"What's the weather?\")]\n    })\n\n    evaluation = await async_judge(outputs=result[\"messages\"])\n    assert evaluation[\"score\"] is True\n```\n\n</Accordion>\n\n:::\n\n## LangSmith integration\n\nFor tracking experiments over time, you can log evaluator results to [LangSmith](https://smith.langchain.com/), a platform for building production-grade LLM applications that includes tracing, evaluation, and experimentation tools.\n\nFirst, set up LangSmith by setting the required environment variables:\n\n```bash\nexport LANGSMITH_API_KEY=\"your_langsmith_api_key\"\nexport LANGSMITH_TRACING=\"true\"\n```\n\n:::python\nLangSmith offers two main approaches for running evaluations: [pytest](/langsmith/pytest) integration and the `evaluate` function.\n\n<Accordion title=\"Using pytest integration\">\n\n```python\nimport pytest\nfrom langsmith import testing as t\nfrom agentevals.trajectory.llm import create_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT\n\ntrajectory_evaluator = create_trajectory_llm_as_judge(\n    model=\"openai:o3-mini\",\n    prompt=TRAJECTORY_ACCURACY_PROMPT,\n)\n\n@pytest.mark.langsmith\ndef test_trajectory_accuracy():\n    result = agent.invoke({\n        \"messages\": [HumanMessage(content=\"What's the weather in SF?\")]\n    })\n\n    reference_trajectory = [\n        HumanMessage(content=\"What's the weather in SF?\"),\n        AIMessage(content=\"\", tool_calls=[\n            {\"id\": \"call_1\", \"name\": \"get_weather\", \"", "metadata": {"source": "test.mdx"}}
{"text": "trajectory_evaluator = create_trajectory_llm_as_judge(\n    model=\"openai:o3-mini\",\n    prompt=TRAJECTORY_ACCURACY_PROMPT,\n)\n\n@pytest.mark.langsmith\ndef test_trajectory_accuracy():\n    result = agent.invoke({\n        \"messages\": [HumanMessage(content=\"What's the weather in SF?\")]\n    })\n\n    reference_trajectory = [\n        HumanMessage(content=\"What's the weather in SF?\"),\n        AIMessage(content=\"\", tool_calls=[\n            {\"id\": \"call_1\", \"name\": \"get_weather\", \"args\": {\"city\": \"SF\"}},\n        ]),\n        ToolMessage(content=\"It's 75 degrees and sunny in SF.\", tool_call_id=\"call_1\"),\n        AIMessage(content=\"The weather in SF is 75 degrees and sunny.\"),\n    ]\n\n    # Log inputs, outputs, and reference outputs to LangSmith\n    t.log_inputs({})\n    t.log_outputs({\"messages\": result[\"messages\"]})\n    t.log_reference_outputs({\"messages\": reference_trajectory})\n\n    trajectory_evaluator(\n        outputs=result[\"messages\"],\n        reference_outputs=reference_trajectory\n    )\n```\n\nRun the evaluation with pytest:\n\n```bash\npytest test_trajectory.py --langsmith-output\n```\n\nResults will be automatically logged to LangSmith.\n\n</Accordion>\n\n<Accordion title=\"Using the evaluate function\">\n\nAlternatively, you can create a dataset in LangSmith and use the `evaluate` function:\n\n```python\nfrom langsmith import Client\nfrom agentevals.trajectory.llm import create_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT\n\nclient = Client()\n\ntrajectory_evaluator = create_trajectory_llm_as_judge(\n    model=\"openai:o3-mini\",\n    prompt=TRAJECTORY_ACCURACY_PROMPT,\n)\n\ndef run_agent(inputs):\n    \"\"\"Your agent function that returns trajectory messages.\"\"\"\n    return agent.invoke(inputs)[\"messages\"]\n\nexperiment_results = client.evaluate(\n    run_agent,\n    data=\"your_dataset_name\",\n    evaluators=[trajectory_evaluator]\n)\n```\n\nResults will be automatically logged to LangSmith.\n\n\n</Accordion>\n\n<Tip>\nTo learn more about evaluating your agent, see the [LangSmith docs](/langsmith/pytest).\n</Tip>\n:::\n\n:::js\nLangSmith offers two main approaches for running evaluations: [Vitest/Jest](/langsmith/vitest-jest) integration and the `evaluate` function.\n\n<Accordion title", "metadata": {"source": "test.mdx"}}
{"text": ")\n\ndef run_agent(inputs):\n    \"\"\"Your agent function that returns trajectory messages.\"\"\"\n    return agent.invoke(inputs)[\"messages\"]\n\nexperiment_results = client.evaluate(\n    run_agent,\n    data=\"your_dataset_name\",\n    evaluators=[trajectory_evaluator]\n)\n```\n\nResults will be automatically logged to LangSmith.\n\n\n</Accordion>\n\n<Tip>\nTo learn more about evaluating your agent, see the [LangSmith docs](/langsmith/pytest).\n</Tip>\n:::\n\n:::js\nLangSmith offers two main approaches for running evaluations: [Vitest/Jest](/langsmith/vitest-jest) integration and the `evaluate` function.\n\n<Accordion title=\"Using vitest/jest integration\">\n\n```ts\nimport * as ls from \"langsmith/vitest\";\n// import * as ls from \"langsmith/jest\";\n\nimport { createTrajectoryLLMAsJudge, TRAJECTORY_ACCURACY_PROMPT } from \"agentevals\";\n\nconst trajectoryEvaluator = createTrajectoryLLMAsJudge({\n  model: \"openai:o3-mini\",\n  prompt: TRAJECTORY_ACCURACY_PROMPT,\n});\n\nls.describe(\"trajectory accuracy\", () => {\n  ls.test(\"accurate trajectory\", {\n    inputs: {\n      messages: [\n        {\n          role: \"user\",\n          content: \"What is the weather in SF?\"\n        }\n      ]\n    },\n    referenceOutputs: {\n      messages: [\n        new HumanMessage(\"What is the weather in SF?\"),\n        new AIMessage({\n          content: \"\",\n          tool_calls: [\n            { id: \"call_1\", name: \"get_weather\", args: { city: \"SF\" } }\n          ]\n        }),\n        new ToolMessage({\n          content: \"It's 75 degrees and sunny in SF.\",\n          tool_call_id: \"call_1\"\n        }),\n        new AIMessage(\"The weather in SF is 75 degrees and sunny.\"),\n      ],\n    },\n  }, async ({ inputs, referenceOutputs }) => {\n    const result = await agent.invoke({\n      messages: [new HumanMessage(\"What is the weather in SF?\")]\n    });\n\n    ls.logOutputs({ messages: result.messages });\n\n    await trajectoryEvaluator({\n      inputs,\n      outputs: result.messages,\n      referenceOutputs,\n    });\n  });\n});\n```\n\nRun the evaluation with your test", "metadata": {"source": "test.mdx"}}
{"text": " SF.\",\n          tool_call_id: \"call_1\"\n        }),\n        new AIMessage(\"The weather in SF is 75 degrees and sunny.\"),\n      ],\n    },\n  }, async ({ inputs, referenceOutputs }) => {\n    const result = await agent.invoke({\n      messages: [new HumanMessage(\"What is the weather in SF?\")]\n    });\n\n    ls.logOutputs({ messages: result.messages });\n\n    await trajectoryEvaluator({\n      inputs,\n      outputs: result.messages,\n      referenceOutputs,\n    });\n  });\n});\n```\n\nRun the evaluation with your test runner:\n\n```bash\nvitest run test_trajectory.eval.ts\n# or\njest test_trajectory.eval.ts\n```\n\n</Accordion>\n\n<Accordion title=\"Using the evaluate function\">\n\nAlternatively, you can create a dataset in LangSmith and use the `evaluate` function:\n\n```ts\nimport { evaluate } from \"langsmith/evaluation\";\nimport { createTrajectoryLLMAsJudge, TRAJECTORY_ACCURACY_PROMPT } from \"agentevals\";\n\nconst trajectoryEvaluator = createTrajectoryLLMAsJudge({\n  model: \"openai:o3-mini\",\n  prompt: TRAJECTORY_ACCURACY_PROMPT,\n});\n\nasync function runAgent(inputs: any) {\n  const result = await agent.invoke(inputs);\n  return result.messages;\n}\n\nawait evaluate(\n  runAgent,\n  {\n    data: \"your_dataset_name\",\n    evaluators: [trajectoryEvaluator],\n  }\n);\n```\n\nResults will be automatically logged to LangSmith.\n\n</Accordion>\n\n<Tip>\nTo learn more about evaluating your agent, see the [LangSmith docs](/langsmith/vitest-jest).\n</Tip>\n:::\n\n:::python\n## Recording & replaying HTTP calls\n\nIntegration tests that call real LLM APIs can be slow and expensive, especially when run frequently in CI/CD pipelines. We recommend using a library for recording HTTP requests and responses, then replaying them in subsequent runs without making actual network calls.\n\nYou can use [`vcrpy`](https://pypi.org/project/vcrpy/1.5.2/) to achieve this. If you're using `pytest`, the [`pytest-recording` plugin](https://pypi.org/project/pytest-recording/) provides a simple way to enable this with minimal configuration. Request/responses are recorded in cassettes, which are then used to mock the real network calls in subsequent runs.\n\nSet up your `conftest.py` file to filter out sensitive information from the cassettes:\n\n```py conftest.py\nimport pytest\n\n@pytest.fixture(scope=\"session\")\ndef vcr_config():\n    return {\n        \"filter_headers\": [\n            (\"authorization\", \"XXXX\"),\n       ", "metadata": {"source": "test.mdx"}}
{"text": "`](https://pypi.org/project/vcrpy/1.5.2/) to achieve this. If you're using `pytest`, the [`pytest-recording` plugin](https://pypi.org/project/pytest-recording/) provides a simple way to enable this with minimal configuration. Request/responses are recorded in cassettes, which are then used to mock the real network calls in subsequent runs.\n\nSet up your `conftest.py` file to filter out sensitive information from the cassettes:\n\n```py conftest.py\nimport pytest\n\n@pytest.fixture(scope=\"session\")\ndef vcr_config():\n    return {\n        \"filter_headers\": [\n            (\"authorization\", \"XXXX\"),\n            (\"x-api-key\", \"XXXX\"),\n            # ... other headers you want to mask\n        ],\n        \"filter_query_parameters\": [\n            (\"api_key\", \"XXXX\"),\n            (\"key\", \"XXXX\"),\n        ],\n    }\n```\n\nThen configure your project to recognise the `vcr` marker:\n\n<CodeGroup>\n```ini pytest.ini\n[pytest]\nmarkers =\n    vcr: record/replay HTTP via VCR\naddopts = --record-mode=once\n```\n```toml pyproject.toml\n[tool.pytest.ini_options]\nmarkers = [\n  \"vcr: record/replay HTTP via VCR\"\n]\naddopts = \"--record-mode=once\"\n```\n</CodeGroup>\n\n<Info>\nThe `--record-mode=once` option records HTTP interactions on the first run and replays them on subsequent runs.\n</Info>\n\nNow, simply decorate your tests with the `vcr` marker:\n\n```python\n@pytest.mark.vcr()\ndef test_agent_trajectory():\n    # ...\n```\n\nThe first time you run this test, your agent will make real network calls and pytest will generate a cassette file `test_agent_trajectory.yaml` in the `tests/cassettes` directory. Subsequent runs will use that cassette to mock the real network calls, granted the agent's requests don't change from the previous run. If they do, the test will fail and you'll need to delete the cassette and rerun the test to record fresh interactions.\n\n<Warning>\nWhen you modify prompts, add new tools, or change expected trajectories, your saved cassettes will become outdated and your existing tests **will fail**. You should delete the corresponding cassette files and rerun the tests to record fresh interactions.\n</Warning>\n:::\n", "metadata": {"source": "test.mdx"}}
{"text": "---\ntitle: Model Context Protocol (MCP)\n---\n\n:::python\n[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the [`langchain-mcp-adapters`](https://github.com/langchain-ai/langchain-mcp-adapters) library.\n:::\n:::js\n[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the [`@langchain/mcp-adapters`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-mcp-adapters) library.\n:::\n\n## Quickstart\n\n:::python\nInstall the `langchain-mcp-adapters` library:\n\n<CodeGroup>\n```bash pip\npip install langchain-mcp-adapters\n```\n\n```bash uv\nuv add langchain-mcp-adapters\n```\n</CodeGroup>\n\n`langchain-mcp-adapters` enables agents to use tools defined across one or more MCP servers.\n\n<Note>\n    `MultiServerMCPClient` is **stateless by default**. Each tool invocation creates a fresh MCP `ClientSession`, executes the tool, and then cleans up. See the [stateful sessions](#stateful-sessions) section for more details.\n</Note>\n\n```python Accessing multiple MCP servers icon=\"server\"\nfrom langchain_mcp_adapters.client import MultiServerMCPClient  # [!code highlight]\nfrom langchain.agents import create_agent\n\n\nclient = MultiServerMCPClient(  # [!code highlight]\n    {\n        \"math\": {\n            \"transport\": \"stdio\",  # Local subprocess communication\n            \"command\": \"python\",\n            # Absolute path to your math_server.py file\n            \"args\": [\"/path/to/math_server.py\"],\n        },\n        \"weather\": {\n            \"transport\": \"http\",  # HTTP-based remote server\n            # Ensure you start your weather server on port 8000\n            \"url\": \"http://localhost:8000/mcp\",\n        }\n    }\n)\n\ntools = await client.get_tools()  # [!code highlight]\nagent = create_agent(\n    \"claude-sonnet-4-5-20250929\",\n    tools  # [!code highlight]\n)\nmath_response = await agent.ainvoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what's (3 + 5) x 12?\"}]}\n)\nweather_response = await agent.ainvoke(\n    {\"messages\": [{\"role\":", "metadata": {"source": "mcp.mdx"}}
{"text": "\",  # HTTP-based remote server\n            # Ensure you start your weather server on port 8000\n            \"url\": \"http://localhost:8000/mcp\",\n        }\n    }\n)\n\ntools = await client.get_tools()  # [!code highlight]\nagent = create_agent(\n    \"claude-sonnet-4-5-20250929\",\n    tools  # [!code highlight]\n)\nmath_response = await agent.ainvoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what's (3 + 5) x 12?\"}]}\n)\nweather_response = await agent.ainvoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in nyc?\"}]}\n)\n```\n:::\n\n:::js\nInstall the `@langchain/mcp-adapters` library:\n\n<CodeGroup>\n```bash npm\nnpm install @langchain/mcp-adapters\n```\n\n```bash pnpm\npnpm add @langchain/mcp-adapters\n```\n\n```bash yarn\nyarn add @langchain/mcp-adapters\n```\n\n```bash bun\nbun add @langchain/mcp-adapters\n```\n</CodeGroup>\n\n`@langchain/mcp-adapters` enables agents to use tools defined across one or more MCP servers.\n\n<Note>\n    `MultiServerMCPClient` is **stateless by default**. Each tool invocation creates a fresh MCP `ClientSession`, executes the tool, and then cleans up. See the [stateful sessions](#stateful-sessions) section for more details.\n</Note>\n\n```ts Accessing multiple MCP servers icon=\"server\"\nimport { MultiServerMCPClient } from \"@langchain/mcp-adapters\";  // [!code highlight]\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport { createAgent } from \"langchain\";\n\nconst client = new MultiServerMCPClient({  // [!code highlight]\n    math: {\n        transport: \"stdio\",  // Local subprocess communication\n        command: \"node\",\n        // Replace with absolute path to your math_server.js file\n        args: [\"/path/to/math_server.js\"],\n    },\n    weather: {\n        transport: \"http\",  // HTTP-based remote server\n        // Ensure you start your weather server on port 8000\n        url: \"http://localhost:8000/mcp\",\n    },\n});\n\nconst tools = await client.getTools();  // [!code highlight]\nconst agent = createAgent({\n    model: \"claude-sonnet-4-5-20250929\",\n    tools,  // [!code highlight]\n});\n\nconst mathResponse = await agent.invoke({\n    messages: [{ role: \"user\", content: \"what's (3 + 5) x 12?\" }],\n});\n\nconst weatherResponse =", "metadata": {"source": "mcp.mdx"}}
{"text": ": [\"/path/to/math_server.js\"],\n    },\n    weather: {\n        transport: \"http\",  // HTTP-based remote server\n        // Ensure you start your weather server on port 8000\n        url: \"http://localhost:8000/mcp\",\n    },\n});\n\nconst tools = await client.getTools();  // [!code highlight]\nconst agent = createAgent({\n    model: \"claude-sonnet-4-5-20250929\",\n    tools,  // [!code highlight]\n});\n\nconst mathResponse = await agent.invoke({\n    messages: [{ role: \"user\", content: \"what's (3 + 5) x 12?\" }],\n});\n\nconst weatherResponse = await agent.invoke({\n    messages: [{ role: \"user\", content: \"what is the weather in nyc?\" }],\n});\n```\n:::\n\n## Custom servers\n\n:::python\n\nTo create a custom MCP server, use the [FastMCP](https://gofastmcp.com/getting-started/welcome) library:\n\n\n<CodeGroup>\n```bash pip\npip install fastmcp\n```\n\n```bash uv\nuv add fastmcp\n```\n</CodeGroup>\n:::\n\n:::js\nTo create your own MCP servers, you can use the `@modelcontextprotocol/sdk` library. This library provides a simple way to define [tools](https://modelcontextprotocol.io/docs/learn/server-concepts#tools-ai-actions) and run them as servers.\n\n<CodeGroup>\n```bash npm\nnpm install @modelcontextprotocol/sdk\n```\n\n```bash pnpm\npnpm add @modelcontextprotocol/sdk\n```\n\n```bash yarn\nyarn add @modelcontextprotocol/sdk\n```\n\n```bash bun\nbun add @modelcontextprotocol/sdk\n```\n</CodeGroup>\n:::\n\nTo test your agent with MCP tool servers, use the following examples:\n\n\n:::python\n<CodeGroup>\n```python title=\"Math server (stdio transport)\" icon=\"floppy-disk\"\nfrom fastmcp import FastMCP\n\nmcp = FastMCP(\"Math\")\n\n@mcp.tool()\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n@mcp.tool()\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply two numbers\"\"\"\n    return a * b\n\nif __name__ == \"__main__\":\n    mcp.run(transport=\"stdio\")\n```\n\n```python title=\"Weather server (streamable HTTP transport)\" icon=\"wifi\"\nfrom fastmcp import FastMCP\n\nmcp = FastMCP(\"Weather\")\n\n@mcp.tool()\nasync def get_weather(location: str) -> str:\n    \"\"\"Get weather for location.\"\"\"\n    return \"It's always sunny in New York\"\n\nif __name__ == \"__main__\":\n    mcp.run(transport=\"streamable-http\")\n```", "metadata": {"source": "mcp.mdx"}}
{"text": "\n    return a + b\n\n@mcp.tool()\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply two numbers\"\"\"\n    return a * b\n\nif __name__ == \"__main__\":\n    mcp.run(transport=\"stdio\")\n```\n\n```python title=\"Weather server (streamable HTTP transport)\" icon=\"wifi\"\nfrom fastmcp import FastMCP\n\nmcp = FastMCP(\"Weather\")\n\n@mcp.tool()\nasync def get_weather(location: str) -> str:\n    \"\"\"Get weather for location.\"\"\"\n    return \"It's always sunny in New York\"\n\nif __name__ == \"__main__\":\n    mcp.run(transport=\"streamable-http\")\n```\n</CodeGroup>\n:::\n\n:::js\n```typescript title=\"Math server (stdio transport)\" icon=\"floppy-disk\"\nimport { Server } from \"@modelcontextprotocol/sdk/server/index.js\";\nimport { StdioServerTransport } from \"@modelcontextprotocol/sdk/server/stdio.js\";\nimport {\n    CallToolRequestSchema,\n    ListToolsRequestSchema,\n} from \"@modelcontextprotocol/sdk/types.js\";\n\nconst server = new Server(\n    {\n        name: \"math-server\",\n        version: \"0.1.0\",\n    },\n    {\n        capabilities: {\n            tools: {},\n        },\n    }\n);\n\nserver.setRequestHandler(ListToolsRequestSchema, async () => {\n    return {\n        tools: [\n        {\n            name: \"add\",\n            description: \"Add two numbers\",\n            inputSchema: {\n                type: \"object\",\n                properties: {\n                    a: {\n                        type: \"number\",\n                        description: \"First number\",\n                    },\n                    b: {\n                        type: \"number\",\n                        description: \"Second number\",\n                    },\n                },\n                required: [\"a", "metadata": {"source": "mcp.mdx"}}
{"text": "     type: \"number\",\n                        description: \"First number\",\n                    },\n                    b: {\n                        type: \"number\",\n                        description: \"Second number\",\n                    },\n                },\n                required: [\"a\", \"b\"],\n            },\n        },\n        {\n            name: \"multiply\",\n            description: \"Multiply two numbers\",\n            inputSchema: {\n                type: \"object\",\n                properties: {\n                    a: {\n                        type: \"number\",\n                        description: \"First number\",\n                    },\n                    b: {\n                        type: \"number\",\n                        description: \"Second number\",\n                    },\n                },\n                required: [\"a\", \"b\"],\n            },\n        },\n        ],\n    };\n});\n\nserver.setRequestHandler(CallToolRequestSchema, async (request) => {\n    switch (request.params.name) {\n        case \"add\": {\n            const { a, b } = request.params.arguments as { a: number; b: number };\n            return {\n                content: [\n                {\n                    type: \"text\",\n                    text: String(a + b", "metadata": {"source": "mcp.mdx"}}
{"text": " },\n        ],\n    };\n});\n\nserver.setRequestHandler(CallToolRequestSchema, async (request) => {\n    switch (request.params.name) {\n        case \"add\": {\n            const { a, b } = request.params.arguments as { a: number; b: number };\n            return {\n                content: [\n                {\n                    type: \"text\",\n                    text: String(a + b),\n                },\n                ],\n            };\n        }\n        case \"multiply\": {\n            const { a, b } = request.params.arguments as { a: number; b: number };\n            return {\n                content: [\n                {\n                    type: \"text\",\n                    text: String(a * b),\n                },\n                ],\n            };\n        }\n        default:\n            throw new Error(`Unknown tool: ${request.params.name}`);\n    }\n});\n\nasync function main() {\n    const transport = new StdioServerTransport();\n    await server.connect(transport);\n    console.error(\"Math MCP server running on stdio\");\n}\n\nmain();\n```\n\n```typescript title=\"Weather server (SSE transport)\" icon=\"wifi\"\nimport { Server } from \"@modelcontextprotocol/sdk/server/index.js\";\nimport { SSEServerTransport } from \"@modelcontextprotocol/sdk/server/sse.js\";\nimport {\n    CallToolRequestSchema,\n    ListToolsRequestSchema,\n} from \"@modelcontextprotocol/sdk/types.js\";\nimport express from \"express\";\n\nconst app = express();\napp.use(express.json());\n\nconst server = new Server(\n    {\n        name: \"weather-server\",\n        version: \"0.1.0\",\n    },\n    {\n        capabilities: {\n            tools: {},\n        },\n    }\n", "metadata": {"source": "mcp.mdx"}}
{"text": " \"@modelcontextprotocol/sdk/server/index.js\";\nimport { SSEServerTransport } from \"@modelcontextprotocol/sdk/server/sse.js\";\nimport {\n    CallToolRequestSchema,\n    ListToolsRequestSchema,\n} from \"@modelcontextprotocol/sdk/types.js\";\nimport express from \"express\";\n\nconst app = express();\napp.use(express.json());\n\nconst server = new Server(\n    {\n        name: \"weather-server\",\n        version: \"0.1.0\",\n    },\n    {\n        capabilities: {\n            tools: {},\n        },\n    }\n);\n\nserver.setRequestHandler(ListToolsRequestSchema, async () => {\n    return {\n        tools: [\n        {\n            name: \"get_weather\",\n            description: \"Get weather for location\",\n            inputSchema: {\n            type: \"object\",\n            properties: {\n                location: {\n                type: \"string\",\n                description: \"Location to get weather for\",\n                },\n            },\n            required: [\"location\"],\n            },\n        },\n        ],\n    };\n});\n\nserver.setRequestHandler(CallToolRequestSchema, async (request) => {\n    switch (request.params.name) {\n        case \"get_weather\": {\n            const { location } = request.params.arguments as { location: string };\n            return {\n                content: [\n                    {\n                        type: \"text\",\n                        text: `It's always sunny in ${location}`,\n                    },\n                ],\n            };\n        }\n        default:\n            throw new Error(`Unknown tool: ${request.params.name}`);\n    }\n});\n\napp.post(\"/mcp\", async (req, res", "metadata": {"source": "mcp.mdx"}}
{"text": "              {\n                        type: \"text\",\n                        text: `It's always sunny in ${location}`,\n                    },\n                ],\n            };\n        }\n        default:\n            throw new Error(`Unknown tool: ${request.params.name}`);\n    }\n});\n\napp.post(\"/mcp\", async (req, res) => {\n    const transport = new SSEServerTransport(\"/mcp\", res);\n    await server.connect(transport);\n});\n\nconst PORT = process.env.PORT || 8000;\napp.listen(PORT, () => {\n    console.log(`Weather MCP server running on port ${PORT}`);\n});\n```\n:::\n\n## Transports\n\nMCP supports different transport mechanisms for client-server communication.\n\n### HTTP\n\nThe `http` transport (also referred to as `streamable-http`) uses HTTP requests for client-server communication. See the [MCP HTTP transport specification](https://modelcontextprotocol.io/specification/2025-03-26/basic/transports#streamable-http) for more details.\n\n:::python\n```python\nclient = MultiServerMCPClient(\n    {\n        \"weather\": {\n            \"transport\": \"http\",\n            \"url\": \"http://localhost:8000/mcp\",\n        }\n    }\n)\n```\n:::\n\n:::js\n```typescript\nconst client = new MultiServerMCPClient({\n    weather: {\n        transport: \"sse\",\n        url: \"http://localhost:8000/mcp\",\n    },\n});\n```\n:::\n\n#### Passing headers\n\n:::python\nWhen connecting to MCP servers over HTTP, you can include custom headers (e.g., for authentication or tracing) using the `headers` field in the connection configuration. This is supported for `sse` (deprecated by MCP spec) and `streamable_http` transports.\n\n```python Passing headers with MultiServerMCPClient\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langchain.agents import create_agent\n\nclient = MultiServerMCPClient(\n    {\n        \"weather\": {\n            \"transport\": \"http\",\n            \"url\": \"http://localhost:8000/mcp\",\n            \"headers\": {  # [!code highlight]\n                \"Authorization\": \"Bearer YOUR_TOKEN\",  # [!code", "metadata": {"source": "mcp.mdx"}}
{"text": "` field in the connection configuration. This is supported for `sse` (deprecated by MCP spec) and `streamable_http` transports.\n\n```python Passing headers with MultiServerMCPClient\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langchain.agents import create_agent\n\nclient = MultiServerMCPClient(\n    {\n        \"weather\": {\n            \"transport\": \"http\",\n            \"url\": \"http://localhost:8000/mcp\",\n            \"headers\": {  # [!code highlight]\n                \"Authorization\": \"Bearer YOUR_TOKEN\",  # [!code highlight]\n                \"X-Custom-Header\": \"custom-value\"  # [!code highlight]\n            },  # [!code highlight]\n        }\n    }\n)\ntools = await client.get_tools()\nagent = create_agent(\"openai:gpt-4.1\", tools)\nresponse = await agent.ainvoke({\"messages\": \"what is the weather in nyc?\"})\n```\n:::\n\n#### Authentication\n\n:::python\nThe `langchain-mcp-adapters` library uses the official [MCP SDK](https://github.com/modelcontextprotocol/python-sdk) under the hood, which allows you to provide a custom authentication mechanism by implementing the `httpx.Auth` interface.\n\n```python\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\n\nclient = MultiServerMCPClient(\n    {\n        \"weather\": {\n            \"transport\": \"http\",\n            \"url\": \"http://localhost:8000/mcp\",\n            \"auth\": auth, # [!code highlight]\n        }\n    }\n)\n```\n\n\n* [Example custom auth implementation](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/clients/simple-auth-client/mcp_simple_auth_client/main.py)\n* [Built-in OAuth flow](https://github.com/modelcontextprotocol/python-sdk/blob/main/src/mcp/client/auth.py#L179)\n:::\n\n### stdio\n\nClient launches server as a subprocess and communicates via standard input/output. Best for local tools and simple setups.\n\n<Note>\n    Unlike HTTP transports, `stdio` connections are inherently **stateful**\u2014the subprocess persists for the lifetime of the client connection. However, when using `MultiServerMCPClient` without explicit session management, each tool call still creates a new session. See [stateful sessions](#stateful-sessions) for managing persistent connections.\n</Note>\n\n:::python\n```python\nclient = MultiServerMCPClient(\n    {\n        \"math\": {\n        ", "metadata": {"source": "mcp.mdx"}}
{"text": " OAuth flow](https://github.com/modelcontextprotocol/python-sdk/blob/main/src/mcp/client/auth.py#L179)\n:::\n\n### stdio\n\nClient launches server as a subprocess and communicates via standard input/output. Best for local tools and simple setups.\n\n<Note>\n    Unlike HTTP transports, `stdio` connections are inherently **stateful**\u2014the subprocess persists for the lifetime of the client connection. However, when using `MultiServerMCPClient` without explicit session management, each tool call still creates a new session. See [stateful sessions](#stateful-sessions) for managing persistent connections.\n</Note>\n\n:::python\n```python\nclient = MultiServerMCPClient(\n    {\n        \"math\": {\n            \"transport\": \"stdio\",\n            \"command\": \"python\",\n            \"args\": [\"/path/to/math_server.py\"],\n        }\n    }\n)\n```\n:::\n\n:::js\n```typescript\nconst client = new MultiServerMCPClient({\n    math: {\n        transport: \"stdio\",\n        command: \"node\",\n        args: [\"/path/to/math_server.js\"],\n    },\n});\n```\n:::\n\n:::python\n\n## Stateful sessions\n\nBy default, `MultiServerMCPClient` is **stateless**\u2014each tool invocation creates a fresh MCP session, executes the tool, and then cleans up.\n\nIf you need to control the [lifecycle](https://modelcontextprotocol.io/specification/2025-03-26/basic/lifecycle) of an MCP session (for example, when working with a stateful server that maintains context across tool calls), you can create a persistent `ClientSession` using `client.session()`.\n\n```python Using MCP ClientSession for stateful tool usage\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langchain_mcp_adapters.tools import load_mcp_tools\nfrom langchain.agents import create_agent\n\nclient = MultiServerMCPClient({...})\n\n# Create a session explicitly\nasync with client.session(\"server_name\") as session:  # [!code highlight]\n    # Pass the session to load tools, resources, or prompts\n    tools = await load_mcp_tools(session)  # [!code highlight]\n    agent = create_agent(\n        \"anthropic:claude-3-7-sonnet-latest\",\n        tools\n    )\n```\n:::\n\n## Core features\n\n### Tools\n\n[Tools](https://modelcontextprotocol.io/docs/concepts/tools) allow MCP servers to expose executable functions that LLMs can invoke to perform actions\u2014such as querying databases, calling APIs, or interacting with external systems. LangChain converts MCP tools into LangChain [tools](/oss/langchain/tools), making them directly usable in any LangChain agent or workflow.\n\n#### Loading tools\n\nUse `client.get_tools()` to", "metadata": {"source": "mcp.mdx"}}
{"text": " Pass the session to load tools, resources, or prompts\n    tools = await load_mcp_tools(session)  # [!code highlight]\n    agent = create_agent(\n        \"anthropic:claude-3-7-sonnet-latest\",\n        tools\n    )\n```\n:::\n\n## Core features\n\n### Tools\n\n[Tools](https://modelcontextprotocol.io/docs/concepts/tools) allow MCP servers to expose executable functions that LLMs can invoke to perform actions\u2014such as querying databases, calling APIs, or interacting with external systems. LangChain converts MCP tools into LangChain [tools](/oss/langchain/tools), making them directly usable in any LangChain agent or workflow.\n\n#### Loading tools\n\nUse `client.get_tools()` to retrieve tools from MCP servers and pass them to your agent:\n\n:::python\n```python\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langchain.agents import create_agent\n\nclient = MultiServerMCPClient({...})\ntools = await client.get_tools()  # [!code highlight]\nagent = create_agent(\"claude-sonnet-4-5-20250929\", tools)\n```\n:::\n\n:::js\n```typescript\nimport { MultiServerMCPClient } from \"@langchain/mcp-adapters\";\nimport { createAgent } from \"langchain\";\n\nconst client = new MultiServerMCPClient({...});\nconst tools = await client.getTools();  // [!code highlight]\nconst agent = createAgent({ model: \"claude-sonnet-4-5-20250929\", tools });\n```\n:::\n\n:::python\n\n#### Structured content\n\nMCP tools can return [structured content](https://modelcontextprotocol.io/specification/2025-03-26/server/tools#structured-content) alongside the human-readable text response. This is useful when a tool needs to return machine-parseable data (like JSON) in addition to text that gets shown to the model.\n\nWhen an MCP tool returns `structuredContent`, the adapter wraps it in an [`MCPToolArtifact`](/docs/reference/langchain-mcp-adapters#MCPToolArtifact) and returns it as the tool's artifact. You can access this using the `artifact` field on the `ToolMessage`. You can also use [interceptors](#tool-interceptors) to process or transform structured content automatically.\n\n**Extracting structured content from artifact**\n\nAfter invoking your agent, you can access the structured content from tool messages in the response:\n\n```python\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langchain.agents import create_agent\nfrom langchain.messages import ToolMessage\n\nclient = MultiServerMCPClient({...})\ntools = await client.get_tools()\nagent = create_agent(\"claude-sonnet-4-5-20250929\", tools)\n\nresult = await agent.ainvoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Get data from the server\"}]}\n)\n\n# Extract structured content from tool messages\nfor message in result[\"messages\"]:\n    if isinstance(message, ToolMessage) and message.artifact", "metadata": {"source": "mcp.mdx"}}
{"text": " content automatically.\n\n**Extracting structured content from artifact**\n\nAfter invoking your agent, you can access the structured content from tool messages in the response:\n\n```python\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langchain.agents import create_agent\nfrom langchain.messages import ToolMessage\n\nclient = MultiServerMCPClient({...})\ntools = await client.get_tools()\nagent = create_agent(\"claude-sonnet-4-5-20250929\", tools)\n\nresult = await agent.ainvoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Get data from the server\"}]}\n)\n\n# Extract structured content from tool messages\nfor message in result[\"messages\"]:\n    if isinstance(message, ToolMessage) and message.artifact:\n        structured_content = message.artifact[\"structured_content\"]\n```\n\n**Appending structured content via interceptor**\n\nIf you want structured content to be visible in the conversation history (visible to the model), you can use an [interceptor](#tool-interceptors) to automatically append structured content to the tool result:\n\n```python\nimport json\n\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langchain_mcp_adapters.interceptors import MCPToolCallRequest\nfrom mcp.types import TextContent\n\nasync def append_structured_content(request: MCPToolCallRequest, handler):\n    \"\"\"Append structured content from artifact to tool message.\"\"\"\n    result = await handler(request)\n    if result.structuredContent:\n        result.content += [\n            TextContent(type=\"text\", text=json.dumps(result.structuredContent)),\n        ]\n    return result\n\nclient = MultiServerMCPClient({...}, tool_interceptors=[append_structured_content])\n```\n\n#### Multimodal tool content\n\nMCP tools can return [multimodal content](https://modelcontextprotocol.io/specification/2025-03-26/server/tools#tool-result) (images, text, etc.) in their responses. When an MCP server returns content with multiple parts (e.g., text and images), the adapter converts them to LangChain's [standard content blocks](/oss/langchain/messages#standard-content-blocks). You can access the standardized representation via the `content_blocks` property on the `ToolMessage`:\n\n```python\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langchain.agents import create_agent\n\nclient = MultiServerMCPClient({...})\ntools = await client.get_tools()\nagent = create_agent(\"claude-sonnet-4-5-20250929\", tools)\n\nresult = await agent.ainvoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Take a screenshot of the current page\"}]}\n)\n\n# Access multimodal content from tool messages\nfor message in result[\"messages\"]:\n    if message.type == \"tool\":\n        # Raw content in provider-native format\n        print(f\"Raw content:", "metadata": {"source": "mcp.mdx"}}
{"text": " via the `content_blocks` property on the `ToolMessage`:\n\n```python\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langchain.agents import create_agent\n\nclient = MultiServerMCPClient({...})\ntools = await client.get_tools()\nagent = create_agent(\"claude-sonnet-4-5-20250929\", tools)\n\nresult = await agent.ainvoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Take a screenshot of the current page\"}]}\n)\n\n# Access multimodal content from tool messages\nfor message in result[\"messages\"]:\n    if message.type == \"tool\":\n        # Raw content in provider-native format\n        print(f\"Raw content: {message.content}\")\n\n        # Standardized content blocks  # [!code highlight]\n        for block in message.content_blocks:  # [!code highlight]\n            if block[\"type\"] == \"text\":  # [!code highlight]\n                print(f\"Text: {block['text']}\")  # [!code highlight]\n            elif block[\"type\"] == \"image\":  # [!code highlight]\n                print(f\"Image URL: {block.get('url')}\")  # [!code highlight]\n                print(f\"Image base64: {block.get('base64', '')[:50]}...\")  # [!code highlight]\n```\n\nThis allows you to handle multimodal tool responses in a provider-agnostic way, regardless of how the underlying MCP server formats its content.\n\n### Resources\n\n[Resources](https://modelcontextprotocol.io/docs/concepts/resources) allow MCP servers to expose data\u2014such as files, database records, or API responses\u2014that can be read by clients. LangChain converts MCP resources into [Blob](/docs/reference/langchain-core/documents#Blob) objects, which provide a unified interface for handling both text and binary content.\n\n#### Loading resources\n\nUse `client.get_resources()` to load resources from an MCP server:\n\n```python\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\n\nclient = MultiServerMCPClient({...})\n\n# Load all resources from a server\nblobs = await client.get_resources(\"server_name\")  # [!code highlight]\n\n# Or load specific resources by URI\nblobs = await client.get_resources(\"server_name\", uris=[\"file:///path/to/file.txt\"])  # [!code highlight]\n\nfor blob in blobs:\n    print(f\"URI: {blob.metadata['uri']}, MIME type: {blob.mimetype}\")\n    print(blob.as_string())  # For text content\n```\n\nYou can also use [`load_mcp_resources`](/docs/reference/langchain-mcp-adapters#load_mcp_resources) directly with a session for more control", "metadata": {"source": "mcp.mdx"}}
{"text": "ServerMCPClient\n\nclient = MultiServerMCPClient({...})\n\n# Load all resources from a server\nblobs = await client.get_resources(\"server_name\")  # [!code highlight]\n\n# Or load specific resources by URI\nblobs = await client.get_resources(\"server_name\", uris=[\"file:///path/to/file.txt\"])  # [!code highlight]\n\nfor blob in blobs:\n    print(f\"URI: {blob.metadata['uri']}, MIME type: {blob.mimetype}\")\n    print(blob.as_string())  # For text content\n```\n\nYou can also use [`load_mcp_resources`](/docs/reference/langchain-mcp-adapters#load_mcp_resources) directly with a session for more control:\n\n```python\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langchain_mcp_adapters.resources import load_mcp_resources\n\nclient = MultiServerMCPClient({...})\n\nasync with client.session(\"server_name\") as session:\n    # Load all resources\n    blobs = await load_mcp_resources(session)\n\n    # Or load specific resources by URI\n    blobs = await load_mcp_resources(session, uris=[\"file:///path/to/file.txt\"])\n```\n\n### Prompts\n\n[Prompts](https://modelcontextprotocol.io/docs/concepts/prompts) allow MCP servers to expose reusable prompt templates that can be retrieved and used by clients. LangChain converts MCP prompts into [messages](/docs/concepts/messages), making them easy to integrate into chat-based workflows.\n\n#### Loading prompts\n\nUse `client.get_prompt()` to load a prompt from an MCP server:\n\n```python\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\n\nclient = MultiServerMCPClient({...})\n\n# Load a prompt by name\nmessages = await client.get_prompt(\"server_name\", \"summarize\")  # [!code highlight]\n\n# Load a prompt with arguments\nmessages = await client.get_prompt(  # [!code highlight]\n    \"server_name\",  # [!code highlight]\n    \"code_review\",  # [!code highlight]\n    arguments={\"language\": \"python\", \"focus\": \"security\"}  # [!code highlight]\n)  # [!code highlight]\n\n# Use the messages in your workflow\nfor message in messages:\n    print(f\"{message.type}: {message.content}\")\n```\n\nYou can also use [`load_mcp_prompt`](/docs/reference/langchain-mcp-adapters#load_mcp_prompt) directly with a session for more control:\n\n```python\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langchain_mcp_adapters.prompts import load_mcp_prompt\n\nclient = MultiServerMCPClient({...})\n\nasync with client.session(\"server_name\") as session:\n    # Load a prompt by name\n    messages = await load_mcp_prompt(session, \"sum", "metadata": {"source": "mcp.mdx"}}
{"text": " [!code highlight]\n)  # [!code highlight]\n\n# Use the messages in your workflow\nfor message in messages:\n    print(f\"{message.type}: {message.content}\")\n```\n\nYou can also use [`load_mcp_prompt`](/docs/reference/langchain-mcp-adapters#load_mcp_prompt) directly with a session for more control:\n\n```python\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langchain_mcp_adapters.prompts import load_mcp_prompt\n\nclient = MultiServerMCPClient({...})\n\nasync with client.session(\"server_name\") as session:\n    # Load a prompt by name\n    messages = await load_mcp_prompt(session, \"summarize\")\n\n    # Load a prompt with arguments\n    messages = await load_mcp_prompt(\n        session,\n        \"code_review\",\n        arguments={\"language\": \"python\", \"focus\": \"security\"}\n    )\n```\n\n## Advanced features\n\n### Tool interceptors\n\nMCP servers run as separate processes\u2014they can't access LangGraph runtime information like the [store](/oss/langgraph/persistence#memory-store), [context](/oss/langchain/context-engineering), or agent state. **Interceptors bridge this gap** by giving you access to this runtime context during MCP tool execution.\n\nInterceptors also provide middleware-like control over tool calls: you can modify requests, implement retries, add headers dynamically, or short-circuit execution entirely.\n\n| Section | Description |\n|---------|-------------|\n| [Accessing runtime context](#accessing-runtime-context) | Read user IDs, API keys, store data, and agent state |\n| [State updates and commands](#state-updates-and-commands) | Update agent state or control graph flow with `Command` |\n| [Writing interceptors](#writing-interceptors) | Patterns for modifying requests, composing interceptors, and error handling |\n\n#### Accessing runtime context\n\nWhen MCP tools are used within a LangChain agent (via `create_agent`), interceptors receive access to the `ToolRuntime` context. This provides access to the tool call ID, state, config, and store\u2014enabling powerful patterns for accessing user data, persisting information, and controlling agent behavior.\n\n<Tabs>\n  <Tab title=\"Runtime context\">\n    Access user-specific configuration like user IDs, API keys, or permissions that are passed at invocation time:\n\n    ```python Inject user context into MCP tool calls\n    from dataclasses import dataclass\n    from langchain_mcp_adapters.client import MultiServerMCPClient\n    from langchain_mcp_adapters.interceptors import MCPToolCallRequest\n    from langchain.agents import create_agent\n\n    @dataclass\n    class Context:\n        user_id: str\n        api_key: str\n\n    async def inject_user_context(\n        request: MCPToolCallRequest,\n        handler,\n    ):\n        \"\"\"", "metadata": {"source": "mcp.mdx"}}
{"text": "  Access user-specific configuration like user IDs, API keys, or permissions that are passed at invocation time:\n\n    ```python Inject user context into MCP tool calls\n    from dataclasses import dataclass\n    from langchain_mcp_adapters.client import MultiServerMCPClient\n    from langchain_mcp_adapters.interceptors import MCPToolCallRequest\n    from langchain.agents import create_agent\n\n    @dataclass\n    class Context:\n        user_id: str\n        api_key: str\n\n    async def inject_user_context(\n        request: MCPToolCallRequest,\n        handler,\n    ):\n        \"\"\"Inject user credentials into MCP tool calls.\"\"\"\n        runtime = request.runtime\n        user_id = runtime.context.user_id  # [!code highlight]\n        api_key = runtime.context.api_key  # [!code highlight]\n\n        # Add user context to tool arguments\n        modified_request = request.override(\n            args={**request.args, \"user_id\": user_id}\n        )\n        return await handler(modified_request)\n\n    client = MultiServerMCPClient(\n        {...},\n        tool_interceptors=[inject_user_context],\n    )\n    tools = await client.get_tools()\n    agent = create_agent(\"gpt-4.1\", tools, context_schema=Context)\n\n    # Invoke with user context\n    result = await agent.ainvoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"Search my orders\"}]},\n        context={\"user_id\": \"user_123\", \"api_key\": \"sk-...\"}\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"Store\">\n    Access long-term memory to retrieve user preferences or persist data across conversations:\n\n    ```python Access user preferences from store\n    from dataclasses import dataclass\n    from langchain_mcp_adapters.client import MultiServerMCPClient\n    from langchain_mcp_adapters.interceptors import MCPToolCallRequest\n    from langchain.agents import create_agent\n    from langgraph.store.memory import InMemoryStore\n\n    @dataclass\n    class Context:\n        user_id: str\n\n    async def personalize_search(\n        request: MCPToolCallRequest,\n        handler,\n    ):\n        \"\"\"Personalize MCP tool calls using stored preferences.\"\"\"\n        runtime = request.runtime\n        user_id = runtime.context.user_id\n       ", "metadata": {"source": "mcp.mdx"}}
{"text": "   from langchain_mcp_adapters.client import MultiServerMCPClient\n    from langchain_mcp_adapters.interceptors import MCPToolCallRequest\n    from langchain.agents import create_agent\n    from langgraph.store.memory import InMemoryStore\n\n    @dataclass\n    class Context:\n        user_id: str\n\n    async def personalize_search(\n        request: MCPToolCallRequest,\n        handler,\n    ):\n        \"\"\"Personalize MCP tool calls using stored preferences.\"\"\"\n        runtime = request.runtime\n        user_id = runtime.context.user_id\n        store = runtime.store  # [!code highlight]\n\n        # Read user preferences from store\n        prefs = store.get((\"preferences\",), user_id)  # [!code highlight]\n\n        if prefs and request.name == \"search\":\n            # Apply user's preferred language and result limit\n            modified_args = {\n                **request.args,\n                \"language\": prefs.value.get(\"language\", \"en\"),\n                \"limit\": prefs.value.get(\"result_limit\", 10),\n            }\n            request = request.override(args=modified_args)\n\n        return await handler(request)\n\n    client = MultiServerMCPClient(\n        {...},\n        tool_interceptors=[personalize_search],\n    )\n    tools = await client.get_tools()\n    agent = create_agent(\n        \"gpt-4.1\",\n        tools,\n        context_schema=Context,\n        store=InMemoryStore()\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"State\">\n    Access conversation state to make decisions based on the current session:\n\n    ```python Filter tools based on authentication state\n    from langchain_mcp_adapters.client import MultiServerMCPClient\n    from langchain_mcp_adapters.interceptors import MCPToolCallRequest\n    from langchain.messages import ToolMessage\n\n    async def require_authentication(\n        request: MCPToolCallRequest,\n        handler,\n    ):\n        \"\"\"Block sensitive MCP tools if user is not authenticated.\"\"\"\n        runtime = request.runtime\n        state = runtime.state  # [!code highlight]\n        is_authenticated = state.get(\"authent", "metadata": {"source": "mcp.mdx"}}
{"text": "\n    Access conversation state to make decisions based on the current session:\n\n    ```python Filter tools based on authentication state\n    from langchain_mcp_adapters.client import MultiServerMCPClient\n    from langchain_mcp_adapters.interceptors import MCPToolCallRequest\n    from langchain.messages import ToolMessage\n\n    async def require_authentication(\n        request: MCPToolCallRequest,\n        handler,\n    ):\n        \"\"\"Block sensitive MCP tools if user is not authenticated.\"\"\"\n        runtime = request.runtime\n        state = runtime.state  # [!code highlight]\n        is_authenticated = state.get(\"authenticated\", False)  # [!code highlight]\n\n        sensitive_tools = [\"delete_file\", \"update_settings\", \"export_data\"]\n\n        if request.name in sensitive_tools and not is_authenticated:\n            # Return error instead of calling tool\n            return ToolMessage(\n                content=\"Authentication required. Please log in first.\",\n                tool_call_id=runtime.tool_call_id,\n            )\n\n        return await handler(request)\n\n    client = MultiServerMCPClient(\n        {...},\n        tool_interceptors=[require_authentication],\n    )\n    ```\n  </Tab>\n\n  <Tab title=\"Tool call ID\">\n    Access the tool call ID to return properly formatted responses or track tool executions:\n\n    ```python Return custom responses with tool call ID\n    from langchain_mcp_adapters.client import MultiServerMCPClient\n    from langchain_mcp_adapters.interceptors import MCPToolCallRequest\n    from langchain.messages import ToolMessage\n\n    async def rate_limit_interceptor(\n        request: MCPToolCallRequest,\n        handler,\n    ):\n        \"\"\"Rate limit expensive MCP tool calls.\"\"\"\n        runtime = request.runtime\n        tool_call_id = runtime.tool_call_id  # [!code highlight]\n\n        # Check rate limit (simplified example)\n        if is_rate_limited(request.name):\n            return ToolMessage(\n                content=\"Rate limit exceeded. Please try again later.\",\n                tool_call_id=tool_call_id,  # [!code highlight]\n            )\n\n        result = await handler(request)\n\n        # Log successful tool call\n  ", "metadata": {"source": "mcp.mdx"}}
{"text": "\n        runtime = request.runtime\n        tool_call_id = runtime.tool_call_id  # [!code highlight]\n\n        # Check rate limit (simplified example)\n        if is_rate_limited(request.name):\n            return ToolMessage(\n                content=\"Rate limit exceeded. Please try again later.\",\n                tool_call_id=tool_call_id,  # [!code highlight]\n            )\n\n        result = await handler(request)\n\n        # Log successful tool call\n        log_tool_execution(tool_call_id, request.name, success=True)\n\n        return result\n\n    client = MultiServerMCPClient(\n        {...},\n        tool_interceptors=[rate_limit_interceptor],\n    )\n    ```\n  </Tab>\n</Tabs>\n\nFor more context engineering patterns, see [Context engineering](/oss/langchain/context-engineering) and [Tools](/oss/langchain/tools).\n\n#### State updates and commands\n\nInterceptors can return `Command` objects to update agent state or control graph execution flow. This is useful for tracking task progress, switching between agents, or ending execution early.\n\n```python Mark task complete and switch agents\nfrom langchain.agents import AgentState, create_agent\nfrom langchain_mcp_adapters.interceptors import MCPToolCallRequest\nfrom langchain.messages import ToolMessage\nfrom langgraph.types import Command\n\nasync def handle_task_completion(\n    request: MCPToolCallRequest,\n    handler,\n):\n    \"\"\"Mark task complete and hand off to summary agent.\"\"\"\n    result = await handler(request)\n\n    if request.name == \"submit_order\":\n        return Command(\n            update={\n                \"messages\": [result] if isinstance(result, ToolMessage) else [],\n                \"task_status\": \"completed\",  # [!code highlight]\n            },\n            goto=\"summary_agent\",  # [!code highlight]\n        )\n\n    return result\n```\n\nUse `Command` with `goto=\"__end__\"` to end execution early:\n\n```python End agent run on completion\nasync def end_on_success(\n    request: MCPToolCallRequest,\n    handler,\n):\n    \"\"\"End agent run when task is marked complete.\"\"\"\n    result = await handler(request)\n\n    if request.name == \"mark_complete\":\n        return Command(\n            update={\"messages\": [result], \"status\":", "metadata": {"source": "mcp.mdx"}}
{"text": "\",  # [!code highlight]\n            },\n            goto=\"summary_agent\",  # [!code highlight]\n        )\n\n    return result\n```\n\nUse `Command` with `goto=\"__end__\"` to end execution early:\n\n```python End agent run on completion\nasync def end_on_success(\n    request: MCPToolCallRequest,\n    handler,\n):\n    \"\"\"End agent run when task is marked complete.\"\"\"\n    result = await handler(request)\n\n    if request.name == \"mark_complete\":\n        return Command(\n            update={\"messages\": [result], \"status\": \"done\"},\n            goto=\"__end__\",  # [!code highlight]\n        )\n\n    return result\n```\n\n#### Custom interceptors\n\nInterceptors are async functions that wrap tool execution, enabling request/response modification, retry logic, and other cross-cutting concerns. They follow an \"onion\" pattern where the first interceptor in the list is the outermost layer.\n\n**Basic pattern**\n\nAn interceptor is an async function that receives a request and a handler. You can modify the request before calling the handler, modify the response after, or skip the handler entirely.\n\n```python Basic interceptor pattern\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langchain_mcp_adapters.interceptors import MCPToolCallRequest\n\nasync def logging_interceptor(\n    request: MCPToolCallRequest,\n    handler,\n):\n    \"\"\"Log tool calls before and after execution.\"\"\"\n    print(f\"Calling tool: {request.name} with args: {request.args}\")\n    result = await handler(request)\n    print(f\"Tool {request.name} returned: {result}\")\n    return result\n\nclient = MultiServerMCPClient(\n    {\"math\": {\"transport\": \"stdio\", \"command\": \"python\", \"args\": [\"/path/to/server.py\"]}},\n    tool_interceptors=[logging_interceptor],  # [!code highlight]\n)\n```\n\n**Modifying requests**\n\nUse `request.override()` to create a modified request. This follows an immutable pattern, leaving the original request unchanged.\n\n```python Modifying tool arguments\nasync def double_args_interceptor(\n    request: MCPToolCallRequest,\n    handler,\n):\n    \"\"\"Double all numeric arguments before execution.\"\"\"\n    modified_args = {k: v * 2 for k, v in request.args.items()}\n    modified_request = request.override(args=modified_args)  # [!code highlight]\n    return await handler(modified_request)\n\n# Original call: add(a=2, b=3) becomes add(a=4, b=6)\n```\n\n**Modifying headers at runtime**\n\nInterceptors can modify HTTP headers dynamically based on the request context:\n\n```python Dynamic header modification\nasync def auth_header_interceptor", "metadata": {"source": "mcp.mdx"}}
{"text": " an immutable pattern, leaving the original request unchanged.\n\n```python Modifying tool arguments\nasync def double_args_interceptor(\n    request: MCPToolCallRequest,\n    handler,\n):\n    \"\"\"Double all numeric arguments before execution.\"\"\"\n    modified_args = {k: v * 2 for k, v in request.args.items()}\n    modified_request = request.override(args=modified_args)  # [!code highlight]\n    return await handler(modified_request)\n\n# Original call: add(a=2, b=3) becomes add(a=4, b=6)\n```\n\n**Modifying headers at runtime**\n\nInterceptors can modify HTTP headers dynamically based on the request context:\n\n```python Dynamic header modification\nasync def auth_header_interceptor(\n    request: MCPToolCallRequest,\n    handler,\n):\n    \"\"\"Add authentication headers based on the tool being called.\"\"\"\n    token = get_token_for_tool(request.name)\n    modified_request = request.override(\n        headers={\"Authorization\": f\"Bearer {token}\"}  # [!code highlight]\n    )\n    return await handler(modified_request)\n```\n\n**Composing interceptors**\n\nMultiple interceptors compose in \"onion\" order \u2014 the first interceptor in the list is the outermost layer:\n\n```python Composing multiple interceptors\nasync def outer_interceptor(request, handler):\n    print(\"outer: before\")\n    result = await handler(request)\n    print(\"outer: after\")\n    return result\n\nasync def inner_interceptor(request, handler):\n    print(\"inner: before\")\n    result = await handler(request)\n    print(\"inner: after\")\n    return result\n\nclient = MultiServerMCPClient(\n    {...},\n    tool_interceptors=[outer_interceptor, inner_interceptor],  # [!code highlight]\n)\n\n# Execution order:\n# outer: before -> inner: before -> tool execution -> inner: after -> outer: after\n```\n\n**Error handling**\n\nUse interceptors to catch tool execution errors and implement retry logic:\n\n```python Retry on error\nimport asyncio\n\nasync def retry_interceptor(\n    request: MCPToolCallRequest,\n    handler,\n    max_retries: int = 3,\n    delay: float = 1.0,\n):\n    \"\"\"Retry failed tool calls with exponential backoff.\"\"\"\n    last_error = None\n    for attempt in range(max_retries):\n        try:\n            return await handler(request)\n        except Exception as e:\n            last_error = e\n            if attempt < max_retries - 1:\n                wait_time = delay * (2 ** attempt)  # Exponential backoff\n                print(f\"", "metadata": {"source": "mcp.mdx"}}
{"text": ",\n    max_retries: int = 3,\n    delay: float = 1.0,\n):\n    \"\"\"Retry failed tool calls with exponential backoff.\"\"\"\n    last_error = None\n    for attempt in range(max_retries):\n        try:\n            return await handler(request)\n        except Exception as e:\n            last_error = e\n            if attempt < max_retries - 1:\n                wait_time = delay * (2 ** attempt)  # Exponential backoff\n                print(f\"Tool {request.name} failed (attempt {attempt + 1}), retrying in {wait_time}s...\")\n                await asyncio.sleep(wait_time)\n    raise last_error\n\nclient = MultiServerMCPClient(\n    {...},\n    tool_interceptors=[retry_interceptor],  # [!code highlight]\n)\n```\n\nYou can also catch specific error types and return fallback values:\n\n```python Error handling with fallback\nasync def fallback_interceptor(\n    request: MCPToolCallRequest,\n    handler,\n):\n    \"\"\"Return a fallback value if tool execution fails.\"\"\"\n    try:\n        return await handler(request)\n    except TimeoutError:\n        return f\"Tool {request.name} timed out. Please try again later.\"\n    except ConnectionError:\n        return f\"Could not connect to {request.name} service. Using cached data.\"\n```\n\n### Progress notifications\n\nSubscribe to progress updates for long-running tool executions:\n\n```python Progress callback\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langchain_mcp_adapters.callbacks import Callbacks, CallbackContext\n\nasync def on_progress(\n    progress: float,\n    total: float | None,\n    message: str | None,\n    context: CallbackContext,\n):\n    \"\"\"Handle progress updates from MCP servers.\"\"\"\n    percent = (progress / total * 100) if total else progress\n    tool_info = f\" ({context.tool_name})\" if context.tool_name else \"\"\n    print(f\"[{context.server_name}{tool_info}] Progress: {percent:.1f}% - {message}\")\n\nclient = MultiServerMCPClient(\n    {...},\n    callbacks=Callbacks(on_progress=on_progress),  # [!code highlight]\n)\n```\n\nThe `CallbackContext` provides:\n- `server_name`: Name of the MCP server\n- `tool_name`: Name of the tool being executed (available during tool calls)\n\n### Logging\n\nThe MCP protocol supports [logging](https://modelcontextprotocol.io/specification/2025-03-26/server/", "metadata": {"source": "mcp.mdx"}}
{"text": " = (progress / total * 100) if total else progress\n    tool_info = f\" ({context.tool_name})\" if context.tool_name else \"\"\n    print(f\"[{context.server_name}{tool_info}] Progress: {percent:.1f}% - {message}\")\n\nclient = MultiServerMCPClient(\n    {...},\n    callbacks=Callbacks(on_progress=on_progress),  # [!code highlight]\n)\n```\n\nThe `CallbackContext` provides:\n- `server_name`: Name of the MCP server\n- `tool_name`: Name of the tool being executed (available during tool calls)\n\n### Logging\n\nThe MCP protocol supports [logging](https://modelcontextprotocol.io/specification/2025-03-26/server/utilities/logging#log-levels) notifications from servers. Use the `Callbacks` class to subscribe to these events.\n\n```python Logging callback\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langchain_mcp_adapters.callbacks import Callbacks, CallbackContext\nfrom mcp.types import LoggingMessageNotificationParams\n\nasync def on_logging_message(\n    params: LoggingMessageNotificationParams,\n    context: CallbackContext,\n):\n    \"\"\"Handle log messages from MCP servers.\"\"\"\n    print(f\"[{context.server_name}] {params.level}: {params.data}\")\n\nclient = MultiServerMCPClient(\n    {...},\n    callbacks=Callbacks(on_logging_message=on_logging_message),  # [!code highlight]\n)\n```\n\n### Elicitation\n\n[Elicitation](https://modelcontextprotocol.io/specification/2025-11-25/client/elicitation#elicitation) allows MCP servers to request additional input from users during tool execution. Instead of requiring all inputs upfront, servers can interactively ask for information as needed.\n\n#### Server setup\n\nDefine a tool that uses `ctx.elicit()` to request user input with a schema:\n\n```python MCP server with elicitation\nfrom pydantic import BaseModel\nfrom mcp.server.fastmcp import Context, FastMCP\n\nserver = FastMCP(\"Profile\")\n\nclass UserDetails(BaseModel):\n    email: str\n    age: int\n\n@server.tool()\nasync def create_profile(name: str, ctx: Context) -> str:\n    \"\"\"Create a user profile, requesting details via elicitation.\"\"\"\n    result = await ctx.elicit(  # [!code highlight]\n        message=f\"Please provide details for {name}'s profile:\",  # [!code highlight]\n        schema=UserDetails,  # [!code highlight]\n    )  # [!code highlight]\n    if result.action == \"accept\" and result.data:\n        return f\"Created profile for {name}: email={result.data.email}, age={result.data.age}\"\n    if result.action == \"decline\":\n        return f\"User declined. Created minimal profile for {name}.\"", "metadata": {"source": "mcp.mdx"}}
{"text": "_profile(name: str, ctx: Context) -> str:\n    \"\"\"Create a user profile, requesting details via elicitation.\"\"\"\n    result = await ctx.elicit(  # [!code highlight]\n        message=f\"Please provide details for {name}'s profile:\",  # [!code highlight]\n        schema=UserDetails,  # [!code highlight]\n    )  # [!code highlight]\n    if result.action == \"accept\" and result.data:\n        return f\"Created profile for {name}: email={result.data.email}, age={result.data.age}\"\n    if result.action == \"decline\":\n        return f\"User declined. Created minimal profile for {name}.\"\n    return \"Profile creation cancelled.\"\n\nif __name__ == \"__main__\":\n    server.run(transport=\"http\")\n```\n\n#### Client setup\n\nHandle elicitation requests by providing a callback to `MultiServerMCPClient`:\n\n```python Handling elicitation requests\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langchain_mcp_adapters.callbacks import Callbacks, CallbackContext\nfrom mcp.shared.context import RequestContext\nfrom mcp.types import ElicitRequestParams, ElicitResult\n\nasync def on_elicitation(\n    mcp_context: RequestContext,\n    params: ElicitRequestParams,\n    context: CallbackContext,\n) -> ElicitResult:\n    \"\"\"Handle elicitation requests from MCP servers.\"\"\"\n    # In a real application, you would prompt the user for input\n    # based on params.message and params.requestedSchema\n    return ElicitResult(  # [!code highlight]\n        action=\"accept\",  # [!code highlight]\n        content={\"email\": \"user@example.com\", \"age\": 25},  # [!code highlight]\n    )  # [!code highlight]\n\nclient = MultiServerMCPClient(\n    {\n        \"profile\": {\n            \"url\": \"http://localhost:8000/mcp\",\n            \"transport\": \"http\",\n        }\n    },\n    callbacks=Callbacks(on_elicitation=on_elicitation),  # [!code highlight]\n)\n```\n\n#### Response actions\n\nThe elicitation callback can return one of three actions:\n\n| Action | Description |\n|--------|-------------|\n| `accept` | User provided valid input. Include the data in the `content` field. |\n| `decline` | User chose not to provide the requested information. |\n| `cancel` | User cancelled the operation entirely. |\n\n```python Response action examples\n# Accept with data\nElicitResult(action=\"accept\", content={\"email\": \"user@example.com\", \"age\": 25})\n\n# Decline (user doesn't want to provide info)\nElicitResult(action=\"decline\")\n\n# Cancel (abort the operation)\n", "metadata": {"source": "mcp.mdx"}}
{"text": " }\n    },\n    callbacks=Callbacks(on_elicitation=on_elicitation),  # [!code highlight]\n)\n```\n\n#### Response actions\n\nThe elicitation callback can return one of three actions:\n\n| Action | Description |\n|--------|-------------|\n| `accept` | User provided valid input. Include the data in the `content` field. |\n| `decline` | User chose not to provide the requested information. |\n| `cancel` | User cancelled the operation entirely. |\n\n```python Response action examples\n# Accept with data\nElicitResult(action=\"accept\", content={\"email\": \"user@example.com\", \"age\": 25})\n\n# Decline (user doesn't want to provide info)\nElicitResult(action=\"decline\")\n\n# Cancel (abort the operation)\nElicitResult(action=\"cancel\")\n```\n\n:::\n\n## Additional resources\n\n* [MCP documentation](https://modelcontextprotocol.io/introduction)\n* [MCP Transport documentation](https://modelcontextprotocol.io/docs/concepts/transports)\n:::python\n* [`langchain-mcp-adapters`](https://github.com/langchain-ai/langchain-mcp-adapters)\n:::\n:::js\n* [`@langchain/mcp-adapters`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-mcp-adapters/)\n:::\n", "metadata": {"source": "mcp.mdx"}}
{"text": "---\ntitle: Build a voice agent with LangChain\nsidebarTitle: Voice agent\n---\n\n## Overview\n\nChat interfaces have dominated how we interact with AI, but recent breakthroughs in multimodal AI are opening up exciting new possibilities. High-quality generative models and expressive text-to-speech (TTS) systems now make it possible to build agents that feel less like tools and more like conversational partners.\n\nVoice agents are one example of this. Instead of relying on a keyboard and mouse to type inputs into an agent, you can use spoken words to interact with it. This can be a more natural and engaging way to interact with AI, and can be especially useful for certain contexts.\n\n### What are voice agents?\n\nVoice agents are [agents](/oss/langchain/agents) that can engage in natural spoken conversations with users. These agents combine speech recognition, natural language processing, generative AI, and text-to-speech technologies to create seamless, natural conversations.\n\nThey're suited for a variety of use cases, including:\n\n- Customer support\n- Personal assistants\n- Hands-free interfaces\n- Coaching and training\n\n### How do voice agents work?\n\nAt a high level, every voice agent needs to handle three tasks:\n\n1. **Listen** - capture audio and transcribe it\n2. **Think** - interpret intent, reason, plan\n3. **Speak** - generate audio and stream it back to the user\n\nThe difference lies in how these steps are sequenced and coupled. In practice, production agents follow one of two main architectures:\n\n#### 1. STT > Agent > TTS architecture (The \"Sandwich\")\n\nThe Sandwich architecture composes three distinct components: speech-to-text (STT), a text-based LangChain agent, and text-to-speech (TTS).\n\n```mermaid\nflowchart LR\n    A[User Audio] --> B[Speech-to-Text]\n    B --> C[LangChain Agent]\n    C --> D[Text-to-Speech]\n    D --> E[Audio Output]\n```\n\n**Pros:**\n- Full control over each component (swap STT/TTS providers as needed)\n- Access to latest capabilities from modern text-modality models\n- Transparent behavior with clear boundaries between components\n\n**Cons:**\n- Requires orchestrating multiple services\n- Additional complexity in managing the pipeline\n- Conversion from speech to text loses information (e.g., tone, emotion)\n\n#### 2. Speech-to-Speech architecture (S2S)\n\nSpeech-to-speech uses a multimodal model that processes audio input and generates audio output natively.\n\n```mermaid\nflowchart LR\n    A[User Audio] --> B[Multimodal Model]\n    B --> C[Audio Output]\n```\n\n**Pros:**\n- Simpler architecture with fewer moving parts\n- Typically lower latency for simple interactions\n- Direct audio processing captures tone and other nuances of speech\n\n**Cons:**\n- Limited model options, greater risk of provider lock-in\n- Features may lag behind text-modality models\n- Less transparency in how audio is processed\n- Reduced controllability and customization options\n\nThis guide demonstrates the **sandwich architecture** to balance performance, controllability, and access to modern model capabilities. The sandwich can achieve sub-700ms latency with some STT and TTS providers while maintaining control over modular components.\n\n### Demo Application overview\n\nWe'll walk through building a voice-based agent using the sandwich architecture. The agent will manage orders", "metadata": {"source": "voice-agent.mdx"}}
{"text": "`mermaid\nflowchart LR\n    A[User Audio] --> B[Multimodal Model]\n    B --> C[Audio Output]\n```\n\n**Pros:**\n- Simpler architecture with fewer moving parts\n- Typically lower latency for simple interactions\n- Direct audio processing captures tone and other nuances of speech\n\n**Cons:**\n- Limited model options, greater risk of provider lock-in\n- Features may lag behind text-modality models\n- Less transparency in how audio is processed\n- Reduced controllability and customization options\n\nThis guide demonstrates the **sandwich architecture** to balance performance, controllability, and access to modern model capabilities. The sandwich can achieve sub-700ms latency with some STT and TTS providers while maintaining control over modular components.\n\n### Demo Application overview\n\nWe'll walk through building a voice-based agent using the sandwich architecture. The agent will manage orders for a sandwich shop. The application will demonstrate all three components of the sandwich architecture, using [AssemblyAI](https://www.assemblyai.com/) for STT and [Cartesia](https://cartesia.ai/) for TTS (although adapters can be built for most providers).\n\nAn end-to-end reference application is available in the [voice-sandwich-demo](https://github.com/langchain-ai/voice-sandwich-demo) repository. We will walk through that application here.\n\nThe demo uses WebSockets for real-time bidirectional communication between the browser and server. The same architecture can be adapted for other transports like telephony systems (Twilio, Vonage) or WebRTC connections.\n\n### Architecture\n\nThe demo implements a streaming pipeline where each stage processes data asynchronously:\n\n**Client (Browser)**\n- Captures microphone audio and encodes it as PCM\n- Establishes WebSocket connection to the backend server\n- Streams audio chunks to the server in real-time\n- Receives and plays back synthesized speech audio\n\n:::python\n**Server (Python)**\n:::\n:::js\n**Server (Node.js)**\n:::\n\n- Accepts WebSocket connections from clients\n- Orchestrates the three-step pipeline:\n  - [Speech-to-text (STT)](#1-speech-to-text): Forwards audio to the STT provider (e.g., AssemblyAI), receives transcript events\n  - [Agent](#2-langchain-agent): Processes transcripts with LangChain agent, streams response tokens\n  - [Text-to-speech (TTS)](#3-text-to-speech): Sends agent responses to the TTS provider (e.g., Cartesia), receives audio chunks\n\n- Returns synthesized audio to the client for playback\n\n:::python\nThe pipeline uses async generators to enable streaming at each stage. This allows downstream components to begin processing before upstream stages complete, minimizing end-to-end latency.\n:::\n:::js\nThe pipeline uses async iterators to enable streaming at each stage. This allows downstream components to begin processing before upstream stages complete, minimizing end-to-end latency.\n:::\n\n## Setup\n\nFor detailed installation instructions and setup, see the [repository README](https://github.com/langchain-ai/voice-sandwich-demo#readme).\n\n## 1. Speech-to-text\n\nThe STT stage transforms an incoming audio stream into text transcripts. The implementation uses a producer-consumer pattern to handle audio streaming and transcript reception concurrently.\n\n### Key concepts\n\n**Producer-Consumer Pattern**: Audio", "metadata": {"source": "voice-agent.mdx"}}
{"text": "), receives audio chunks\n\n- Returns synthesized audio to the client for playback\n\n:::python\nThe pipeline uses async generators to enable streaming at each stage. This allows downstream components to begin processing before upstream stages complete, minimizing end-to-end latency.\n:::\n:::js\nThe pipeline uses async iterators to enable streaming at each stage. This allows downstream components to begin processing before upstream stages complete, minimizing end-to-end latency.\n:::\n\n## Setup\n\nFor detailed installation instructions and setup, see the [repository README](https://github.com/langchain-ai/voice-sandwich-demo#readme).\n\n## 1. Speech-to-text\n\nThe STT stage transforms an incoming audio stream into text transcripts. The implementation uses a producer-consumer pattern to handle audio streaming and transcript reception concurrently.\n\n### Key concepts\n\n**Producer-Consumer Pattern**: Audio chunks are sent to the STT service concurrently with receiving transcript events. This allows transcription to begin before all audio has arrived.\n\n**Event Types**:\n- `stt_chunk`: Partial transcripts provided as the STT service processes audio\n- `stt_output`: Final, formatted transcripts that trigger agent processing\n\n**WebSocket Connection**: Maintains a persistent connection to AssemblyAI's real-time STT API, configured for 16kHz PCM audio with automatic turn formatting.\n\n### Implementation\n\n:::python\n```python\nfrom typing import AsyncIterator\nimport asyncio\nfrom assemblyai_stt import AssemblyAISTT\nfrom events import VoiceAgentEvent\n\nasync def stt_stream(\n    audio_stream: AsyncIterator[bytes],\n) -> AsyncIterator[VoiceAgentEvent]:\n    \"\"\"\n    Transform stream: Audio (Bytes) \u2192 Voice Events (VoiceAgentEvent)\n\n    Uses a producer-consumer pattern where:\n    - Producer: Reads audio chunks and sends them to AssemblyAI\n    - Consumer: Receives transcription events from AssemblyAI\n    \"\"\"\n    stt = AssemblyAISTT(sample_rate=16000)\n\n    async def send_audio():\n        \"\"\"Background task that pumps audio chunks to AssemblyAI.\"\"\"\n        try:\n            async for audio_chunk in audio_stream:\n                await stt.send_audio(audio_chunk)\n        finally:\n            # Signal completion when audio stream ends\n            await stt.close()\n\n    # Launch audio sending in background\n    send_task = asyncio.create_task(send_audio())\n\n    try:\n        # Receive and yield transcription events as they arrive\n        async for event in stt.receive_events():\n            yield event\n    finally:\n        # Cleanup\n        with contextlib.suppress(asyncio.CancelledError):\n            send_task.cancel()\n            await send_task\n        await stt.close()\n```\n:::\n\n::", "metadata": {"source": "voice-agent.mdx"}}
{"text": "         await stt.close()\n\n    # Launch audio sending in background\n    send_task = asyncio.create_task(send_audio())\n\n    try:\n        # Receive and yield transcription events as they arrive\n        async for event in stt.receive_events():\n            yield event\n    finally:\n        # Cleanup\n        with contextlib.suppress(asyncio.CancelledError):\n            send_task.cancel()\n            await send_task\n        await stt.close()\n```\n:::\n\n:::js\n```typescript\nimport { AssemblyAISTT } from \"./assemblyai\";\nimport type { VoiceAgentEvent } from \"./types\";\n\nasync function* sttStream(\n  audioStream: AsyncIterable<Uint8Array>\n): AsyncGenerator<VoiceAgentEvent> {\n  const stt = new AssemblyAISTT({ sampleRate: 16000 });\n  const passthrough = writableIterator<VoiceAgentEvent>();\n\n  // Producer: pump audio chunks to AssemblyAI\n  const producer = (async () => {\n    try {\n      for await (const audioChunk of audioStream) {\n        await stt.sendAudio(audioChunk);\n      }\n    } finally {\n      await stt.close();\n    }\n  })();\n\n  // Consumer: receive transcription events\n  const consumer = (async () => {\n    for await (const event of stt.receiveEvents()) {\n      passthrough.push(event);\n    }\n  })();\n\n  try {\n    // Yield events as they arrive\n    yield* passthrough;\n  } finally {\n    // Wait for producer and consumer to complete\n    await Promise.all([producer, consumer]);\n  }\n}\n```\n:::\n\nThe application implements an AssemblyAI client to manage the WebSocket connection and message parsing. See below for implementations; similar adapters can be constructed for other STT providers.\n\n<Accordion title=\"AssemblyAI Client\">\n\n:::python\n```python\nclass AssemblyAISTT:\n    def __init__(self, api_key: str | None = None, sample_rate: int = 16000):\n        self.api_key = api_key or os.getenv(\"ASSEMBLYAI_API_KEY\")\n        self.sample_rate = sample_rate\n        self._ws: WebSocketClientProtocol | None = None\n\n    async def send_audio(self, audio_chunk: bytes) -> None:\n        \"\"\"Send PCM audio bytes to AssemblyAI.\"\"\"\n        ws = await self._ensure_connection()\n        await ws.send(audio_chunk)\n\n    async def receive_events(self) -> AsyncIterator[STTEvent]:\n", "metadata": {"source": "voice-agent.mdx"}}
{"text": "   def __init__(self, api_key: str | None = None, sample_rate: int = 16000):\n        self.api_key = api_key or os.getenv(\"ASSEMBLYAI_API_KEY\")\n        self.sample_rate = sample_rate\n        self._ws: WebSocketClientProtocol | None = None\n\n    async def send_audio(self, audio_chunk: bytes) -> None:\n        \"\"\"Send PCM audio bytes to AssemblyAI.\"\"\"\n        ws = await self._ensure_connection()\n        await ws.send(audio_chunk)\n\n    async def receive_events(self) -> AsyncIterator[STTEvent]:\n        \"\"\"Yield STT events as they arrive from AssemblyAI.\"\"\"\n        async for raw_message in self._ws:\n            message = json.loads(raw_message)\n\n            if message[\"type\"] == \"Turn\":\n                # Final formatted transcript\n                if message.get(\"turn_is_formatted\"):\n                    yield STTOutputEvent.create(message[\"transcript\"])\n                # Partial transcript\n                else:\n                    yield STTChunkEvent.create(message[\"transcript\"])\n\n    async def _ensure_connection(self) -> WebSocketClientProtocol:\n        \"\"\"Establish WebSocket connection if not already connected.\"\"\"\n        if self._ws is None:\n            url = f\"wss://streaming.assemblyai.com/v3/ws?sample_rate={self.sample_rate}&format_turns=true\"\n            self._ws = await websockets.connect(\n                url,\n                additional_headers={\"Authorization\": self.api_key}\n            )\n        return self._ws\n```\n:::\n\n:::js\n```typescript\nexport class AssemblyAISTT {\n  protected _bufferIterator = writableIterator<VoiceAgentEvent.STTEvent>();\n  protected _connectionPromise: Promise<WebSocket> | null = null;\n\n  async sendAudio(buffer: Uint8Array): Promise<void> {\n    const conn = await this._connection;\n    conn.send(buffer);\n  }\n\n  async *receiveEvents(): AsyncGenerator<VoiceAgentEvent.STTEvent> {\n    yield* this._bufferIterator;\n  }\n\n  protected get _connection(): Promise<WebSocket> {\n    if (this", "metadata": {"source": "voice-agent.mdx"}}
{"text": "_headers={\"Authorization\": self.api_key}\n            )\n        return self._ws\n```\n:::\n\n:::js\n```typescript\nexport class AssemblyAISTT {\n  protected _bufferIterator = writableIterator<VoiceAgentEvent.STTEvent>();\n  protected _connectionPromise: Promise<WebSocket> | null = null;\n\n  async sendAudio(buffer: Uint8Array): Promise<void> {\n    const conn = await this._connection;\n    conn.send(buffer);\n  }\n\n  async *receiveEvents(): AsyncGenerator<VoiceAgentEvent.STTEvent> {\n    yield* this._bufferIterator;\n  }\n\n  protected get _connection(): Promise<WebSocket> {\n    if (this._connectionPromise) return this._connectionPromise;\n\n    this._connectionPromise = new Promise((resolve, reject) => {\n      const params = new URLSearchParams({\n        sample_rate: this.sampleRate.toString(),\n        format_turns: \"true\",\n      });\n      const url = `wss://streaming.assemblyai.com/v3/ws?${params}`;\n      const ws = new WebSocket(url, {\n        headers: { Authorization: this.apiKey },\n      });\n\n      ws.on(\"open\", () => resolve(ws));\n\n      ws.on(\"message\", (data) => {\n        const message = JSON.parse(data.toString());\n        if (message.type === \"Turn\") {\n          if (message.turn_is_formatted) {\n            this._bufferIterator.push({\n              type: \"stt_output\",\n              transcript: message.transcript,\n              ts: Date.now()\n            });\n          } else {\n            this._bufferIterator.push({\n              type: \"stt_chunk\",\n              transcript: message.transcript,\n              ts: Date.now()\n            });\n          }\n        }\n      });\n    });\n\n    return this._connectionPromise;\n  }\n}\n```\n:::\n\n</Accordion>\n\n## 2. LangChain agent\n\nThe agent stage processes text transcripts through a LangChain [agent](/oss/langchain/agents) and streams the response tokens. In this case, we stream all [text content blocks](/oss/langchain/messages#textcontentblock) generated by the agent.\n\n### Key concepts\n\n", "metadata": {"source": "voice-agent.mdx"}}
{"text": "   type: \"stt_chunk\",\n              transcript: message.transcript,\n              ts: Date.now()\n            });\n          }\n        }\n      });\n    });\n\n    return this._connectionPromise;\n  }\n}\n```\n:::\n\n</Accordion>\n\n## 2. LangChain agent\n\nThe agent stage processes text transcripts through a LangChain [agent](/oss/langchain/agents) and streams the response tokens. In this case, we stream all [text content blocks](/oss/langchain/messages#textcontentblock) generated by the agent.\n\n### Key concepts\n\n**Streaming Responses**: The agent uses [`stream_mode=\"messages\"`](/oss/langchain/streaming#llm-tokens) to emit response tokens as they're generated, rather than waiting for the complete response. This enables the TTS stage to begin synthesis immediately.\n\n**Conversation Memory**: A [checkpointer](/oss/langchain/short-term-memory) maintains conversation state across turns using a unique thread ID. This allows the agent to reference previous exchanges in the conversation.\n\n### Implementation\n\n:::python\n```python\nfrom uuid import uuid4\nfrom langchain.agents import create_agent\nfrom langchain.messages import HumanMessage\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n# Define agent tools\ndef add_to_order(item: str, quantity: int) -> str:\n    \"\"\"Add an item to the customer's sandwich order.\"\"\"\n    return f\"Added {quantity} x {item} to the order.\"\n\ndef confirm_order(order_summary: str) -> str:\n    \"\"\"Confirm the final order with the customer.\"\"\"\n    return f\"Order confirmed: {order_summary}. Sending to kitchen.\"\n\n# Create agent with tools and memory\nagent = create_agent(\n    model=\"anthropic:claude-haiku-4-5\",  # Select your model\n    tools=[add_to_order, confirm_order],\n    system_prompt=\"\"\"You are a helpful sandwich shop assistant.\n    Your goal is to take the user's order. Be concise and friendly.\n    Do NOT use emojis, special characters, or markdown.\n    Your responses will be read by a text-to-speech engine.\"\"\",\n    checkpointer=InMemorySaver(),\n)\n\nasync def agent_stream(\n    event_stream: AsyncIterator[VoiceAgentEvent],\n) -> AsyncIterator[VoiceAgentEvent]:\n    \"\"\"\n    Transform stream: Voice Events \u2192 Voice Events (with Agent Responses)\n\n    Passes through all upstream events and adds agent_chunk events\n    when processing STT transcripts.\n    \"\"\"\n    # Generate unique thread ID for conversation memory\n    thread_id = str(uuid4())\n\n    async for event in event_stream:\n        # Pass through all upstream events\n        yield event\n\n        # Process final transcripts through the agent\n    ", "metadata": {"source": "voice-agent.mdx"}}
{"text": " be read by a text-to-speech engine.\"\"\",\n    checkpointer=InMemorySaver(),\n)\n\nasync def agent_stream(\n    event_stream: AsyncIterator[VoiceAgentEvent],\n) -> AsyncIterator[VoiceAgentEvent]:\n    \"\"\"\n    Transform stream: Voice Events \u2192 Voice Events (with Agent Responses)\n\n    Passes through all upstream events and adds agent_chunk events\n    when processing STT transcripts.\n    \"\"\"\n    # Generate unique thread ID for conversation memory\n    thread_id = str(uuid4())\n\n    async for event in event_stream:\n        # Pass through all upstream events\n        yield event\n\n        # Process final transcripts through the agent\n        if event.type == \"stt_output\":\n            # Stream agent response with conversation context\n            stream = agent.astream(\n                {\"messages\": [HumanMessage(content=event.transcript)]},\n                {\"configurable\": {\"thread_id\": thread_id}},\n                stream_mode=\"messages\",\n            )\n\n            # Yield agent response chunks as they arrive\n            async for message, _ in stream:\n                if message.text:\n                    yield AgentChunkEvent.create(message.text)\n```\n:::\n\n:::js\n```typescript\nimport { createAgent } from \"langchain\";\nimport { HumanMessage } from \"@langchain/core/messages\";\nimport { MemorySaver } from \"@langchain/langgraph\";\nimport { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\nimport { v4 as uuidv4 } from \"uuid\";\n\n// Define agent tools\nconst addToOrder = tool(\n  async ({ item, quantity }) => {\n    return `Added ${quantity} x ${item} to the order.`;\n  },\n  {\n    name: \"add_to_order\",\n    description: \"Add an item to the customer's sandwich order.\",\n    schema: z.object({\n      item: z.string(),\n      quantity: z.number(),\n    }),\n  }\n);\n\nconst confirmOrder = tool(\n  async ({ orderSummary }) => {\n    return `Order confirmed: ${orderSummary}. Sending to kitchen.`;\n  },\n  {\n    name: \"confirm_order\",\n    description: \"Confirm the final order with the customer.\",\n    schema: z.object({\n      orderSummary: z.string().describe(\"Summary of the order\"),\n    }),\n  }\n);\n\n// Create agent with tools and memory\nconst agent = createAgent({\n ", "metadata": {"source": "voice-agent.mdx"}}
{"text": ": \"add_to_order\",\n    description: \"Add an item to the customer's sandwich order.\",\n    schema: z.object({\n      item: z.string(),\n      quantity: z.number(),\n    }),\n  }\n);\n\nconst confirmOrder = tool(\n  async ({ orderSummary }) => {\n    return `Order confirmed: ${orderSummary}. Sending to kitchen.`;\n  },\n  {\n    name: \"confirm_order\",\n    description: \"Confirm the final order with the customer.\",\n    schema: z.object({\n      orderSummary: z.string().describe(\"Summary of the order\"),\n    }),\n  }\n);\n\n// Create agent with tools and memory\nconst agent = createAgent({\n  model: \"claude-haiku-4-5\",\n  tools: [addToOrder, confirmOrder],\n  checkpointer: new MemorySaver(),\n  systemPrompt: `You are a helpful sandwich shop assistant.\nYour goal is to take the user's order. Be concise and friendly.\nDo NOT use emojis, special characters, or markdown.\nYour responses will be read by a text-to-speech engine.`,\n});\n\nasync function* agentStream(\n  eventStream: AsyncIterable<VoiceAgentEvent>\n): AsyncGenerator<VoiceAgentEvent> {\n  // Generate unique thread ID for conversation memory\n  const threadId = uuidv4();\n\n  for await (const event of eventStream) {\n    // Pass through all upstream events\n    yield event;\n\n    // Process final transcripts through the agent\n    if (event.type === \"stt_output\") {\n      const stream = await agent.stream(\n        { messages: [new HumanMessage(event.transcript)] },\n        {\n          configurable: { thread_id: threadId },\n          streamMode: \"messages\",\n        }\n      );\n\n      // Yield agent response chunks as they arrive\n      for await (const [message] of stream) {\n        yield { type: \"agent_chunk\", text: message.text, ts: Date.now() };\n      }\n    }\n  }\n}\n```\n:::\n\n## 3. Text-to-speech\n\nThe TTS stage synthesizes agent response text into audio and streams it back to the client. Like the STT stage, it uses a producer-consumer pattern to handle concurrent text sending and audio reception.\n\n### Key concepts\n\n**Concurrent Processing**: The implementation merges two async streams:\n- **Upstream processing**: Passes through all events and sends agent text chunks to the TTS provider\n- **Audio reception**: Receives synthesized audio chunks from the TTS provider\n\n**Streaming TTS**: Some providers (such as [Cartesia](https://cartesia.ai/)) begin synthesizing audio as soon as it receives text, enabling audio playback to start before the agent finishes generating its complete response.\n\n**Event Passthrough**: All upstream events flow through unchanged, allowing the client or other observers to track the full pipeline state.\n\n### Implementation", "metadata": {"source": "voice-agent.mdx"}}
{"text": "\n## 3. Text-to-speech\n\nThe TTS stage synthesizes agent response text into audio and streams it back to the client. Like the STT stage, it uses a producer-consumer pattern to handle concurrent text sending and audio reception.\n\n### Key concepts\n\n**Concurrent Processing**: The implementation merges two async streams:\n- **Upstream processing**: Passes through all events and sends agent text chunks to the TTS provider\n- **Audio reception**: Receives synthesized audio chunks from the TTS provider\n\n**Streaming TTS**: Some providers (such as [Cartesia](https://cartesia.ai/)) begin synthesizing audio as soon as it receives text, enabling audio playback to start before the agent finishes generating its complete response.\n\n**Event Passthrough**: All upstream events flow through unchanged, allowing the client or other observers to track the full pipeline state.\n\n### Implementation\n\n:::python\n```python\nfrom cartesia_tts import CartesiaTTS\nfrom utils import merge_async_iters\n\nasync def tts_stream(\n    event_stream: AsyncIterator[VoiceAgentEvent],\n) -> AsyncIterator[VoiceAgentEvent]:\n    \"\"\"\n    Transform stream: Voice Events \u2192 Voice Events (with Audio)\n\n    Merges two concurrent streams:\n    1. process_upstream(): passes through events and sends text to Cartesia\n    2. tts.receive_events(): yields audio chunks from Cartesia\n    \"\"\"\n    tts = CartesiaTTS()\n\n    async def process_upstream() -> AsyncIterator[VoiceAgentEvent]:\n        \"\"\"Process upstream events and send agent text to Cartesia.\"\"\"\n        async for event in event_stream:\n            # Pass through all events\n            yield event\n            # Send agent text to Cartesia for synthesis\n            if event.type == \"agent_chunk\":\n                await tts.send_text(event.text)\n\n    try:\n        # Merge upstream events with TTS audio events\n        # Both streams run concurrently\n        async for event in merge_async_iters(\n            process_upstream(),\n            tts.receive_events()\n        ):\n            yield event\n    finally:\n        await tts.close()\n```\n:::\n\n:::js\n```typescript\nimport { CartesiaTTS } from \"./cartesia\";\n\nasync function* ttsStream(\n  eventStream: AsyncIterable<VoiceAgentEvent>\n): AsyncGenerator<VoiceAgentEvent> {\n  const tts = new CartesiaTTS();\n  const passthrough = writableIterator<VoiceAgentEvent>();\n\n  // Producer: read upstream events and send text to Cartesia\n  const producer = (async () => {\n    try {\n      for await (", "metadata": {"source": "voice-agent.mdx"}}
{"text": "           tts.receive_events()\n        ):\n            yield event\n    finally:\n        await tts.close()\n```\n:::\n\n:::js\n```typescript\nimport { CartesiaTTS } from \"./cartesia\";\n\nasync function* ttsStream(\n  eventStream: AsyncIterable<VoiceAgentEvent>\n): AsyncGenerator<VoiceAgentEvent> {\n  const tts = new CartesiaTTS();\n  const passthrough = writableIterator<VoiceAgentEvent>();\n\n  // Producer: read upstream events and send text to Cartesia\n  const producer = (async () => {\n    try {\n      for await (const event of eventStream) {\n        passthrough.push(event);\n        if (event.type === \"agent_chunk\") {\n          await tts.sendText(event.text);\n        }\n      }\n    } finally {\n      await tts.close();\n    }\n  })();\n\n  // Consumer: receive audio from Cartesia\n  const consumer = (async () => {\n    for await (const event of tts.receiveEvents()) {\n      passthrough.push(event);\n    }\n  })();\n\n  try {\n    // Yield events from both producer and consumer\n    yield* passthrough;\n  } finally {\n    await Promise.all([producer, consumer]);\n  }\n}\n```\n:::\n\nThe application implements an Cartesia client to manage the WebSocket connection and audio streaming. See below for implementations; similar adapters can be constructed for other TTS providers.\n\n<Accordion title=\"Cartesia Client\">\n\n:::python\n```python\nimport base64\nimport json\nimport websockets\n\nclass CartesiaTTS:\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        voice_id: str = \"f6ff7c0c-e396-40a9-a70b-f7607edb6937\",\n        model_id: str = \"sonic-3\",\n        sample_rate: int = 24000,\n        encoding: str = \"pcm_s16le\",\n    ):\n        self.api_key = api_key or os.getenv(\"CARTESIA_API_KEY\")\n        self.voice_id = voice_id\n        self.model_id = model_id\n        self.sample_rate = sample_rate\n        self.encoding = encoding\n        self._ws: WebSocketClientProtocol | None = None\n\n    def _generate_context_id(self) -> str:\n        \"\"\"Generate a valid", "metadata": {"source": "voice-agent.mdx"}}
{"text": ": str = \"sonic-3\",\n        sample_rate: int = 24000,\n        encoding: str = \"pcm_s16le\",\n    ):\n        self.api_key = api_key or os.getenv(\"CARTESIA_API_KEY\")\n        self.voice_id = voice_id\n        self.model_id = model_id\n        self.sample_rate = sample_rate\n        self.encoding = encoding\n        self._ws: WebSocketClientProtocol | None = None\n\n    def _generate_context_id(self) -> str:\n        \"\"\"Generate a valid context_id for Cartesia.\"\"\"\n        timestamp = int(time.time() * 1000)\n        counter = self._context_counter\n        self._context_counter += 1\n        return f\"ctx_{timestamp}_{counter}\"\n\n    async def send_text(self, text: str | None) -> None:\n        \"\"\"Send text to Cartesia for synthesis.\"\"\"\n        if not text or not text.strip():\n            return\n\n        ws = await self._ensure_connection()\n        payload = {\n            \"model_id\": self.model_id,\n            \"transcript\": text,\n            \"voice\": {\n                \"mode\": \"id\",\n                \"id\": self.voice_id,\n            },\n            \"output_format\": {\n                \"container\": \"raw\",\n                \"encoding\": self.encoding,\n                \"sample_rate\": self.sample_rate,\n            },\n            \"language\": self.language,\n            \"context_id\": self._generate_context_id(),\n        }\n        await ws.send(json.dumps(payload))\n\n    async def receive_events(self) -> AsyncIterator[TTSChunkEvent]:\n        \"\"\"Yield audio chunks as they arrive from Cartesia.\"\"\"\n        async for raw_message in self._ws:\n            message = json.loads(raw_message)\n\n            # Decode and yield audio chunks\n            if \"data\" in message and message[\"", "metadata": {"source": "voice-agent.mdx"}}
{"text": "            \"language\": self.language,\n            \"context_id\": self._generate_context_id(),\n        }\n        await ws.send(json.dumps(payload))\n\n    async def receive_events(self) -> AsyncIterator[TTSChunkEvent]:\n        \"\"\"Yield audio chunks as they arrive from Cartesia.\"\"\"\n        async for raw_message in self._ws:\n            message = json.loads(raw_message)\n\n            # Decode and yield audio chunks\n            if \"data\" in message and message[\"data\"]:\n                audio_chunk = base64.b64decode(message[\"data\"])\n                if audio_chunk:\n                    yield TTSChunkEvent.create(audio_chunk)\n\n    async def _ensure_connection(self) -> WebSocketClientProtocol:\n        \"\"\"Establish WebSocket connection if not already connected.\"\"\"\n        if self._ws is None:\n            url = (\n                f\"wss://api.cartesia.ai/tts/websocket\"\n                f\"?api_key={self.api_key}&cartesia_version={self.cartesia_version}\"\n            )\n            self._ws = await websockets.connect(url)\n\n        return self._ws\n```\n:::\n\n:::js\n```typescript\nexport class CartesiaTTS {\n  protected _bufferIterator = writableIterator<VoiceAgentEvent.TTSEvent>();\n  protected _connectionPromise: Promise<WebSocket> | null = null;\n\n  async sendText(text: string | null): Promise<void> {\n    if (!text || !text.trim()) return;\n\n    const conn = await this._connection;\n    const payload = { text, try_trigger_generation: false };\n    conn.send(JSON.stringify(payload));\n  }\n\n  async *receiveEvents(): AsyncGenerator<VoiceAgentEvent.TTSEvent> {\n    yield* this._bufferIterator;\n  }\n\n  protected _generateContextId(): string {\n    const timestamp = Date.now();\n    const counter = this._contextCounter++;\n    return `ctx_${timestamp}_${counter}`;\n  }\n\n  protected get _connection(): Promise<WebSocket> {\n    if (this._connectionPromise) return this._connectionPromise;\n\n    this._connectionPromise = new Promise((resolve, reject) => {\n      const params = new URLSearchParams({\n  ", "metadata": {"source": "voice-agent.mdx"}}
{"text": ";\n    const payload = { text, try_trigger_generation: false };\n    conn.send(JSON.stringify(payload));\n  }\n\n  async *receiveEvents(): AsyncGenerator<VoiceAgentEvent.TTSEvent> {\n    yield* this._bufferIterator;\n  }\n\n  protected _generateContextId(): string {\n    const timestamp = Date.now();\n    const counter = this._contextCounter++;\n    return `ctx_${timestamp}_${counter}`;\n  }\n\n  protected get _connection(): Promise<WebSocket> {\n    if (this._connectionPromise) return this._connectionPromise;\n\n    this._connectionPromise = new Promise((resolve, reject) => {\n      const params = new URLSearchParams({\n        api_key: this.apiKey,\n        cartesia_version: this.cartesiaVersion,\n      });\n      const url = `wss://api.cartesia.ai/tts/websocket?${params.toString()}`;\n      const ws = new WebSocket(url);\n\n      ws.on(\"open\", () => {\n        resolve(ws);\n      });\n\n      ws.on(\"message\", (data: WebSocket.RawData) => {\n        const message: CartesiaTTSResponse = JSON.parse(data.toString());\n        if (message.data) {\n          this._bufferIterator.push({\n            type: \"tts_chunk\",\n            audio: message.data,\n            ts: Date.now(),\n          });\n        } else if (message.error) {\n          throw new Error(`Cartesia error: ${message.error}`);\n        }\n      });\n    });\n\n    return this._connectionPromise;\n  }\n}\n```\n:::\n</Accordion>\n\n\n## Putting it all together\n\nThe complete pipeline chains the three stages together:\n\n:::python\n```python\nfrom langchain_core.runnables import RunnableGenerator\n\npipeline = (\n    RunnableGenerator(stt_stream)      # Audio \u2192 STT events\n    | RunnableGenerator(agent_stream)  # STT events \u2192 Agent events\n    | RunnableGenerator(tts_stream)    # Agent events \u2192 TTS audio\n)\n\n# Use in WebSocket endpoint\n@app.websocket(\"/ws\")\nasync def websocket_endpoint(websocket: WebSocket):\n    await websocket.accept()\n\n    async def websocket_audio_stream():\n        \"\"\"Yield audio bytes from WebSocket.\"\"\"\n        while True:\n          ", "metadata": {"source": "voice-agent.mdx"}}
{"text": "python\nfrom langchain_core.runnables import RunnableGenerator\n\npipeline = (\n    RunnableGenerator(stt_stream)      # Audio \u2192 STT events\n    | RunnableGenerator(agent_stream)  # STT events \u2192 Agent events\n    | RunnableGenerator(tts_stream)    # Agent events \u2192 TTS audio\n)\n\n# Use in WebSocket endpoint\n@app.websocket(\"/ws\")\nasync def websocket_endpoint(websocket: WebSocket):\n    await websocket.accept()\n\n    async def websocket_audio_stream():\n        \"\"\"Yield audio bytes from WebSocket.\"\"\"\n        while True:\n            data = await websocket.receive_bytes()\n            yield data\n\n    # Transform audio through pipeline\n    output_stream = pipeline.atransform(websocket_audio_stream())\n\n    # Send TTS audio back to client\n    async for event in output_stream:\n        if event.type == \"tts_chunk\":\n            await websocket.send_bytes(event.audio)\n```\n\nWe use [RunnableGenerators](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.base.RunnableGenerator) to compose each step of the pipeline. This is an abstraction LangChain uses internally to manage [streaming across components](https://reference.langchain.com/python/langchain_core/runnables/).\n:::\n\n:::js\n```typescript\n// using https://hono.dev/\napp.get(\"/ws\", upgradeWebSocket(async () => {\n  const inputStream = writableIterator<Uint8Array>();\n\n  // Chain the three stages\n  const transcriptEventStream = sttStream(inputStream);\n  const agentEventStream = agentStream(transcriptEventStream);\n  const outputEventStream = ttsStream(agentEventStream);\n\n  // Process pipeline and send TTS audio to client\n  const flushPromise = (async () => {\n    for await (const event of outputEventStream) {\n      if (event.type === \"tts_chunk\") {\n        currentSocket?.send(event.audio);\n      }\n    }\n  })();\n\n  return {\n    onMessage(event) {\n      // Push incoming audio into pipeline\n      const data = event.data;\n      if (Buffer.isBuffer(data)) {\n        inputStream.push(new Uint8Array(data));\n      }\n    },\n    async onClose() {\n      inputStream.cancel();\n      await flushPromise;\n    },\n  };\n}));\n```\n:::\n\nEach stage processes events independently and concurrently: audio transcription begins as soon as audio arrives, the agent starts reasoning as soon as a transcript is available, and speech synthesis begins as soon as agent", "metadata": {"source": "voice-agent.mdx"}}
{"text": ".send(event.audio);\n      }\n    }\n  })();\n\n  return {\n    onMessage(event) {\n      // Push incoming audio into pipeline\n      const data = event.data;\n      if (Buffer.isBuffer(data)) {\n        inputStream.push(new Uint8Array(data));\n      }\n    },\n    async onClose() {\n      inputStream.cancel();\n      await flushPromise;\n    },\n  };\n}));\n```\n:::\n\nEach stage processes events independently and concurrently: audio transcription begins as soon as audio arrives, the agent starts reasoning as soon as a transcript is available, and speech synthesis begins as soon as agent text is generated. This architecture can achieve sub-700ms latency to support natural conversation.\n\nFor more on building agents with LangChain, see the [Agents guide](/oss/langchain/agents).\n", "metadata": {"source": "voice-agent.mdx"}}
{"text": "---\ntitle: Overview\ndescription: Control and customize agent execution at every step\n---\n\nMiddleware provides a way to more tightly control what happens inside the agent. Middleware is useful for the following:\n\n- Tracking agent behavior with logging, analytics, and debugging.\n- Transforming prompts, [tool selection](/oss/langchain/middleware/built-in#llm-tool-selector), and output formatting.\n- Adding [retries](/oss/langchain/middleware/built-in#tool-retry), [fallbacks](/oss/langchain/middleware/built-in#model-fallback), and early termination logic.\n- Applying [rate limits](/oss/langchain/middleware/built-in#model-call-limit), guardrails, and [PII detection](/oss/langchain/middleware/built-in#pii-detection).\n\n:::python\nAdd middleware by passing them to @[`create_agent`]:\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import SummarizationMiddleware, HumanInTheLoopMiddleware\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[...],\n    middleware=[\n        SummarizationMiddleware(...),\n        HumanInTheLoopMiddleware(...)\n    ],\n)\n```\n:::\n\n:::js\nAdd middleware by passing them to `createAgent`:\n\n```typescript\nimport {\n  createAgent,\n  summarizationMiddleware,\n  humanInTheLoopMiddleware,\n} from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [...],\n  middleware: [summarizationMiddleware, humanInTheLoopMiddleware],\n});\n```\n:::\n\n## The agent loop\n\nThe core agent loop involves calling a model, letting it choose tools to execute, and then finishing when it calls no more tools:\n\n<img\n    src=\"/oss/images/core_agent_loop.png\"\n    alt=\"Core agent loop diagram\"\n    style={{height: \"200px\", width: \"auto\", justifyContent: \"center\"}}\n    className=\"rounded-lg block mx-auto\"\n/>\n\nMiddleware exposes hooks before and after each of those steps:\n\n<img\n    src=\"/oss/images/middleware_final.png\"\n    alt=\"Middleware flow diagram\"\n    style={{height: \"300px\", width: \"auto\", justifyContent: \"center\"}}\n    className=\"rounded-lg mx-auto\"\n/>\n\n## Additional resources\n\n<CardGroup cols={2}>\n    <Card title=\"Built-in middleware\" icon=\"box\" href=\"/oss/langchain/middleware/built-in\">\n        Explore built-in middleware for common use cases.\n    </Card>\n    <Card title=\"Custom middleware\" icon=\"code\" href=\"/oss/langchain/middleware/custom\">\n        Build your own middleware with hooks and decorators.\n    </Card>\n    <Card title=\"Middleware API reference\" icon=\"book\" href=\"https://reference.", "metadata": {"source": "middleware/overview.mdx"}}
{"text": "=\"Middleware flow diagram\"\n    style={{height: \"300px\", width: \"auto\", justifyContent: \"center\"}}\n    className=\"rounded-lg mx-auto\"\n/>\n\n## Additional resources\n\n<CardGroup cols={2}>\n    <Card title=\"Built-in middleware\" icon=\"box\" href=\"/oss/langchain/middleware/built-in\">\n        Explore built-in middleware for common use cases.\n    </Card>\n    <Card title=\"Custom middleware\" icon=\"code\" href=\"/oss/langchain/middleware/custom\">\n        Build your own middleware with hooks and decorators.\n    </Card>\n    <Card title=\"Middleware API reference\" icon=\"book\" href=\"https://reference.langchain.com/python/langchain/middleware/\">\n        Complete API reference for middleware.\n    </Card>\n    <Card title=\"Testing agents\" icon=\"scale-unbalanced\" href=\"/oss/langchain/test\">\n        Test your agents with LangSmith.\n    </Card>\n</CardGroup>\n", "metadata": {"source": "middleware/overview.mdx"}}
{"text": "---\ntitle: Built-in middleware\ndescription: Prebuilt middleware for common agent use cases\n---\n\nLangChain provides prebuilt middleware for common use cases. Each middleware is production-ready and configurable for your specific needs.\n\n## Provider-agnostic middleware\n\nThe following middleware work with any LLM provider:\n\n:::python\n\n| Middleware | Description |\n|------------|-------------|\n| [Summarization](#summarization) | Automatically summarize conversation history when approaching token limits. |\n| [Human-in-the-loop](#human-in-the-loop) | Pause execution for human approval of tool calls. |\n| [Model call limit](#model-call-limit) | Limit the number of model calls to prevent excessive costs. |\n| [Tool call limit](#tool-call-limit) | Control tool execution by limiting call counts. |\n| [Model fallback](#model-fallback) | Automatically fallback to alternative models when primary fails. |\n| [PII detection](#pii-detection) | Detect and handle Personally Identifiable Information (PII). |\n| [To-do list](#to-do-list) | Equip agents with task planning and tracking capabilities. |\n| [LLM tool selector](#llm-tool-selector) | Use an LLM to select relevant tools before calling main model. |\n| [Tool retry](#tool-retry) | Automatically retry failed tool calls with exponential backoff. |\n| [Model retry](#model-retry) | Automatically retry failed model calls with exponential backoff. |\n| [LLM tool emulator](#llm-tool-emulator) | Emulate tool execution using an LLM for testing purposes. |\n| [Context editing](#context-editing) | Manage conversation context by trimming or clearing tool uses. |\n| [Shell tool](#shell-tool) | Expose a persistent shell session to agents for command execution. |\n| [File search](#file-search) | Provide Glob and Grep search tools over filesystem files. |\n\n:::\n\n:::js\n\n| Middleware | Description |\n|------------|-------------|\n| [Summarization](#summarization) | Automatically summarize conversation history when approaching token limits. |\n| [Human-in-the-loop](#human-in-the-loop) | Pause execution for human approval of tool calls. |\n| [Model call limit](#model-call-limit) | Limit the number of model calls to prevent excessive costs. |\n| [Tool call limit](#tool-call-limit) | Control tool execution by limiting call counts. |\n| [Model fallback](#model-fallback) | Automatically fallback to alternative models when primary fails. |\n| [PII detection](#pii-detection) | Detect and handle Personally Identifiable Information (PII). |\n| [To-do list](#to-do-list) | Equip agents with task planning and tracking capabilities. |\n| [LLM tool selector](#llm-tool-selector) | Use an LLM to select relevant tools before calling main model. |\n| [Tool retry](#tool-retry) | Automatically retry failed tool calls with exponential backoff. |\n| [Model retry](#model-retry) | Automatically retry failed model calls with exponential backoff. |\n| [LLM tool emulator](#llm-tool-emulator) | Emulate tool execution using an LLM for testing purposes. |\n|", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": ". |\n| [Model fallback](#model-fallback) | Automatically fallback to alternative models when primary fails. |\n| [PII detection](#pii-detection) | Detect and handle Personally Identifiable Information (PII). |\n| [To-do list](#to-do-list) | Equip agents with task planning and tracking capabilities. |\n| [LLM tool selector](#llm-tool-selector) | Use an LLM to select relevant tools before calling main model. |\n| [Tool retry](#tool-retry) | Automatically retry failed tool calls with exponential backoff. |\n| [Model retry](#model-retry) | Automatically retry failed model calls with exponential backoff. |\n| [LLM tool emulator](#llm-tool-emulator) | Emulate tool execution using an LLM for testing purposes. |\n| [Context editing](#context-editing) | Manage conversation context by trimming or clearing tool uses. |\n\n:::\n\n### Summarization\n\nAutomatically summarize conversation history when approaching token limits, preserving recent messages while compressing older context. Summarization is useful for the following:\n- Long-running conversations that exceed context windows.\n- Multi-turn dialogues with extensive history.\n- Applications where preserving full conversation context matters.\n\n:::python\n**API reference:** @[`SummarizationMiddleware`]\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import SummarizationMiddleware\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[your_weather_tool, your_calculator_tool],\n    middleware=[\n        SummarizationMiddleware(\n            model=\"gpt-4.1-mini\",\n            trigger=(\"tokens\", 4000),\n            keep=(\"messages\", 20),\n        ),\n    ],\n)\n```\n:::\n\n:::js\n```typescript\nimport { createAgent, summarizationMiddleware } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [weatherTool, calculatorTool],\n  middleware: [\n    summarizationMiddleware({\n      model: \"gpt-4.1-mini\",\n      trigger: { tokens: 4000 },\n      keep: { messages: 20 },\n    }),\n  ],\n});\n```\n:::\n\n<Accordion title=\"Configuration options\">\n\n:::python\n\n<Tip>\n    The `fraction` conditions for `trigger` and `keep` (shown below) rely on a chat model's [profile data](/oss/langchain/models#model-profiles) if using `langchain>=1.1`. If data are not available, use another condition or specify manually:\n\n    ```python\n    from langchain.chat_models import init_chat_model\n\n    custom_profile = {\n        \"max_input_tokens\": 100_000,\n        # ...\n    }\n    model = init_chat_model(\"", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": "      keep: { messages: 20 },\n    }),\n  ],\n});\n```\n:::\n\n<Accordion title=\"Configuration options\">\n\n:::python\n\n<Tip>\n    The `fraction` conditions for `trigger` and `keep` (shown below) rely on a chat model's [profile data](/oss/langchain/models#model-profiles) if using `langchain>=1.1`. If data are not available, use another condition or specify manually:\n\n    ```python\n    from langchain.chat_models import init_chat_model\n\n    custom_profile = {\n        \"max_input_tokens\": 100_000,\n        # ...\n    }\n    model = init_chat_model(\"gpt-4.1\", profile=custom_profile)\n    ```\n</Tip>\n\n<ParamField body=\"model\" type=\"string | BaseChatModel\" required>\n    Model for generating summaries. Can be a model identifier string (e.g., `'openai:gpt-4.1-mini'`) or a `BaseChatModel` instance. See @[`init_chat_model`][init_chat_model(model)] for more information.\n</ParamField>\n\n<ParamField body=\"trigger\" type=\"ContextSize | list[ContextSize] | None\">\n    Condition(s) for triggering summarization. Can be:\n\n    - A single @[`ContextSize`] tuple (specified condition must be met)\n    - A list of @[`ContextSize`] tuples (any condition must be met - OR logic)\n\n    Condition should be one of the following:\n\n    - `fraction` (float): Fraction of model's context size (0-1)\n    - `tokens` (int): Absolute token count\n    - `messages` (int): Message count\n\n    At least one condition must be specified. If not provided, summarization will not trigger automatically.\n\n    See the API reference for @[`ContextSize`] for more information.\n</ParamField>\n\n<ParamField body=\"keep\" type=\"ContextSize\" default=\"('messages', 20)\">\n    How much context to preserve after summarization. Specify exactly one of:\n\n    - `fraction` (float): Fraction of model's context size to keep (0-1)\n    - `tokens` (int): Absolute token count to keep\n    - `messages` (int): Number of recent messages to keep\n\n    See the API reference for @[`ContextSize`] for more information.\n</ParamField>\n\n<ParamField body=\"token_counter\" type=\"function\">\n    Custom token counting function. Defaults to character-based counting.\n</ParamField>\n\n<ParamField body=\"summary_prompt\" type=\"string\">\n    Custom prompt template for summarization. Uses built-in template if not specified. The template should include `{messages}` placeholder where conversation history will be inserted.\n</ParamField>\n\n<ParamField body=\"trim_tokens_to_summarize\" type=\"number\" default=\"4000\">\n    Maximum number of tokens to include when generating the summary. Messages will be trimmed to fit this limit before summarization.\n</Param", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": " count to keep\n    - `messages` (int): Number of recent messages to keep\n\n    See the API reference for @[`ContextSize`] for more information.\n</ParamField>\n\n<ParamField body=\"token_counter\" type=\"function\">\n    Custom token counting function. Defaults to character-based counting.\n</ParamField>\n\n<ParamField body=\"summary_prompt\" type=\"string\">\n    Custom prompt template for summarization. Uses built-in template if not specified. The template should include `{messages}` placeholder where conversation history will be inserted.\n</ParamField>\n\n<ParamField body=\"trim_tokens_to_summarize\" type=\"number\" default=\"4000\">\n    Maximum number of tokens to include when generating the summary. Messages will be trimmed to fit this limit before summarization.\n</ParamField>\n\n<ParamField body=\"summary_prefix\" type=\"string\" deprecated>\n    **Deprecated:** Use `summary_prompt` to provide the full prompt instead.\n</ParamField>\n\n<ParamField body=\"max_tokens_before_summary\" type=\"number\" deprecated>\n    **Deprecated:** Use `trigger: (\"tokens\", value)` instead. Token threshold for triggering summarization.\n</ParamField>\n\n<ParamField body=\"messages_to_keep\" type=\"number\" deprecated>\n    **Deprecated:** Use `keep: (\"messages\", value)` instead. Recent messages to preserve.\n</ParamField>\n:::\n\n:::js\n<Tip>\n    The `fraction` conditions for `trigger` and `keep` (shown below) rely on a chat model's [profile data](/oss/langchain/models#model-profiles) if using `langchain@1.1.0`. If data are not available, use another condition or specify manually:\n    ```typescript\n    const customProfile: ModelProfile = {\n        maxInputTokens: 100_000,\n        // ...\n    }\n    model = await initChatModel(\"...\", {\n        profile: customProfile,\n    });\n    ```\n</Tip>\n\n<ParamField body=\"model\" type=\"string | BaseChatModel\" required>\n    Model for generating summaries. Can be a model identifier string (e.g., `'openai:gpt-4.1-mini'`) or a `BaseChatModel` instance.\n</ParamField>\n\n<ParamField body=\"trigger\" type=\"object | object[]\">\n    Conditions for triggering summarization. Can be:\n\n    - A single condition object (all properties must be met - AND logic)\n    - An array of condition objects (any condition must be met - OR logic)\n\n    Each condition can include:\n    - `fraction` (number): Fraction of model's context size (0-1)\n    - `tokens` (number): Absolute token count\n    - `messages` (number): Message count\n\n    At least one property must be specified per condition. If not provided, summarization will not trigger automatically.\n</ParamField>\n\n<ParamField body=\"keep\" type=\"object\" default=\"{messages: 20}\">\n    How much context to preserve after summarization. Spec", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": "ParamField>\n\n<ParamField body=\"trigger\" type=\"object | object[]\">\n    Conditions for triggering summarization. Can be:\n\n    - A single condition object (all properties must be met - AND logic)\n    - An array of condition objects (any condition must be met - OR logic)\n\n    Each condition can include:\n    - `fraction` (number): Fraction of model's context size (0-1)\n    - `tokens` (number): Absolute token count\n    - `messages` (number): Message count\n\n    At least one property must be specified per condition. If not provided, summarization will not trigger automatically.\n</ParamField>\n\n<ParamField body=\"keep\" type=\"object\" default=\"{messages: 20}\">\n    How much context to preserve after summarization. Specify exactly one of:\n\n    - `fraction` (number): Fraction of model's context size to keep (0-1)\n    - `tokens` (number): Absolute token count to keep\n    - `messages` (number): Number of recent messages to keep\n</ParamField>\n\n<ParamField body=\"tokenCounter\" type=\"function\">\n    Custom token counting function. Defaults to character-based counting.\n</ParamField>\n\n<ParamField body=\"summaryPrompt\" type=\"string\">\n    Custom prompt template for summarization. Uses built-in template if not specified. The template should include `{messages}` placeholder where conversation history will be inserted.\n</ParamField>\n\n<ParamField body=\"trimTokensToSummarize\" type=\"number\" default=\"4000\">\n    Maximum number of tokens to include when generating the summary. Messages will be trimmed to fit this limit before summarization.\n</ParamField>\n\n<ParamField body=\"summaryPrefix\" type=\"string\">\n    Prefix to add to the summary message. If not provided, a default prefix is used.\n</ParamField>\n\n<ParamField body=\"maxTokensBeforeSummary\" type=\"number\" deprecated>\n    **Deprecated:** Use `trigger: { tokens: value }` instead. Token threshold for triggering summarization.\n</ParamField>\n\n<ParamField body=\"messagesToKeep\" type=\"number\" deprecated>\n    **Deprecated:** Use `keep: { messages: value }` instead. Recent messages to preserve.\n</ParamField>\n:::\n\n</Accordion>\n\n<Accordion title=\"Full example\">\n\nThe summarization middleware monitors message token counts and automatically summarizes older messages when thresholds are reached.\n\n**Trigger conditions** control when summarization runs:\n- Single condition object (specified must be met)\n- Array of conditions (any condition must be met - OR logic)\n- Each condition can use `fraction` (of model's context size), `tokens` (absolute count), or `messages` (message count)\n\n**Keep condition** control how much context to preserve (specify exactly one):\n- `fraction` - Fraction of model's context size to keep\n- `tokens` - Absolute token count to keep\n- `messages` - Number of recent messages to keep\n\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import SummarizationMiddleware\n\n\n# Single condition: trigger if tokens >= 4000\nagent = create_agent(\n    model=\"gpt-", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": " messages when thresholds are reached.\n\n**Trigger conditions** control when summarization runs:\n- Single condition object (specified must be met)\n- Array of conditions (any condition must be met - OR logic)\n- Each condition can use `fraction` (of model's context size), `tokens` (absolute count), or `messages` (message count)\n\n**Keep condition** control how much context to preserve (specify exactly one):\n- `fraction` - Fraction of model's context size to keep\n- `tokens` - Absolute token count to keep\n- `messages` - Number of recent messages to keep\n\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import SummarizationMiddleware\n\n\n# Single condition: trigger if tokens >= 4000\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[your_weather_tool, your_calculator_tool],\n    middleware=[\n        SummarizationMiddleware(\n            model=\"gpt-4.1-mini\",\n            trigger=(\"tokens\", 4000),\n            keep=(\"messages\", 20),\n        ),\n    ],\n)\n\n# Multiple conditions: trigger if number of tokens >= 3000 OR messages >= 6\nagent2 = create_agent(\n    model=\"gpt-4.1\",\n    tools=[your_weather_tool, your_calculator_tool],\n    middleware=[\n        SummarizationMiddleware(\n            model=\"gpt-4.1-mini\",\n            trigger=[\n                (\"tokens\", 3000),\n                (\"messages\", 6),\n            ],\n            keep=(\"messages\", 20),\n        ),\n    ],\n)\n\n# Using fractional limits\nagent3 = create_agent(\n    model=\"gpt-4.1\",\n    tools=[your_weather_tool, your_calculator_tool],\n    middleware=[\n        SummarizationMiddleware(\n            model=\"gpt-4.1-mini\",\n            trigger=(\"fraction\", 0.8),\n            keep=(\"fraction\", 0.3),\n        ),\n    ],\n)\n```\n:::\n\n:::js\n```typescript\nimport { createAgent, summarizationMiddleware } from \"langchain\";\n\n// Single condition\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [weatherTool, calculatorTool],\n  middleware: [\n    summarizationMiddleware({\n      model: \"gpt-4.1-mini\",\n      trigger: { tokens: 4000", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": "izationMiddleware(\n            model=\"gpt-4.1-mini\",\n            trigger=(\"fraction\", 0.8),\n            keep=(\"fraction\", 0.3),\n        ),\n    ],\n)\n```\n:::\n\n:::js\n```typescript\nimport { createAgent, summarizationMiddleware } from \"langchain\";\n\n// Single condition\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [weatherTool, calculatorTool],\n  middleware: [\n    summarizationMiddleware({\n      model: \"gpt-4.1-mini\",\n      trigger: { tokens: 4000, messages: 10 },\n      keep: { messages: 20 },\n    }),\n  ],\n});\n\n// Multiple conditions\nconst agent2 = createAgent({\n  model: \"gpt-4.1\",\n  tools: [weatherTool, calculatorTool],\n  middleware: [\n    summarizationMiddleware({\n      model: \"gpt-4.1-mini\",\n      trigger: [\n        { tokens: 3000, messages: 6 },\n      ],\n      keep: { messages: 20 },\n    }),\n  ],\n});\n\n// Using fractional limits\nconst agent3 = createAgent({\n  model: \"gpt-4.1\",\n  tools: [weatherTool, calculatorTool],\n  middleware: [\n    summarizationMiddleware({\n      model: \"gpt-4.1-mini\",\n      trigger: { fraction: 0.8 },\n      keep: { fraction: 0.3 },\n    }),\n  ],\n});\n```\n:::\n\n</Accordion>\n\n### Human-in-the-loop\n\nPause agent execution for human approval, editing, or rejection of tool calls before they execute. [Human-in-the-loop](/oss/langchain/human-in-the-loop) is useful for the following:\n\n- High-stakes operations requiring human approval (e.g. database writes, financial transactions).\n- Compliance workflows where human oversight is mandatory.\n- Long-running conversations where human feedback guides the agent.\n\n:::python\n**API reference:** @[`HumanInTheLoopMiddleware`]\n:::\n\n<Warning>\n    Human-in-the-loop middleware requires a [checkpointer](/oss/langgraph/persistence#checkpoints) to maintain state across interruptions.\n</Warning>\n\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n\ndef read_email_tool(email_id: str) -> str:\n    \"\"\"Mock function to read an email by its ID.\"\"\"\n    return f\"Email content for ID: {email_id}\"\n\ndef send_email_tool(recipient: str, subject: str, body: str) -> str:\n    \"\"\"Mock function to send an email.\"\"\"\n   ", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": "TheLoopMiddleware`]\n:::\n\n<Warning>\n    Human-in-the-loop middleware requires a [checkpointer](/oss/langgraph/persistence#checkpoints) to maintain state across interruptions.\n</Warning>\n\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n\ndef read_email_tool(email_id: str) -> str:\n    \"\"\"Mock function to read an email by its ID.\"\"\"\n    return f\"Email content for ID: {email_id}\"\n\ndef send_email_tool(recipient: str, subject: str, body: str) -> str:\n    \"\"\"Mock function to send an email.\"\"\"\n    return f\"Email sent to {recipient} with subject '{subject}'\"\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[your_read_email_tool, your_send_email_tool],\n    checkpointer=InMemorySaver(),\n    middleware=[\n        HumanInTheLoopMiddleware(\n            interrupt_on={\n                \"your_send_email_tool\": {\n                    \"allowed_decisions\": [\"approve\", \"edit\", \"reject\"],\n                },\n                \"your_read_email_tool\": False,\n            }\n        ),\n    ],\n)\n```\n:::\n\n:::js\n```typescript\nimport { createAgent, humanInTheLoopMiddleware } from \"langchain\";\n\nfunction readEmailTool(emailId: string): string {\n  /** Mock function to read an email by its ID. */\n  return `Email content for ID: ${emailId}`;\n}\n\nfunction sendEmailTool(recipient: string, subject: string, body: string): string {\n  /** Mock function to send an email. */\n  return `Email sent to ${recipient} with subject '${subject}'`;\n}\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [readEmailTool, sendEmailTool],\n  middleware: [\n    humanInTheLoopMiddleware({\n      interruptOn: {\n        sendEmailTool: {\n          allowedDecisions: [\"approve\", \"edit\", \"reject\"],\n        },\n        readEmailTool: false,\n      }\n    })\n  ]\n});\n```\n:::\n\n<Tip>\n    For complete examples, configuration options, and integration patterns, see the [Human-in-the-loop documentation](/oss/langchain/human-in-the-loop).\n</Tip>\n\n:::python\n<Callout icon=\"circle-play\" iconType=\"solid\">\n    Watch", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": "\",\n  tools: [readEmailTool, sendEmailTool],\n  middleware: [\n    humanInTheLoopMiddleware({\n      interruptOn: {\n        sendEmailTool: {\n          allowedDecisions: [\"approve\", \"edit\", \"reject\"],\n        },\n        readEmailTool: false,\n      }\n    })\n  ]\n});\n```\n:::\n\n<Tip>\n    For complete examples, configuration options, and integration patterns, see the [Human-in-the-loop documentation](/oss/langchain/human-in-the-loop).\n</Tip>\n\n:::python\n<Callout icon=\"circle-play\" iconType=\"solid\">\n    Watch this [video guide](https://www.youtube.com/watch?v=SpfT6-YAVPk) demonstrating Human-in-the-loop middleware behavior.\n</Callout>\n:::\n\n:::js\n<Callout icon=\"circle-play\" iconType=\"solid\">\n    Watch this [video guide](https://www.youtube.com/watch?v=tdOeUVERukA) demonstrating Human-in-the-loop middleware behavior.\n</Callout>\n:::\n\n### Model call limit\n\nLimit the number of model calls to prevent infinite loops or excessive costs. Model call limit is useful for the following:\n\n- Preventing runaway agents from making too many API calls.\n- Enforcing cost controls on production deployments.\n- Testing agent behavior within specific call budgets.\n\n:::python\n**API reference:** @[`ModelCallLimitMiddleware`]\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import ModelCallLimitMiddleware\nfrom langgraph.checkpoint.memory import InMemorySaver\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    checkpointer=InMemorySaver(),  # Required for thread limiting\n    tools=[],\n    middleware=[\n        ModelCallLimitMiddleware(\n            thread_limit=10,\n            run_limit=5,\n            exit_behavior=\"end\",\n        ),\n    ],\n)\n```\n:::\n\n:::js\n```typescript\nimport { createAgent, modelCallLimitMiddleware } from \"langchain\";\nimport { MemorySaver } from \"@langchain/langgraph\";\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  checkpointer: new MemorySaver(), // Required for thread limiting\n  tools: [],\n  middleware: [\n    modelCallLimitMiddleware({\n      threadLimit: 10,\n      runLimit: 5,\n      exitBehavior: \"end\",\n    }),\n  ],\n});\n```\n:::\n\n:::python\n<Callout icon=\"circle-play\" iconType=\"solid\">\n    Watch this [video guide](https://www.youtube.com/watch?v=nJEER0uaNkE) demonstrating Model Call Limit middleware behavior", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": "\nimport { createAgent, modelCallLimitMiddleware } from \"langchain\";\nimport { MemorySaver } from \"@langchain/langgraph\";\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  checkpointer: new MemorySaver(), // Required for thread limiting\n  tools: [],\n  middleware: [\n    modelCallLimitMiddleware({\n      threadLimit: 10,\n      runLimit: 5,\n      exitBehavior: \"end\",\n    }),\n  ],\n});\n```\n:::\n\n:::python\n<Callout icon=\"circle-play\" iconType=\"solid\">\n    Watch this [video guide](https://www.youtube.com/watch?v=nJEER0uaNkE) demonstrating Model Call Limit middleware behavior.\n</Callout>\n:::\n\n:::js\n<Callout icon=\"circle-play\" iconType=\"solid\">\n    Watch this [video guide](https://www.youtube.com/watch?v=x5jLQTFXR0Y) demonstrating Model Call Limit middleware behavior.\n</Callout>\n:::\n\n<Accordion title=\"Configuration options\">\n\n:::python\n<ParamField body=\"thread_limit\" type=\"number\">\n    Maximum model calls across all runs in a thread. Defaults to no limit.\n</ParamField>\n\n<ParamField body=\"run_limit\" type=\"number\">\n    Maximum model calls per single invocation. Defaults to no limit.\n</ParamField>\n\n<ParamField body=\"exit_behavior\" type=\"string\" default=\"end\">\n    Behavior when limit is reached. Options: `'end'` (graceful termination) or `'error'` (raise exception)\n</ParamField>\n:::\n\n:::js\n<ParamField body=\"threadLimit\" type=\"number\">\n    Maximum model calls across all runs in a thread. Defaults to no limit.\n</ParamField>\n\n<ParamField body=\"runLimit\" type=\"number\">\n    Maximum model calls per single invocation. Defaults to no limit.\n</ParamField>\n\n<ParamField body=\"exitBehavior\" type=\"string\" default=\"end\">\n    Behavior when limit is reached. Options: `'end'` (graceful termination) or `'error'` (throw exception)\n</ParamField>\n:::\n\n</Accordion>\n\n\n### Tool call limit\n\nControl agent execution by limiting the number of tool calls, either globally across all tools or for specific tools. Tool call limits are useful for the following:\n\n- Preventing excessive calls to expensive external APIs.\n- Limiting web searches or database queries.\n- Enforcing rate limits on specific tool usage.\n- Protecting against runaway agent loops.\n\n:::python\n**API reference:** @[`ToolCallLimitMiddleware`]\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import ToolCallLimitMiddleware\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[search_tool, database_tool],\n    middleware=[\n        # Global limit\n        ToolCallLimitMiddleware(thread_limit=20, run_limit=10),\n        # Tool-specific limit\n", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": " or for specific tools. Tool call limits are useful for the following:\n\n- Preventing excessive calls to expensive external APIs.\n- Limiting web searches or database queries.\n- Enforcing rate limits on specific tool usage.\n- Protecting against runaway agent loops.\n\n:::python\n**API reference:** @[`ToolCallLimitMiddleware`]\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import ToolCallLimitMiddleware\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[search_tool, database_tool],\n    middleware=[\n        # Global limit\n        ToolCallLimitMiddleware(thread_limit=20, run_limit=10),\n        # Tool-specific limit\n        ToolCallLimitMiddleware(\n            tool_name=\"search\",\n            thread_limit=5,\n            run_limit=3,\n        ),\n    ],\n)\n```\n:::\n\n:::js\n```typescript\nimport { createAgent, toolCallLimitMiddleware } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [searchTool, databaseTool],\n  middleware: [\n    toolCallLimitMiddleware({ threadLimit: 20, runLimit: 10 }),\n    toolCallLimitMiddleware({\n      toolName: \"search\",\n      threadLimit: 5,\n      runLimit: 3,\n    }),\n  ],\n});\n```\n:::\n\n:::python\n<Callout icon=\"circle-play\" iconType=\"solid\">\n    Watch this [video guide](https://www.youtube.com/watch?v=6gYlaJJ8t0w) demonstrating Tool Call Limit middleware behavior.\n</Callout>\n:::\n\n:::js\n<Callout icon=\"circle-play\" iconType=\"solid\">\n    Watch this [video guide](https://www.youtube.com/watch?v=oL6am5UqODY) demonstrating Tool Call Limit middleware behavior.\n</Callout>\n:::\n\n<Accordion title=\"Configuration options\">\n\n:::python\n<ParamField body=\"tool_name\" type=\"string\">\n    Name of specific tool to limit. If not provided, limits apply to **all tools globally**.\n</ParamField>\n\n<ParamField body=\"thread_limit\" type=\"number\">\n    Maximum tool calls across all runs in a thread (conversation). Persists across multiple invocations with the same thread ID. Requires a checkpointer to maintain state. `None` means no thread limit.\n</ParamField>\n\n<ParamField body=\"run_limit\" type=\"number\">\n    Maximum tool calls per single invocation (one user message \u2192 response cycle). Resets with each new user message. `None` means no run limit.\n\n    **Note:** At least one of `thread_limit` or `run_limit` must be specified.\n</ParamField>\n\n<ParamField body=\"exit_behavior\" type=\"string\" default=\"continue\">\n   ", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": "\">\n    Name of specific tool to limit. If not provided, limits apply to **all tools globally**.\n</ParamField>\n\n<ParamField body=\"thread_limit\" type=\"number\">\n    Maximum tool calls across all runs in a thread (conversation). Persists across multiple invocations with the same thread ID. Requires a checkpointer to maintain state. `None` means no thread limit.\n</ParamField>\n\n<ParamField body=\"run_limit\" type=\"number\">\n    Maximum tool calls per single invocation (one user message \u2192 response cycle). Resets with each new user message. `None` means no run limit.\n\n    **Note:** At least one of `thread_limit` or `run_limit` must be specified.\n</ParamField>\n\n<ParamField body=\"exit_behavior\" type=\"string\" default=\"continue\">\n    Behavior when limit is reached:\n\n    - `'continue'` (default) - Block exceeded tool calls with error messages, let other tools and the model continue. The model decides when to end based on the error messages.\n    - `'error'` - Raise a `ToolCallLimitExceededError` exception, stopping execution immediately\n    - `'end'` - Stop execution immediately with a `ToolMessage` and AI message for the exceeded tool call. Only works when limiting a single tool; raises `NotImplementedError` if other tools have pending calls.\n</ParamField>\n:::\n\n:::js\n<ParamField body=\"toolName\" type=\"string\">\n    Name of specific tool to limit. If not provided, limits apply to **all tools globally**.\n</ParamField>\n\n<ParamField body=\"threadLimit\" type=\"number\">\n    Maximum tool calls across all runs in a thread (conversation). Persists across multiple invocations with the same thread ID. Requires a checkpointer to maintain state. `undefined` means no thread limit.\n</ParamField>\n\n<ParamField body=\"runLimit\" type=\"number\">\n    Maximum tool calls per single invocation (one user message \u2192 response cycle). Resets with each new user message. `undefined` means no run limit.\n\n    **Note:** At least one of `threadLimit` or `runLimit` must be specified.\n</ParamField>\n\n<ParamField body=\"exitBehavior\" type=\"string\" default=\"continue\">\n    Behavior when limit is reached:\n\n    - `'continue'` (default) - Block exceeded tool calls with error messages, let other tools and the model continue. The model decides when to end based on the error messages.\n    - `'error'` - Throw a `ToolCallLimitExceededError` exception, stopping execution immediately\n    - `'end'` - Stop execution immediately with a ToolMessage and AI message for the exceeded tool call. Only works when limiting a single tool; throws error if other tools have pending calls.\n</ParamField>\n:::\n\n</Accordion>\n\n<Accordion title=\"Full example\">\n\nSpecify limits with:\n- **Thread limit** - Max calls across all runs in a conversation (requires checkpointer)\n- **Run limit** - Max calls per single invocation (resets each turn)\n\nExit behaviors:\n- `'continue'` (default) - Block exceeded calls with error messages, agent continues\n- `'error'` - Raise exception immediately\n- `'end'` - Stop with ToolMessage + AI message (single-tool scenarios only)\n\n", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": "   - `'error'` - Throw a `ToolCallLimitExceededError` exception, stopping execution immediately\n    - `'end'` - Stop execution immediately with a ToolMessage and AI message for the exceeded tool call. Only works when limiting a single tool; throws error if other tools have pending calls.\n</ParamField>\n:::\n\n</Accordion>\n\n<Accordion title=\"Full example\">\n\nSpecify limits with:\n- **Thread limit** - Max calls across all runs in a conversation (requires checkpointer)\n- **Run limit** - Max calls per single invocation (resets each turn)\n\nExit behaviors:\n- `'continue'` (default) - Block exceeded calls with error messages, agent continues\n- `'error'` - Raise exception immediately\n- `'end'` - Stop with ToolMessage + AI message (single-tool scenarios only)\n\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import ToolCallLimitMiddleware\n\n\nglobal_limiter = ToolCallLimitMiddleware(thread_limit=20, run_limit=10)\nsearch_limiter = ToolCallLimitMiddleware(tool_name=\"search\", thread_limit=5, run_limit=3)\ndatabase_limiter = ToolCallLimitMiddleware(tool_name=\"query_database\", thread_limit=10)\nstrict_limiter = ToolCallLimitMiddleware(tool_name=\"scrape_webpage\", run_limit=2, exit_behavior=\"error\")\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[search_tool, database_tool, scraper_tool],\n    middleware=[global_limiter, search_limiter, database_limiter, strict_limiter],\n)\n```\n:::\n\n:::js\n```typescript\nimport { createAgent, toolCallLimitMiddleware } from \"langchain\";\n\nconst globalLimiter = toolCallLimitMiddleware({ threadLimit: 20, runLimit: 10 });\nconst searchLimiter = toolCallLimitMiddleware({ toolName: \"search\", threadLimit: 5, runLimit: 3 });\nconst databaseLimiter = toolCallLimitMiddleware({ toolName: \"query_database\", threadLimit: 10 });\nconst strictLimiter = toolCallLimitMiddleware({ toolName: \"scrape_webpage\", runLimit: 2, exitBehavior: \"error\" });\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [searchTool, databaseTool, scraperTool],\n  middleware: [globalLimiter, searchLimiter, databaseLimiter, strictLimiter],\n});\n```\n:::\n\n</Accordion>\n\n\n### Model fallback\n\nAutomatically fallback to alternative models when the primary model fails. Model fallback is useful for the following:\n\n- Building resilient agents that handle model outages.\n- Cost optimization by falling back to cheaper models.\n- Provider redundancy across OpenAI, Anthropic, etc.\n\n:::python\n**API reference:** @[`ModelFallbackMiddleware`]\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import ModelFallbackMiddleware\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[],\n    middleware=[\n        ModelFallbackMiddleware(", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": "ware: [globalLimiter, searchLimiter, databaseLimiter, strictLimiter],\n});\n```\n:::\n\n</Accordion>\n\n\n### Model fallback\n\nAutomatically fallback to alternative models when the primary model fails. Model fallback is useful for the following:\n\n- Building resilient agents that handle model outages.\n- Cost optimization by falling back to cheaper models.\n- Provider redundancy across OpenAI, Anthropic, etc.\n\n:::python\n**API reference:** @[`ModelFallbackMiddleware`]\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import ModelFallbackMiddleware\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[],\n    middleware=[\n        ModelFallbackMiddleware(\n            \"gpt-4.1-mini\",\n            \"claude-3-5-sonnet-20241022\",\n        ),\n    ],\n)\n```\n:::\n\n:::js\n```typescript\nimport { createAgent, modelFallbackMiddleware } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [],\n  middleware: [\n    modelFallbackMiddleware(\n      \"gpt-4.1-mini\",\n      \"claude-3-5-sonnet-20241022\"\n    ),\n  ],\n});\n```\n:::\n\n:::python\n<Callout icon=\"circle-play\" iconType=\"solid\">\n    Watch this [video guide](https://www.youtube.com/watch?v=8rCRO0DUeIM) demonstrating Model Fallback middleware behavior.\n</Callout>\n:::\n\n<Accordion title=\"Configuration options\">\n\n:::python\n<ParamField body=\"first_model\" type=\"string | BaseChatModel\" required>\n    First fallback model to try when the primary model fails. Can be a model identifier string (e.g., `'openai:gpt-4.1-mini'`) or a `BaseChatModel` instance.\n</ParamField>\n\n<ParamField body=\"*additional_models\" type=\"string | BaseChatModel\">\n    Additional fallback models to try in order if previous models fail\n</ParamField>\n:::\n\n:::js\nThe middleware accepts a variable number of string arguments representing fallback models in order:\n\n<ParamField body=\"...models\" type=\"string[]\" required>\n  One or more fallback model strings to try in order when the primary model fails\n\n  ```typescript\n  modelFallbackMiddleware(\n    \"first-fallback-model\",\n    \"second-fallback-model\",\n    // ... more models\n  )\n  ```\n</ParamField>\n:::\n\n</Accordion>\n\n### PII detection\n\nDetect and handle Personally Identifiable Information (PII) in conversations using configurable strategies. PII detection is useful for the following:\n\n- Healthcare and financial applications with compliance requirements.\n- Customer service agents that need to sanitize logs.\n- Any application handling sensitive user data.\n", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": ">\n:::\n\n:::js\nThe middleware accepts a variable number of string arguments representing fallback models in order:\n\n<ParamField body=\"...models\" type=\"string[]\" required>\n  One or more fallback model strings to try in order when the primary model fails\n\n  ```typescript\n  modelFallbackMiddleware(\n    \"first-fallback-model\",\n    \"second-fallback-model\",\n    // ... more models\n  )\n  ```\n</ParamField>\n:::\n\n</Accordion>\n\n### PII detection\n\nDetect and handle Personally Identifiable Information (PII) in conversations using configurable strategies. PII detection is useful for the following:\n\n- Healthcare and financial applications with compliance requirements.\n- Customer service agents that need to sanitize logs.\n- Any application handling sensitive user data.\n\n:::python\n**API reference:** @[`PIIMiddleware`]\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import PIIMiddleware\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[],\n    middleware=[\n        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_input=True),\n        PIIMiddleware(\"credit_card\", strategy=\"mask\", apply_to_input=True),\n    ],\n)\n```\n:::\n\n:::js\n```typescript\nimport { createAgent, piiMiddleware } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [],\n  middleware: [\n    piiMiddleware(\"email\", { strategy: \"redact\", applyToInput: true }),\n    piiMiddleware(\"credit_card\", { strategy: \"mask\", applyToInput: true }),\n  ],\n});\n```\n:::\n\n#### Custom PII types\n\nYou can create custom PII types by providing a `detector` parameter. This allows you to detect patterns specific to your use case beyond the built-in types.\n\n**Three ways to create custom detectors:**\n\n1. **Regex pattern string** - Simple pattern matching\n:::js\n1. **RegExp object** - More control over regex flags\n:::\n1. **Custom function** - Complex detection logic with validation\n\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import PIIMiddleware\nimport re\n\n\n# Method 1: Regex pattern string\nagent1 = create_agent(\n    model=\"gpt-4.1\",\n    tools=[],\n    middleware=[\n        PIIMiddleware(\n            \"api_key\",\n            detector=r\"sk-[a-zA-Z0-9]{32}\",\n            strategy=\"block\",\n        ),\n    ],\n)\n\n# Method 2: Compiled regex pattern\nagent2 = create_agent(\n    model=\"gpt-4.1\",\n    tools=[],\n    middleware=[\n   ", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": "agents.middleware import PIIMiddleware\nimport re\n\n\n# Method 1: Regex pattern string\nagent1 = create_agent(\n    model=\"gpt-4.1\",\n    tools=[],\n    middleware=[\n        PIIMiddleware(\n            \"api_key\",\n            detector=r\"sk-[a-zA-Z0-9]{32}\",\n            strategy=\"block\",\n        ),\n    ],\n)\n\n# Method 2: Compiled regex pattern\nagent2 = create_agent(\n    model=\"gpt-4.1\",\n    tools=[],\n    middleware=[\n        PIIMiddleware(\n            \"phone_number\",\n            detector=re.compile(r\"\\+?\\d{1,3}[\\s.-]?\\d{3,4}[\\s.-]?\\d{4}\"),\n            strategy=\"mask\",\n        ),\n    ],\n)\n\n# Method 3: Custom detector function\ndef detect_ssn(content: str) -> list[dict[str, str | int]]:\n    \"\"\"Detect SSN with validation.\n\n    Returns a list of dictionaries with 'text', 'start', and 'end' keys.\n    \"\"\"\n    import re\n    matches = []\n    pattern = r\"\\d{3}-\\d{2}-\\d{4}\"\n    for match in re.finditer(pattern, content):\n        ssn = match.group(0)\n        # Validate: first 3 digits shouldn't be 000, 666, or 900-999\n        first_three = int(ssn[:3])\n        if first_three not in [0, 666] and not (900 <= first_three <= 999):\n            matches.append({\n                \"text\": ssn,\n                \"start\": match.start(),\n                \"end\": match.end(),\n            })\n    return matches\n\nagent3 = create_agent(\n    model=\"gpt-4.1\",\n    tools=[],\n    middleware=[\n        PIIMiddleware(\n            \"ssn\",\n            detector=detect_ssn,\n            strategy=\"hash\",\n        ),\n    ],\n)\n```\n:::\n\n:::js\n```typescript\nimport { createAgent, piiMiddleware, type PIIMatch } from \"langchain\";\n\n// Method 1: Regex pattern string", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": "          \"end\": match.end(),\n            })\n    return matches\n\nagent3 = create_agent(\n    model=\"gpt-4.1\",\n    tools=[],\n    middleware=[\n        PIIMiddleware(\n            \"ssn\",\n            detector=detect_ssn,\n            strategy=\"hash\",\n        ),\n    ],\n)\n```\n:::\n\n:::js\n```typescript\nimport { createAgent, piiMiddleware, type PIIMatch } from \"langchain\";\n\n// Method 1: Regex pattern string\nconst agent1 = createAgent({\n  model: \"gpt-4.1\",\n  tools: [],\n  middleware: [\n    piiMiddleware(\"api_key\", {\n      detector: \"sk-[a-zA-Z0-9]{32}\",\n      strategy: \"block\",\n    }),\n  ],\n});\n\n// Method 2: RegExp object\nconst agent2 = createAgent({\n  model: \"gpt-4.1\",\n  tools: [],\n  middleware: [\n    piiMiddleware(\"phone_number\", {\n      detector: /\\+?\\d{1,3}[\\s.-]?\\d{3,4}[\\s.-]?\\d{4}/,\n      strategy: \"mask\",\n    }),\n  ],\n});\n\n// Method 3: Custom detector function\nfunction detectSSN(content: string): PIIMatch[] {\n  const matches: PIIMatch[] = [];\n  const pattern = /\\d{3}-\\d{2}-\\d{4}/g;\n  let match: RegExpExecArray | null;\n\n  while ((match = pattern.exec(content)) !== null) {\n    const ssn = match[0];\n    // Validate: first 3 digits shouldn't be 000, 666, or 900-999\n    const firstThree = parseInt(ssn.substring(0, 3), 10);\n    if (firstThree !== 0 && firstThree !== 666 && !(firstThree >= 900 && firstThree <= 999)) {\n      matches.push({\n        text: ssn,\n        start: match.index ?? 0,\n        end: (match.index ?? 0) + ssn.length,\n      });\n    }\n  }\n  return matches;\n}\n\nconst agent3 = createAgent({\n  model: \"gpt-4.1\",\n  tools: [],\n  middleware: [\n    piiMiddleware(\"ssn\", {\n      detector: detectSSN,\n      strategy: \"hash\",\n    }),\n  ],\n});\n```\n:::\n\n**Custom detector function signature:**\n\nThe detector function must accept a string (content) and return matches:\n\n:::python\nReturns a list of dictionaries with", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": "push({\n        text: ssn,\n        start: match.index ?? 0,\n        end: (match.index ?? 0) + ssn.length,\n      });\n    }\n  }\n  return matches;\n}\n\nconst agent3 = createAgent({\n  model: \"gpt-4.1\",\n  tools: [],\n  middleware: [\n    piiMiddleware(\"ssn\", {\n      detector: detectSSN,\n      strategy: \"hash\",\n    }),\n  ],\n});\n```\n:::\n\n**Custom detector function signature:**\n\nThe detector function must accept a string (content) and return matches:\n\n:::python\nReturns a list of dictionaries with `text`, `start`, and `end` keys:\n```python\ndef detector(content: str) -> list[dict[str, str | int]]:\n    return [\n        {\"text\": \"matched_text\", \"start\": 0, \"end\": 12},\n        # ... more matches\n    ]\n```\n:::\n:::js\nReturns an array of `PIIMatch` objects:\n```typescript\ninterface PIIMatch {\n  text: string;    // The matched text\n  start: number;   // Start index in content\n  end: number;      // End index in content\n}\n\nfunction detector(content: string): PIIMatch[] {\n  return [\n    { text: \"matched_text\", start: 0, end: 12 },\n    // ... more matches\n  ];\n}\n```\n:::\n\n<Tip>\n    For custom detectors:\n\n    - Use regex strings for simple patterns\n    - Use RegExp objects when you need flags (e.g., case-insensitive matching)\n    - Use custom functions when you need validation logic beyond pattern matching\n    - Custom functions give you full control over detection logic and can implement complex validation rules\n</Tip>\n\n<Accordion title=\"Configuration options\">\n\n:::python\n<ParamField body=\"pii_type\" type=\"string\" required>\n    Type of PII to detect. Can be a built-in type (`email`, `credit_card`, `ip`, `mac_address`, `url`) or a custom type name.\n</ParamField>\n\n<ParamField body=\"strategy\" type=\"string\" default=\"redact\">\n    How to handle detected PII. Options:\n\n    - `'block'` - Raise exception when detected\n    - `'redact'` - Replace with `[REDACTED_{PII_TYPE}]`\n    - `'mask'` - Partially mask (e.g., `****-****-****-1234`)\n    - `'hash'` - Replace with deterministic hash\n</ParamField>\n\n<ParamField body=\"detector\" type=\"function | regex\">\n    Custom detector function or regex pattern. If not provided, uses built-in detector for the PII type.\n</ParamField>\n\n<ParamField body=\"apply_to_input\" type=\"boolean\" default=\"True\">\n    Check user messages before model call\n</ParamField>\n\n", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": "\" type=\"string\" default=\"redact\">\n    How to handle detected PII. Options:\n\n    - `'block'` - Raise exception when detected\n    - `'redact'` - Replace with `[REDACTED_{PII_TYPE}]`\n    - `'mask'` - Partially mask (e.g., `****-****-****-1234`)\n    - `'hash'` - Replace with deterministic hash\n</ParamField>\n\n<ParamField body=\"detector\" type=\"function | regex\">\n    Custom detector function or regex pattern. If not provided, uses built-in detector for the PII type.\n</ParamField>\n\n<ParamField body=\"apply_to_input\" type=\"boolean\" default=\"True\">\n    Check user messages before model call\n</ParamField>\n\n<ParamField body=\"apply_to_output\" type=\"boolean\" default=\"False\">\n    Check AI messages after model call\n</ParamField>\n\n<ParamField body=\"apply_to_tool_results\" type=\"boolean\" default=\"False\">\n    Check tool result messages after execution\n</ParamField>\n:::\n\n:::js\n<ParamField body=\"piiType\" type=\"string\" required>\n    Type of PII to detect. Can be a built-in type (`email`, `credit_card`, `ip`, `mac_address`, `url`) or a custom type name.\n</ParamField>\n\n<ParamField body=\"strategy\" type=\"string\" default=\"redact\">\n    How to handle detected PII. Options:\n\n    - `'block'` - Throw error when detected\n    - `'redact'` - Replace with `[REDACTED_TYPE]`\n    - `'mask'` - Partially mask (e.g., `****-****-****-1234`)\n    - `'hash'` - Replace with deterministic hash (e.g., `<email_hash:a1b2c3d4>`)\n</ParamField>\n\n<ParamField body=\"detector\" type=\"RegExp | string | function\">\n    Custom detector. Can be:\n\n    - `RegExp` - Regex pattern for matching\n    - `string` - Regex pattern string (e.g., `\"sk-[a-zA-Z0-9]{32}\"`)\n    - `function` - Custom detector function `(content: string) => PIIMatch[]`\n\n    If not provided, uses built-in detector for the PII type.\n</ParamField>\n\n<ParamField body=\"applyToInput\" type=\"boolean\" default=\"true\">\n    Check user messages before model call\n</ParamField>\n\n<ParamField body=\"applyToOutput\" type=\"boolean\" default=\"false\">\n    Check AI messages after model call\n</ParamField>\n\n<ParamField body=\"applyToToolResults\" type=\"boolean\" default=\"false\">\n    Check tool result messages after execution\n</ParamField>\n:::\n\n</Accordion>\n\n### To-do list\n\nEquip agents with task planning and tracking capabilities for complex multi-step tasks. To-do lists are useful for the following:\n\n- Complex multi-step tasks requiring coordination across multiple tools.\n- Long-running operations where progress visibility is important.\n\n<Note>", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": " provided, uses built-in detector for the PII type.\n</ParamField>\n\n<ParamField body=\"applyToInput\" type=\"boolean\" default=\"true\">\n    Check user messages before model call\n</ParamField>\n\n<ParamField body=\"applyToOutput\" type=\"boolean\" default=\"false\">\n    Check AI messages after model call\n</ParamField>\n\n<ParamField body=\"applyToToolResults\" type=\"boolean\" default=\"false\">\n    Check tool result messages after execution\n</ParamField>\n:::\n\n</Accordion>\n\n### To-do list\n\nEquip agents with task planning and tracking capabilities for complex multi-step tasks. To-do lists are useful for the following:\n\n- Complex multi-step tasks requiring coordination across multiple tools.\n- Long-running operations where progress visibility is important.\n\n<Note>\n    This middleware automatically provides agents with a `write_todos` tool and system prompts to guide effective task planning.\n</Note>\n\n:::python\n**API reference:** @[`TodoListMiddleware`]\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import TodoListMiddleware\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[read_file, write_file, run_tests],\n    middleware=[TodoListMiddleware()],\n)\n```\n:::\n\n:::js\n```typescript\nimport { createAgent, todoListMiddleware } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [readFile, writeFile, runTests],\n  middleware: [todoListMiddleware()],\n});\n```\n:::\n\n:::python\n<Callout icon=\"circle-play\" iconType=\"solid\">\n    Watch this [video guide](https://www.youtube.com/watch?v=yTWocbVKQxw) demonstrating To-do List middleware behavior.\n</Callout>\n:::\n\n:::js\n<Callout icon=\"circle-play\" iconType=\"solid\">\n    Watch this [video guide](https://www.youtube.com/watch?v=dwvhZ1z_Pas) demonstrating To-do List middleware behavior.\n</Callout>\n:::\n\n<Accordion title=\"Configuration options\">\n\n:::python\n<ParamField body=\"system_prompt\" type=\"string\">\n    Custom system prompt for guiding todo usage. Uses built-in prompt if not specified.\n</ParamField>\n\n<ParamField body=\"tool_description\" type=\"string\">\n    Custom description for the `write_todos` tool. Uses built-in description if not specified.\n</ParamField>\n:::\n\n:::js\nNo configuration options available (uses defaults).\n:::\n\n</Accordion>\n\n\n### LLM tool selector\n\nUse an LLM to intelligently select relevant tools before calling the main model. LLM tool selectors are useful for the following:\n\n- Agents with many tools (10+) where most aren't relevant per query.\n- Reducing token usage by filtering irrelevant tools.\n- Improving model focus and accuracy.\n\nThis middleware uses structured output to ask an LLM which tools are most relevant for the current query. The structured output schema defines", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": "   Custom system prompt for guiding todo usage. Uses built-in prompt if not specified.\n</ParamField>\n\n<ParamField body=\"tool_description\" type=\"string\">\n    Custom description for the `write_todos` tool. Uses built-in description if not specified.\n</ParamField>\n:::\n\n:::js\nNo configuration options available (uses defaults).\n:::\n\n</Accordion>\n\n\n### LLM tool selector\n\nUse an LLM to intelligently select relevant tools before calling the main model. LLM tool selectors are useful for the following:\n\n- Agents with many tools (10+) where most aren't relevant per query.\n- Reducing token usage by filtering irrelevant tools.\n- Improving model focus and accuracy.\n\nThis middleware uses structured output to ask an LLM which tools are most relevant for the current query. The structured output schema defines the available tool names and descriptions. Model providers often add this structured output information to the system prompt behind the scenes.\n\n:::python\n**API reference:** @[`LLMToolSelectorMiddleware`]\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import LLMToolSelectorMiddleware\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[tool1, tool2, tool3, tool4, tool5, ...],\n    middleware=[\n        LLMToolSelectorMiddleware(\n            model=\"gpt-4.1-mini\",\n            max_tools=3,\n            always_include=[\"search\"],\n        ),\n    ],\n)\n```\n:::\n\n:::js\n```typescript\nimport { createAgent, llmToolSelectorMiddleware } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [tool1, tool2, tool3, tool4, tool5, ...],\n  middleware: [\n    llmToolSelectorMiddleware({\n      model: \"gpt-4.1-mini\",\n      maxTools: 3,\n      alwaysInclude: [\"search\"],\n    }),\n  ],\n});\n```\n:::\n\n<Accordion title=\"Configuration options\">\n\n:::python\n<ParamField body=\"model\" type=\"string | BaseChatModel\">\n    Model for tool selection. Can be a model identifier string (e.g., `'openai:gpt-4.1-mini'`) or a `BaseChatModel` instance. See @[`init_chat_model`][init_chat_model(model)] for more information.\n\n    Defaults to the agent's main model.\n</ParamField>\n\n<ParamField body=\"system_prompt\" type=\"string\">\n    Instructions for the selection model. Uses built-in prompt if not specified.\n</ParamField>\n\n<ParamField body=\"max_tools\" type=\"number\">\n    Maximum number of tools to select. If the model selects more, only the first max_tools will be used. No limit if not specified.\n</ParamField>\n\n<ParamField body=\"always_include\" type=\"", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": " type=\"string | BaseChatModel\">\n    Model for tool selection. Can be a model identifier string (e.g., `'openai:gpt-4.1-mini'`) or a `BaseChatModel` instance. See @[`init_chat_model`][init_chat_model(model)] for more information.\n\n    Defaults to the agent's main model.\n</ParamField>\n\n<ParamField body=\"system_prompt\" type=\"string\">\n    Instructions for the selection model. Uses built-in prompt if not specified.\n</ParamField>\n\n<ParamField body=\"max_tools\" type=\"number\">\n    Maximum number of tools to select. If the model selects more, only the first max_tools will be used. No limit if not specified.\n</ParamField>\n\n<ParamField body=\"always_include\" type=\"list[string]\">\n    Tool names to always include regardless of selection. These do not count against the max_tools limit.\n</ParamField>\n:::\n\n:::js\n<ParamField body=\"model\" type=\"string | BaseChatModel\">\n    Model for tool selection. Can be a model identifier string (e.g., `'openai:gpt-4.1-mini'`) or a `BaseChatModel` instance. Defaults to the agent's main model.\n</ParamField>\n\n<ParamField body=\"systemPrompt\" type=\"string\">\n    Instructions for the selection model. Uses built-in prompt if not specified.\n</ParamField>\n\n<ParamField body=\"maxTools\" type=\"number\">\n    Maximum number of tools to select. If the model selects more, only the first maxTools will be used. No limit if not specified.\n</ParamField>\n\n<ParamField body=\"alwaysInclude\" type=\"string[]\">\n    Tool names to always include regardless of selection. These do not count against the maxTools limit.\n</ParamField>\n:::\n\n</Accordion>\n\n\n### Tool retry\n\nAutomatically retry failed tool calls with configurable exponential backoff. Tool retry is useful for the following:\n\n- Handling transient failures in external API calls.\n- Improving reliability of network-dependent tools.\n- Building resilient agents that gracefully handle temporary errors.\n\n:::python\n**API reference:** @[`ToolRetryMiddleware`]\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import ToolRetryMiddleware\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[search_tool, database_tool],\n    middleware=[\n        ToolRetryMiddleware(\n            max_retries=3,\n            backoff_factor=2.0,\n            initial_delay=1.0,\n        ),\n    ],\n)\n```\n:::\n\n:::js\n**API reference:** @[`toolRetryMiddleware`]\n\n```typescript\nimport { createAgent, toolRetryMiddleware } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [searchTool, databaseTool],\n  middleware: [\n  ", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": "   tools=[search_tool, database_tool],\n    middleware=[\n        ToolRetryMiddleware(\n            max_retries=3,\n            backoff_factor=2.0,\n            initial_delay=1.0,\n        ),\n    ],\n)\n```\n:::\n\n:::js\n**API reference:** @[`toolRetryMiddleware`]\n\n```typescript\nimport { createAgent, toolRetryMiddleware } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [searchTool, databaseTool],\n  middleware: [\n    toolRetryMiddleware({\n      maxRetries: 3,\n      backoffFactor: 2.0,\n      initialDelayMs: 1000,\n    }),\n  ],\n});\n```\n:::\n\n<Accordion title=\"Configuration options\">\n\n:::python\n<ParamField body=\"max_retries\" type=\"number\" default=\"2\">\n    Maximum number of retry attempts after the initial call (3 total attempts with default)\n</ParamField>\n\n<ParamField body=\"tools\" type=\"list[BaseTool | str]\">\n    Optional list of tools or tool names to apply retry logic to. If `None`, applies to all tools.\n</ParamField>\n\n<ParamField body=\"retry_on\" type=\"tuple[type[Exception], ...] | callable\" default=\"(Exception,)\">\n    Either a tuple of exception types to retry on, or a callable that takes an exception and returns `True` if it should be retried.\n</ParamField>\n\n<ParamField body=\"on_failure\" type=\"string | callable\" default=\"return_message\">\n    Behavior when all retries are exhausted. Options:\n    - `'return_message'` - Return a `ToolMessage` with error details (allows LLM to handle failure)\n    - `'raise'` - Re-raise the exception (stops agent execution)\n    - Custom callable - Function that takes the exception and returns a string for the `ToolMessage` content\n</ParamField>\n\n<ParamField body=\"backoff_factor\" type=\"number\" default=\"2.0\">\n    Multiplier for exponential backoff. Each retry waits `initial_delay * (backoff_factor ** retry_number)` seconds. Set to `0.0` for constant delay.\n</ParamField>\n\n<ParamField body=\"initial_delay\" type=\"number\" default=\"1.0\">\n    Initial delay in seconds before first retry\n</ParamField>\n\n<ParamField body=\"max_delay\" type=\"number\" default=\"60.0\">\n    Maximum delay in seconds between retries (caps exponential backoff growth)\n</ParamField>\n\n<ParamField body=\"jitter\" type=\"boolean\" default=\"true\">\n    Whether to add random jitter (`\u00b125%`) to delay to avoid thundering herd\n</ParamField>\n:::\n\n:::js\n<ParamField body=\"maxRetries\" type=\"number\" default=\"2\">\n ", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": " backoff. Each retry waits `initial_delay * (backoff_factor ** retry_number)` seconds. Set to `0.0` for constant delay.\n</ParamField>\n\n<ParamField body=\"initial_delay\" type=\"number\" default=\"1.0\">\n    Initial delay in seconds before first retry\n</ParamField>\n\n<ParamField body=\"max_delay\" type=\"number\" default=\"60.0\">\n    Maximum delay in seconds between retries (caps exponential backoff growth)\n</ParamField>\n\n<ParamField body=\"jitter\" type=\"boolean\" default=\"true\">\n    Whether to add random jitter (`\u00b125%`) to delay to avoid thundering herd\n</ParamField>\n:::\n\n:::js\n<ParamField body=\"maxRetries\" type=\"number\" default=\"2\">\n    Maximum number of retry attempts after the initial call (3 total attempts with default). Must be >= 0.\n</ParamField>\n\n<ParamField body=\"tools\" type=\"(ClientTool | ServerTool | string)[]\">\n    Optional array of tools or tool names to apply retry logic to. Can be a list of `BaseTool` instances or tool name strings. If `undefined`, applies to all tools.\n</ParamField>\n\n<ParamField body=\"retryOn\" type=\"((error: Error) => boolean) | (new (...args: any[]) => Error)[]\" default=\"() => true\">\n    Either an array of error constructors to retry on, or a function that takes an error and returns `true` if it should be retried. Default is to retry on all errors.\n</ParamField>\n\n<ParamField body=\"onFailure\" type=\"'error' | 'continue' | ((error: Error) => string)\" default=\"continue\">\n    Behavior when all retries are exhausted. Options:\n    - `'continue'` (default) - Return a `ToolMessage` with error details, allowing the LLM to handle the failure and potentially recover\n    - `'error'` - Re-raise the exception, stopping agent execution\n    - Custom function - Function that takes the exception and returns a string for the `ToolMessage` content, allowing custom error formatting\n\n    **Deprecated values:** `'raise'` (use `'error'` instead) and `'return_message'` (use `'continue'` instead). These deprecated values still work but will show a warning.\n</ParamField>\n\n<ParamField body=\"backoffFactor\" type=\"number\" default=\"2.0\">\n    Multiplier for exponential backoff. Each retry waits `initialDelayMs * (backoffFactor ** retryNumber)` milliseconds. Set to `0.0` for constant delay. Must be >= 0.\n</ParamField>\n\n<ParamField body=\"initialDelayMs\" type=\"number\" default=\"1000\">\n    Initial delay in milliseconds before first retry. Must be >= 0.\n</ParamField>\n\n<ParamField body=\"maxDelayMs\" type=\"number\" default=\"60000\">\n    Maximum delay in milliseconds between retries (caps exponential backoff growth). Must be >= 0.\n</ParamField>\n\n<ParamField body=\"jitter\" type=\"boolean\" default=\"true\">\n    Whether to add random jitter (`\u00b125%`) to delay to avoid thundering herd\n</ParamField>\n:::\n\n</Acc", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": "   Multiplier for exponential backoff. Each retry waits `initialDelayMs * (backoffFactor ** retryNumber)` milliseconds. Set to `0.0` for constant delay. Must be >= 0.\n</ParamField>\n\n<ParamField body=\"initialDelayMs\" type=\"number\" default=\"1000\">\n    Initial delay in milliseconds before first retry. Must be >= 0.\n</ParamField>\n\n<ParamField body=\"maxDelayMs\" type=\"number\" default=\"60000\">\n    Maximum delay in milliseconds between retries (caps exponential backoff growth). Must be >= 0.\n</ParamField>\n\n<ParamField body=\"jitter\" type=\"boolean\" default=\"true\">\n    Whether to add random jitter (`\u00b125%`) to delay to avoid thundering herd\n</ParamField>\n:::\n\n</Accordion>\n\n<Accordion title=\"Full example\">\n\nThe middleware automatically retries failed tool calls with exponential backoff.\n\n:::python\n**Key configuration:**\n- `max_retries` - Number of retry attempts (default: 2)\n- `backoff_factor` - Multiplier for exponential backoff (default: 2.0)\n- `initial_delay` - Starting delay in seconds (default: 1.0)\n- `max_delay` - Cap on delay growth (default: 60.0)\n- `jitter` - Add random variation (default: True)\n\n**Failure handling:**\n- `on_failure='return_message'` - Return error message\n- `on_failure='raise'` - Re-raise exception\n- Custom function - Function returning error message\n:::\n:::js\n**Key configuration:**\n- `maxRetries` - Number of retry attempts (default: 2)\n- `backoffFactor` - Multiplier for exponential backoff (default: 2.0)\n- `initialDelayMs` - Starting delay in milliseconds (default: 1000ms)\n- `maxDelayMs` - Cap on delay growth (default: 60000ms)\n- `jitter` - Add random variation (default: true)\n\n**Failure handling:**\n- `onFailure: \"continue\"` (default) - Return error message\n- `onFailure: \"error\"` - Re-raise exception\n- Custom function - Function returning error message\n:::\n\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import ToolRetryMiddleware\n\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[search_tool, database_tool, api_tool],\n    middleware=[\n        ToolRetryMiddleware(\n            max_retries=3,\n            backoff_factor=2.0,\n            initial_delay=1.0,\n            max_delay=60.0,\n            jitter=True,\n            tools=[\"api_tool\"],\n            retry_on=(ConnectionError, TimeoutError),\n            on_failure=\"continue\",\n   ", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": "   tools=[search_tool, database_tool, api_tool],\n    middleware=[\n        ToolRetryMiddleware(\n            max_retries=3,\n            backoff_factor=2.0,\n            initial_delay=1.0,\n            max_delay=60.0,\n            jitter=True,\n            tools=[\"api_tool\"],\n            retry_on=(ConnectionError, TimeoutError),\n            on_failure=\"continue\",\n        ),\n    ],\n)\n```\n:::\n\n:::js\n```typescript\nimport { createAgent, toolRetryMiddleware } from \"langchain\";\nimport { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n\n// Basic usage with default settings (2 retries, exponential backoff)\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [searchTool, databaseTool],\n  middleware: [toolRetryMiddleware()],\n});\n\n// Retry specific exceptions only\nconst retry = toolRetryMiddleware({\n  maxRetries: 4,\n  retryOn: [TimeoutError, NetworkError],\n  backoffFactor: 1.5,\n});\n\n// Custom exception filtering\nfunction shouldRetry(error: Error): boolean {\n  // Only retry on 5xx errors\n  if (error.name === \"HTTPError\" && \"statusCode\" in error) {\n    const statusCode = (error as any).statusCode;\n    return 500 <= statusCode && statusCode < 600;\n  }\n  return false;\n}\n\nconst retryWithFilter = toolRetryMiddleware({\n  maxRetries: 3,\n  retryOn: shouldRetry,\n});\n\n// Apply to specific tools with custom error handling\nconst formatError = (error: Error) =>\n  \"Database temporarily unavailable. Please try again later.\";\n\nconst retrySpecificTools = toolRetryMiddleware({\n  maxRetries: 4,\n  tools: [\"search_database\"],\n  onFailure: formatError,\n});\n\n// Apply to specific tools using BaseTool instances\nconst searchDatabase = tool(\n  async ({ query }) => {\n    // Search implementation\n    return results;\n  },\n  {\n    name: \"search_database\",\n    description: \"Search the database\",\n    schema: z.object({ query: z.string() }),\n  }\n);\n\nconst retryWithToolInstance = toolRetryMiddleware({\n  maxRetries: 4,\n  tools: [searchDatabase], // Pass BaseTool instance\n});\n\n// Constant backoff (no exponential growth)\nconst constantBackoff = toolRetryMiddleware({\n  maxRetries: 5,\n  backoffFactor: 0.0, // No exponential growth\n  initialDelayMs: 2000, // Always wait 2 seconds\n});\n\n// Raise exception on failure\nconst strictRetry = toolRetryMiddleware({\n  maxRetries: 2", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": " tool(\n  async ({ query }) => {\n    // Search implementation\n    return results;\n  },\n  {\n    name: \"search_database\",\n    description: \"Search the database\",\n    schema: z.object({ query: z.string() }),\n  }\n);\n\nconst retryWithToolInstance = toolRetryMiddleware({\n  maxRetries: 4,\n  tools: [searchDatabase], // Pass BaseTool instance\n});\n\n// Constant backoff (no exponential growth)\nconst constantBackoff = toolRetryMiddleware({\n  maxRetries: 5,\n  backoffFactor: 0.0, // No exponential growth\n  initialDelayMs: 2000, // Always wait 2 seconds\n});\n\n// Raise exception on failure\nconst strictRetry = toolRetryMiddleware({\n  maxRetries: 2,\n  onFailure: \"error\", // Re-raise exception instead of returning message\n});\n```\n:::\n\n</Accordion>\n\n### Model retry\n\nAutomatically retry failed model calls with configurable exponential backoff. Model retry is useful for the following:\n\n- Handling transient failures in model API calls.\n- Improving reliability of network-dependent model requests.\n- Building resilient agents that gracefully handle temporary model errors.\n\n:::python\n**API reference:** @[`ModelRetryMiddleware`]\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import ModelRetryMiddleware\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[search_tool, database_tool],\n    middleware=[\n        ModelRetryMiddleware(\n            max_retries=3,\n            backoff_factor=2.0,\n            initial_delay=1.0,\n        ),\n    ],\n)\n```\n:::\n\n:::js\n**API reference:** @[`modelRetryMiddleware`]\n\n```typescript\nimport { createAgent, modelRetryMiddleware } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [searchTool, databaseTool],\n  middleware: [\n    modelRetryMiddleware({\n      maxRetries: 3,\n      backoffFactor: 2.0,\n      initialDelayMs: 1000,\n    }),\n  ],\n});\n```\n:::\n\n<Accordion title=\"Configuration options\">\n\n:::python\n<ParamField body=\"max_retries\" type=\"number\" default=\"2\">\n    Maximum number of retry attempts after the initial call (3 total attempts with default)\n</ParamField>\n\n<ParamField body=\"retry_on\" type=\"tuple[type[Exception], ...] | callable\" default=\"(Exception,)\">\n    Either a tuple of exception types to retry on, or a callable that takes an exception and returns `True` if it should be retried.\n</ParamField>\n\n<ParamField body=\"on_failure\" type=\"string | callable\" default=\"continue\">\n    Behavior when", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": "offFactor: 2.0,\n      initialDelayMs: 1000,\n    }),\n  ],\n});\n```\n:::\n\n<Accordion title=\"Configuration options\">\n\n:::python\n<ParamField body=\"max_retries\" type=\"number\" default=\"2\">\n    Maximum number of retry attempts after the initial call (3 total attempts with default)\n</ParamField>\n\n<ParamField body=\"retry_on\" type=\"tuple[type[Exception], ...] | callable\" default=\"(Exception,)\">\n    Either a tuple of exception types to retry on, or a callable that takes an exception and returns `True` if it should be retried.\n</ParamField>\n\n<ParamField body=\"on_failure\" type=\"string | callable\" default=\"continue\">\n    Behavior when all retries are exhausted. Options:\n    - `'continue'` (default) - Return an `AIMessage` with error details, allowing the agent to potentially handle the failure gracefully\n    - `'error'` - Re-raise the exception (stops agent execution)\n    - Custom callable - Function that takes the exception and returns a string for the `AIMessage` content\n</ParamField>\n\n<ParamField body=\"backoff_factor\" type=\"number\" default=\"2.0\">\n    Multiplier for exponential backoff. Each retry waits `initial_delay * (backoff_factor ** retry_number)` seconds. Set to `0.0` for constant delay.\n</ParamField>\n\n<ParamField body=\"initial_delay\" type=\"number\" default=\"1.0\">\n    Initial delay in seconds before first retry\n</ParamField>\n\n<ParamField body=\"max_delay\" type=\"number\" default=\"60.0\">\n    Maximum delay in seconds between retries (caps exponential backoff growth)\n</ParamField>\n\n<ParamField body=\"jitter\" type=\"boolean\" default=\"true\">\n    Whether to add random jitter (`\u00b125%`) to delay to avoid thundering herd\n</ParamField>\n:::\n\n:::js\n<ParamField body=\"maxRetries\" type=\"number\" default=\"2\">\n    Maximum number of retry attempts after the initial call (3 total attempts with default). Must be >= 0.\n</ParamField>\n\n<ParamField body=\"retryOn\" type=\"((error: Error) => boolean) | (new (...args: any[]) => Error)[]\" default=\"() => true\">\n    Either an array of error constructors to retry on, or a function that takes an error and returns `true` if it should be retried. Default is to retry on all errors.\n</ParamField>\n\n<ParamField body=\"onFailure\" type=\"'error' | 'continue' | ((error: Error) => string)\" default=\"continue\">\n    Behavior when all retries are exhausted. Options:\n    - `'continue'` (default) - Return an `AIMessage` with error details, allowing the agent to potentially handle the failure gracefully\n    - `'error'` - Re-raise the exception, stopping agent execution\n    - Custom function - Function that takes the exception and returns a string for the `AIMessage` content, allowing custom error formatting\n</ParamField>\n\n<ParamField body=\"backoffFactor\" type=\"number\" default=\"", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": "   Either an array of error constructors to retry on, or a function that takes an error and returns `true` if it should be retried. Default is to retry on all errors.\n</ParamField>\n\n<ParamField body=\"onFailure\" type=\"'error' | 'continue' | ((error: Error) => string)\" default=\"continue\">\n    Behavior when all retries are exhausted. Options:\n    - `'continue'` (default) - Return an `AIMessage` with error details, allowing the agent to potentially handle the failure gracefully\n    - `'error'` - Re-raise the exception, stopping agent execution\n    - Custom function - Function that takes the exception and returns a string for the `AIMessage` content, allowing custom error formatting\n</ParamField>\n\n<ParamField body=\"backoffFactor\" type=\"number\" default=\"2.0\">\n    Multiplier for exponential backoff. Each retry waits `initialDelayMs * (backoffFactor ** retryNumber)` milliseconds. Set to `0.0` for constant delay. Must be >= 0.\n</ParamField>\n\n<ParamField body=\"initialDelayMs\" type=\"number\" default=\"1000\">\n    Initial delay in milliseconds before first retry. Must be >= 0.\n</ParamField>\n\n<ParamField body=\"maxDelayMs\" type=\"number\" default=\"60000\">\n    Maximum delay in milliseconds between retries (caps exponential backoff growth). Must be >= 0.\n</ParamField>\n\n<ParamField body=\"jitter\" type=\"boolean\" default=\"true\">\n    Whether to add random jitter (`\u00b125%`) to delay to avoid thundering herd\n</ParamField>\n:::\n\n</Accordion>\n\n<Accordion title=\"Full example\">\n\nThe middleware automatically retries failed model calls with exponential backoff.\n\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import ModelRetryMiddleware\n\n\n# Basic usage with default settings (2 retries, exponential backoff)\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[search_tool],\n    middleware=[ModelRetryMiddleware()],\n)\n\n# Custom exception filtering\nclass TimeoutError(Exception):\n    \"\"\"Custom exception for timeout errors.\"\"\"\n    pass\n\nclass ConnectionError(Exception):\n    \"\"\"Custom exception for connection errors.\"\"\"\n    pass\n\n# Retry specific exceptions only\nretry = ModelRetryMiddleware(\n    max_retries=4,\n    retry_on=(TimeoutError, ConnectionError),\n    backoff_factor=1.5,\n)\n\n\ndef should_retry(error: Exception) -> bool:\n    # Only retry on rate limit errors\n    if isinstance(error, TimeoutError):\n        return True\n    # Or check for specific HTTP status codes\n    if hasattr(error, \"status_code\"):\n        return error.status_code in (429, 503)\n    return False\n\nretry_with_filter = ModelRetryMiddleware(\n    max_retries=3,\n    retry_on=should_retry,\n)\n\n# Return error", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": " = ModelRetryMiddleware(\n    max_retries=4,\n    retry_on=(TimeoutError, ConnectionError),\n    backoff_factor=1.5,\n)\n\n\ndef should_retry(error: Exception) -> bool:\n    # Only retry on rate limit errors\n    if isinstance(error, TimeoutError):\n        return True\n    # Or check for specific HTTP status codes\n    if hasattr(error, \"status_code\"):\n        return error.status_code in (429, 503)\n    return False\n\nretry_with_filter = ModelRetryMiddleware(\n    max_retries=3,\n    retry_on=should_retry,\n)\n\n# Return error message instead of raising\nretry_continue = ModelRetryMiddleware(\n    max_retries=4,\n    on_failure=\"continue\",  # Return AIMessage with error instead of raising\n)\n\n# Custom error message formatting\ndef format_error(error: Exception) -> str:\n    return f\"Model call failed: {error}. Please try again later.\"\n\nretry_with_formatter = ModelRetryMiddleware(\n    max_retries=4,\n    on_failure=format_error,\n)\n\n# Constant backoff (no exponential growth)\nconstant_backoff = ModelRetryMiddleware(\n    max_retries=5,\n    backoff_factor=0.0,  # No exponential growth\n    initial_delay=2.0,  # Always wait 2 seconds\n)\n\n# Raise exception on failure\nstrict_retry = ModelRetryMiddleware(\n    max_retries=2,\n    on_failure=\"error\",  # Re-raise exception instead of returning message\n)\n```\n:::\n\n:::js\n```typescript\nimport { createAgent, modelRetryMiddleware } from \"langchain\";\n\n// Basic usage with default settings (2 retries, exponential backoff)\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [searchTool],\n  middleware: [modelRetryMiddleware()],\n});\n\nclass TimeoutError extends Error {\n    // ...\n}\nclass NetworkError extends Error {\n    // ...\n}\n\n// Retry specific exceptions only\nconst retry = modelRetryMiddleware({\n  maxRetries: 4,\n  retryOn: [TimeoutError, NetworkError],\n  backoffFactor: 1.5,\n});\n\n// Custom exception filtering\nfunction shouldRetry(error: Error): boolean {\n  // Only retry on rate limit errors\n  if (error.name === \"RateLimitError\") {\n    return true;\n  }\n  // Or check for specific HTTP status codes\n  if (error.name === \"HTTPError\" && \"statusCode\" in error) {\n    const statusCode = (error as any).statusCode;\n    return statusCode === 429 || statusCode === 503;\n  }\n  return false;\n}\n\nconst retryWithFilter = modelRetryMiddleware({\n  maxRetries: 3,\n  retryOn: shouldRetry,\n});\n\n// Return error message", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": "Middleware({\n  maxRetries: 4,\n  retryOn: [TimeoutError, NetworkError],\n  backoffFactor: 1.5,\n});\n\n// Custom exception filtering\nfunction shouldRetry(error: Error): boolean {\n  // Only retry on rate limit errors\n  if (error.name === \"RateLimitError\") {\n    return true;\n  }\n  // Or check for specific HTTP status codes\n  if (error.name === \"HTTPError\" && \"statusCode\" in error) {\n    const statusCode = (error as any).statusCode;\n    return statusCode === 429 || statusCode === 503;\n  }\n  return false;\n}\n\nconst retryWithFilter = modelRetryMiddleware({\n  maxRetries: 3,\n  retryOn: shouldRetry,\n});\n\n// Return error message instead of raising\nconst retryContinue = modelRetryMiddleware({\n  maxRetries: 4,\n  onFailure: \"continue\", // Return AIMessage with error instead of throwing\n});\n\n// Custom error message formatting\nconst formatError = (error: Error) =>\n  `Model call failed: ${error.message}. Please try again later.`;\n\nconst retryWithFormatter = modelRetryMiddleware({\n  maxRetries: 4,\n  onFailure: formatError,\n});\n\n// Constant backoff (no exponential growth)\nconst constantBackoff = modelRetryMiddleware({\n  maxRetries: 5,\n  backoffFactor: 0.0, // No exponential growth\n  initialDelayMs: 2000, // Always wait 2 seconds\n});\n\n// Raise exception on failure\nconst strictRetry = modelRetryMiddleware({\n  maxRetries: 2,\n  onFailure: \"error\", // Re-raise exception instead of returning message\n});\n```\n:::\n\n</Accordion>\n\n### LLM tool emulator\n\nEmulate tool execution using an LLM for testing purposes, replacing actual tool calls with AI-generated responses. LLM tool emulators are useful for the following:\n\n- Testing agent behavior without executing real tools.\n- Developing agents when external tools are unavailable or expensive.\n- Prototyping agent workflows before implementing actual tools.\n\n:::python\n**API reference:** @[`LLMToolEmulator`]\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import LLMToolEmulator\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[get_weather, search_database, send_email],\n    middleware=[\n        LLMToolEmulator(),  # Emulate all tools\n    ],\n)\n```\n:::\n\n:::js\n```typescript\nimport { createAgent, toolEmulatorMiddleware } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [getWeather, searchDatabase, sendEmail],\n  middleware: [\n    toolEmulatorMiddleware(), // Emulate all tools\n  ],\n});\n```\n:::\n\n<Accordion title=\"Configuration options\">\n\n:::python\n<ParamField body=\"tools\" type=\"list[str | BaseTool]\">\n    List of tool names (str) or BaseTool instances to emulate. If `None` (default), ALL tools will be em", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": "email],\n    middleware=[\n        LLMToolEmulator(),  # Emulate all tools\n    ],\n)\n```\n:::\n\n:::js\n```typescript\nimport { createAgent, toolEmulatorMiddleware } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [getWeather, searchDatabase, sendEmail],\n  middleware: [\n    toolEmulatorMiddleware(), // Emulate all tools\n  ],\n});\n```\n:::\n\n<Accordion title=\"Configuration options\">\n\n:::python\n<ParamField body=\"tools\" type=\"list[str | BaseTool]\">\n    List of tool names (str) or BaseTool instances to emulate. If `None` (default), ALL tools will be emulated. If empty list `[]`, no tools will be emulated. If array with tool names/instances, only those tools will be emulated.\n</ParamField>\n\n<ParamField body=\"model\" type=\"string | BaseChatModel\">\n    Model to use for generating emulated tool responses. Can be a model identifier string (e.g., `'anthropic:claude-sonnet-4-5-20250929'`) or a `BaseChatModel` instance. Defaults to the agent's model if not specified. See @[`init_chat_model`][init_chat_model(model)] for more information.\n</ParamField>\n:::\n\n:::js\n<ParamField body=\"tools\" type=\"(string | ClientTool | ServerTool)[]\">\n    List of tool names (string) or tool instances to emulate. If `undefined` (default), ALL tools will be emulated. If empty array `[]`, no tools will be emulated. If array with tool names/instances, only those tools will be emulated.\n</ParamField>\n\n<ParamField body=\"model\" type=\"string | BaseChatModel\">\n    Model to use for generating emulated tool responses. Can be a model identifier string (e.g., `'anthropic:claude-sonnet-4-5-20250929'`) or a `BaseChatModel` instance. Defaults to the agent's model if not specified.\n</ParamField>\n:::\n\n</Accordion>\n\n<Accordion title=\"Full example\">\n\nThe middleware uses an LLM to generate plausible responses for tool calls instead of executing the actual tools.\n\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import LLMToolEmulator\nfrom langchain.tools import tool\n\n\n@tool\ndef get_weather(location: str) -> str:\n    \"\"\"Get the current weather for a location.\"\"\"\n    return f\"Weather in {location}\"\n\n@tool\ndef send_email(to: str, subject: str, body: str) -> str:\n    \"\"\"Send an email.\"\"\"\n    return \"Email sent\"\n\n\n# Emulate all tools (default behavior)\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[get_weather, send_email],\n    middleware=[LLMToolEmulator()],\n)\n\n# Emulate specific tools only\nagent2 = create_agent(\n    model=\"gpt-4.1\",\n", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": "from langchain.agents.middleware import LLMToolEmulator\nfrom langchain.tools import tool\n\n\n@tool\ndef get_weather(location: str) -> str:\n    \"\"\"Get the current weather for a location.\"\"\"\n    return f\"Weather in {location}\"\n\n@tool\ndef send_email(to: str, subject: str, body: str) -> str:\n    \"\"\"Send an email.\"\"\"\n    return \"Email sent\"\n\n\n# Emulate all tools (default behavior)\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[get_weather, send_email],\n    middleware=[LLMToolEmulator()],\n)\n\n# Emulate specific tools only\nagent2 = create_agent(\n    model=\"gpt-4.1\",\n    tools=[get_weather, send_email],\n    middleware=[LLMToolEmulator(tools=[\"get_weather\"])],\n)\n\n# Use custom model for emulation\nagent4 = create_agent(\n    model=\"gpt-4.1\",\n    tools=[get_weather, send_email],\n    middleware=[LLMToolEmulator(model=\"claude-sonnet-4-5-20250929\")],\n)\n```\n:::\n\n:::js\n```typescript\nimport { createAgent, toolEmulatorMiddleware, tool } from \"langchain\";\nimport * as z from \"zod\";\n\nconst getWeather = tool(\n  async ({ location }) => `Weather in ${location}`,\n  {\n    name: \"get_weather\",\n    description: \"Get the current weather for a location\",\n    schema: z.object({ location: z.string() }),\n  }\n);\n\nconst sendEmail = tool(\n  async ({ to, subject, body }) => \"Email sent\",\n  {\n    name: \"send_email\",\n    description: \"Send an email\",\n    schema: z.object({\n      to: z.string(),\n      subject: z.string(),\n      body: z.string(),\n    }),\n  }\n);\n\n// Emulate all tools (default behavior)\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [getWeather, sendEmail],\n  middleware: [toolEmulatorMiddleware()],\n});\n\n// Emulate specific tools by name\nconst agent2 = createAgent({\n  model: \"gpt-4.1\",\n  tools: [getWeather, sendEmail],\n  middleware: [\n    toolEmulatorMiddleware({\n      tools: [\"get_weather\"],\n    }),\n  ],\n});\n\n// Emulate specific tools by passing tool instances\nconst agent3 = createAgent({\n  model: \"gpt-4.1\",\n  tools: [getWeather, sendEmail],\n  middleware: [\n    toolEmulatorMiddleware({\n      tools: [getWeather],\n    }),\n  ],\n});\n\n// Use custom model for emulation\nconst agent5 = createAgent({\n  model: \"gpt-4.1\",\n  tools: [getWeather, sendEmail],\n  middleware:", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": " specific tools by name\nconst agent2 = createAgent({\n  model: \"gpt-4.1\",\n  tools: [getWeather, sendEmail],\n  middleware: [\n    toolEmulatorMiddleware({\n      tools: [\"get_weather\"],\n    }),\n  ],\n});\n\n// Emulate specific tools by passing tool instances\nconst agent3 = createAgent({\n  model: \"gpt-4.1\",\n  tools: [getWeather, sendEmail],\n  middleware: [\n    toolEmulatorMiddleware({\n      tools: [getWeather],\n    }),\n  ],\n});\n\n// Use custom model for emulation\nconst agent5 = createAgent({\n  model: \"gpt-4.1\",\n  tools: [getWeather, sendEmail],\n  middleware: [\n    toolEmulatorMiddleware({\n      model: \"claude-sonnet-4-5-20250929\",\n    }),\n  ],\n});\n```\n:::\n\n</Accordion>\n\n### Context editing\n\nManage conversation context by clearing older tool call outputs when token limits are reached, while preserving recent results. This helps keep context windows manageable in long conversations with many tool calls. Context editing is useful for the following:\n\n- Long conversations with many tool calls that exceed token limits\n- Reducing token costs by removing older tool outputs that are no longer relevant\n- Maintaining only the most recent N tool results in context\n\n:::python\n**API reference:** @[`ContextEditingMiddleware`], @[`ClearToolUsesEdit`]\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import ContextEditingMiddleware, ClearToolUsesEdit\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[],\n    middleware=[\n        ContextEditingMiddleware(\n            edits=[\n                ClearToolUsesEdit(\n                    trigger=100000,\n                    keep=3,\n                ),\n            ],\n        ),\n    ],\n)\n```\n:::\n\n:::js\n```typescript\nimport { createAgent, contextEditingMiddleware, ClearToolUsesEdit } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [],\n  middleware: [\n    contextEditingMiddleware({\n      edits: [\n        new ClearToolUsesEdit({\n          triggerTokens: 100000,\n          keep: 3,\n        }),\n      ],\n    }),\n  ],\n});\n```\n:::\n\n<Accordion title=\"Configuration options\">\n\n:::python\n<ParamField body=\"edits\" type=\"list[Context", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": "\n```\n:::\n\n:::js\n```typescript\nimport { createAgent, contextEditingMiddleware, ClearToolUsesEdit } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [],\n  middleware: [\n    contextEditingMiddleware({\n      edits: [\n        new ClearToolUsesEdit({\n          triggerTokens: 100000,\n          keep: 3,\n        }),\n      ],\n    }),\n  ],\n});\n```\n:::\n\n<Accordion title=\"Configuration options\">\n\n:::python\n<ParamField body=\"edits\" type=\"list[ContextEdit]\" default=\"[ClearToolUsesEdit()]\">\n    List of @[`ContextEdit`] strategies to apply\n</ParamField>\n\n<ParamField body=\"token_count_method\" type=\"string\" default=\"approximate\">\n    Token counting method. Options: `'approximate'` or `'model'`\n</ParamField>\n\n**@[`ClearToolUsesEdit`] options:**\n\n<ParamField body=\"trigger\" type=\"number\" default=\"100000\">\n    Token count that triggers the edit. When the conversation exceeds this token count, older tool outputs will be cleared.\n</ParamField>\n\n<ParamField body=\"clear_at_least\" type=\"number\" default=\"0\">\n    Minimum number of tokens to reclaim when the edit runs. If set to 0, clears as much as needed.\n</ParamField>\n\n<ParamField body=\"keep\" type=\"number\" default=\"3\">\n    Number of most recent tool results that must be preserved. These will never be cleared.\n</ParamField>\n\n<ParamField body=\"clear_tool_inputs\" type=\"boolean\" default=\"False\">\n    Whether to clear the originating tool call parameters on the AI message. When `True`, tool call arguments are replaced with empty objects.\n</ParamField>\n\n<ParamField body=\"exclude_tools\" type=\"list[string]\" default=\"()\">\n    List of tool names to exclude from clearing. These tools will never have their outputs cleared.\n</ParamField>\n\n<ParamField body=\"placeholder\" type=\"string\" default=\"[cleared]\">\n    Placeholder text inserted for cleared tool outputs. This replaces the original tool message content.\n</ParamField>\n:::\n\n:::js\n<ParamField body=\"edits\" type=\"ContextEdit[]\" default=\"[new ClearToolUsesEdit()]\">\n    Array of @[`ContextEdit`] strategies to apply\n</ParamField>\n\n**@[`ClearToolUsesEdit`] options:**\n\n<ParamField body=\"triggerTokens\" type=\"number\" default=\"100000\">\n    Token count that triggers the edit. When the conversation exceeds this token count, older tool outputs will be cleared.\n</ParamField>\n\n<ParamField body=\"clearAtLeast\" type=\"number\" default=\"0\">\n    Minimum number of tokens to reclaim when the edit runs. If set to 0, clears as much as needed.\n</ParamField>\n\n<ParamField body=\"keep\" type=\"number\" default=\"3\">\n    Number of most recent tool results", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": "\n:::\n\n:::js\n<ParamField body=\"edits\" type=\"ContextEdit[]\" default=\"[new ClearToolUsesEdit()]\">\n    Array of @[`ContextEdit`] strategies to apply\n</ParamField>\n\n**@[`ClearToolUsesEdit`] options:**\n\n<ParamField body=\"triggerTokens\" type=\"number\" default=\"100000\">\n    Token count that triggers the edit. When the conversation exceeds this token count, older tool outputs will be cleared.\n</ParamField>\n\n<ParamField body=\"clearAtLeast\" type=\"number\" default=\"0\">\n    Minimum number of tokens to reclaim when the edit runs. If set to 0, clears as much as needed.\n</ParamField>\n\n<ParamField body=\"keep\" type=\"number\" default=\"3\">\n    Number of most recent tool results that must be preserved. These will never be cleared.\n</ParamField>\n\n<ParamField body=\"clearToolInputs\" type=\"boolean\" default=\"false\">\n    Whether to clear the originating tool call parameters on the AI message. When `true`, tool call arguments are replaced with empty objects.\n</ParamField>\n\n<ParamField body=\"excludeTools\" type=\"string[]\" default=\"[]\">\n    List of tool names to exclude from clearing. These tools will never have their outputs cleared.\n</ParamField>\n\n<ParamField body=\"placeholder\" type=\"string\" default=\"[cleared]\">\n    Placeholder text inserted for cleared tool outputs. This replaces the original tool message content.\n</ParamField>\n:::\n\n</Accordion>\n\n<Accordion title=\"Full example\">\n\nThe middleware applies context editing strategies when token limits are reached. The most common strategy is `ClearToolUsesEdit`, which clears older tool results while preserving recent ones.\n\n**How it works:**\n1. Monitor token count in conversation\n2. When threshold is reached, clear older tool outputs\n3. Keep most recent N tool results\n4. Optionally preserve tool call arguments for context\n\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import ContextEditingMiddleware, ClearToolUsesEdit\n\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[search_tool, your_calculator_tool, database_tool],\n    middleware=[\n        ContextEditingMiddleware(\n            edits=[\n                ClearToolUsesEdit(\n                    trigger=2000,\n                    keep=3,\n                    clear_tool_inputs=False,\n                    exclude_tools=[],\n                    placeholder=\"[cleared]\",\n                ),\n            ],\n        ),\n    ],\n)\n```\n:::\n\n:::js\n``", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": "       ClearToolUsesEdit(\n                    trigger=2000,\n                    keep=3,\n                    clear_tool_inputs=False,\n                    exclude_tools=[],\n                    placeholder=\"[cleared]\",\n                ),\n            ],\n        ),\n    ],\n)\n```\n:::\n\n:::js\n```typescript\nimport { createAgent, contextEditingMiddleware, ClearToolUsesEdit } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [searchTool, calculatorTool, databaseTool],\n  middleware: [\n    contextEditingMiddleware({\n      edits: [\n        new ClearToolUsesEdit({\n          triggerTokens: 2000,\n          keep: 3,\n          clearToolInputs: false,\n          excludeTools: [],\n          placeholder: \"[cleared]\",\n        }),\n      ],\n    }),\n  ],\n});\n```\n:::\n\n</Accordion>\n\n:::python\n### Shell tool\n\nExpose a persistent shell session to agents for command execution. Shell tool middleware is useful for the following:\n\n- Agents that need to execute system commands\n- Development and deployment automation tasks\n- Testing and validation workflows\n- File system operations and script execution\n\n<Warning>\n    **Security consideration**: Use appropriate execution policies (`HostExecutionPolicy`, `DockerExecutionPolicy`, or `CodexSandboxExecutionPolicy`) to match your deployment's security requirements.\n</Warning>\n\n<Note>\n    **Limitation**: Persistent shell sessions do not currently work with interrupts (human-in-the-loop). We anticipate adding support for this in the future.\n</Note>\n\n**API reference:** @[`ShellToolMiddleware`]\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import (\n    ShellToolMiddleware,\n    HostExecutionPolicy,\n)\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[search_tool],\n    middleware=[\n        ShellToolMiddleware(\n            workspace_root=\"/workspace\",\n            execution_policy=HostExecutionPolicy(),\n        ),\n    ],\n)\n```\n\n<Accordion title=\"Configuration options\">\n\n<ParamField body=\"workspace_root\" type=\"str | Path | None\">\n    Base directory for the shell session", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": "ware`]\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import (\n    ShellToolMiddleware,\n    HostExecutionPolicy,\n)\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[search_tool],\n    middleware=[\n        ShellToolMiddleware(\n            workspace_root=\"/workspace\",\n            execution_policy=HostExecutionPolicy(),\n        ),\n    ],\n)\n```\n\n<Accordion title=\"Configuration options\">\n\n<ParamField body=\"workspace_root\" type=\"str | Path | None\">\n    Base directory for the shell session. If omitted, a temporary directory is created when the agent starts and removed when it ends.\n</ParamField>\n\n<ParamField body=\"startup_commands\" type=\"tuple[str, ...] | list[str] | str | None\">\n    Optional commands executed sequentially after the session starts\n</ParamField>\n\n<ParamField body=\"shutdown_commands\" type=\"tuple[str, ...] | list[str] | str | None\">\n    Optional commands executed before the session shuts down\n</ParamField>\n\n<ParamField body=\"execution_policy\" type=\"BaseExecutionPolicy | None\">\n    Execution policy controlling timeouts, output limits, and resource configuration. Options:\n\n    - `HostExecutionPolicy` - Full host access (default); best for trusted environments where the agent already runs inside a container or VM\n    - `DockerExecutionPolicy` - Launches a separate Docker container for each agent run, providing harder isolation\n    - `CodexSandboxExecutionPolicy` - Reuses the Codex CLI sandbox for additional syscall/filesystem restrictions\n</ParamField>\n\n<ParamField body=\"redaction_rules\" type=\"tuple[RedactionRule, ...] | list[RedactionRule] | None\">\n    Optional redaction rules to sanitize command output before returning it to the model.\n    <Warning>\n    Redaction rules are applied post execution and do not prevent exfiltration of secrets or sensitive data when using `HostExecutionPolicy`.\n    </Warning>\n</ParamField>\n\n<ParamField body=\"tool_description\" type=\"str | None\">\n    Optional override for the registered shell tool description\n</ParamField>\n\n<ParamField body=\"shell_command\" type=\"Sequence[str] | str | None\">\n    Optional shell executable (string) or argument sequence used to launch the persistent session. Defaults to `/bin/bash`.\n</ParamField>\n\n<ParamField body=\"env\" type=\"Mapping[str, Any] | None\">\n    Optional environment variables to supply to the shell session. Values are coerced to strings before command execution.\n</ParamField>\n\n</Accordion>\n\n<Accordion title=\"Full example\">\n\nThe middleware provides a single persistent shell session that agents can use to execute commands sequentially.\n\n**Execution policies:**\n- `HostExecutionPolicy` (default) - Native execution with full host access\n- `DockerExecutionPolicy` - Isolated Docker container execution\n- `CodexSandboxExecutionPolicy` - Sandboxed execution via Codex CLI", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": " body=\"shell_command\" type=\"Sequence[str] | str | None\">\n    Optional shell executable (string) or argument sequence used to launch the persistent session. Defaults to `/bin/bash`.\n</ParamField>\n\n<ParamField body=\"env\" type=\"Mapping[str, Any] | None\">\n    Optional environment variables to supply to the shell session. Values are coerced to strings before command execution.\n</ParamField>\n\n</Accordion>\n\n<Accordion title=\"Full example\">\n\nThe middleware provides a single persistent shell session that agents can use to execute commands sequentially.\n\n**Execution policies:**\n- `HostExecutionPolicy` (default) - Native execution with full host access\n- `DockerExecutionPolicy` - Isolated Docker container execution\n- `CodexSandboxExecutionPolicy` - Sandboxed execution via Codex CLI\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import (\n    ShellToolMiddleware,\n    HostExecutionPolicy,\n    DockerExecutionPolicy,\n    RedactionRule,\n)\n\n\n# Basic shell tool with host execution\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[search_tool],\n    middleware=[\n        ShellToolMiddleware(\n            workspace_root=\"/workspace\",\n            execution_policy=HostExecutionPolicy(),\n        ),\n    ],\n)\n\n# Docker isolation with startup commands\nagent_docker = create_agent(\n    model=\"gpt-4.1\",\n    tools=[],\n    middleware=[\n        ShellToolMiddleware(\n            workspace_root=\"/workspace\",\n            startup_commands=[\"pip install requests\", \"export PYTHONPATH=/workspace\"],\n            execution_policy=DockerExecutionPolicy(\n                image=\"python:3.11-slim\",\n                command_timeout=60.0,\n            ),\n        ),\n    ],\n)\n\n# With output redaction (applied post execution)\nagent_redacted = create_agent(\n    model=\"gpt-4.1\",\n    tools=[],\n    middleware=[\n        ShellToolMiddleware(\n            workspace_root=\"/workspace\",\n            redaction_rules=[\n                RedactionRule(pii_type=\"api_key\", detector=r\"sk-[a-zA-Z0-9]{32}\"),\n            ],\n        ),\n    ],\n)\n```\n\n</Accordion>\n\n### File search\n\nProvide Glob and Grep search tools over a filesystem. File search middleware", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": " post execution)\nagent_redacted = create_agent(\n    model=\"gpt-4.1\",\n    tools=[],\n    middleware=[\n        ShellToolMiddleware(\n            workspace_root=\"/workspace\",\n            redaction_rules=[\n                RedactionRule(pii_type=\"api_key\", detector=r\"sk-[a-zA-Z0-9]{32}\"),\n            ],\n        ),\n    ],\n)\n```\n\n</Accordion>\n\n### File search\n\nProvide Glob and Grep search tools over a filesystem. File search middleware is useful for the following:\n\n- Code exploration and analysis\n- Finding files by name patterns\n- Searching code content with regex\n- Large codebases where file discovery is needed\n\n**API reference:** @[`FilesystemFileSearchMiddleware`]\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import FilesystemFileSearchMiddleware\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[],\n    middleware=[\n        FilesystemFileSearchMiddleware(\n            root_path=\"/workspace\",\n            use_ripgrep=True,\n        ),\n    ],\n)\n```\n\n<Accordion title=\"Configuration options\">\n\n<ParamField body=\"root_path\" type=\"str\" required>\n    Root directory to search. All file operations are relative to this path.\n</ParamField>\n\n<ParamField body=\"use_ripgrep\" type=\"bool\" default=\"True\">\n    Whether to use ripgrep for search. Falls back to Python regex if ripgrep is unavailable.\n</ParamField>\n\n<ParamField body=\"max_file_size_mb\" type=\"int\" default=\"10\">\n    Maximum file size to search in MB. Files larger than this are skipped.\n</ParamField>\n\n</Accordion>\n\n<Accordion title=\"Full example\">\n\nThe middleware adds two search tools to agents:\n\n**Glob tool** - Fast file pattern matching:\n- Supports patterns like `**/*.py`, `src/**/*.ts`\n- Returns matching file paths sorted by modification time\n\n**Grep tool** - Content search with regex:\n- Full regex syntax support\n- Filter by file patterns with `include` parameter\n- Three output modes: `files_with_matches`, `content`, `count`\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import FilesystemFileSearchMiddleware\nfrom langchain.messages import HumanMessage\n\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[],\n    middleware=[\n        FilesystemFileSearchMiddleware(\n            root_path=\"/workspace\",\n            use_ripgrep=True,\n   ", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": "src/**/*.ts`\n- Returns matching file paths sorted by modification time\n\n**Grep tool** - Content search with regex:\n- Full regex syntax support\n- Filter by file patterns with `include` parameter\n- Three output modes: `files_with_matches`, `content`, `count`\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import FilesystemFileSearchMiddleware\nfrom langchain.messages import HumanMessage\n\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[],\n    middleware=[\n        FilesystemFileSearchMiddleware(\n            root_path=\"/workspace\",\n            use_ripgrep=True,\n            max_file_size_mb=10,\n        ),\n    ],\n)\n\n# Agent can now use glob_search and grep_search tools\nresult = agent.invoke({\n    \"messages\": [HumanMessage(\"Find all Python files containing 'async def'\")]\n})\n\n# The agent will use:\n# 1. glob_search(pattern=\"**/*.py\") to find Python files\n# 2. grep_search(pattern=\"async def\", include=\"*.py\") to find async functions\n```\n</Accordion>\n\n:::\n\n## Provider-specific middleware\n\nThese middleware are optimized for specific LLM providers. See each provider's documentation for full details and examples.\n\n<Columns cols={2}>\n\n:::python\n    <Card title=\"Anthropic\" href=\"/oss/integrations/middleware/anthropic\" icon=\"anthropic\" arrow>\n        Prompt caching, bash tool, text editor, memory, and file search middleware for Claude models.\n    </Card>\n    <Card title=\"OpenAI\" href=\"/oss/integrations/middleware/openai\" icon=\"openai\" arrow>\n        Content moderation middleware for OpenAI models.\n    </Card>\n:::\n\n:::js\n    <Card title=\"Anthropic\" href=\"/oss/integrations/middleware/anthropic\" icon=\"anthropic\" arrow>\n        Prompt caching, bash tool, text editor, memory, and file search middleware for Claude models.\n    </Card>\n:::\n</Columns>\n", "metadata": {"source": "middleware/built-in.mdx"}}
{"text": "---\ntitle: Custom middleware\n---\n\nBuild custom middleware by implementing hooks that run at specific points in the agent execution flow.\n\n## Hooks\n\nMiddleware provides two styles of hooks to intercept agent execution:\n\n<CardGroup cols={2}>\n    <Card title=\"Node-style hooks\" icon=\"share-nodes\" href=\"#node-style-hooks\">\n        Run sequentially at specific execution points.\n    </Card>\n    <Card title=\"Wrap-style hooks\" icon=\"container-storage\" href=\"#wrap-style-hooks\">\n        Run around each model or tool call.\n    </Card>\n</CardGroup>\n\n### Node-style hooks\n\nRun sequentially at specific execution points. Use for logging, validation, and state updates.\n\n**Available hooks:**\n\n:::python\n- `before_agent` - Before agent starts (once per invocation)\n- `before_model` - Before each model call\n- `after_model` - After each model response\n- `after_agent` - After agent completes (once per invocation)\n:::\n\n:::js\n- `beforeAgent` - Before agent starts (once per invocation)\n- `beforeModel` - Before each model call\n- `afterModel` - After each model response\n- `afterAgent` - After agent completes (once per invocation)\n:::\n\n**Example:**\n\n:::python\n\n<Tabs>\n<Tab title=\"Decorator\">\n\n```python\nfrom langchain.agents.middleware import before_model, after_model, AgentState\nfrom langchain.messages import AIMessage\nfrom langgraph.runtime import Runtime\nfrom typing import Any\n\n\n@before_model(can_jump_to=[\"end\"])\ndef check_message_limit(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n    if len(state[\"messages\"]) >= 50:\n        return {\n            \"messages\": [AIMessage(\"Conversation limit reached.\")],\n            \"jump_to\": \"end\"\n        }\n    return None\n\n@after_model\ndef log_response(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n    print(f\"Model returned: {state['messages'][-1].content}\")\n    return None\n```\n\n</Tab>\n\n<Tab title=\"Class\">\n\n```python\nfrom langchain.agents.middleware import AgentMiddleware, AgentState, hook_config\nfrom langchain.messages import AIMessage\nfrom langgraph.runtime import Runtime\nfrom typing import Any\n\nclass MessageLimitMiddleware(AgentMiddleware):\n    def __init__(self, max_messages: int = 50):\n        super().__init__()\n        self.max_messages = max_messages\n\n    @hook_config(can_jump_to=[\"end\"])\n    def before_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n        if len(state[\"messages\"]) == self.max_messages:\n", "metadata": {"source": "middleware/custom.mdx"}}
{"text": "</Tab>\n\n<Tab title=\"Class\">\n\n```python\nfrom langchain.agents.middleware import AgentMiddleware, AgentState, hook_config\nfrom langchain.messages import AIMessage\nfrom langgraph.runtime import Runtime\nfrom typing import Any\n\nclass MessageLimitMiddleware(AgentMiddleware):\n    def __init__(self, max_messages: int = 50):\n        super().__init__()\n        self.max_messages = max_messages\n\n    @hook_config(can_jump_to=[\"end\"])\n    def before_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n        if len(state[\"messages\"]) == self.max_messages:\n            return {\n                \"messages\": [AIMessage(\"Conversation limit reached.\")],\n                \"jump_to\": \"end\"\n            }\n        return None\n\n    def after_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n        print(f\"Model returned: {state['messages'][-1].content}\")\n        return None\n```\n\n</Tab>\n</Tabs>\n\n:::\n\n:::js\n\n```typescript\nimport { createMiddleware, AIMessage } from \"langchain\";\n\nconst createMessageLimitMiddleware = (maxMessages: number = 50) => {\n  return createMiddleware({\n    name: \"MessageLimitMiddleware\",\n    beforeModel: {\n      canJumpTo: [\"end\"],\n      hook: (state) => {\n        if (state.messages.length === maxMessages) {\n          return {\n            messages: [new AIMessage(\"Conversation limit reached.\")],\n            jumpTo: \"end\",\n          };\n        }\n        return;\n      }\n    },\n    afterModel: (state) => {\n      const lastMessage = state.messages[state.messages.length - 1];\n      console.log(`Model returned: ${lastMessage.content}`);\n      return;\n    },\n  });\n};\n```\n\n:::\n\n### Wrap-style hooks\n\nIntercept execution and control when the handler is called. Use for retries, caching, and transformation.\n\nYou decide if the handler is called zero times (short-circuit), once (normal flow), or multiple times (retry logic).\n\n**Available hooks:**\n\n:::python\n- `wrap_model_call` - Around each model call\n- `wrap_tool_call` - Around each tool call\n:::\n\n:::js\n- `wrapModelCall` - Around each", "metadata": {"source": "middleware/custom.mdx"}}
{"text": "    afterModel: (state) => {\n      const lastMessage = state.messages[state.messages.length - 1];\n      console.log(`Model returned: ${lastMessage.content}`);\n      return;\n    },\n  });\n};\n```\n\n:::\n\n### Wrap-style hooks\n\nIntercept execution and control when the handler is called. Use for retries, caching, and transformation.\n\nYou decide if the handler is called zero times (short-circuit), once (normal flow), or multiple times (retry logic).\n\n**Available hooks:**\n\n:::python\n- `wrap_model_call` - Around each model call\n- `wrap_tool_call` - Around each tool call\n:::\n\n:::js\n- `wrapModelCall` - Around each model call\n- `wrapToolCall` - Around each tool call\n:::\n\n**Example:**\n\n:::python\n\n<Tabs>\n<Tab title=\"Decorator\">\n\n```python\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\nfrom typing import Callable\n\n\n@wrap_model_call\ndef retry_model(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n    for attempt in range(3):\n        try:\n            return handler(request)\n        except Exception as e:\n            if attempt == 2:\n                raise\n            print(f\"Retry {attempt + 1}/3 after error: {e}\")\n```\n\n</Tab>\n\n<Tab title=\"Class\">\n\n```python\nfrom langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse\nfrom typing import Callable\n\nclass RetryMiddleware(AgentMiddleware):\n    def __init__(self, max_retries: int = 3):\n        super().__init__()\n        self.max_retries = max_retries\n\n    def wrap_model_call(\n        self,\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse],\n    ) -> ModelResponse:\n        for attempt in range(self.max_retries):\n            try:\n                return handler(request)\n            except Exception as e:\n                if attempt == self.max_retries - 1:\n                    raise\n                print(f\"Retry {attempt + 1}/{self.max_retries} after error: {e}\")\n```\n\n</Tab>\n</Tabs>\n\n:::\n\n:::js\n\n", "metadata": {"source": "middleware/custom.mdx"}}
{"text": "Response],\n    ) -> ModelResponse:\n        for attempt in range(self.max_retries):\n            try:\n                return handler(request)\n            except Exception as e:\n                if attempt == self.max_retries - 1:\n                    raise\n                print(f\"Retry {attempt + 1}/{self.max_retries} after error: {e}\")\n```\n\n</Tab>\n</Tabs>\n\n:::\n\n:::js\n\n```typescript\nimport { createMiddleware } from \"langchain\";\n\nconst createRetryMiddleware = (maxRetries: number = 3) => {\n  return createMiddleware({\n    name: \"RetryMiddleware\",\n    wrapModelCall: (request, handler) => {\n      for (let attempt = 0; attempt < maxRetries; attempt++) {\n        try {\n          return handler(request);\n        } catch (e) {\n          if (attempt === maxRetries - 1) {\n            throw e;\n          }\n          console.log(`Retry ${attempt + 1}/${maxRetries} after error: ${e}`);\n        }\n      }\n      throw new Error(\"Unreachable\");\n    },\n  });\n};\n```\n\n:::\n\n## Create middleware\n\n:::python\n\nYou can create middleware in two ways:\n\n<CardGroup cols={2}>\n  <Card title=\"Decorator-based middleware\" icon=\"at\" href=\"#decorator-based-middleware\">\n    Quick and simple for single-hook middleware. Use decorators to wrap individual functions.\n  </Card>\n  <Card title=\"Class-based middleware\" icon=\"brackets-curly\" href=\"#class-based-middleware\">\n    More powerful for complex middleware with multiple hooks or configuration.\n  </Card>\n</CardGroup>\n\n### Decorator-based middleware\n\nQuick and simple for single-hook middleware. Use decorators to wrap individual functions.\n\n**Available decorators:**\n\n**Node-style:**\n- @[`@before_agent`] - Runs before agent starts (once per invocation)\n- @[`@before_model`] - Runs before each model call\n- @[`@after_model`] - Runs after each model response\n- @[`@after_agent`] - Runs after agent completes (once per invocation)\n\n**Wrap-style:**\n- @[`@wrap_model_call`] - Wraps each model call with custom logic\n- @[`@wrap_tool_call`] - Wraps each tool call with custom logic\n\n**Convenience:**\n- @[`@dynamic_", "metadata": {"source": "middleware/custom.mdx"}}
{"text": " </Card>\n</CardGroup>\n\n### Decorator-based middleware\n\nQuick and simple for single-hook middleware. Use decorators to wrap individual functions.\n\n**Available decorators:**\n\n**Node-style:**\n- @[`@before_agent`] - Runs before agent starts (once per invocation)\n- @[`@before_model`] - Runs before each model call\n- @[`@after_model`] - Runs after each model response\n- @[`@after_agent`] - Runs after agent completes (once per invocation)\n\n**Wrap-style:**\n- @[`@wrap_model_call`] - Wraps each model call with custom logic\n- @[`@wrap_tool_call`] - Wraps each tool call with custom logic\n\n**Convenience:**\n- @[`@dynamic_prompt`] - Generates dynamic system prompts\n\n**Example:**\n\n```python\nfrom langchain.agents.middleware import (\n    before_model,\n    wrap_model_call,\n    AgentState,\n    ModelRequest,\n    ModelResponse,\n)\nfrom langchain.agents import create_agent\nfrom langgraph.runtime import Runtime\nfrom typing import Any, Callable\n\n\n@before_model\ndef log_before_model(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n    print(f\"About to call model with {len(state['messages'])} messages\")\n    return None\n\n@wrap_model_call\ndef retry_model(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n    for attempt in range(3):\n        try:\n            return handler(request)\n        except Exception as e:\n            if attempt == 2:\n                raise\n            print(f\"Retry {attempt + 1}/3 after error: {e}\")\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    middleware=[log_before_model, retry_model],\n    tools=[...],\n)\n```\n\n**When to use decorators:**\n- Single hook needed\n- No complex configuration\n- Quick prototyping\n\n### Class-based middleware\n\nMore powerful for complex middleware with multiple hooks or configuration. Use classes when you need to define both sync and async implementations for the same hook, or when you want to combine multiple hooks in a single middleware.\n\n**Example:**\n\n```python\nfrom langchain.agents.middleware import (\n    AgentMiddleware,\n    AgentState,\n    ModelRequest,\n    ModelResponse,\n)\nfrom langgraph.runtime import Runtime\nfrom typing import Any, Callable\n\nclass LoggingMiddleware(AgentMiddleware):\n    def before_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n        print(f\"About to call model with {len(state['messages'])} messages\")\n   ", "metadata": {"source": "middleware/custom.mdx"}}
{"text": " Quick prototyping\n\n### Class-based middleware\n\nMore powerful for complex middleware with multiple hooks or configuration. Use classes when you need to define both sync and async implementations for the same hook, or when you want to combine multiple hooks in a single middleware.\n\n**Example:**\n\n```python\nfrom langchain.agents.middleware import (\n    AgentMiddleware,\n    AgentState,\n    ModelRequest,\n    ModelResponse,\n)\nfrom langgraph.runtime import Runtime\nfrom typing import Any, Callable\n\nclass LoggingMiddleware(AgentMiddleware):\n    def before_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n        print(f\"About to call model with {len(state['messages'])} messages\")\n        return None\n\n    def after_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n        print(f\"Model returned: {state['messages'][-1].content}\")\n        return None\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    middleware=[LoggingMiddleware()],\n    tools=[...],\n)\n```\n\n**When to use classes:**\n- Defining both sync and async implementations for the same hook\n- Multiple hooks needed in a single middleware\n- Complex configuration required (e.g., configurable thresholds, custom models)\n- Reuse across projects with init-time configuration\n\n:::\n\n:::js\n\nUse the `createMiddleware` function to define custom middleware:\n\n```typescript\nimport { createMiddleware } from \"langchain\";\n\nconst loggingMiddleware = createMiddleware({\n  name: \"LoggingMiddleware\",\n  beforeModel: (state) => {\n    console.log(`About to call model with ${state.messages.length} messages`);\n    return;\n  },\n  afterModel: (state) => {\n    const lastMessage = state.messages[state.messages.length - 1];\n    console.log(`Model returned: ${lastMessage.content}`);\n    return;\n  },\n});\n```\n\n:::\n\n## Custom state schema\n\nMiddleware can extend the agent's state with custom properties. This enables middleware to:\n\n- **Track state across execution**: Maintain counters, flags, or other values that persist throughout the agent's execution lifecycle\n:::python\n- **Share data between hooks**: Pass information from `before_model` to `after_model` or between different middleware instances\n:::\n:::js\n- **Share data between hooks**: Pass information from `beforeModel` to `afterModel` or between different middleware instances\n:::\n- **Implement cross-cutting concerns**: Add functionality like rate limiting, usage tracking, user context, or audit logging without modifying the core agent logic\n- **Make conditional decisions**: Use accumulated state to determine whether to continue execution, jump to different nodes, or modify behavior dynamically\n\n:::python\n\n<Tabs>\n<Tab title=\"Decorator\">\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain.messages import HumanMessage\nfrom langchain.agents.middleware import AgentState", "metadata": {"source": "middleware/custom.mdx"}}
{"text": " flags, or other values that persist throughout the agent's execution lifecycle\n:::python\n- **Share data between hooks**: Pass information from `before_model` to `after_model` or between different middleware instances\n:::\n:::js\n- **Share data between hooks**: Pass information from `beforeModel` to `afterModel` or between different middleware instances\n:::\n- **Implement cross-cutting concerns**: Add functionality like rate limiting, usage tracking, user context, or audit logging without modifying the core agent logic\n- **Make conditional decisions**: Use accumulated state to determine whether to continue execution, jump to different nodes, or modify behavior dynamically\n\n:::python\n\n<Tabs>\n<Tab title=\"Decorator\">\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain.messages import HumanMessage\nfrom langchain.agents.middleware import AgentState, before_model, after_model\nfrom typing_extensions import NotRequired\nfrom typing import Any\nfrom langgraph.runtime import Runtime\n\n\nclass CustomState(AgentState):\n    model_call_count: NotRequired[int]\n    user_id: NotRequired[str]\n\n\n@before_model(state_schema=CustomState, can_jump_to=[\"end\"])\ndef check_call_limit(state: CustomState, runtime: Runtime) -> dict[str, Any] | None:\n    count = state.get(\"model_call_count\", 0)\n    if count > 10:\n        return {\"jump_to\": \"end\"}\n    return None\n\n\n@after_model(state_schema=CustomState)\ndef increment_counter(state: CustomState, runtime: Runtime) -> dict[str, Any] | None:\n    return {\"model_call_count\": state.get(\"model_call_count\", 0) + 1}\n\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    middleware=[check_call_limit, increment_counter],\n    tools=[],\n)\n\n# Invoke with custom state\nresult = agent.invoke({\n    \"messages\": [HumanMessage(\"Hello\")],\n    \"model_call_count\": 0,\n    \"user_id\": \"user-123\",\n})\n```\n\n</Tab>\n\n<Tab title=\"Class\">\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain.messages import HumanMessage\nfrom langchain.agents.middleware import AgentState, AgentMiddleware\nfrom typing_extensions import NotRequired\nfrom typing import Any\n\n\nclass CustomState(AgentState):\n    model_call_count: NotRequired[int]\n    user_id: NotRequired[str]\n\n\nclass CallCounterMiddleware(AgentMiddleware[CustomState]):\n    state_schema = CustomState\n\n    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n        count = state.get(\"model_call_count\", 0)\n        if count > 10:\n            return {\"jump_to\": \"end\"}\n        return None\n\n    def after_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n", "metadata": {"source": "middleware/custom.mdx"}}
{"text": "from typing_extensions import NotRequired\nfrom typing import Any\n\n\nclass CustomState(AgentState):\n    model_call_count: NotRequired[int]\n    user_id: NotRequired[str]\n\n\nclass CallCounterMiddleware(AgentMiddleware[CustomState]):\n    state_schema = CustomState\n\n    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n        count = state.get(\"model_call_count\", 0)\n        if count > 10:\n            return {\"jump_to\": \"end\"}\n        return None\n\n    def after_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n        return {\"model_call_count\": state.get(\"model_call_count\", 0) + 1}\n\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    middleware=[CallCounterMiddleware()],\n    tools=[],\n)\n\n# Invoke with custom state\nresult = agent.invoke({\n    \"messages\": [HumanMessage(\"Hello\")],\n    \"model_call_count\": 0,\n    \"user_id\": \"user-123\",\n})\n```\n\n</Tab>\n</Tabs>\n\n:::\n\n:::js\n\n```typescript\nimport { createMiddleware, createAgent, HumanMessage } from \"langchain\";\nimport { StateSchema } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\nconst CustomState = new StateSchema({\n  modelCallCount: z.number().default(0),\n  userId: z.string().optional(),\n});\n\nconst callCounterMiddleware = createMiddleware({\n  name: \"CallCounterMiddleware\",\n  stateSchema: CustomState,\n  beforeModel: {\n    canJumpTo: [\"end\"],\n    hook: (state) => {\n      if (state.modelCallCount > 10) {\n        return { jumpTo: \"end\" };\n      }\n\n      return;\n    },\n  },\n  afterModel: (state) => {\n    return { modelCallCount: state.modelCallCount + 1 };\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [...],\n  middleware: [callCounterMiddleware],\n});\n\nconst result = await agent.invoke({\n  messages: [new HumanMessage(\"Hello\")],\n  modelCallCount: 0,\n  userId: \"user-123\",\n});\n```\n\nState fields can be either public or private. Fields that start with an underscore (`_`) are considered private and will not be included in the agent's result. Only public fields (those without a leading underscore) are returned.\n\nThis is useful for storing internal middleware state that shouldn't be exposed to the caller, such as temporary tracking variables or internal flags:\n\n```typescript\nimport { StateSchema } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\nconst PrivateState = new StateSchema({\n  //", "metadata": {"source": "middleware/custom.mdx"}}
{"text": "  model: \"gpt-4.1\",\n  tools: [...],\n  middleware: [callCounterMiddleware],\n});\n\nconst result = await agent.invoke({\n  messages: [new HumanMessage(\"Hello\")],\n  modelCallCount: 0,\n  userId: \"user-123\",\n});\n```\n\nState fields can be either public or private. Fields that start with an underscore (`_`) are considered private and will not be included in the agent's result. Only public fields (those without a leading underscore) are returned.\n\nThis is useful for storing internal middleware state that shouldn't be exposed to the caller, such as temporary tracking variables or internal flags:\n\n```typescript\nimport { StateSchema } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\nconst PrivateState = new StateSchema({\n  // Public field - included in invoke result\n  publicCounter: z.number().default(0),\n  // Private field - excluded from invoke result\n  _internalFlag: z.boolean().default(false),\n});\n\nconst middleware = createMiddleware({\n  name: \"ExampleMiddleware\",\n  stateSchema: PrivateState,\n  afterModel: (state) => {\n    // Both fields are accessible during execution\n    if (state._internalFlag) {\n      return { publicCounter: state.publicCounter + 1 };\n    }\n    return { _internalFlag: true };\n  },\n});\n\nconst result = await agent.invoke({\n  messages: [new HumanMessage(\"Hello\")],\n  publicCounter: 0\n});\n\n// result only contains publicCounter, not _internalFlag\nconsole.log(result.publicCounter); // 1\nconsole.log(result._internalFlag); // undefined\n```\n\n:::\n\n:::js\n## Custom context\n\nMiddleware can define a custom context schema to access per-invocation metadata. Unlike state, context is read-only and not persisted between invocations. This makes it ideal for:\n\n- **User information**: Pass user ID, roles, or preferences that don't change during execution\n- **Configuration overrides**: Provide per-invocation settings like rate limits or feature flags\n- **Tenant/workspace context**: Include organization-specific data for multi-tenant applications\n- **Request metadata**: Pass request IDs, API keys, or other metadata needed by middleware\n\nDefine a context schema using Zod and access it via `runtime.context` in middleware hooks. Required fields in the context schema will be enforced at the TypeScript level, ensuring you must provide them when calling `agent.invoke()`.\n\n```typescript\nimport { createAgent, createMiddleware, HumanMessage } from \"langchain\";\nimport * as z from \"zod\";\n\nconst contextSchema = z.object({\n  userId: z.string(),\n  tenantId: z.string(),\n  apiKey: z.string().optional(),\n});\n\nconst userContextMiddleware = createMiddleware({\n  name: \"UserContextMiddleware\",\n  contextSchema,\n  wrapModelCall: (request, handler) => {\n    // Access context from runtime\n    const { userId, tenantId } = request.runtime.context;\n\n    // Add user context to system message\n    const contextText = `User ID: ${userId}, Tenant: ${tenantId}`;\n    const newSystemMessage = request", "metadata": {"source": "middleware/custom.mdx"}}
{"text": " calling `agent.invoke()`.\n\n```typescript\nimport { createAgent, createMiddleware, HumanMessage } from \"langchain\";\nimport * as z from \"zod\";\n\nconst contextSchema = z.object({\n  userId: z.string(),\n  tenantId: z.string(),\n  apiKey: z.string().optional(),\n});\n\nconst userContextMiddleware = createMiddleware({\n  name: \"UserContextMiddleware\",\n  contextSchema,\n  wrapModelCall: (request, handler) => {\n    // Access context from runtime\n    const { userId, tenantId } = request.runtime.context;\n\n    // Add user context to system message\n    const contextText = `User ID: ${userId}, Tenant: ${tenantId}`;\n    const newSystemMessage = request.systemMessage.concat(contextText);\n\n    return handler({\n      ...request,\n      systemMessage: newSystemMessage,\n    });\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  middleware: [userContextMiddleware],\n  tools: [],\n  contextSchema,\n});\n\nconst result = await agent.invoke(\n  { messages: [new HumanMessage(\"Hello\")] },\n  // Required fields (userId, tenantId) must be provided\n  {\n    context: {\n      userId: \"user-123\",\n      tenantId: \"acme-corp\",\n    },\n  }\n);\n```\n\n**Required context fields**: When you define required fields in your `contextSchema` (fields without `.optional()` or `.default()`), TypeScript will enforce that these fields must be provided during `agent.invoke()` calls. This ensures type safety and prevents runtime errors from missing required context.\n\n```typescript\n// This will cause a TypeScript error if userId or tenantId are missing\nconst result = await agent.invoke(\n  { messages: [new HumanMessage(\"Hello\")] },\n  { context: { userId: \"user-123\" } } // Error: tenantId is required\n);\n```\n:::\n\n## Execution order\n\nWhen using multiple middleware, understand how they execute:\n\n:::python\n```python\nagent = create_agent(\n    model=\"gpt-4.1\",\n    middleware=[middleware1, middleware2, middleware3],\n    tools=[...],\n)\n```\n:::\n\n:::js\n```typescript\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  middleware: [middleware1, middleware2, middleware3],\n  tools: [...],\n});\n```\n:::\n\n<Accordion title=\"Execution flow\">\n\n**Before hooks run in order:**\n1. `middleware1.before_agent()`\n2. `middleware2.before_agent()`\n3. `middleware3.before_agent()`\n\n__Agent loop starts__\n\n4. `middleware1.before_model()`\n5. `middleware2.before_model()`\n6. `middleware3.before_model()`\n\n**Wrap hooks nest like function calls:**\n\n7.", "metadata": {"source": "middleware/custom.mdx"}}
{"text": "  tools=[...],\n)\n```\n:::\n\n:::js\n```typescript\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  middleware: [middleware1, middleware2, middleware3],\n  tools: [...],\n});\n```\n:::\n\n<Accordion title=\"Execution flow\">\n\n**Before hooks run in order:**\n1. `middleware1.before_agent()`\n2. `middleware2.before_agent()`\n3. `middleware3.before_agent()`\n\n__Agent loop starts__\n\n4. `middleware1.before_model()`\n5. `middleware2.before_model()`\n6. `middleware3.before_model()`\n\n**Wrap hooks nest like function calls:**\n\n7. `middleware1.wrap_model_call()` \u2192 `middleware2.wrap_model_call()` \u2192 `middleware3.wrap_model_call()` \u2192 model\n\n**After hooks run in reverse order:**\n\n8. `middleware3.after_model()`\n9. `middleware2.after_model()`\n10. `middleware1.after_model()`\n\n__Agent loop ends__\n\n11. `middleware3.after_agent()`\n12. `middleware2.after_agent()`\n13. `middleware1.after_agent()`\n\n</Accordion>\n\n**Key rules:**\n- `before_*` hooks: First to last\n- `after_*` hooks: Last to first (reverse)\n- `wrap_*` hooks: Nested (first middleware wraps all others)\n\n## Agent jumps\n\nTo exit early from middleware, return a dictionary with `jump_to`:\n\n**Available jump targets:**\n- `'end'`: Jump to the end of the agent execution (or the first `after_agent` hook)\n- `'tools'`: Jump to the tools node\n- `'model'`: Jump to the model node (or the first `before_model` hook)\n\n:::python\n\n<Tabs>\n<Tab title=\"Decorator\">\n\n```python\nfrom langchain.agents.middleware import after_model, hook_config, AgentState\nfrom langchain.messages import AIMessage\nfrom langgraph.runtime import Runtime\nfrom typing import Any\n\n\n@after_model\n@hook_config(can_jump_to=[\"end\"])\ndef check_for_blocked(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n    last_message = state[\"messages\"][-1]\n    if \"BLOCKED\" in last_message.content:\n        return {\n            \"messages\": [AIMessage(\"I cannot respond to that request.\")],\n            \"jump_to\": \"end\"\n        }\n    return None\n```\n\n</Tab>\n\n<Tab title=\"Class\">\n\n```python\nfrom langchain.agents.middleware import AgentMiddleware, hook_config, AgentState\nfrom langchain.messages import AIMessage\nfrom langgraph.runtime import Runtime\nfrom typing import Any\n\nclass BlockedContentMiddleware(AgentMiddleware):\n", "metadata": {"source": "middleware/custom.mdx"}}
{"text": ": Runtime) -> dict[str, Any] | None:\n    last_message = state[\"messages\"][-1]\n    if \"BLOCKED\" in last_message.content:\n        return {\n            \"messages\": [AIMessage(\"I cannot respond to that request.\")],\n            \"jump_to\": \"end\"\n        }\n    return None\n```\n\n</Tab>\n\n<Tab title=\"Class\">\n\n```python\nfrom langchain.agents.middleware import AgentMiddleware, hook_config, AgentState\nfrom langchain.messages import AIMessage\nfrom langgraph.runtime import Runtime\nfrom typing import Any\n\nclass BlockedContentMiddleware(AgentMiddleware):\n    @hook_config(can_jump_to=[\"end\"])\n    def after_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n        last_message = state[\"messages\"][-1]\n        if \"BLOCKED\" in last_message.content:\n            return {\n                \"messages\": [AIMessage(\"I cannot respond to that request.\")],\n                \"jump_to\": \"end\"\n            }\n        return None\n```\n\n</Tab>\n</Tabs>\n\n:::\n\n:::js\n\n```typescript\nimport { createAgent, createMiddleware, AIMessage } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  middleware: [\n    createMiddleware({\n      name: \"BlockedContentMiddleware\",\n      beforeModel: {\n        canJumpTo: [\"end\"],\n        hook: (state) => {\n          if (state.messages.at(-1)?.content.includes(\"BLOCKED\")) {\n            return {\n              messages: [new AIMessage(\"I cannot respond to that request.\")],\n              jumpTo: \"end\" as const,\n            };\n          }\n          return;\n        },\n      },\n    }),\n  ],\n});\n\nconst result = await agent.invoke({\n    messages: \"Hello, world! BLOCKED\"\n});\n\n/**\n * Expected output:\n * I cannot respond to that request.\n */\nconsole.log(result.messages.at(-1)?.content);\n```\n:::\n\n## Best practices\n\n1. Keep middleware focused - each should do one thing well\n2. Handle errors gracefully - don't let middleware errors crash the agent\n3. **Use appropriate hook types**:\n", "metadata": {"source": "middleware/custom.mdx"}}
{"text": "           jumpTo: \"end\" as const,\n            };\n          }\n          return;\n        },\n      },\n    }),\n  ],\n});\n\nconst result = await agent.invoke({\n    messages: \"Hello, world! BLOCKED\"\n});\n\n/**\n * Expected output:\n * I cannot respond to that request.\n */\nconsole.log(result.messages.at(-1)?.content);\n```\n:::\n\n## Best practices\n\n1. Keep middleware focused - each should do one thing well\n2. Handle errors gracefully - don't let middleware errors crash the agent\n3. **Use appropriate hook types**:\n    - Node-style for sequential logic (logging, validation)\n    - Wrap-style for control flow (retry, fallback, caching)\n4. Clearly document any custom state properties\n5. Unit test middleware independently before integrating\n6. Consider execution order - place critical middleware first in the list\n7. Use built-in middleware when possible\n\n## Examples\n\n### Dynamic model selection\n\n:::python\n\n<Tabs>\n<Tab title=\"Decorator\">\n\n```python\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\nfrom langchain.chat_models import init_chat_model\nfrom typing import Callable\n\n\ncomplex_model = init_chat_model(\"gpt-4.1\")\nsimple_model = init_chat_model(\"gpt-4.1-mini\")\n\n@wrap_model_call\ndef dynamic_model(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n    # Use different model based on conversation length\n    if len(request.messages) > 10:\n        model = complex_model\n    else:\n        model = simple_model\n    return handler(request.override(model=model))\n```\n\n</Tab>\n\n<Tab title=\"Class\">\n\n```python\nfrom langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse\nfrom langchain.chat_models import init_chat_model\nfrom typing import Callable\n\ncomplex_model = init_chat_model(\"gpt-4.1\")\nsimple_model = init_chat_model(\"gpt-4.1-mini\")\n\nclass DynamicModelMiddleware(AgentMiddleware):\n    def wrap_model_call(\n        self,\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse],\n    ) -> ModelResponse:\n        # Use different model based on conversation length\n        if len(request.messages) > 10:\n            model = complex_model\n        else:\n            model = simple_model\n        return handler(request.override(model=model))\n```\n\n</Tab>\n</Tabs>", "metadata": {"source": "middleware/custom.mdx"}}
{"text": "_chat_model(\"gpt-4.1-mini\")\n\nclass DynamicModelMiddleware(AgentMiddleware):\n    def wrap_model_call(\n        self,\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse],\n    ) -> ModelResponse:\n        # Use different model based on conversation length\n        if len(request.messages) > 10:\n            model = complex_model\n        else:\n            model = simple_model\n        return handler(request.override(model=model))\n```\n\n</Tab>\n</Tabs>\n\n:::\n\n:::js\n\n```typescript\nimport { createMiddleware, initChatModel } from \"langchain\";\n\nconst dynamicModelMiddleware = createMiddleware({\n  name: \"DynamicModelMiddleware\",\n  wrapModelCall: (request, handler) => {\n    const modifiedRequest = { ...request };\n    if (request.messages.length > 10) {\n      modifiedRequest.model = initChatModel(\"gpt-4.1\");\n    } else {\n      modifiedRequest.model = initChatModel(\"gpt-4.1-mini\");\n    }\n    return handler(modifiedRequest);\n  },\n});\n```\n\n:::\n\n### Tool call monitoring\n\n:::python\n\n<Tabs>\n<Tab title=\"Decorator\">\n\n```python\nfrom langchain.agents.middleware import wrap_tool_call\nfrom langchain.tools.tool_node import ToolCallRequest\nfrom langchain.messages import ToolMessage\nfrom langgraph.types import Command\nfrom typing import Callable\n\n\n@wrap_tool_call\ndef monitor_tool(\n    request: ToolCallRequest,\n    handler: Callable[[ToolCallRequest], ToolMessage | Command],\n) -> ToolMessage | Command:\n    print(f\"Executing tool: {request.tool_call['name']}\")\n    print(f\"Arguments: {request.tool_call['args']}\")\n    try:\n        result = handler(request)\n        print(f\"Tool completed successfully\")\n        return result\n    except Exception as e:\n        print(f\"Tool failed: {e}\")\n        raise\n```\n\n</Tab>\n\n<Tab title=\"Class\">\n\n```python\nfrom langchain.tools.tool_node import ToolCallRequest\nfrom langchain.agents.middleware import AgentMiddleware\nfrom langchain.messages import ToolMessage\nfrom langgraph.types import Command\nfrom typing import Callable\n\nclass ToolMonitoringMiddleware(AgentMiddleware):\n    def wrap_tool_call(\n        self,\n        request: ToolCallRequest,\n        handler: Callable[[ToolCallRequest], ToolMessage | Command],\n    ) -> ToolMessage | Command:\n        print(", "metadata": {"source": "middleware/custom.mdx"}}
{"text": "\n    except Exception as e:\n        print(f\"Tool failed: {e}\")\n        raise\n```\n\n</Tab>\n\n<Tab title=\"Class\">\n\n```python\nfrom langchain.tools.tool_node import ToolCallRequest\nfrom langchain.agents.middleware import AgentMiddleware\nfrom langchain.messages import ToolMessage\nfrom langgraph.types import Command\nfrom typing import Callable\n\nclass ToolMonitoringMiddleware(AgentMiddleware):\n    def wrap_tool_call(\n        self,\n        request: ToolCallRequest,\n        handler: Callable[[ToolCallRequest], ToolMessage | Command],\n    ) -> ToolMessage | Command:\n        print(f\"Executing tool: {request.tool_call['name']}\")\n        print(f\"Arguments: {request.tool_call['args']}\")\n        try:\n            result = handler(request)\n            print(f\"Tool completed successfully\")\n            return result\n        except Exception as e:\n            print(f\"Tool failed: {e}\")\n            raise\n```\n\n</Tab>\n</Tabs>\n\n:::\n\n:::js\n\n```typescript\nimport { createMiddleware } from \"langchain\";\n\nconst toolMonitoringMiddleware = createMiddleware({\n  name: \"ToolMonitoringMiddleware\",\n  wrapToolCall: (request, handler) => {\n    console.log(`Executing tool: ${request.toolCall.name}`);\n    console.log(`Arguments: ${JSON.stringify(request.toolCall.args)}`);\n    try {\n      const result = handler(request);\n      console.log(\"Tool completed successfully\");\n      return result;\n    } catch (e) {\n      console.log(`Tool failed: ${e}`);\n      throw e;\n    }\n  },\n});\n```\n\n:::\n\n### Dynamically selecting tools\n\nSelect relevant tools at runtime to improve performance and accuracy. This section covers filtering pre-registered tools. For registering tools that are discovered at runtime (e.g., from MCP servers), see [Runtime tool registration](/oss/langchain/agents#dynamic-tools).\n\n**Benefits:**\n- **Shorter prompts** - Reduce complexity by exposing only relevant tools\n- **Better accuracy** - Models choose correctly from fewer options\n- **Permission control** - Dynamically filter tools based on user access\n\n:::python\n\n<Tabs>\n<Tab title=\"Decorator\">\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\nfrom typing import Callable\n\n\n@wrap_model_call\ndef select_tools(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse],\n) -> Model", "metadata": {"source": "middleware/custom.mdx"}}
{"text": " section covers filtering pre-registered tools. For registering tools that are discovered at runtime (e.g., from MCP servers), see [Runtime tool registration](/oss/langchain/agents#dynamic-tools).\n\n**Benefits:**\n- **Shorter prompts** - Reduce complexity by exposing only relevant tools\n- **Better accuracy** - Models choose correctly from fewer options\n- **Permission control** - Dynamically filter tools based on user access\n\n:::python\n\n<Tabs>\n<Tab title=\"Decorator\">\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\nfrom typing import Callable\n\n\n@wrap_model_call\ndef select_tools(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n    \"\"\"Middleware to select relevant tools based on state/context.\"\"\"\n    # Select a small, relevant subset of tools based on state/context\n    relevant_tools = select_relevant_tools(request.state, request.runtime)\n    return handler(request.override(tools=relevant_tools))\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=all_tools,  # All available tools need to be registered upfront\n    middleware=[select_tools],\n)\n```\n\n</Tab>\n\n<Tab title=\"Class\">\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse\nfrom typing import Callable\n\n\nclass ToolSelectorMiddleware(AgentMiddleware):\n    def wrap_model_call(\n        self,\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse],\n    ) -> ModelResponse:\n        \"\"\"Middleware to select relevant tools based on state/context.\"\"\"\n        # Select a small, relevant subset of tools based on state/context\n        relevant_tools = select_relevant_tools(request.state, request.runtime)\n        return handler(request.override(tools=relevant_tools))\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=all_tools,  # All available tools need to be registered upfront\n    middleware=[ToolSelectorMiddleware()],\n)\n```\n\n</Tab>\n</Tabs>\n\n:::\n\n:::js\n\n```typescript\nimport { createAgent, createMiddleware } from \"langchain\";\n\nconst toolSelectorMiddleware = createMiddleware({\n  name: \"ToolSelector\",\n  wrapModelCall: (request, handler) => {\n    // Select a small, relevant subset of tools based on state/context\n    const relevantTools = selectRelevantTools(request.state, request.runtime);\n    const modifiedRequest = { ...request, tools: relevantTools };\n    return handler(modifiedRequest);\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: allTools,\n  middleware: [toolSelectorMiddleware", "metadata": {"source": "middleware/custom.mdx"}}
{"text": " middleware=[ToolSelectorMiddleware()],\n)\n```\n\n</Tab>\n</Tabs>\n\n:::\n\n:::js\n\n```typescript\nimport { createAgent, createMiddleware } from \"langchain\";\n\nconst toolSelectorMiddleware = createMiddleware({\n  name: \"ToolSelector\",\n  wrapModelCall: (request, handler) => {\n    // Select a small, relevant subset of tools based on state/context\n    const relevantTools = selectRelevantTools(request.state, request.runtime);\n    const modifiedRequest = { ...request, tools: relevantTools };\n    return handler(modifiedRequest);\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: allTools,\n  middleware: [toolSelectorMiddleware],\n});\n```\n\n:::\n\n### Working with system messages\n\n:::python\n\nModify system messages in middleware using the `system_message` field on `ModelRequest`. The `system_message` field contains a @[`SystemMessage`] object (even if the agent was created with a string `system_prompt`).\n\n**Example: Adding context to system message**\n\n<Tabs>\n<Tab title=\"Decorator\">\n\n```python\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\nfrom langchain.messages import SystemMessage\nfrom typing import Callable\n\n\n@wrap_model_call\ndef add_context(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n    # Always work with content blocks\n    new_content = list(request.system_message.content_blocks) + [\n        {\"type\": \"text\", \"text\": \"Additional context.\"}\n    ]\n    new_system_message = SystemMessage(content=new_content)\n    return handler(request.override(system_message=new_system_message))\n```\n\n</Tab>\n\n<Tab title=\"Class\">\n\n```python\nfrom langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse\nfrom langchain.messages import SystemMessage\nfrom typing import Callable\n\n\nclass ContextMiddleware(AgentMiddleware):\n    def wrap_model_call(\n        self,\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse],\n    ) -> ModelResponse:\n        # Always work with content blocks\n        new_content = list(request.system_message.content_blocks) + [\n            {\"type\": \"text\", \"text\": \"Additional context.\"}\n        ]\n        new_system_message = SystemMessage(content=new_content)\n        return handler(request.override(system_message=new_system_message))\n```\n\n</Tab>\n</Tabs>\n\n**Example: Working with cache control (Anthropic)**\n\nWhen working with Anthropic models, you can use structured content blocks with cache control directives to cache large system prompts:\n\n<T", "metadata": {"source": "middleware/custom.mdx"}}
{"text": ": Callable[[ModelRequest], ModelResponse],\n    ) -> ModelResponse:\n        # Always work with content blocks\n        new_content = list(request.system_message.content_blocks) + [\n            {\"type\": \"text\", \"text\": \"Additional context.\"}\n        ]\n        new_system_message = SystemMessage(content=new_content)\n        return handler(request.override(system_message=new_system_message))\n```\n\n</Tab>\n</Tabs>\n\n**Example: Working with cache control (Anthropic)**\n\nWhen working with Anthropic models, you can use structured content blocks with cache control directives to cache large system prompts:\n\n<Tabs>\n<Tab title=\"Decorator\">\n\n```python\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\nfrom langchain.messages import SystemMessage\nfrom typing import Callable\n\n\n@wrap_model_call\ndef add_cached_context(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n    # Always work with content blocks\n    new_content = list(request.system_message.content_blocks) + [\n        {\n            \"type\": \"text\",\n            \"text\": \"Here is a large document to analyze:\\n\\n<document>...</document>\",\n            # content up until this point is cached\n            \"cache_control\": {\"type\": \"ephemeral\"}\n        }\n    ]\n\n    new_system_message = SystemMessage(content=new_content)\n    return handler(request.override(system_message=new_system_message))\n```\n\n</Tab>\n\n<Tab title=\"Class\">\n\n```python\nfrom langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse\nfrom langchain.messages import SystemMessage\nfrom typing import Callable\n\n\nclass CachedContextMiddleware(AgentMiddleware):\n    def wrap_model_call(\n        self,\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse],\n    ) -> ModelResponse:\n        # Always work with content blocks\n        new_content = list(request.system_message.content_blocks) + [\n            {\n                \"type\": \"text\",\n                \"text\": \"Here is a large document to analyze:\\n\\n<document>...</document>\",\n                \"cache_control\": {\"type\": \"ephemeral\"}  # This content will be cached\n            }\n        ]\n\n  ", "metadata": {"source": "middleware/custom.mdx"}}
{"text": "      handler: Callable[[ModelRequest], ModelResponse],\n    ) -> ModelResponse:\n        # Always work with content blocks\n        new_content = list(request.system_message.content_blocks) + [\n            {\n                \"type\": \"text\",\n                \"text\": \"Here is a large document to analyze:\\n\\n<document>...</document>\",\n                \"cache_control\": {\"type\": \"ephemeral\"}  # This content will be cached\n            }\n        ]\n\n        new_system_message = SystemMessage(content=new_content)\n        return handler(request.override(system_message=new_system_message))\n```\n\n</Tab>\n</Tabs>\n\n**Notes:**\n- `ModelRequest.system_message` is always a @[`SystemMessage`] object, even if the agent was created with `system_prompt=\"string\"`\n- Use `SystemMessage.content_blocks` to access content as a list of blocks, regardless of whether the original content was a string or list\n- When modifying system messages, use `content_blocks` and append new blocks to preserve existing structure\n- You can pass @[`SystemMessage`] objects directly to `create_agent`'s `system_prompt` parameter for advanced use cases like cache control\n\n:::\n\n:::js\nModify system messages in middleware using the `systemMessage` field in `ModelRequest`. It contains a @[`SystemMessage`] object (even if the agent was created with a string @[`systemPrompt`]).\n\n**Example: Chaining middleware** - Different middleware can use different approaches:\n\n```typescript\nimport { createMiddleware, SystemMessage, createAgent } from \"langchain\";\n\n// Middleware 1: Uses systemMessage with simple concatenation\nconst myMiddleware = createMiddleware({\n  name: \"MyMiddleware\",\n  wrapModelCall: async (request, handler) => {\n    return handler({\n      ...request,\n      systemMessage: request.systemMessage.concat(`Additional context.`),\n    });\n  },\n});\n\n// Middleware 2: Uses systemMessage with structured content (preserves structure)\nconst myOtherMiddleware = createMiddleware({\n  name: \"MyOtherMiddleware\",\n  wrapModelCall: async (request, handler) => {\n    return handler({\n      ...request,\n      systemMessage: request.systemMessage.concat(\n        new SystemMessage({\n          content: [\n            {\n              type: \"text\",\n              text: \" More additional context. This will be cached.\",\n              cache_control: { type: \"ephemeral\", ttl: \"5m\" },\n      ", "metadata": {"source": "middleware/custom.mdx"}}
{"text": " with structured content (preserves structure)\nconst myOtherMiddleware = createMiddleware({\n  name: \"MyOtherMiddleware\",\n  wrapModelCall: async (request, handler) => {\n    return handler({\n      ...request,\n      systemMessage: request.systemMessage.concat(\n        new SystemMessage({\n          content: [\n            {\n              type: \"text\",\n              text: \" More additional context. This will be cached.\",\n              cache_control: { type: \"ephemeral\", ttl: \"5m\" },\n            },\n          ],\n        })\n      ),\n    });\n  },\n});\n\nconst agent = createAgent({\n  model: \"anthropic:claude-3-5-sonnet\",\n  systemPrompt: \"You are a helpful assistant.\",\n  middleware: [myMiddleware, myOtherMiddleware],\n});\n```\n\nThe resulting system message will be:\n```typescript\nnew SystemMessage({\n  content: [\n    { type: \"text\", text: \"You are a helpful assistant.\" },\n    { type: \"text\", text: \"Additional context.\" },\n    {\n        type: \"text\",\n        text: \" More additional context. This will be cached.\",\n        cache_control: { type: \"ephemeral\", ttl: \"5m\" },\n    },\n  ],\n});\n```\n\nUse @[`SystemMessage.concat`] to preserve cache control metadata or structured content blocks created by other middleware.\n\n:::\n\n## Additional resources\n\n- [Middleware API reference](https://reference.langchain.com/python/langchain/middleware/)\n- [Built-in middleware](/oss/langchain/middleware/built-in)\n- [Testing agents](/oss/langchain/test)\n", "metadata": {"source": "middleware/custom.mdx"}}
{"text": "---\ntitle: Build a multi-source knowledge base with routing\nsidebarTitle: \"Router: Knowledge base\"\n---\n\nimport ChatModelTabsPy from '/snippets/chat-model-tabs.mdx';\nimport ChatModelTabsJs from '/snippets/chat-model-tabs-js.mdx';\n\n## Overview\n\nThe **router pattern** is a [multi-agent](/oss/langchain/multi-agent) architecture where a routing step classifies input and directs it to specialized agents, with results synthesized into a combined response. This pattern excels when your organization's knowledge lives across distinct **verticals**\u2014separate knowledge domains that each require their own agent with specialized tools and prompts.\n\nIn this tutorial, you'll build a multi-source knowledge base router that demonstrates these benefits through a realistic enterprise scenario. The system will coordinate three specialists:\n\n- A **GitHub agent** that searches code, issues, and pull requests.\n- A **Notion agent** that searches internal documentation and wikis.\n- A **Slack agent** that searches relevant threads and discussions.\n\nWhen a user asks \"How do I authenticate API requests?\", the router decomposes the query into source-specific sub-questions, routes them to the relevant agents in parallel, and synthesizes results into a coherent answer.\n\n```mermaid\ngraph LR\n    A([Query]) --> B[Classify]\n    B --> C[GitHub agent]\n    B --> D[Notion agent]\n    B --> E[Slack agent]\n    C --> F[Synthesize]\n    D --> F\n    E --> F\n    F --> G([Combined answer])\n```\n\n### Why use a router?\n\nThe router pattern provides several advantages:\n\n- **Parallel execution**: Query multiple sources simultaneously, reducing latency compared to sequential approaches.\n- **Specialized agents**: Each vertical has focused tools and prompts optimized for its domain.\n- **Selective routing**: Not every query needs every source\u2014the router intelligently selects relevant verticals.\n- **Targeted sub-questions**: Each agent receives a question tailored to its domain, improving result quality.\n- **Clean synthesis**: Results from multiple sources are combined into a single, coherent response.\n\n### Concepts\n\nWe will cover the following concepts:\n\n- [Multi-agent systems](/oss/langchain/multi-agent)\n- [StateGraph](/oss/langchain/graphs) for workflow orchestration\n- [Send API](/oss/langchain/send) for parallel execution\n\n<Tip>\n**Router vs. Subagents**: The [subagents pattern](/oss/langchain/multi-agent/subagents) can also route to multiple agents. Use the router pattern when you need specialized preprocessing, custom routing logic, or want explicit control over parallel execution. Use the subagents pattern when you want the LLM to decide which agents to call dynamically.\n</Tip>\n\n## Setup\n\n### Installation\n\nThis tutorial requires the `langchain` and `langgraph` packages:\n\n:::python\n<CodeGroup>\n```bash pip\npip install langchain langgraph\n```\n```bash uv\nuv add langchain langgraph\n```\n```bash conda\nconda install langchain langgraph -c conda-forge\n```\n</CodeGroup>\n:::\n\n:::js\n<CodeGroup>\n```bash npm\nnpm install langchain @langchain", "metadata": {"source": "multi-agent/router-knowledge-base.mdx"}}
{"text": " vs. Subagents**: The [subagents pattern](/oss/langchain/multi-agent/subagents) can also route to multiple agents. Use the router pattern when you need specialized preprocessing, custom routing logic, or want explicit control over parallel execution. Use the subagents pattern when you want the LLM to decide which agents to call dynamically.\n</Tip>\n\n## Setup\n\n### Installation\n\nThis tutorial requires the `langchain` and `langgraph` packages:\n\n:::python\n<CodeGroup>\n```bash pip\npip install langchain langgraph\n```\n```bash uv\nuv add langchain langgraph\n```\n```bash conda\nconda install langchain langgraph -c conda-forge\n```\n</CodeGroup>\n:::\n\n:::js\n<CodeGroup>\n```bash npm\nnpm install langchain @langchain/langgraph\n```\n```bash yarn\nyarn add langchain @langchain/langgraph\n```\n```bash pnpm\npnpm add langchain @langchain/langgraph\n```\n</CodeGroup>\n:::\n\nFor more details, see our [Installation guide](/oss/langchain/install).\n\n### LangSmith\n\nSet up [LangSmith](https://smith.langchain.com) to inspect what is happening inside your agent. Then set the following environment variables:\n\n:::python\n<CodeGroup>\n```bash bash\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n```\n```python python\nimport getpass\nimport os\n\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```\n</CodeGroup>\n:::\n\n:::js\n<CodeGroup>\n```bash bash\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n```\n```typescript typescript\nprocess.env.LANGSMITH_TRACING = \"true\";\nprocess.env.LANGSMITH_API_KEY = \"...\";\n```\n</CodeGroup>\n:::\n\n### Select an LLM\n\nSelect a chat model from LangChain's suite of integrations:\n\n:::python\n<ChatModelTabsPy />\n:::\n\n:::js\n<ChatModelTabsJs />\n:::\n\n## 1. Define state\n\nFirst, define the state schemas. We use three types:\n\n- **`AgentInput`**: Simple state passed to each subagent (just a query)\n- **`AgentOutput`**: Result returned by each subagent (source name + result)\n- **`RouterState`**: Main workflow state tracking the query, classifications, results, and final answer\n\n:::python\n```python\nfrom typing import Annotated, Literal, TypedDict\nimport operator\n\n\nclass AgentInput(TypedDict):\n    \"\"\"Simple input state for each subagent.\"\"\"\n    query: str\n\n\nclass AgentOutput(TypedDict):\n    \"\"\"Output from each subagent.\"\"\"\n    source: str\n    result: str\n\n\nclass Classification(TypedDict):\n    \"\"\"A single routing decision: which agent to call with what query.\"\"\"\n    source: Literal[\"github", "metadata": {"source": "multi-agent/router-knowledge-base.mdx"}}
{"text": " **`AgentInput`**: Simple state passed to each subagent (just a query)\n- **`AgentOutput`**: Result returned by each subagent (source name + result)\n- **`RouterState`**: Main workflow state tracking the query, classifications, results, and final answer\n\n:::python\n```python\nfrom typing import Annotated, Literal, TypedDict\nimport operator\n\n\nclass AgentInput(TypedDict):\n    \"\"\"Simple input state for each subagent.\"\"\"\n    query: str\n\n\nclass AgentOutput(TypedDict):\n    \"\"\"Output from each subagent.\"\"\"\n    source: str\n    result: str\n\n\nclass Classification(TypedDict):\n    \"\"\"A single routing decision: which agent to call with what query.\"\"\"\n    source: Literal[\"github\", \"notion\", \"slack\"]\n    query: str\n\n\nclass RouterState(TypedDict):\n    query: str\n    classifications: list[Classification]\n    results: Annotated[list[AgentOutput], operator.add]  # Reducer collects parallel results\n    final_answer: str\n```\n:::\n\n:::js\n```typescript\nimport { StateSchema, ReducedValue } from \"@langchain/langgraph\";\nimport { z } from \"zod/v4\";\n\nconst AgentOutput = z.object({\n  source: z.string(),\n  result: z.string(),\n});\n\nconst RouterState = new StateSchema({\n  query: z.string(),\n  classifications: z.array(\n    z.object({\n      source: z.enum([\"github\", \"notion\", \"slack\"]),\n      query: z.string(),\n    })\n  ),\n  results: new ReducedValue(\n    z.array(AgentOutput).default(() => []),\n    { reducer: (current, update) => current.concat(update) }\n  ),\n  finalAnswer: z.string(),\n});\n```\n:::\n\nThe `results` field uses a **reducer** (`operator.add` in Python, a concat function in JS) to collect outputs from parallel agent executions into a single list.\n\n## 2. Define tools for each vertical\n\nCreate tools for each knowledge domain. In a production system, these would call actual APIs. For this tutorial, we use stub implementations that return mock data. We define 7 tools across 3 verticals: GitHub (search code, issues, PRs), Notion (search docs, get page), and Slack (search messages, get thread).\n\n:::python\n```python expandable\nfrom langchain.tools import tool\n\n\n@tool\ndef search_code(query: str, repo: str = \"main\") -> str:\n    \"\"\"Search code in GitHub repositories.\"\"\"\n    return f\"Found code matching '{query}' in {repo}: authentication middleware in src/auth.py\"\n\n\n@tool\ndef search_issues(query: str) -> str:\n    \"\"\"Search GitHub issues and pull requests.\"\"\"\n    return f\"Found 3 issues matching '{query}': #142 (API auth docs), #89 (OAuth flow), #203 (token refresh)\"\n\n\n@tool\ndef search_prs(query: str) -> str:\n    \"\"\"", "metadata": {"source": "multi-agent/router-knowledge-base.mdx"}}
{"text": "search code, issues, PRs), Notion (search docs, get page), and Slack (search messages, get thread).\n\n:::python\n```python expandable\nfrom langchain.tools import tool\n\n\n@tool\ndef search_code(query: str, repo: str = \"main\") -> str:\n    \"\"\"Search code in GitHub repositories.\"\"\"\n    return f\"Found code matching '{query}' in {repo}: authentication middleware in src/auth.py\"\n\n\n@tool\ndef search_issues(query: str) -> str:\n    \"\"\"Search GitHub issues and pull requests.\"\"\"\n    return f\"Found 3 issues matching '{query}': #142 (API auth docs), #89 (OAuth flow), #203 (token refresh)\"\n\n\n@tool\ndef search_prs(query: str) -> str:\n    \"\"\"Search pull requests for implementation details.\"\"\"\n    return f\"PR #156 added JWT authentication, PR #178 updated OAuth scopes\"\n\n\n@tool\ndef search_notion(query: str) -> str:\n    \"\"\"Search Notion workspace for documentation.\"\"\"\n    return f\"Found documentation: 'API Authentication Guide' - covers OAuth2 flow, API keys, and JWT tokens\"\n\n\n@tool\ndef get_page(page_id: str) -> str:\n    \"\"\"Get a specific Notion page by ID.\"\"\"\n    return f\"Page content: Step-by-step authentication setup instructions\"\n\n\n@tool\ndef search_slack(query: str) -> str:\n    \"\"\"Search Slack messages and threads.\"\"\"\n    return f\"Found discussion in #engineering: 'Use Bearer tokens for API auth, see docs for refresh flow'\"\n\n\n@tool\ndef get_thread(thread_id: str) -> str:\n    \"\"\"Get a specific Slack thread.\"\"\"\n    return f\"Thread discusses best practices for API key rotation\"\n```\n:::\n\n:::js\n```typescript expandable\nimport { tool } from \"langchain\";\nimport { z } from \"zod\";\n\nconst searchCode = tool(\n  async ({ query, repo }) => {\n    return `Found code matching '${query}' in ${repo || \"main\"}: authentication middleware in src/auth.py`;\n  },\n  {\n    name: \"search_code\",\n    description: \"Search code in GitHub repositories.\",\n    schema: z.object({\n      query: z.string(),\n      repo: z.string().optional().default(\"main\"),\n    }),\n  }\n);\n\nconst searchIssues = tool(\n  async ({ query }) => {\n    return `Found 3 issues matching '${query}': #142 (API auth docs), #89 (OAuth flow), #203 (token refresh)`;\n  },\n  {\n    name: \"search_issues\",\n    description: \"Search GitHub issues and pull requests.\",\n    schema: z.object({\n      query: z.string(),\n    }),\n  }\n);\n\nconst searchPrs = tool(\n  async ({ query }) => {\n    return `PR #156 added JWT authentication, PR #178 updated OAuth scopes`;\n  },\n  {\n    name: \"search_prs\",", "metadata": {"source": "multi-agent/router-knowledge-base.mdx"}}
{"text": " repo: z.string().optional().default(\"main\"),\n    }),\n  }\n);\n\nconst searchIssues = tool(\n  async ({ query }) => {\n    return `Found 3 issues matching '${query}': #142 (API auth docs), #89 (OAuth flow), #203 (token refresh)`;\n  },\n  {\n    name: \"search_issues\",\n    description: \"Search GitHub issues and pull requests.\",\n    schema: z.object({\n      query: z.string(),\n    }),\n  }\n);\n\nconst searchPrs = tool(\n  async ({ query }) => {\n    return `PR #156 added JWT authentication, PR #178 updated OAuth scopes`;\n  },\n  {\n    name: \"search_prs\",\n    description: \"Search pull requests for implementation details.\",\n    schema: z.object({\n      query: z.string(),\n    }),\n  }\n);\n\nconst searchNotion = tool(\n  async ({ query }) => {\n    return `Found documentation: 'API Authentication Guide' - covers OAuth2 flow, API keys, and JWT tokens`;\n  },\n  {\n    name: \"search_notion\",\n    description: \"Search Notion workspace for documentation.\",\n    schema: z.object({\n      query: z.string(),\n    }),\n  }\n);\n\nconst getPage = tool(\n  async ({ pageId }) => {\n    return `Page content: Step-by-step authentication setup instructions`;\n  },\n  {\n    name: \"get_page\",\n    description: \"Get a specific Notion page by ID.\",\n    schema: z.object({\n      pageId: z.string(),\n    }),\n  }\n);\n\nconst searchSlack = tool(\n  async ({ query }) => {\n    return `Found discussion in #engineering: 'Use Bearer tokens for API auth, see docs for refresh flow'`;\n  },\n  {\n    name: \"search_slack\",\n    description: \"Search Slack messages and threads.\",\n    schema: z.object({\n      query: z.string(),\n    }),\n  }\n);\n\nconst getThread = tool(\n  async ({ threadId }) => {\n    return `Thread discusses best practices for API key rotation`;\n  },\n  {\n    name: \"get_thread\",\n    description: \"Get a specific Slack thread.\",\n    schema: z.object({\n      threadId: z.string(),\n    }),\n  }\n);\n```\n:::\n\n## 3. Create specialized agents\n\nCreate an agent for each vertical. Each agent has domain-specific tools and a prompt optimized for its knowledge source. All three follow the same pattern\u2014only the tools and system prompt differ.\n\n:::python\n```python expandable\nfrom langchain.agents import create_agent\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"openai:gpt-4.1\")\n\ngithub_agent = create_agent(\n    model,\n    tools=[", "metadata": {"source": "multi-agent/router-knowledge-base.mdx"}}
{"text": " API key rotation`;\n  },\n  {\n    name: \"get_thread\",\n    description: \"Get a specific Slack thread.\",\n    schema: z.object({\n      threadId: z.string(),\n    }),\n  }\n);\n```\n:::\n\n## 3. Create specialized agents\n\nCreate an agent for each vertical. Each agent has domain-specific tools and a prompt optimized for its knowledge source. All three follow the same pattern\u2014only the tools and system prompt differ.\n\n:::python\n```python expandable\nfrom langchain.agents import create_agent\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"openai:gpt-4.1\")\n\ngithub_agent = create_agent(\n    model,\n    tools=[search_code, search_issues, search_prs],\n    system_prompt=(\n        \"You are a GitHub expert. Answer questions about code, \"\n        \"API references, and implementation details by searching \"\n        \"repositories, issues, and pull requests.\"\n    ),\n)\n\nnotion_agent = create_agent(\n    model,\n    tools=[search_notion, get_page],\n    system_prompt=(\n        \"You are a Notion expert. Answer questions about internal \"\n        \"processes, policies, and team documentation by searching \"\n        \"the organization's Notion workspace.\"\n    ),\n)\n\nslack_agent = create_agent(\n    model,\n    tools=[search_slack, get_thread],\n    system_prompt=(\n        \"You are a Slack expert. Answer questions by searching \"\n        \"relevant threads and discussions where team members have \"\n        \"shared knowledge and solutions.\"\n    ),\n)\n```\n:::\n\n:::js\n```typescript expandable\nimport { createAgent } from \"langchain\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst llm = new ChatOpenAI({ model: \"gpt-4.1\" });\n\nconst githubAgent = createAgent({\n  model: llm,\n  tools: [searchCode, searchIssues, searchPrs],\n  systemPrompt: `\nYou are a GitHub expert. Answer questions about code,\nAPI references, and implementation details by searching\nrepositories, issues, and pull requests.\n  `.trim(),\n});\n\nconst notionAgent = createAgent({\n  model: llm,\n  tools: [searchNotion, getPage],\n  systemPrompt: `\nYou are a Notion expert. Answer questions about internal\nprocesses, policies, and team documentation by searching\nthe organization's Notion workspace.\n  `.trim(),\n});\n\nconst slackAgent = createAgent({\n  model: llm,\n  tools: [searchSlack, getThread],\n  systemPrompt: `\nYou are a Slack expert. Answer questions by searching\nrelevant threads and discussions where team members have\nshared knowledge and solutions.\n  `.trim(),\n});\n```\n:::\n\n## 4. Build the router", "metadata": {"source": "multi-agent/router-knowledge-base.mdx"}}
{"text": "You are a GitHub expert. Answer questions about code,\nAPI references, and implementation details by searching\nrepositories, issues, and pull requests.\n  `.trim(),\n});\n\nconst notionAgent = createAgent({\n  model: llm,\n  tools: [searchNotion, getPage],\n  systemPrompt: `\nYou are a Notion expert. Answer questions about internal\nprocesses, policies, and team documentation by searching\nthe organization's Notion workspace.\n  `.trim(),\n});\n\nconst slackAgent = createAgent({\n  model: llm,\n  tools: [searchSlack, getThread],\n  systemPrompt: `\nYou are a Slack expert. Answer questions by searching\nrelevant threads and discussions where team members have\nshared knowledge and solutions.\n  `.trim(),\n});\n```\n:::\n\n## 4. Build the router workflow\n\nNow build the router workflow using a StateGraph. The workflow has four main steps:\n\n1. **Classify**: Analyze the query and determine which agents to invoke with what sub-questions\n2. **Route**: Fan out to selected agents in parallel using `Send`\n3. **Query agents**: Each agent receives a simple `AgentInput` and returns an `AgentOutput`\n4. **Synthesize**: Combine collected results into a coherent response\n\n:::python\n```python\nfrom pydantic import BaseModel, Field\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.types import Send\n\nrouter_llm = init_chat_model(\"openai:gpt-4.1-mini\")\n\n\n# Define structured output schema for the classifier\nclass ClassificationResult(BaseModel):  # [!code highlight]\n    \"\"\"Result of classifying a user query into agent-specific sub-questions.\"\"\"\n    classifications: list[Classification] = Field(\n        description=\"List of agents to invoke with their targeted sub-questions\"\n    )\n\n\ndef classify_query(state: RouterState) -> dict:\n    \"\"\"Classify query and determine which agents to invoke.\"\"\"\n    structured_llm = router_llm.with_structured_output(ClassificationResult)  # [!code highlight]\n\n    result = structured_llm.invoke([\n        {\n            \"role\": \"system\",\n            \"content\": \"\"\"Analyze this query and determine which knowledge bases to consult.\nFor each relevant source, generate a targeted sub-question optimized for that source.\n\nAvailable sources:\n- github: Code, API references, implementation details, issues, pull requests\n- notion: Internal documentation, processes, policies, team wikis\n- slack: Team discussions, informal knowledge sharing, recent conversations\n\nReturn ONLY the sources that are relevant to the query. Each source should have\na targeted sub-question optimized for that specific knowledge domain.\n\nExample for \"How do I authenticate API requests?\":\n- github: \"What authentication code exists? Search for auth middleware, JWT handling\"\n- notion: \"What authentication documentation exists? Look for API auth guides\"\n(slack omitted because it's not relevant for this technical question)\"\"\"\n        },\n        {\"role\": \"user\", \"content\": state[\"query\"]}\n    ])\n\n    return {\"classifications\": result", "metadata": {"source": "multi-agent/router-knowledge-base.mdx"}}
{"text": " a targeted sub-question optimized for that source.\n\nAvailable sources:\n- github: Code, API references, implementation details, issues, pull requests\n- notion: Internal documentation, processes, policies, team wikis\n- slack: Team discussions, informal knowledge sharing, recent conversations\n\nReturn ONLY the sources that are relevant to the query. Each source should have\na targeted sub-question optimized for that specific knowledge domain.\n\nExample for \"How do I authenticate API requests?\":\n- github: \"What authentication code exists? Search for auth middleware, JWT handling\"\n- notion: \"What authentication documentation exists? Look for API auth guides\"\n(slack omitted because it's not relevant for this technical question)\"\"\"\n        },\n        {\"role\": \"user\", \"content\": state[\"query\"]}\n    ])\n\n    return {\"classifications\": result.classifications}\n\n\ndef route_to_agents(state: RouterState) -> list[Send]:\n    \"\"\"Fan out to agents based on classifications.\"\"\"\n    return [\n        Send(c[\"source\"], {\"query\": c[\"query\"]})  # [!code highlight]\n        for c in state[\"classifications\"]\n    ]\n\n\ndef query_github(state: AgentInput) -> dict:\n    \"\"\"Query the GitHub agent.\"\"\"\n    result = github_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": state[\"query\"]}]  # [!code highlight]\n    })\n    return {\"results\": [{\"source\": \"github\", \"result\": result[\"messages\"][-1].content}]}\n\n\ndef query_notion(state: AgentInput) -> dict:\n    \"\"\"Query the Notion agent.\"\"\"\n    result = notion_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": state[\"query\"]}]  # [!code highlight]\n    })\n    return {\"results\": [{\"source\": \"notion\", \"result\": result[\"messages\"][-1].content}]}\n\n\ndef query_slack(state: AgentInput) -> dict:\n    \"\"\"Query the Slack agent.\"\"\"\n    result = slack_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": state[\"query\"]}]  # [!code highlight]\n    })\n    return {\"results\": [{\"source\": \"slack\", \"result\": result[\"messages\"][-1].content}]}\n\n\ndef synthesize_results(state: RouterState) -> dict:\n    \"\"\"Combine results from all agents into a coherent answer.\"\"\"\n    if not state[\"results\"]:\n        return {\"final_answer\": \"No results found from any knowledge source.\"}\n\n    # Format results for synthesis\n    formatted = [\n        f\"**From {r['source'].title()}:**\\n{r['result']}\"\n        for r in state[\"results\"]\n    ]\n\n    synthesis_response = router_llm.invoke([\n        {\n            \"role\": \"system\",\n", "metadata": {"source": "multi-agent/router-knowledge-base.mdx"}}
{"text": " [{\"source\": \"slack\", \"result\": result[\"messages\"][-1].content}]}\n\n\ndef synthesize_results(state: RouterState) -> dict:\n    \"\"\"Combine results from all agents into a coherent answer.\"\"\"\n    if not state[\"results\"]:\n        return {\"final_answer\": \"No results found from any knowledge source.\"}\n\n    # Format results for synthesis\n    formatted = [\n        f\"**From {r['source'].title()}:**\\n{r['result']}\"\n        for r in state[\"results\"]\n    ]\n\n    synthesis_response = router_llm.invoke([\n        {\n            \"role\": \"system\",\n            \"content\": f\"\"\"Synthesize these search results to answer the original question: \"{state['query']}\"\n\n- Combine information from multiple sources without redundancy\n- Highlight the most relevant and actionable information\n- Note any discrepancies between sources\n- Keep the response concise and well-organized\"\"\"\n        },\n        {\"role\": \"user\", \"content\": \"\\n\\n\".join(formatted)}\n    ])\n\n    return {\"final_answer\": synthesis_response.content}\n```\n:::\n\n:::js\n```typescript\nimport { StateGraph, START, END, Send } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst routerLlm = new ChatOpenAI({ model: \"gpt-4.1-mini\" });\n\n\n// Define structured output schema for the classifier\nconst ClassificationResultSchema = z.object({  // [!code highlight]\n  classifications: z.array(z.object({\n    source: z.enum([\"github\", \"notion\", \"slack\"]),\n    query: z.string(),\n  })).describe(\"List of agents to invoke with their targeted sub-questions\"),\n});\n\n\nasync function classifyQuery(state: typeof RouterState.State) {\n  const structuredLlm = routerLlm.withStructuredOutput(ClassificationResultSchema);  // [!code highlight]\n\n  const result = await structuredLlm.invoke([\n    {\n      role: \"system\",\n      content: `Analyze this query and determine which knowledge bases to consult.\nFor each relevant source, generate a targeted sub-question optimized for that source.\n\nAvailable sources:\n- github: Code, API references, implementation details, issues, pull requests\n- notion: Internal documentation, processes, policies, team wikis\n- slack: Team discussions, informal knowledge sharing, recent conversations\n\nReturn ONLY the sources that are relevant to the query. Each source should have\na targeted sub-question optimized for that specific knowledge domain.\n\nExample for \"How do I authenticate API requests?\":\n- github: \"What authentication code exists? Search for auth middleware, JWT handling\"\n- notion: \"What authentication documentation exists? Look for API auth guides\"\n(slack omitted because it's not relevant for this technical question)`\n    },\n    { role: \"user\", content: state.query }\n  ]);\n\n  return { classifications: result.classifications };\n}", "metadata": {"source": "multi-agent/router-knowledge-base.mdx"}}
{"text": "\nFor each relevant source, generate a targeted sub-question optimized for that source.\n\nAvailable sources:\n- github: Code, API references, implementation details, issues, pull requests\n- notion: Internal documentation, processes, policies, team wikis\n- slack: Team discussions, informal knowledge sharing, recent conversations\n\nReturn ONLY the sources that are relevant to the query. Each source should have\na targeted sub-question optimized for that specific knowledge domain.\n\nExample for \"How do I authenticate API requests?\":\n- github: \"What authentication code exists? Search for auth middleware, JWT handling\"\n- notion: \"What authentication documentation exists? Look for API auth guides\"\n(slack omitted because it's not relevant for this technical question)`\n    },\n    { role: \"user\", content: state.query }\n  ]);\n\n  return { classifications: result.classifications };\n}\n\n\nfunction routeToAgents(state: typeof RouterState.State): Send[] {\n  return state.classifications.map(\n    (c) => new Send(c.source, { query: c.query })  // [!code highlight]\n  );\n}\n\n\nasync function queryGithub(state: AgentInput) {\n  const result = await githubAgent.invoke({\n    messages: [{ role: \"user\", content: state.query }]  // [!code highlight]\n  });\n  return { results: [{ source: \"github\", result: result.messages.at(-1)?.content }] };\n}\n\n\nasync function queryNotion(state: AgentInput) {\n  const result = await notionAgent.invoke({\n    messages: [{ role: \"user\", content: state.query }]  // [!code highlight]\n  });\n  return { results: [{ source: \"notion\", result: result.messages.at(-1)?.content }] };\n}\n\n\nasync function querySlack(state: AgentInput) {\n  const result = await slackAgent.invoke({\n    messages: [{ role: \"user\", content: state.query }]  // [!code highlight]\n  });\n  return { results: [{ source: \"slack\", result: result.messages.at(-1)?.content }] };\n}\n\n\nasync function synthesizeResults(state: typeof RouterState.State) {\n  if (state.results.length === 0) {\n    return { finalAnswer: \"No results found from any knowledge source.\" };\n  }\n\n  // Format results for synthesis\n  const formatted = state.results.map(\n    (r) => `**From ${r.source.charAt(0).toUpperCase() + r.source.slice(1)}:**\\n${r.result}`\n  );\n\n  const synthesisResponse = await routerLlm.invoke([\n    {\n      role: \"system\",\n      content: `Synthesize these search results to answer the original question: \"${state.query}\"\n\n- Combine information from multiple sources without redundancy\n- Highlight the most relevant and actionable information\n- Note any discrepancies between sources\n- Keep the response concise and well-organized`\n    },\n    { role: \"user\", content: formatted.join(\"\\n\\n\") }\n  ]);\n\n  return { finalAnswer: synthesisResponse.content };\n}\n```\n:::\n\n## 5.", "metadata": {"source": "multi-agent/router-knowledge-base.mdx"}}
{"text": "    (r) => `**From ${r.source.charAt(0).toUpperCase() + r.source.slice(1)}:**\\n${r.result}`\n  );\n\n  const synthesisResponse = await routerLlm.invoke([\n    {\n      role: \"system\",\n      content: `Synthesize these search results to answer the original question: \"${state.query}\"\n\n- Combine information from multiple sources without redundancy\n- Highlight the most relevant and actionable information\n- Note any discrepancies between sources\n- Keep the response concise and well-organized`\n    },\n    { role: \"user\", content: formatted.join(\"\\n\\n\") }\n  ]);\n\n  return { finalAnswer: synthesisResponse.content };\n}\n```\n:::\n\n## 5. Compile the workflow\n\nNow assemble the workflow by connecting nodes with edges. The key is using `add_conditional_edges` with the routing function to enable parallel execution:\n\n:::python\n```python\nworkflow = (\n    StateGraph(RouterState)\n    .add_node(\"classify\", classify_query)\n    .add_node(\"github\", query_github)\n    .add_node(\"notion\", query_notion)\n    .add_node(\"slack\", query_slack)\n    .add_node(\"synthesize\", synthesize_results)\n    .add_edge(START, \"classify\")\n    .add_conditional_edges(\"classify\", route_to_agents, [\"github\", \"notion\", \"slack\"])\n    .add_edge(\"github\", \"synthesize\")\n    .add_edge(\"notion\", \"synthesize\")\n    .add_edge(\"slack\", \"synthesize\")\n    .add_edge(\"synthesize\", END)\n    .compile()\n)\n```\n:::\n\n:::js\n```typescript\nconst workflow = new StateGraph(RouterState)\n  .addNode(\"classify\", classifyQuery)\n  .addNode(\"github\", queryGithub)\n  .addNode(\"notion\", queryNotion)\n  .addNode(\"slack\", querySlack)\n  .addNode(\"synthesize\", synthesizeResults)\n  .addEdge(START, \"classify\")\n  .addConditionalEdges(\"classify\", routeToAgents, [\"github\", \"notion\", \"slack\"])\n  .addEdge(\"github\", \"synthesize\")\n  .addEdge(\"notion\", \"synthesize\")\n  .addEdge(\"slack\", \"synthesize\")\n  .addEdge(\"synthesize\", END)\n  .compile();\n```\n:::\n\nThe `add_conditional_edges` call connects the classify node to the agent nodes through the `route_to_agents` function. When `route_to_agents` returns multiple `Send` objects, those nodes execute in parallel.\n\n## 6. Use the router\n\nTest your router with queries that span multiple knowledge domains:\n\n:::python\n```python\nresult = workflow.invoke({\n    \"query\": \"How do I authenticate API requests?\"\n})\n\nprint(\"Original query:\", result[\"query", "metadata": {"source": "multi-agent/router-knowledge-base.mdx"}}
{"text": " [\"github\", \"notion\", \"slack\"])\n  .addEdge(\"github\", \"synthesize\")\n  .addEdge(\"notion\", \"synthesize\")\n  .addEdge(\"slack\", \"synthesize\")\n  .addEdge(\"synthesize\", END)\n  .compile();\n```\n:::\n\nThe `add_conditional_edges` call connects the classify node to the agent nodes through the `route_to_agents` function. When `route_to_agents` returns multiple `Send` objects, those nodes execute in parallel.\n\n## 6. Use the router\n\nTest your router with queries that span multiple knowledge domains:\n\n:::python\n```python\nresult = workflow.invoke({\n    \"query\": \"How do I authenticate API requests?\"\n})\n\nprint(\"Original query:\", result[\"query\"])\nprint(\"\\nClassifications:\")\nfor c in result[\"classifications\"]:\n    print(f\"  {c['source']}: {c['query']}\")\nprint(\"\\n\" + \"=\" * 60 + \"\\n\")\nprint(\"Final Answer:\")\nprint(result[\"final_answer\"])\n```\n:::\n\n:::js\n```typescript\nconst result = await workflow.invoke({\n  query: \"How do I authenticate API requests?\"\n});\n\nconsole.log(\"Original query:\", result.query);\nconsole.log(\"\\nClassifications:\");\nfor (const c of result.classifications) {\n  console.log(`  ${c.source}: ${c.query}`);\n}\nconsole.log(\"\\n\" + \"=\".repeat(60) + \"\\n\");\nconsole.log(\"Final Answer:\");\nconsole.log(result.finalAnswer);\n```\n:::\n\nExpected output:\n```\nOriginal query: How do I authenticate API requests?\n\nClassifications:\n  github: What authentication code exists? Search for auth middleware, JWT handling\n  notion: What authentication documentation exists? Look for API auth guides\n\n============================================================\n\nFinal Answer:\nTo authenticate API requests, you have several options:\n\n1. **JWT Tokens**: The recommended approach for most use cases.\n   Implementation details are in `src/auth.py` (PR #156).\n\n2. **OAuth2 Flow**: For third-party integrations, follow the OAuth2\n   flow documented in Notion's 'API Authentication Guide'.\n\n3. **API Keys**: For server-to-server communication, use Bearer tokens\n   in the Authorization header.\n\nFor token refresh handling, see issue #203 and PR #178 for the latest\nOAuth scope updates.\n```\n\nThe router analyzed the query, classified it to determine which agents to invoke (GitHub and Notion, but not Slack for this technical question), queried both agents in parallel, and synthesized the results into a coherent answer.\n\n## 7. Understanding the architecture\n\nThe router workflow follows a clear pattern:\n\n### Classification phase\n\nThe `classify_query` function uses **structured output** to analyze the user's query and determine which agents to invoke. This is where the routing intelligence lives:\n\n- Uses a Pydantic model (Python) or Zod schema (JS) to ensure valid output\n- Returns a list of `Classification` objects, each with a `source` and targeted `query`\n- Only includes relevant sources\u2014irrelevant ones are simply omitted\n\n", "metadata": {"source": "multi-agent/router-knowledge-base.mdx"}}
{"text": " Authorization header.\n\nFor token refresh handling, see issue #203 and PR #178 for the latest\nOAuth scope updates.\n```\n\nThe router analyzed the query, classified it to determine which agents to invoke (GitHub and Notion, but not Slack for this technical question), queried both agents in parallel, and synthesized the results into a coherent answer.\n\n## 7. Understanding the architecture\n\nThe router workflow follows a clear pattern:\n\n### Classification phase\n\nThe `classify_query` function uses **structured output** to analyze the user's query and determine which agents to invoke. This is where the routing intelligence lives:\n\n- Uses a Pydantic model (Python) or Zod schema (JS) to ensure valid output\n- Returns a list of `Classification` objects, each with a `source` and targeted `query`\n- Only includes relevant sources\u2014irrelevant ones are simply omitted\n\nThis structured approach is more reliable than free-form JSON parsing and makes the routing logic explicit.\n\n### Parallel execution with send\n\nThe `route_to_agents` function maps classifications to `Send` objects. Each `Send` specifies the target node and the state to pass:\n\n:::python\n```python\n# Classifications: [{\"source\": \"github\", \"query\": \"...\"}, {\"source\": \"notion\", \"query\": \"...\"}]\n# Becomes:\n[Send(\"github\", {\"query\": \"...\"}), Send(\"notion\", {\"query\": \"...\"})]\n# Both agents execute simultaneously, each receiving only the query it needs\n```\n:::\n\n:::js\n```typescript\n// Classifications: [{ source: \"github\", query: \"...\" }, { source: \"notion\", query: \"...\" }]\n// Becomes:\n[new Send(\"github\", { query: \"...\" }), new Send(\"notion\", { query: \"...\" })]\n// Both agents execute simultaneously, each receiving only the query it needs\n```\n:::\n\nEach agent node receives a simple `AgentInput` with just a `query` field\u2014not the full router state. This keeps the interface clean and explicit.\n\n### Result collection with reducers\n\nAgent results flow back to the main state via a **reducer**. Each agent returns:\n\n:::python\n```python\n{\"results\": [{\"source\": \"github\", \"result\": \"...\"}]}\n```\n:::\n\n:::js\n```typescript\n{ results: [{ source: \"github\", result: \"...\" }] }\n```\n:::\n\nThe reducer (`operator.add` in Python) concatenates these lists, collecting all parallel results into `state[\"results\"]`.\n\n### Synthesis phase\n\nAfter all agents complete, the `synthesize_results` function iterates over the collected results:\n\n- Waits for all parallel branches to complete (LangGraph handles this automatically)\n- References the original query to ensure the answer addresses what the user asked\n- Combines information from all sources without redundancy\n\n<Note>\n**Partial results**: In this tutorial, all selected agents must complete before synthesis. For more advanced patterns where you want to handle partial results or timeouts, see the [map-reduce guide](/oss/langchain/map-reduce).\n</Note>\n\n## 8. Complete working example\n\nHere's everything together in a runnable script:\n\n<Expandable title=\"View complete code\" defaultOpen={false}>\n\n:::python\n```python\n\"\"\"\nMulti-Source Knowledge Router Example", "metadata": {"source": "multi-agent/router-knowledge-base.mdx"}}
{"text": " `state[\"results\"]`.\n\n### Synthesis phase\n\nAfter all agents complete, the `synthesize_results` function iterates over the collected results:\n\n- Waits for all parallel branches to complete (LangGraph handles this automatically)\n- References the original query to ensure the answer addresses what the user asked\n- Combines information from all sources without redundancy\n\n<Note>\n**Partial results**: In this tutorial, all selected agents must complete before synthesis. For more advanced patterns where you want to handle partial results or timeouts, see the [map-reduce guide](/oss/langchain/map-reduce).\n</Note>\n\n## 8. Complete working example\n\nHere's everything together in a runnable script:\n\n<Expandable title=\"View complete code\" defaultOpen={false}>\n\n:::python\n```python\n\"\"\"\nMulti-Source Knowledge Router Example\n\nThis example demonstrates the router pattern for multi-agent systems.\nA router classifies queries, routes them to specialized agents in parallel,\nand synthesizes results into a combined response.\n\"\"\"\n\nimport operator\nfrom typing import Annotated, Literal, TypedDict\n\nfrom langchain.agents import create_agent\nfrom langchain.chat_models import init_chat_model\nfrom langchain.tools import tool\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.types import Send\nfrom pydantic import BaseModel, Field\n\n\n# State definitions\nclass AgentInput(TypedDict):\n    \"\"\"Simple input state for each subagent.\"\"\"\n    query: str\n\n\nclass AgentOutput(TypedDict):\n    \"\"\"Output from each subagent.\"\"\"\n    source: str\n    result: str\n\n\nclass Classification(TypedDict):\n    \"\"\"A single routing decision: which agent to call with what query.\"\"\"\n    source: Literal[\"github\", \"notion\", \"slack\"]\n    query: str\n\n\nclass RouterState(TypedDict):\n    query: str\n    classifications: list[Classification]\n    results: Annotated[list[AgentOutput], operator.add]\n    final_answer: str\n\n\n# Structured output schema for classifier\nclass ClassificationResult(BaseModel):\n    \"\"\"Result of classifying a user query into agent-specific sub-questions.\"\"\"\n    classifications: list[Classification] = Field(\n        description=\"List of agents to invoke with their targeted sub-questions\"\n    )\n\n\n# Tools\n@tool\ndef search_code(query: str, repo: str = \"main\") -> str:\n    \"\"\"Search code in GitHub repositories.\"\"\"\n    return f\"Found code matching '{query}' in {repo}: authentication middleware in src/auth.py\"\n\n\n@tool\ndef search_issues(query: str) -> str:\n    \"\"\"Search GitHub issues and pull requests.\"\"\"\n    return f\"Found 3 issues matching '{query}': #142 (API auth docs), #89 (OAuth flow), #203 (token refresh)\"\n\n\n@tool\ndef search_prs(query: str) -> str:\n    \"\"\"Search pull requests for implementation details.\"\"\"\n    return f\"PR #156 added JWT authentication, PR #178 updated OAuth scopes\"\n\n\n@tool\ndef search_notion(query: str) -> str", "metadata": {"source": "multi-agent/router-knowledge-base.mdx"}}
{"text": "\ndef search_code(query: str, repo: str = \"main\") -> str:\n    \"\"\"Search code in GitHub repositories.\"\"\"\n    return f\"Found code matching '{query}' in {repo}: authentication middleware in src/auth.py\"\n\n\n@tool\ndef search_issues(query: str) -> str:\n    \"\"\"Search GitHub issues and pull requests.\"\"\"\n    return f\"Found 3 issues matching '{query}': #142 (API auth docs), #89 (OAuth flow), #203 (token refresh)\"\n\n\n@tool\ndef search_prs(query: str) -> str:\n    \"\"\"Search pull requests for implementation details.\"\"\"\n    return f\"PR #156 added JWT authentication, PR #178 updated OAuth scopes\"\n\n\n@tool\ndef search_notion(query: str) -> str:\n    \"\"\"Search Notion workspace for documentation.\"\"\"\n    return f\"Found documentation: 'API Authentication Guide' - covers OAuth2 flow, API keys, and JWT tokens\"\n\n\n@tool\ndef get_page(page_id: str) -> str:\n    \"\"\"Get a specific Notion page by ID.\"\"\"\n    return f\"Page content: Step-by-step authentication setup instructions\"\n\n\n@tool\ndef search_slack(query: str) -> str:\n    \"\"\"Search Slack messages and threads.\"\"\"\n    return f\"Found discussion in #engineering: 'Use Bearer tokens for API auth, see docs for refresh flow'\"\n\n\n@tool\ndef get_thread(thread_id: str) -> str:\n    \"\"\"Get a specific Slack thread.\"\"\"\n    return f\"Thread discusses best practices for API key rotation\"\n\n\n# Models and agents\nmodel = init_chat_model(\"openai:gpt-4.1\")\nrouter_llm = init_chat_model(\"openai:gpt-4.1-mini\")\n\ngithub_agent = create_agent(\n    model,\n    tools=[search_code, search_issues, search_prs],\n    system_prompt=(\n        \"You are a GitHub expert. Answer questions about code, \"\n        \"API references, and implementation details by searching \"\n        \"repositories, issues, and pull requests.\"\n    ),\n)\n\nnotion_agent = create_agent(\n    model,\n    tools=[search_notion, get_page],\n    system_prompt=(\n        \"You are a Notion expert. Answer questions about internal \"\n        \"processes, policies, and team documentation by searching \"\n        \"the organization's Notion workspace.\"\n    ),\n)\n\nslack_agent = create_agent(\n    model,\n    tools=[search_slack, get_thread],\n    system_prompt=(\n        \"You are a Slack expert. Answer questions by searching \"\n        \"relevant threads and discussions where team members have \"\n        \"shared knowledge and solutions.\"\n    ),\n)\n\n\n# Workflow nodes\ndef classify_query(state: RouterState) -> dict:\n    \"\"\"Classify query and determine which agents to", "metadata": {"source": "multi-agent/router-knowledge-base.mdx"}}
{"text": "prompt=(\n        \"You are a Notion expert. Answer questions about internal \"\n        \"processes, policies, and team documentation by searching \"\n        \"the organization's Notion workspace.\"\n    ),\n)\n\nslack_agent = create_agent(\n    model,\n    tools=[search_slack, get_thread],\n    system_prompt=(\n        \"You are a Slack expert. Answer questions by searching \"\n        \"relevant threads and discussions where team members have \"\n        \"shared knowledge and solutions.\"\n    ),\n)\n\n\n# Workflow nodes\ndef classify_query(state: RouterState) -> dict:\n    \"\"\"Classify query and determine which agents to invoke.\"\"\"\n    structured_llm = router_llm.with_structured_output(ClassificationResult)\n\n    result = structured_llm.invoke([\n        {\n            \"role\": \"system\",\n            \"content\": \"\"\"Analyze this query and determine which knowledge bases to consult.\nFor each relevant source, generate a targeted sub-question optimized for that source.\n\nAvailable sources:\n- github: Code, API references, implementation details, issues, pull requests\n- notion: Internal documentation, processes, policies, team wikis\n- slack: Team discussions, informal knowledge sharing, recent conversations\n\nReturn ONLY the sources that are relevant to the query.\"\"\"\n        },\n        {\"role\": \"user\", \"content\": state[\"query\"]}\n    ])\n\n    return {\"classifications\": result.classifications}\n\n\ndef route_to_agents(state: RouterState) -> list[Send]:\n    \"\"\"Fan out to agents based on classifications.\"\"\"\n    return [\n        Send(c[\"source\"], {\"query\": c[\"query\"]})\n        for c in state[\"classifications\"]\n    ]\n\n\ndef query_github(state: AgentInput) -> dict:\n    \"\"\"Query the GitHub agent.\"\"\"\n    result = github_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": state[\"query\"]}]\n    })\n    return {\"results\": [{\"source\": \"github\", \"result\": result[\"messages\"][-1].content}]}\n\n\ndef query_notion(state: AgentInput) -> dict:\n    \"\"\"Query the Notion agent.\"\"\"\n    result = notion_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": state[\"query\"]}]\n    })\n    return {\"results\": [{\"source\": \"notion\", \"result\": result[\"messages\"][-1].content}]}\n\n\ndef query_slack(state: AgentInput) -> dict:\n    \"\"\"Query the Slack agent.\"\"\"\n    result = slack_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": state[\"query\"]}]\n    })\n    return {\"results\": [{\"source\": \"slack", "metadata": {"source": "multi-agent/router-knowledge-base.mdx"}}
{"text": "ages\"][-1].content}]}\n\n\ndef query_notion(state: AgentInput) -> dict:\n    \"\"\"Query the Notion agent.\"\"\"\n    result = notion_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": state[\"query\"]}]\n    })\n    return {\"results\": [{\"source\": \"notion\", \"result\": result[\"messages\"][-1].content}]}\n\n\ndef query_slack(state: AgentInput) -> dict:\n    \"\"\"Query the Slack agent.\"\"\"\n    result = slack_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": state[\"query\"]}]\n    })\n    return {\"results\": [{\"source\": \"slack\", \"result\": result[\"messages\"][-1].content}]}\n\n\ndef synthesize_results(state: RouterState) -> dict:\n    \"\"\"Combine results from all agents into a coherent answer.\"\"\"\n    if not state[\"results\"]:\n        return {\"final_answer\": \"No results found from any knowledge source.\"}\n\n    formatted = [\n        f\"**From {r['source'].title()}:**\\n{r['result']}\"\n        for r in state[\"results\"]\n    ]\n\n    synthesis_response = router_llm.invoke([\n        {\n            \"role\": \"system\",\n            \"content\": f\"\"\"Synthesize these search results to answer the original question: \"{state['query']}\"\n\n- Combine information from multiple sources without redundancy\n- Highlight the most relevant and actionable information\n- Note any discrepancies between sources\n- Keep the response concise and well-organized\"\"\"\n        },\n        {\"role\": \"user\", \"content\": \"\\n\\n\".join(formatted)}\n    ])\n\n    return {\"final_answer\": synthesis_response.content}\n\n\n# Build workflow\nworkflow = (\n    StateGraph(RouterState)\n    .add_node(\"classify\", classify_query)\n    .add_node(\"github\", query_github)\n    .add_node(\"notion\", query_notion)\n    .add_node(\"slack\", query_slack)\n    .add_node(\"synthesize\", synthesize_results)\n    .add_edge(START, \"classify\")\n    .add_conditional_edges(\"classify\", route_to_agents, [\"github\", \"notion\", \"slack\"])\n    .add_edge(\"github\", \"synthesize\")\n    .add_edge(\"notion\", \"synthesize\")\n    .add_edge(\"slack\", \"synthesize\")\n    .add_edge(\"synthesize\", END)\n    .compile()\n)\n\n\nif __name__ == \"__main__\":\n    result = workflow.invoke({\n        \"query\": \"How do I authenticate API requests?\"\n    })\n\n   ", "metadata": {"source": "multi-agent/router-knowledge-base.mdx"}}
{"text": ")\n    .add_node(\"synthesize\", synthesize_results)\n    .add_edge(START, \"classify\")\n    .add_conditional_edges(\"classify\", route_to_agents, [\"github\", \"notion\", \"slack\"])\n    .add_edge(\"github\", \"synthesize\")\n    .add_edge(\"notion\", \"synthesize\")\n    .add_edge(\"slack\", \"synthesize\")\n    .add_edge(\"synthesize\", END)\n    .compile()\n)\n\n\nif __name__ == \"__main__\":\n    result = workflow.invoke({\n        \"query\": \"How do I authenticate API requests?\"\n    })\n\n    print(\"Original query:\", result[\"query\"])\n    print(\"\\nClassifications:\")\n    for c in result[\"classifications\"]:\n        print(f\"  {c['source']}: {c['query']}\")\n    print(\"\\n\" + \"=\" * 60 + \"\\n\")\n    print(\"Final Answer:\")\n    print(result[\"final_answer\"])\n```\n:::\n\n:::js\n```typescript\n/**\n * Multi-Source Knowledge Router Example\n *\n * This example demonstrates the router pattern for multi-agent systems.\n * A router classifies queries, routes them to specialized agents in parallel,\n * and synthesizes results into a combined response.\n */\nimport { z } from \"zod/v4\";\nimport { tool } from \"langchain\";\nimport { StateGraph, START, END, Send, StateSchema, ReducedValue } from \"@langchain/langgraph\";\n\nconst AgentOutput = z.object({\n  source: z.string(),\n  result: z.string(),\n});\n\nconst RouterState = new StateSchema({\n  query: z.string(),\n  classifications: z.array(\n    z.object({\n      source: z.enum([\"github\", \"notion\", \"slack\"]),\n      query: z.string(),\n    })\n  ),\n  results: new ReducedValue(\n    z.array(AgentOutput).default(() => []),\n    { reducer: (current, update) => current.concat(update) }\n  ),\n  finalAnswer: z.string(),\n});\n\nconst searchCode = tool(\n  async ({ query, repo }) => {\n    return `Found code matching '${query}' in ${repo || \"main\"}: authentication middleware in src/auth.py`;\n  },\n  {\n    name: \"search_code\",\n    description: \"Search code in GitHub repositories.\",\n    schema: z.object({\n      query: z.string(),\n      repo: z.string().optional().default(\"main\"),\n    }),\n  }\n);\n\nconst searchIssues = tool(\n  async ({ query }) => {\n    return `Found 3 issues matching '${query}': #142 (API auth docs), #89 (OAuth flow), #203 (token refresh)`;\n  },\n  {\n    name: \"search_issues\",\n ", "metadata": {"source": "multi-agent/router-knowledge-base.mdx"}}
{"text": " tool(\n  async ({ query, repo }) => {\n    return `Found code matching '${query}' in ${repo || \"main\"}: authentication middleware in src/auth.py`;\n  },\n  {\n    name: \"search_code\",\n    description: \"Search code in GitHub repositories.\",\n    schema: z.object({\n      query: z.string(),\n      repo: z.string().optional().default(\"main\"),\n    }),\n  }\n);\n\nconst searchIssues = tool(\n  async ({ query }) => {\n    return `Found 3 issues matching '${query}': #142 (API auth docs), #89 (OAuth flow), #203 (token refresh)`;\n  },\n  {\n    name: \"search_issues\",\n    description: \"Search GitHub issues and pull requests.\",\n    schema: z.object({\n      query: z.string(),\n    }),\n  }\n);\n\nconst searchPrs = tool(\n  async ({ query }) => {\n    return `PR #156 added JWT authentication, PR #178 updated OAuth scopes`;\n  },\n  {\n    name: \"search_prs\",\n    description: \"Search pull requests for implementation details.\",\n    schema: z.object({\n      query: z.string(),\n    }),\n  }\n);\n\nconst searchNotion = tool(\n  async ({ query }) => {\n    return `Found documentation: 'API Authentication Guide' - covers OAuth2 flow, API keys, and JWT tokens`;\n  },\n  {\n    name: \"search_notion\",\n    description: \"Search Notion workspace for documentation.\",\n    schema: z.object({\n      query: z.string(),\n    }),\n  }\n);\n\nconst getPage = tool(\n  async ({ pageId }) => {\n    return `Page content: Step-by-step authentication setup instructions`;\n  },\n  {\n    name: \"get_page\",\n    description: \"Get a specific Notion page by ID.\",\n    schema: z.object({\n      pageId: z.string(),\n    }),\n  }\n);\n\nconst searchSlack = tool(\n  async ({ query }) => {\n    return `Found discussion in #engineering: 'Use Bearer tokens for API auth, see docs for refresh flow'`;\n  },\n  {\n    name: \"search_slack\",\n    description: \"Search Slack messages and threads.\",\n    schema: z.object({\n      query: z.string(),\n    }),\n  }\n);\n\nconst getThread = tool(\n  async ({ threadId }) => {\n    return `Thread discusses best practices for API key rotation`;\n  },\n  {\n    name: \"get_thread\",\n    description: \"Get a specific Slack thread.\",\n    schema: z.object({\n      threadId: z.string(),\n    }),\n  }\n);\n\nimport { createAgent } from \"langchain\";\nimport { ChatOpenAI } from \"@langchain/openai\";", "metadata": {"source": "multi-agent/router-knowledge-base.mdx"}}
{"text": " for API auth, see docs for refresh flow'`;\n  },\n  {\n    name: \"search_slack\",\n    description: \"Search Slack messages and threads.\",\n    schema: z.object({\n      query: z.string(),\n    }),\n  }\n);\n\nconst getThread = tool(\n  async ({ threadId }) => {\n    return `Thread discusses best practices for API key rotation`;\n  },\n  {\n    name: \"get_thread\",\n    description: \"Get a specific Slack thread.\",\n    schema: z.object({\n      threadId: z.string(),\n    }),\n  }\n);\n\nimport { createAgent } from \"langchain\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst llm = new ChatOpenAI({ model: \"gpt-4.1\" });\n\nconst githubAgent = createAgent({\n  model: llm,\n  tools: [searchCode, searchIssues, searchPrs],\n  systemPrompt: `\nYou are a GitHub expert. Answer questions about code,\nAPI references, and implementation details by searching\nrepositories, issues, and pull requests.\n  `.trim(),\n});\n\nconst notionAgent = createAgent({\n  model: llm,\n  tools: [searchNotion, getPage],\n  systemPrompt: `\nYou are a Notion expert. Answer questions about internal\nprocesses, policies, and team documentation by searching\nthe organization's Notion workspace.\n  `.trim(),\n});\n\nconst slackAgent = createAgent({\n  model: llm,\n  tools: [searchSlack, getThread],\n  systemPrompt: `\nYou are a Slack expert. Answer questions by searching\nrelevant threads and discussions where team members have\nshared knowledge and solutions.\n  `.trim(),\n});\n\nconst routerLlm = new ChatOpenAI({ model: \"gpt-4.1-mini\" });\n\n// Define structured output schema for the classifier\nconst ClassificationResultSchema = z.object({\n  // [!code highlight]\n  classifications: z\n    .array(\n      z.object({\n        source: z.enum([\"github\", \"notion\", \"slack\"]),\n        query: z.string(),\n      })\n    )\n    .describe(\"List of agents to invoke with their targeted sub-questions\"),\n});\n\nasync function classifyQuery(state: typeof RouterState.State) {\n  const structuredLlm = routerLlm.withStructuredOutput(\n    ClassificationResultSchema\n  ); // [!code highlight]\n\n  const result = await structuredLlm.invoke([\n    {\n      role: \"system\",\n      content: `Analyze this query and determine which knowledge bases to consult.\nFor each relevant source, generate a targeted sub-question optimized for that source.\n\nAvailable sources:\n- github: Code, API references, implementation details, issues, pull requests\n- notion: Internal documentation, processes, policies, team wikis\n- slack: Team discussions, informal knowledge sharing, recent conversations\n\nReturn ONLY the sources that are relevant to the query. Each source should have\na targeted sub-question optimized for that specific knowledge domain", "metadata": {"source": "multi-agent/router-knowledge-base.mdx"}}
{"text": "\"),\n});\n\nasync function classifyQuery(state: typeof RouterState.State) {\n  const structuredLlm = routerLlm.withStructuredOutput(\n    ClassificationResultSchema\n  ); // [!code highlight]\n\n  const result = await structuredLlm.invoke([\n    {\n      role: \"system\",\n      content: `Analyze this query and determine which knowledge bases to consult.\nFor each relevant source, generate a targeted sub-question optimized for that source.\n\nAvailable sources:\n- github: Code, API references, implementation details, issues, pull requests\n- notion: Internal documentation, processes, policies, team wikis\n- slack: Team discussions, informal knowledge sharing, recent conversations\n\nReturn ONLY the sources that are relevant to the query. Each source should have\na targeted sub-question optimized for that specific knowledge domain.\n\nExample for \"How do I authenticate API requests?\":\n- github: \"What authentication code exists? Search for auth middleware, JWT handling\"\n- notion: \"What authentication documentation exists? Look for API auth guides\"\n(slack omitted because it's not relevant for this technical question)`,\n    },\n    { role: \"user\", content: state.query },\n  ]);\n\n  return { classifications: result.classifications };\n}\n\nfunction routeToAgents(state: typeof RouterState.State): Send[] {\n  return state.classifications.map(\n    (c) => new Send(c.source, { query: c.query }) // [!code highlight]\n  );\n}\n\nasync function queryGithub(state: typeof RouterState.State) {\n  const result = await githubAgent.invoke({\n    messages: [{ role: \"user\", content: state.query }], // [!code highlight]\n  });\n  return {\n    results: [{ source: \"github\", result: result.messages.at(-1)?.content }],\n  };\n}\n\nasync function queryNotion(state: typeof RouterState.State) {\n  const result = await notionAgent.invoke({\n    messages: [{ role: \"user\", content: state.query }], // [!code highlight]\n  });\n  return {\n    results: [{ source: \"notion\", result: result.messages.at(-1)?.content }],\n  };\n}\n\nasync function querySlack(state: typeof RouterState.State) {\n  const result = await slackAgent.invoke({\n    messages: [{ role: \"user\", content: state.query }], // [!code highlight]\n  });\n  return {\n    results: [{ source: \"slack\", result: result.messages.at(-1)?.content }],\n  };\n}\n\nasync function synthesizeResults(state: typeof RouterState.State) {\n  if (state.results.length === 0) {\n    return { finalAnswer: \"No results found from any knowledge source.\" };\n  }\n\n  // Format results for synthesis\n  const formatted = state.results.map(\n    (r) =>\n      `**From ${r.source.charAt(0).toUpperCase() + r.source.slice(1)}:**\\n${r.result}`\n  );\n\n  const synthesisResponse = await routerLlm.invoke([\n", "metadata": {"source": "multi-agent/router-knowledge-base.mdx"}}
{"text": " role: \"user\", content: state.query }], // [!code highlight]\n  });\n  return {\n    results: [{ source: \"slack\", result: result.messages.at(-1)?.content }],\n  };\n}\n\nasync function synthesizeResults(state: typeof RouterState.State) {\n  if (state.results.length === 0) {\n    return { finalAnswer: \"No results found from any knowledge source.\" };\n  }\n\n  // Format results for synthesis\n  const formatted = state.results.map(\n    (r) =>\n      `**From ${r.source.charAt(0).toUpperCase() + r.source.slice(1)}:**\\n${r.result}`\n  );\n\n  const synthesisResponse = await routerLlm.invoke([\n    {\n      role: \"system\",\n      content: `Synthesize these search results to answer the original question: \"${state.query}\"\n\n- Combine information from multiple sources without redundancy\n- Highlight the most relevant and actionable information\n- Note any discrepancies between sources\n- Keep the response concise and well-organized`,\n    },\n    { role: \"user\", content: formatted.join(\"\\n\\n\") },\n  ]);\n\n  return { finalAnswer: synthesisResponse.content };\n}\n\nconst workflow = new StateGraph(RouterState)\n  .addNode(\"classify\", classifyQuery)\n  .addNode(\"github\", queryGithub)\n  .addNode(\"notion\", queryNotion)\n  .addNode(\"slack\", querySlack)\n  .addNode(\"synthesize\", synthesizeResults)\n  .addEdge(START, \"classify\")\n  .addConditionalEdges(\"classify\", routeToAgents, [\"github\", \"notion\", \"slack\"])\n  .addEdge(\"github\", \"synthesize\")\n  .addEdge(\"notion\", \"synthesize\")\n  .addEdge(\"slack\", \"synthesize\")\n  .addEdge(\"synthesize\", END)\n  .compile();\n\nconst result = await workflow.invoke({\n  query: \"How do I authenticate API requests?\",\n});\n\nconsole.log(\"Original query:\", result.query);\nconsole.log(\"\\nClassifications:\");\nfor (const c of result.classifications) {\n  console.log(`  ${c.source}: ${c.query}`);\n}\nconsole.log(`\\n${\"=\".repeat(60)}\\n`);\nconsole.log(\"Final Answer:\");\nconsole.log(result.finalAnswer);\n```\n:::\n\n</Expandable>\n\n## 9. Advanced: Stateful routers\n\nThe router we've built so far is **stateless**\u2014each request is handled independently with no memory between calls. For multi-turn conversations, you need a **stateful** approach.\n\n### Tool wrapper approach\n\nThe simplest way to add conversation memory is to wrap the stateless router as a tool that a conversational agent can call:\n\n:::python\n```python\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n\n@tool\ndef search_knowledge_base(query: str) -> str:\n    \"\"\"Search across multiple knowledge sources (GitHub, Notion, Slack).\n\n    Use this to find", "metadata": {"source": "multi-agent/router-knowledge-base.mdx"}}
{"text": "\\n${\"=\".repeat(60)}\\n`);\nconsole.log(\"Final Answer:\");\nconsole.log(result.finalAnswer);\n```\n:::\n\n</Expandable>\n\n## 9. Advanced: Stateful routers\n\nThe router we've built so far is **stateless**\u2014each request is handled independently with no memory between calls. For multi-turn conversations, you need a **stateful** approach.\n\n### Tool wrapper approach\n\nThe simplest way to add conversation memory is to wrap the stateless router as a tool that a conversational agent can call:\n\n:::python\n```python\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n\n@tool\ndef search_knowledge_base(query: str) -> str:\n    \"\"\"Search across multiple knowledge sources (GitHub, Notion, Slack).\n\n    Use this to find information about code, documentation, or team discussions.\n    \"\"\"\n    result = workflow.invoke({\"query\": query})\n    return result[\"final_answer\"]\n\n\nconversational_agent = create_agent(\n    model,\n    tools=[search_knowledge_base],\n    system_prompt=(\n        \"You are a helpful assistant that answers questions about our organization. \"\n        \"Use the search_knowledge_base tool to find information across our code, \"\n        \"documentation, and team discussions.\"\n    ),\n    checkpointer=InMemorySaver(),\n)\n```\n:::\n\n:::js\n```typescript\nimport { MemorySaver } from \"@langchain/langgraph\";\n\nconst searchKnowledgeBase = tool(\n  async ({ query }) => {\n    const result = await workflow.invoke({ query });\n    return result.finalAnswer;\n  },\n  {\n    name: \"search_knowledge_base\",\n    description: `Search across multiple knowledge sources (GitHub, Notion, Slack).\nUse this to find information about code, documentation, or team discussions.`,\n    schema: z.object({\n      query: z.string().describe(\"The search query\"),\n    }),\n  }\n);\n\nconst conversationalAgent = createAgent({\n  model: llm,\n  tools: [searchKnowledgeBase],\n  systemPrompt: `\nYou are a helpful assistant that answers questions about our organization.\nUse the search_knowledge_base tool to find information across our code,\ndocumentation, and team discussions.\n  `.trim(),\n  checkpointer: new MemorySaver(),\n});\n```\n:::\n\nThis approach keeps the router stateless while the conversational agent handles memory and context. The user can have a multi-turn conversation, and the agent will call the router tool as needed.\n\n:::python\n```python\nconfig = {\"configurable\": {\"thread_id\": \"user-123\"}}\n\nresult = conversational_agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"How do I authenticate API requests?\"}]},\n    config\n)\nprint(result[\"messages\"][-1].content)\n\nresult = conversational_agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What about rate limiting for those endpoints?\"}]},", "metadata": {"source": "multi-agent/router-knowledge-base.mdx"}}
{"text": " and team discussions.\n  `.trim(),\n  checkpointer: new MemorySaver(),\n});\n```\n:::\n\nThis approach keeps the router stateless while the conversational agent handles memory and context. The user can have a multi-turn conversation, and the agent will call the router tool as needed.\n\n:::python\n```python\nconfig = {\"configurable\": {\"thread_id\": \"user-123\"}}\n\nresult = conversational_agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"How do I authenticate API requests?\"}]},\n    config\n)\nprint(result[\"messages\"][-1].content)\n\nresult = conversational_agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What about rate limiting for those endpoints?\"}]},\n    config\n)\nprint(result[\"messages\"][-1].content)\n```\n:::\n\n:::js\n```typescript\nconst config = { configurable: { thread_id: \"user-123\" } };\nlet conversationalAgentResult = await conversationalAgent.invoke(\n  {\n    messages: [\n      { role: \"user\", content: \"How do I authenticate API requests?\" },\n    ],\n  },\n  config\n);\nconsole.log(conversationalAgentResult.messages.at(-1)?.content);\n\nconversationalAgentResult = await conversationalAgent.invoke(\n  {\n    messages: [\n      {\n        role: \"user\",\n        content: \"What about rate limiting for those endpoints?\",\n      },\n    ],\n  },\n  config\n);\nconsole.log(conversationalAgentResult.messages.at(-1)?.content);\n```\n:::\n\n<Tip>\nThe tool wrapper approach is recommended for most use cases. It provides clean separation: the router handles multi-source querying, while the conversational agent handles context and memory.\n</Tip>\n\n### Full persistence approach\n\nIf you need the router itself to maintain state\u2014for example, to use previous search results in routing decisions\u2014use [persistence](/oss/langchain/short-term-memory) to store message history at the router level.\n\n<Warning>\n**Stateful routers add complexity.** When routing to different agents across turns, conversations may feel inconsistent if agents have different tones or prompts. Consider the [handoffs pattern](/oss/langchain/multi-agent/handoffs) or [subagents pattern](/oss/langchain/multi-agent/subagents) instead\u2014both provide clearer semantics for multi-turn conversations with different agents.\n</Warning>\n\n## 10. Key takeaways\n\nThe router pattern excels when you have:\n\n- **Distinct verticals**: Separate knowledge domains that each require specialized tools and prompts\n- **Parallel query needs**: Questions that benefit from querying multiple sources simultaneously\n- **Synthesis requirements**: Results from multiple sources need to be combined into a coherent response\n\nThe pattern has three phases: **decompose** (analyze the query and generate targeted sub-questions), **route** (execute queries in parallel), and **synthesize** (combine results).\n\n<Tip>\n**When to use the router pattern**\n\nUse the router pattern when you have multiple independent knowledge sources, need", "metadata": {"source": "multi-agent/router-knowledge-base.mdx"}}
{"text": "multi-agent/handoffs) or [subagents pattern](/oss/langchain/multi-agent/subagents) instead\u2014both provide clearer semantics for multi-turn conversations with different agents.\n</Warning>\n\n## 10. Key takeaways\n\nThe router pattern excels when you have:\n\n- **Distinct verticals**: Separate knowledge domains that each require specialized tools and prompts\n- **Parallel query needs**: Questions that benefit from querying multiple sources simultaneously\n- **Synthesis requirements**: Results from multiple sources need to be combined into a coherent response\n\nThe pattern has three phases: **decompose** (analyze the query and generate targeted sub-questions), **route** (execute queries in parallel), and **synthesize** (combine results).\n\n<Tip>\n**When to use the router pattern**\n\nUse the router pattern when you have multiple independent knowledge sources, need low-latency parallel queries, and want explicit control over routing logic.\n\nFor simpler cases with dynamic tool selection, consider the [subagents pattern](/oss/langchain/multi-agent/subagents). For workflows where agents need to converse with users sequentially, consider [handoffs](/oss/langchain/multi-agent/handoffs).\n</Tip>\n\n## Next steps\n\n- Learn about [handoffs](/oss/langchain/multi-agent/handoffs) for agent-to-agent conversations\n- Explore the [subagents pattern](/oss/langchain/multi-agent/subagents-personal-assistant) for centralized orchestration\n- Read the [multi-agent overview](/oss/langchain/multi-agent) to compare different patterns\n- Use [LangSmith](https://smith.langchain.com) to debug and monitor your router\n", "metadata": {"source": "multi-agent/router-knowledge-base.mdx"}}
{"text": "---\ntitle: Custom workflow\n---\n\nIn the **custom workflow** architecture, you define your own bespoke execution flow using [LangGraph](/oss/langgraph/overview). You have complete control over the graph structure\u2014including sequential steps, conditional branches, loops, and parallel execution.\n\n```mermaid\ngraph LR\n    A([Input]) --> B{{Conditional}}\n    B -->|path_a| C[Deterministic step]\n    B -->|path_b| D((Agentic step))\n    C --> G([Output])\n    D --> G([Output])\n```\n\n## Key characteristics\n\n* Complete control over graph structure\n* Mix deterministic logic with agentic behavior\n* Support for sequential steps, conditional branches, loops, and parallel execution\n* Embed other patterns as nodes in your workflow\n\n## When to use\n\nUse custom workflows when standard patterns (subagents, skills, etc.) don't fit your requirements, you need to mix deterministic logic with agentic behavior, or your use case requires complex routing or multi-stage processing.\n\nEach node in your workflow can be a simple function, an LLM call, or an entire [agent](/oss/langchain/agents) with [tools](/oss/langchain/tools). You can also compose other architectures within a custom workflow\u2014for example, embedding a multi-agent system as a single node.\n\nFor a complete example of a custom workflow, see the tutorial below.\n\n<Card\n    title=\"Tutorial: Build a multi-source knowledge base with routing\"\n    icon=\"book\"\n    href=\"/oss/langchain/multi-agent/router-knowledge-base\"\n    arrow cta=\"Learn more\"\n>\n    The [router pattern](/oss/langchain/multi-agent/router) is an example of a custom workflow. This tutorial walks through building a router that queries GitHub, Notion, and Slack in parallel, then synthesizes results.\n>\n</Card>\n\n## Basic implementation\n\nThe core insight is that you can call a LangChain agent directly inside any LangGraph node, combining the flexibility of custom workflows with the convenience of pre-built agents:\n\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom langgraph.graph import StateGraph, START, END\n\nagent = create_agent(model=\"openai:gpt-4.1\", tools=[...])\n\ndef agent_node(state: State) -> dict:\n    \"\"\"A LangGraph node that invokes a LangChain agent.\"\"\"\n    result = agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": state[\"query\"]}]\n    })\n    return {\"answer\": result[\"messages\"][-1].content}\n\n# Build a simple workflow\nworkflow = (\n    StateGraph(State)\n    .add_node(\"agent\", agent_node)\n    .add_edge(START, \"agent\")\n    .add_edge(\"agent\", END)\n    .compile()\n)\n```\n:::\n:::js\n```typescript\nimport { z } from \"zod\";\nimport { createAgent } from \"langchain\";\nimport { StateGraph, START, END, StateSchema, MessagesValue } from \"@langchain/langgraph\";\n\nconst agent = createAgent({ model: \"open", "metadata": {"source": "multi-agent/custom-workflow.mdx"}}
{"text": "\n        \"messages\": [{\"role\": \"user\", \"content\": state[\"query\"]}]\n    })\n    return {\"answer\": result[\"messages\"][-1].content}\n\n# Build a simple workflow\nworkflow = (\n    StateGraph(State)\n    .add_node(\"agent\", agent_node)\n    .add_edge(START, \"agent\")\n    .add_edge(\"agent\", END)\n    .compile()\n)\n```\n:::\n:::js\n```typescript\nimport { z } from \"zod\";\nimport { createAgent } from \"langchain\";\nimport { StateGraph, START, END, StateSchema, MessagesValue } from \"@langchain/langgraph\";\n\nconst agent = createAgent({ model: \"openai:gpt-4o\", tools: [...] });\n\nconst AgentState = new StateSchema({\n  messages: MessagesValue,\n  query: z.string(),\n});\n\nconst agentNode: GraphNode<typeof AgentState> = (state) => {\n  // A LangGraph node that invokes a LangChain agent\n  const result = await agent.invoke({\n    messages: [{ role: \"user\", content: state.query }]\n  });\n  return { answer: result.messages.at(-1)?.content };\n}\n\n// Build a simple workflow\nconst workflow = new StateGraph(State)\n  .addNode(\"agent\", agentNode)\n  .addEdge(START, \"agent\")\n  .addEdge(\"agent\", END)\n  .compile();\n```\n:::\n\n## Example: RAG pipeline\n\nA common use case is combining [retrieval](/oss/langchain/retrieval) with an agent. This example builds a WNBA stats assistant that retrieves from a knowledge base and can fetch live news.\n\n<Accordion title=\"Custom RAG workflow\">\n\nThe workflow demonstrates three types of nodes:\n\n- **Model node** (Rewrite): Rewrites the user query for better retrieval using [structured output](/oss/langchain/structured-output).\n- **Deterministic node** (Retrieve): Performs vector similarity search \u2014 no LLM involved.\n- **Agent node** (Agent): Reasons over retrieved context and can fetch additional information via tools.\n\n```mermaid\ngraph LR\n    A([Query]) --> B{{Rewrite}}\n    B --> C[(Retrieve)]\n    C --> D((Agent))\n    D --> E([Response])\n```\n\n<Tip>\nYou can use LangGraph state to pass information between workflow steps. This allows each part of your workflow to read and update structured fields, making it easy to share data and context across nodes.\n</Tip>\n\n:::python\n```python\nfrom typing import TypedDict\nfrom pydantic import BaseModel\nfrom langgraph.graph import StateGraph, START, END\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass State(TypedDict):\n    question: str\n    rewritten_query: str\n    documents: list[str]\n    answer: str\n\n# WNBA knowledge base with rosters, game results, and player", "metadata": {"source": "multi-agent/custom-workflow.mdx"}}
{"text": "([Response])\n```\n\n<Tip>\nYou can use LangGraph state to pass information between workflow steps. This allows each part of your workflow to read and update structured fields, making it easy to share data and context across nodes.\n</Tip>\n\n:::python\n```python\nfrom typing import TypedDict\nfrom pydantic import BaseModel\nfrom langgraph.graph import StateGraph, START, END\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nclass State(TypedDict):\n    question: str\n    rewritten_query: str\n    documents: list[str]\n    answer: str\n\n# WNBA knowledge base with rosters, game results, and player stats\nembeddings = OpenAIEmbeddings()\nvector_store = InMemoryVectorStore(embeddings)\nvector_store.add_texts([\n    # Rosters\n    \"New York Liberty 2024 roster: Breanna Stewart, Sabrina Ionescu, Jonquel Jones, Courtney Vandersloot.\",\n    \"Las Vegas Aces 2024 roster: A'ja Wilson, Kelsey Plum, Jackie Young, Chelsea Gray.\",\n    \"Indiana Fever 2024 roster: Caitlin Clark, Aliyah Boston, Kelsey Mitchell, NaLyssa Smith.\",\n    # Game results\n    \"2024 WNBA Finals: New York Liberty defeated Minnesota Lynx 3-2 to win the championship.\",\n    \"June 15, 2024: Indiana Fever 85, Chicago Sky 79. Caitlin Clark had 23 points and 8 assists.\",\n    \"August 20, 2024: Las Vegas Aces 92, Phoenix Mercury 84. A'ja Wilson scored 35 points.\",\n    # Player stats\n    \"A'ja Wilson 2024 season stats: 26.9 PPG, 11.9 RPG, 2.6 BPG. Won MVP award.\",\n    \"Caitlin Clark 2024 rookie stats: 19.2 PPG, 8.4 APG, 5.7 RPG. Won Rookie of the Year.\",\n    \"Breanna Stewart 2024 stats: 20.4 PPG, 8.5 RPG, 3.5 APG.\",\n])\nretriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n\n@tool\ndef get_latest_news(query: str) -> str:\n    \"\"\"Get the latest WNBA news and updates.\"\"\"\n    # Your news API here\n    return \"Latest: The WNBA announced expanded playoff format for 2025...\"\n\nagent = create_agent(\n    model=\"openai:gpt-4.1\",\n    tools=[get_latest_news],\n)\n\nmodel = ChatOpenAI(model=\"gpt-4.1\")\n\nclass RewrittenQuery(BaseModel):\n    query: str\n\ndef rewrite_query(state: State) -> dict:\n    \"\"\"Rewrite the user query for better retrieval.\"\"\"\n    system_prompt = \"\"\"Rewrite this query to retrieve relevant WNBA information.\nThe knowledge base contains: team rosters, game results with scores, and player statistics (PPG, RPG, APG).\nFocus on specific player names, team names, or stat categories mentioned.\"\"\"\n    response = model.with_structured_output(RewrittenQuery).invoke([\n  ", "metadata": {"source": "multi-agent/custom-workflow.mdx"}}
{"text": "Latest: The WNBA announced expanded playoff format for 2025...\"\n\nagent = create_agent(\n    model=\"openai:gpt-4.1\",\n    tools=[get_latest_news],\n)\n\nmodel = ChatOpenAI(model=\"gpt-4.1\")\n\nclass RewrittenQuery(BaseModel):\n    query: str\n\ndef rewrite_query(state: State) -> dict:\n    \"\"\"Rewrite the user query for better retrieval.\"\"\"\n    system_prompt = \"\"\"Rewrite this query to retrieve relevant WNBA information.\nThe knowledge base contains: team rosters, game results with scores, and player statistics (PPG, RPG, APG).\nFocus on specific player names, team names, or stat categories mentioned.\"\"\"\n    response = model.with_structured_output(RewrittenQuery).invoke([\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": state[\"question\"]}\n    ])\n    return {\"rewritten_query\": response.query}\n\ndef retrieve(state: State) -> dict:\n    \"\"\"Retrieve documents based on the rewritten query.\"\"\"\n    docs = retriever.invoke(state[\"rewritten_query\"])\n    return {\"documents\": [doc.page_content for doc in docs]}\n\ndef call_agent(state: State) -> dict:\n    \"\"\"Generate answer using retrieved context.\"\"\"\n    context = \"\\n\\n\".join(state[\"documents\"])\n    prompt = f\"Context:\\n{context}\\n\\nQuestion: {state['question']}\"\n    response = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": prompt}]})\n    return {\"answer\": response[\"messages\"][-1].content_blocks}\n\nworkflow = (\n    StateGraph(State)\n    .add_node(\"rewrite\", rewrite_query)\n    .add_node(\"retrieve\", retrieve)\n    .add_node(\"agent\", call_agent)\n    .add_edge(START, \"rewrite\")\n    .add_edge(\"rewrite\", \"retrieve\")\n    .add_edge(\"retrieve\", \"agent\")\n    .add_edge(\"agent\", END)\n    .compile()\n)\n\nresult = workflow.invoke({\"question\": \"Who won the 2024 WNBA Championship?\"})\nprint(result[\"answer\"])\n```\n:::\n:::js\n```typescript\nimport { StateGraph, Annotation, START, END } from \"@langchain/langgraph\";\nimport { createAgent, tool } from \"langchain\";\nimport { ChatOpenAI, OpenAIEmbeddings } from \"@langchain/openai\";\nimport { MemoryVectorStore } from \"@langchain/classic/vectorstores/memory\";\nimport * as z from \"zod\";\n\nconst State = Annotation.Root({\n  question: Annotation<string>(),\n  rewrittenQuery: Annotation<string>(),\n  documents: Annotation<string[]>(),\n  answer: Annotation<string>(),\n});\n\n// WNBA knowledge base with rosters, game results, and player stats\nconst embeddings = new OpenAIEmbeddings();\nconst vectorStore = await MemoryVectorStore", "metadata": {"source": "multi-agent/custom-workflow.mdx"}}
{"text": "(result[\"answer\"])\n```\n:::\n:::js\n```typescript\nimport { StateGraph, Annotation, START, END } from \"@langchain/langgraph\";\nimport { createAgent, tool } from \"langchain\";\nimport { ChatOpenAI, OpenAIEmbeddings } from \"@langchain/openai\";\nimport { MemoryVectorStore } from \"@langchain/classic/vectorstores/memory\";\nimport * as z from \"zod\";\n\nconst State = Annotation.Root({\n  question: Annotation<string>(),\n  rewrittenQuery: Annotation<string>(),\n  documents: Annotation<string[]>(),\n  answer: Annotation<string>(),\n});\n\n// WNBA knowledge base with rosters, game results, and player stats\nconst embeddings = new OpenAIEmbeddings();\nconst vectorStore = await MemoryVectorStore.fromTexts(\n  [\n    // Rosters\n    \"New York Liberty 2024 roster: Breanna Stewart, Sabrina Ionescu, Jonquel Jones, Courtney Vandersloot.\",\n    \"Las Vegas Aces 2024 roster: A'ja Wilson, Kelsey Plum, Jackie Young, Chelsea Gray.\",\n    \"Indiana Fever 2024 roster: Caitlin Clark, Aliyah Boston, Kelsey Mitchell, NaLyssa Smith.\",\n    // Game results\n    \"2024 WNBA Finals: New York Liberty defeated Minnesota Lynx 3-2 to win the championship.\",\n    \"June 15, 2024: Indiana Fever 85, Chicago Sky 79. Caitlin Clark had 23 points and 8 assists.\",\n    \"August 20, 2024: Las Vegas Aces 92, Phoenix Mercury 84. A'ja Wilson scored 35 points.\",\n    // Player stats\n    \"A'ja Wilson 2024 season stats: 26.9 PPG, 11.9 RPG, 2.6 BPG. Won MVP award.\",\n    \"Caitlin Clark 2024 rookie stats: 19.2 PPG, 8.4 APG, 5.7 RPG. Won Rookie of the Year.\",\n    \"Breanna Stewart 2024 stats: 20.4 PPG, 8.5 RPG, 3.5 APG.\",\n  ],\n  [{}, {}, {}, {}, {}, {}, {}, {}, {}],\n  embeddings\n);\nconst retriever = vectorStore.asRetriever({ k: 5 });\n\nconst getLatestNews = tool(\n  async ({ query }) => {\n    // Your news API here\n    return \"Latest: The WNBA announced expanded playoff format for 2025...\";\n  },\n  {\n    name: \"get_latest_news\",\n    description: \"Get the latest WNBA news and updates\",\n    schema: z.object({ query: z.string() }),\n  }\n);\n\nconst agent = createAgent({\n  model: \"openai:gpt-4.1\",\n  tools: [getLatestNews],\n});\n\nconst model = new ChatOpenAI({ model: \"gpt-4.1\" });\n\nconst RewrittenQuery = z.object({ query: z.string() });\n\nasync function rewriteQuery(state: typeof State.State) {\n  const systemPrompt = `Rewrite this query to retrieve relevant WNBA information.\nThe knowledge base contains: team rosters, game results with scores, and player statistics (PPG, RPG, APG).\nFocus on specific player names, team names, or stat categories mentioned.`;\n ", "metadata": {"source": "multi-agent/custom-workflow.mdx"}}
{"text": " \"get_latest_news\",\n    description: \"Get the latest WNBA news and updates\",\n    schema: z.object({ query: z.string() }),\n  }\n);\n\nconst agent = createAgent({\n  model: \"openai:gpt-4.1\",\n  tools: [getLatestNews],\n});\n\nconst model = new ChatOpenAI({ model: \"gpt-4.1\" });\n\nconst RewrittenQuery = z.object({ query: z.string() });\n\nasync function rewriteQuery(state: typeof State.State) {\n  const systemPrompt = `Rewrite this query to retrieve relevant WNBA information.\nThe knowledge base contains: team rosters, game results with scores, and player statistics (PPG, RPG, APG).\nFocus on specific player names, team names, or stat categories mentioned.`;\n  const response = await model.withStructuredOutput(RewrittenQuery).invoke([\n    { role: \"system\", content: systemPrompt },\n    { role: \"user\", content: state.question },\n  ]);\n  return { rewrittenQuery: response.query };\n}\n\nasync function retrieve(state: typeof State.State) {\n  const docs = await retriever.invoke(state.rewrittenQuery);\n  return { documents: docs.map((doc) => doc.pageContent) };\n}\n\nasync function callAgent(state: typeof State.State) {\n  const context = state.documents.join(\"\\n\\n\");\n  const prompt = `Context:\\n${context}\\n\\nQuestion: ${state.question}`;\n  const response = await agent.invoke({\n    messages: [{ role: \"user\", content: prompt }],\n  });\n  return { answer: response.messages.at(-1)?.contentBlocks };\n}\n\nconst workflow = new StateGraph(State)\n  .addNode(\"rewrite\", rewriteQuery)\n  .addNode(\"retrieve\", retrieve)\n  .addNode(\"agent\", callAgent)\n  .addEdge(START, \"rewrite\")\n  .addEdge(\"rewrite\", \"retrieve\")\n  .addEdge(\"retrieve\", \"agent\")\n  .addEdge(\"agent\", END)\n  .compile();\n\nconst result = await workflow.invoke({\n  question: \"Who won the 2024 WNBA Championship?\",\n});\nconsole.log(result.answer);\n```\n:::\n\n</Accordion>\n", "metadata": {"source": "multi-agent/custom-workflow.mdx"}}
{"text": "---\ntitle: Build customer support with handoffs\nsidebarTitle: \"Handoffs: Customer support\"\n---\n\nimport ChatModelTabsPy from '/snippets/chat-model-tabs.mdx';\nimport ChatModelTabsJs from '/snippets/chat-model-tabs-js.mdx';\n\nThe [state machine pattern](/oss/langchain/multi-agent/handoffs) describes workflows where an agent's behavior changes as it moves through different states of a task. This tutorial shows how to implement a state machine by using tool calls to dynamically change a single agent's configuration\u2014updating its available tools and instructions based on the current state. The state can be determined from multiple sources: the agent's past actions (tool calls), external state (such as API call results), or even initial user input (for example, by running a classifier to determine user intent).\n\nIn this tutorial, you'll build a customer support agent that does the following:\n\n- Collects warranty information before proceeding.\n- Classifies issues as hardware or software.\n- Provides solutions or escalates to human support.\n- Maintains conversation state across multiple turns.\n\nUnlike the [subagents pattern](/oss/langchain/multi-agent/subagents-personal-assistant) where sub-agents are called as tools, the **state machine pattern** uses a single agent whose configuration changes based on workflow progress. Each \"step\" is just a different configuration (system prompt + tools) of the same underlying agent, selected dynamically based on state.\n\nHere's the workflow we'll build:\n\n```mermaid\n%%{init: {'theme':'base', 'themeVariables': {'primaryColor':'#4CAF50','primaryTextColor':'#fff','primaryBorderColor':'#2E7D32','lineColor':'#666','secondaryColor':'#FF9800','tertiaryColor':'#2196F3'}}}%%\nflowchart TD\n    %% Start\n    Start([\ud83d\udcac Customer reports<br>an issue]) --> Warranty{Is the device<br>under warranty?}\n\n    %% Warranty check\n    Warranty -->|\u2705 Yes| IssueType{What type<br>of issue?}\n    Warranty -->|\u274c No| OutOfWarranty{What type<br>of issue?}\n\n    %% In-Warranty branch\n    IssueType -->|\ud83d\udd29 Hardware| Repair[Provide warranty<br>repair instructions]\n    IssueType -->|\ud83d\udcbb Software| Troubleshoot[Provide troubleshooting<br>steps]\n\n    %% Out-of-Warranty branch\n    OutOfWarranty -->|\ud83d\udd29 Hardware| Escalate[Escalate to human<br>for paid repair options]\n    OutOfWarranty -->|\ud83d\udcbb Software| Troubleshoot\n\n    %% Troubleshooting follow-up\n    Troubleshoot --> Close([\u2705 Issue Resolved])\n    Repair --> Close\n    Escalate --> Close\n\n    %% === Styling ===\n    classDef startEnd fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff\n    classDef decisionNode fill:#2196F3,stroke:#1565C0,stroke-width:2px,color:#fff\n    classDef actionNode fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#", "metadata": {"source": "multi-agent/handoffs-customer-support.mdx"}}
{"text": "\n    OutOfWarranty -->|\ud83d\udd29 Hardware| Escalate[Escalate to human<br>for paid repair options]\n    OutOfWarranty -->|\ud83d\udcbb Software| Troubleshoot\n\n    %% Troubleshooting follow-up\n    Troubleshoot --> Close([\u2705 Issue Resolved])\n    Repair --> Close\n    Escalate --> Close\n\n    %% === Styling ===\n    classDef startEnd fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff\n    classDef decisionNode fill:#2196F3,stroke:#1565C0,stroke-width:2px,color:#fff\n    classDef actionNode fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff\n    classDef escalateNode fill:#f44336,stroke:#c62828,stroke-width:2px,color:#fff\n\n    class Start,Close startEnd\n    class Warranty,IssueType,OutOfWarranty decisionNode\n    class Repair,Troubleshoot actionNode\n    class Escalate escalateNode\n```\n\n## Setup\n\n### Installation\n\nThis tutorial requires the `langchain` package:\n\n:::python\n<CodeGroup>\n```bash pip\npip install langchain\n```\n```bash uv\nuv add langchain\n```\n```bash conda\nconda install langchain -c conda-forge\n```\n</CodeGroup>\n:::\n\n:::js\n<CodeGroup>\n```bash npm\nnpm install langchain\n```\n```bash yarn\nyarn add langchain\n```\n```bash pnpm\npnpm add langchain\n```\n</CodeGroup>\n:::\n\nFor more details, see our [Installation guide](/oss/langchain/install).\n\n### LangSmith\n\nSet up [LangSmith](https://smith.langchain.com) to inspect what is happening inside your agent. Then set the following environment variables:\n\n:::python\n<CodeGroup>\n```bash bash\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n```\n```python python\nimport getpass\nimport os\n\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```\n</CodeGroup>\n:::\n\n:::js\n<CodeGroup>\n```bash bash\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n```\n```typescript typescript\nprocess.env.LANGSMITH_TRACING = \"true\";\nprocess.env.LANGSMITH_API_KEY = \"...\";\n```\n</CodeGroup>\n:::\n\n### Select an LLM\n\nSelect a chat model from LangChain's suite of integrations:\n\n:::python\n<ChatModelTabsPy />\n:::\n\n:::js\n<ChatModelTabsJs />\n:::\n\n## 1. Define custom state\n\nFirst, define a custom state schema that tracks which step is currently active:\n\n:::python\n```python\nfrom langchain.agents import AgentState\nfrom typing_", "metadata": {"source": "multi-agent/handoffs-customer-support.mdx"}}
{"text": ":\n\n:::js\n<CodeGroup>\n```bash bash\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n```\n```typescript typescript\nprocess.env.LANGSMITH_TRACING = \"true\";\nprocess.env.LANGSMITH_API_KEY = \"...\";\n```\n</CodeGroup>\n:::\n\n### Select an LLM\n\nSelect a chat model from LangChain's suite of integrations:\n\n:::python\n<ChatModelTabsPy />\n:::\n\n:::js\n<ChatModelTabsJs />\n:::\n\n## 1. Define custom state\n\nFirst, define a custom state schema that tracks which step is currently active:\n\n:::python\n```python\nfrom langchain.agents import AgentState\nfrom typing_extensions import NotRequired\nfrom typing import Literal\n\n# Define the possible workflow steps\nSupportStep = Literal[\"warranty_collector\", \"issue_classifier\", \"resolution_specialist\"]  # [!code highlight]\n\nclass SupportState(AgentState):  # [!code highlight]\n    \"\"\"State for customer support workflow.\"\"\"\n    current_step: NotRequired[SupportStep]  # [!code highlight]\n    warranty_status: NotRequired[Literal[\"in_warranty\", \"out_of_warranty\"]]\n    issue_type: NotRequired[Literal[\"hardware\", \"software\"]]\n```\n:::\n\n:::js\n```typescript\nimport { StateSchema } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\n// Define the possible workflow steps\nconst SupportStepSchema = z.enum([\"warranty_collector\", \"issue_classifier\", \"resolution_specialist\"]);  // [!code highlight]\nconst WarrantyStatusSchema = z.enum([\"in_warranty\", \"out_of_warranty\"]);\nconst IssueTypeSchema = z.enum([\"hardware\", \"software\"]);\n\n// State for customer support workflow\nconst SupportState = new StateSchema({  // [!code highlight]\n  currentStep: SupportStepSchema.optional(),  // [!code highlight]\n  warrantyStatus: WarrantyStatusSchema.optional(),\n  issueType: IssueTypeSchema.optional(),\n});\n```\n:::\n\nThe `current_step` field is the core of the state machine pattern - it determines which configuration (prompt + tools) is loaded on each turn.\n\n## 2. Create tools that manage workflow state\n\nCreate tools that update the workflow state. These tools allow the agent to record information and transition to the next step.\n\nThe key is using `Command` to update state, including the `current_step` field:\n\n:::python\n```python\nfrom langchain.tools import tool, ToolRuntime\nfrom langchain.messages import ToolMessage\nfrom langgraph.types import Command\n\n@tool\ndef record_warranty_status(\n    status: Literal[\"in_warranty\", \"out_of_warranty\"],\n    runtime: ToolRuntime[None, SupportState],\n) -> Command:  # [!code highlight]\n    \"\"\"Record the customer's warranty status and transition to issue classification.\"\"\"\n    return Command(  # [!code highlight]\n     ", "metadata": {"source": "multi-agent/handoffs-customer-support.mdx"}}
{"text": " turn.\n\n## 2. Create tools that manage workflow state\n\nCreate tools that update the workflow state. These tools allow the agent to record information and transition to the next step.\n\nThe key is using `Command` to update state, including the `current_step` field:\n\n:::python\n```python\nfrom langchain.tools import tool, ToolRuntime\nfrom langchain.messages import ToolMessage\nfrom langgraph.types import Command\n\n@tool\ndef record_warranty_status(\n    status: Literal[\"in_warranty\", \"out_of_warranty\"],\n    runtime: ToolRuntime[None, SupportState],\n) -> Command:  # [!code highlight]\n    \"\"\"Record the customer's warranty status and transition to issue classification.\"\"\"\n    return Command(  # [!code highlight]\n        update={  # [!code highlight]\n            \"messages\": [\n                ToolMessage(\n                    content=f\"Warranty status recorded as: {status}\",\n                    tool_call_id=runtime.tool_call_id,\n                )\n            ],\n            \"warranty_status\": status,\n            \"current_step\": \"issue_classifier\",  # [!code highlight]\n        }\n    )\n\n\n@tool\ndef record_issue_type(\n    issue_type: Literal[\"hardware\", \"software\"],\n    runtime: ToolRuntime[None, SupportState],\n) -> Command:  # [!code highlight]\n    \"\"\"Record the type of issue and transition to resolution specialist.\"\"\"\n    return Command(  # [!code highlight]\n        update={  # [!code highlight]\n            \"messages\": [\n                ToolMessage(\n                    content=f\"Issue type recorded as: {issue_type}\",\n                    tool_call_id=runtime.tool_call_id,\n                )\n            ],\n            \"issue_type\": issue_type,\n            \"current_step\": \"resolution_specialist\",  # [!code highlight]\n        }\n    )\n\n\n@tool\ndef escalate_to_human(reason: str) -> str:\n    \"\"\"Escalate the case to a human support specialist.\"\"\"\n    # In a real system, this would create a ticket, notify staff, etc.\n    return f\"Escalating to human support. Reason: {reason}\"\n\n\n@tool\ndef provide_solution", "metadata": {"source": "multi-agent/handoffs-customer-support.mdx"}}
{"text": "      tool_call_id=runtime.tool_call_id,\n                )\n            ],\n            \"issue_type\": issue_type,\n            \"current_step\": \"resolution_specialist\",  # [!code highlight]\n        }\n    )\n\n\n@tool\ndef escalate_to_human(reason: str) -> str:\n    \"\"\"Escalate the case to a human support specialist.\"\"\"\n    # In a real system, this would create a ticket, notify staff, etc.\n    return f\"Escalating to human support. Reason: {reason}\"\n\n\n@tool\ndef provide_solution(solution: str) -> str:\n    \"\"\"Provide a solution to the customer's issue.\"\"\"\n    return f\"Solution provided: {solution}\"\n```\n:::\n\n:::js\n```typescript\nimport { z } from \"zod\";\nimport { tool, ToolMessage, type ToolRuntime } from \"langchain\";\nimport { Command } from \"@langchain/langgraph\";\n\nconst recordWarrantyStatus = tool(\n  async (input, config: ToolRuntime<typeof SupportState.State>) => {\n    return new Command({ // [!code highlight]\n      update: { // [!code highlight]\n        messages: [\n          new ToolMessage({\n            content: `Warranty status recorded as: ${input.status}`,\n            tool_call_id: config.toolCallId,\n          }),\n        ],\n        warrantyStatus: input.status,\n        currentStep: \"issue_classifier\", // [!code highlight]\n      },\n    });\n  },\n  {\n    name: \"record_warranty_status\",\n    description:\n      \"Record the customer's warranty status and transition to issue classification.\",\n    schema: z.object({\n      status: WarrantyStatusSchema,\n    }),\n  }\n);\n\nconst recordIssueType = tool(\n  async (input, config: ToolRuntime<typeof SupportState.State>) => {\n    return new Command({ // [!code highlight]\n      update: { // [!code highlight]\n        messages: [\n          new ToolMessage({\n            content: `Issue type recorded as: ${input.issueType}`,\n            tool_call_id: config.toolCallId,\n          }),\n        ],\n        issueType: input.issueType,\n        currentStep: \"resolution_specialist\", // [!code highlight]\n      },\n    });\n  },\n  {\n   ", "metadata": {"source": "multi-agent/handoffs-customer-support.mdx"}}
{"text": ": ToolRuntime<typeof SupportState.State>) => {\n    return new Command({ // [!code highlight]\n      update: { // [!code highlight]\n        messages: [\n          new ToolMessage({\n            content: `Issue type recorded as: ${input.issueType}`,\n            tool_call_id: config.toolCallId,\n          }),\n        ],\n        issueType: input.issueType,\n        currentStep: \"resolution_specialist\", // [!code highlight]\n      },\n    });\n  },\n  {\n    name: \"record_issue_type\",\n    description:\n      \"Record the type of issue and transition to resolution specialist.\",\n    schema: z.object({\n      issueType: IssueTypeSchema,\n    }),\n  }\n);\n\nconst escalateToHuman = tool(\n  async (input) => {\n    // In a real system, this would create a ticket, notify staff, etc.\n    return `Escalating to human support. Reason: ${input.reason}`;\n  },\n  {\n    name: \"escalate_to_human\",\n    description: \"Escalate the case to a human support specialist.\",\n    schema: z.object({\n      reason: z.string(),\n    }),\n  }\n);\n\nconst provideSolution = tool(\n  async (input) => {\n    return `Solution provided: ${input.solution}`;\n  },\n  {\n    name: \"provide_solution\",\n    description: \"Provide a solution to the customer's issue.\",\n    schema: z.object({\n      solution: z.string(),\n    }),\n  }\n);\n```\n:::\n\nNotice how `record_warranty_status` and `record_issue_type` return `Command` objects that update both the data (`warranty_status`, `issue_type`) AND the `current_step`. This is how the state machine works - tools control workflow progression.\n\n## 3. Define step configurations\n\nDefine prompts and tools for each step. First, define the prompts for each step:\n\n<Accordion title=\"View complete prompt definitions\">\n\n:::python\n```python\n# Define prompts as constants for easy reference\nWARRANTY_COLLECTOR_PROMPT = \"\"\"You are a customer support agent helping with device issues.\n\nCURRENT STAGE: Warranty verification\n\nAt this step, you need to:\n1. Greet the customer warmly\n2. Ask if their device is under warranty\n3. Use record_warranty_status to record their response and move to the next step\n\nBe conversational and friendly. Don't ask multiple questions at once.\"\"\"\n\nISSUE_CLASSIFIER_PROMPT = \"\"\"You are a customer support agent helping with device issues.\n\nCURRENT STAGE: Issue classification\nCUSTOMER INFO: Warranty status is {warranty_status}\n\nAt this step, you need to:\n1. Ask the", "metadata": {"source": "multi-agent/handoffs-customer-support.mdx"}}
{"text": ":\n\n<Accordion title=\"View complete prompt definitions\">\n\n:::python\n```python\n# Define prompts as constants for easy reference\nWARRANTY_COLLECTOR_PROMPT = \"\"\"You are a customer support agent helping with device issues.\n\nCURRENT STAGE: Warranty verification\n\nAt this step, you need to:\n1. Greet the customer warmly\n2. Ask if their device is under warranty\n3. Use record_warranty_status to record their response and move to the next step\n\nBe conversational and friendly. Don't ask multiple questions at once.\"\"\"\n\nISSUE_CLASSIFIER_PROMPT = \"\"\"You are a customer support agent helping with device issues.\n\nCURRENT STAGE: Issue classification\nCUSTOMER INFO: Warranty status is {warranty_status}\n\nAt this step, you need to:\n1. Ask the customer to describe their issue\n2. Determine if it's a hardware issue (physical damage, broken parts) or software issue (app crashes, performance)\n3. Use record_issue_type to record the classification and move to the next step\n\nIf unclear, ask clarifying questions before classifying.\"\"\"\n\nRESOLUTION_SPECIALIST_PROMPT = \"\"\"You are a customer support agent helping with device issues.\n\nCURRENT STAGE: Resolution\nCUSTOMER INFO: Warranty status is {warranty_status}, issue type is {issue_type}\n\nAt this step, you need to:\n1. For SOFTWARE issues: provide troubleshooting steps using provide_solution\n2. For HARDWARE issues:\n   - If IN WARRANTY: explain warranty repair process using provide_solution\n   - If OUT OF WARRANTY: escalate_to_human for paid repair options\n\nBe specific and helpful in your solutions.\"\"\"\n```\n:::\n\n:::js\n```typescript\n// Define prompts as constants for easy reference\nconst WARRANTY_COLLECTOR_PROMPT = `You are a customer support agent helping with device issues.\n\nCURRENT STAGE: Warranty verification\n\nAt this step, you need to:\n1. Greet the customer warmly\n2. Ask if their device is under warranty\n3. Use record_warranty_status to record their response and move to the next step\n\nBe conversational and friendly. Don't ask multiple questions at once.`;\n\nconst ISSUE_CLASSIFIER_PROMPT = `You are a customer support agent helping with device issues.\n\nCURRENT STAGE: Issue classification\nCUSTOMER INFO: Warranty status is {warranty_status}\n\nAt this step, you need to:\n1. Ask the customer to describe their issue\n2. Determine if it's a hardware issue (physical damage, broken parts) or software issue (app crashes, performance)\n3. Use record_issue_type to record the classification and move to the next step\n\nIf unclear, ask clarifying questions before classifying.`;\n\nconst RESOLUTION_SPECIALIST_PROMPT = `You are a customer support agent helping with device issues.\n\nCURRENT STAGE: Resolution\nCUSTOMER INFO: Warranty status is {warranty_status}, issue type is {issue_type}\n\nAt this step, you need to:\n1. For SOFTWARE issues: provide troubleshooting steps using provide_solution\n2. For HARDWARE issues:\n   - If IN WARRANTY: explain warranty repair process using provide_solution\n   - If OUT OF WARRANTY: escalate_to_human for paid repair", "metadata": {"source": "multi-agent/handoffs-customer-support.mdx"}}
{"text": ". Ask the customer to describe their issue\n2. Determine if it's a hardware issue (physical damage, broken parts) or software issue (app crashes, performance)\n3. Use record_issue_type to record the classification and move to the next step\n\nIf unclear, ask clarifying questions before classifying.`;\n\nconst RESOLUTION_SPECIALIST_PROMPT = `You are a customer support agent helping with device issues.\n\nCURRENT STAGE: Resolution\nCUSTOMER INFO: Warranty status is {warranty_status}, issue type is {issue_type}\n\nAt this step, you need to:\n1. For SOFTWARE issues: provide troubleshooting steps using provide_solution\n2. For HARDWARE issues:\n   - If IN WARRANTY: explain warranty repair process using provide_solution\n   - If OUT OF WARRANTY: escalate_to_human for paid repair options\n\nBe specific and helpful in your solutions.`;\n```\n:::\n\n</Accordion>\n\nThen map step names to their configurations using a dictionary:\n\n:::python\n```python\n# Step configuration: maps step name to (prompt, tools, required_state)\nSTEP_CONFIG = {\n    \"warranty_collector\": {\n        \"prompt\": WARRANTY_COLLECTOR_PROMPT,\n        \"tools\": [record_warranty_status],\n        \"requires\": [],\n    },\n    \"issue_classifier\": {\n        \"prompt\": ISSUE_CLASSIFIER_PROMPT,\n        \"tools\": [record_issue_type],\n        \"requires\": [\"warranty_status\"],\n    },\n    \"resolution_specialist\": {\n        \"prompt\": RESOLUTION_SPECIALIST_PROMPT,\n        \"tools\": [provide_solution, escalate_to_human],\n        \"requires\": [\"warranty_status\", \"issue_type\"],\n    },\n}\n```\n:::\n\n:::js\n```typescript\n// Step configuration: maps step name to (prompt, tools, required_state)\nconst STEP_CONFIG = {\n  warranty_collector: {\n    prompt: WARRANTY_COLLECTOR_PROMPT,\n    tools: [recordWarrantyStatus],\n    requires: [],\n  },\n  issue_classifier: {\n    prompt: ISSUE_CLASSIFIER_PROMPT,\n    tools: [recordIssueType],\n    requires: [\"warrantyStatus\"],\n  },\n  resolution_specialist: {\n    prompt: RESOLUTION_SPECIALIST_PROMPT,\n    tools: [provideSolution, escalateToHuman],\n    requires: [\"warrantyStatus\", \"issueType\"],\n  },\n} as const;\n```\n:::\n\nThis dictionary-based configuration makes it easy to:\n- See all steps at a glance\n- Add new steps (just add another entry)\n- Understand the workflow dependencies (`requires` field)\n- Use prompt templates with state variables (e.g., `{warranty_status}`)\n\n## 4. Create step", "metadata": {"source": "multi-agent/handoffs-customer-support.mdx"}}
{"text": " [],\n  },\n  issue_classifier: {\n    prompt: ISSUE_CLASSIFIER_PROMPT,\n    tools: [recordIssueType],\n    requires: [\"warrantyStatus\"],\n  },\n  resolution_specialist: {\n    prompt: RESOLUTION_SPECIALIST_PROMPT,\n    tools: [provideSolution, escalateToHuman],\n    requires: [\"warrantyStatus\", \"issueType\"],\n  },\n} as const;\n```\n:::\n\nThis dictionary-based configuration makes it easy to:\n- See all steps at a glance\n- Add new steps (just add another entry)\n- Understand the workflow dependencies (`requires` field)\n- Use prompt templates with state variables (e.g., `{warranty_status}`)\n\n## 4. Create step-based middleware\n\nCreate middleware that reads `current_step` from state and applies the appropriate configuration. We'll use the `@wrap_model_call` decorator for a clean implementation:\n\n:::python\n```python\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\nfrom typing import Callable\n\n\n@wrap_model_call  # [!code highlight]\ndef apply_step_config(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n    \"\"\"Configure agent behavior based on the current step.\"\"\"\n    # Get current step (defaults to warranty_collector for first interaction)\n    current_step = request.state.get(\"current_step\", \"warranty_collector\")  # [!code highlight]\n\n    # Look up step configuration\n    stage_config = STEP_CONFIG[current_step]  # [!code highlight]\n\n    # Validate required state exists\n    for key in stage_config[\"requires\"]:\n        if request.state.get(key) is None:\n            raise ValueError(f\"{key} must be set before reaching {current_step}\")\n\n    # Format prompt with state values (supports {warranty_status}, {issue_type}, etc.)\n    system_prompt = stage_config[\"prompt\"].format(**request.state)\n\n    # Inject system prompt and step-specific tools\n    request = request.override(  # [!code highlight]\n        system_prompt=system_prompt,  # [!code highlight]\n        tools=stage_config[\"tools\"],  # [!code highlight]\n    )\n\n    return handler(request)\n```\n:::\n\n:::js\n```typescript\nimport { createMiddleware } from \"langchain\";\n\nconst applyStepMiddleware = createMiddleware({\n  name: \"applyStep\",\n  stateSchema: SupportState,\n  wrapModelCall: async (request, handler) => {\n    // Get current step (defaults to warranty_collector for first interaction)\n    const currentStep = request.state.currentStep ?? \"warranty_collector\"; // [!code highlight]\n\n    // Look up step configuration\n    const stepConfig = STEP_CONFIG[currentStep", "metadata": {"source": "multi-agent/handoffs-customer-support.mdx"}}
{"text": "  system_prompt=system_prompt,  # [!code highlight]\n        tools=stage_config[\"tools\"],  # [!code highlight]\n    )\n\n    return handler(request)\n```\n:::\n\n:::js\n```typescript\nimport { createMiddleware } from \"langchain\";\n\nconst applyStepMiddleware = createMiddleware({\n  name: \"applyStep\",\n  stateSchema: SupportState,\n  wrapModelCall: async (request, handler) => {\n    // Get current step (defaults to warranty_collector for first interaction)\n    const currentStep = request.state.currentStep ?? \"warranty_collector\"; // [!code highlight]\n\n    // Look up step configuration\n    const stepConfig = STEP_CONFIG[currentStep]; // [!code highlight]\n\n    // Validate required state exists\n    for (const key of stepConfig.requires) {\n      if (request.state[key] === undefined) {\n        throw new Error(`${key} must be set before reaching ${currentStep}`);\n      }\n    }\n\n    // Format prompt with state values (supports {warrantyStatus}, {issueType}, etc.)\n    let systemPrompt: string = stepConfig.prompt;\n    for (const [key, value] of Object.entries(request.state)) {\n      systemPrompt = systemPrompt.replace(`{${key}}`, String(value ?? \"\"));\n    }\n\n    // Inject system prompt and step-specific tools\n    return handler({\n      ...request, // [!code highlight]\n      systemPrompt, // [!code highlight]\n      tools: [...stepConfig.tools], // [!code highlight]\n    });\n  },\n});\n```\n:::\n\nThis middleware:\n\n1. **Reads current step**: Gets `current_step` from state (defaults to \"warranty_collector\").\n2. **Looks up configuration**: Finds the matching entry in `STEP_CONFIG`.\n3. **Validates dependencies**: Ensures required state fields exist.\n4. **Formats prompt**: Injects state values into the prompt template.\n5. **Applies configuration**: Overrides the system prompt and available tools.\n\nThe `request.override()` method is key - it allows us to dynamically change the agent's behavior based on state without creating separate agent instances.\n\n## 5. Create the agent\n\nNow create the agent with the step-based middleware and a checkpointer for state persistence:\n\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n# Collect all tools from all step configurations\nall_tools = [\n    record_warranty_status,\n    record_issue_type,\n    provide_solution,\n    escalate_to_human,\n]\n\n# Create the agent with step-based configuration\nagent = create_agent(\n    model,\n    tools=all_tools,\n    state_schema=SupportState,  # [!code highlight]\n   ", "metadata": {"source": "multi-agent/handoffs-customer-support.mdx"}}
{"text": " is key - it allows us to dynamically change the agent's behavior based on state without creating separate agent instances.\n\n## 5. Create the agent\n\nNow create the agent with the step-based middleware and a checkpointer for state persistence:\n\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n# Collect all tools from all step configurations\nall_tools = [\n    record_warranty_status,\n    record_issue_type,\n    provide_solution,\n    escalate_to_human,\n]\n\n# Create the agent with step-based configuration\nagent = create_agent(\n    model,\n    tools=all_tools,\n    state_schema=SupportState,  # [!code highlight]\n    middleware=[apply_step_config],  # [!code highlight]\n    checkpointer=InMemorySaver(),  # [!code highlight]\n)\n```\n:::\n\n:::js\n```typescript\nimport { createAgent } from \"langchain\";\nimport { MemorySaver } from \"@langchain/langgraph\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\n// Collect all tools from all step configurations\nconst allTools = [\n  recordWarrantyStatus,\n  recordIssueType,\n  provideSolution,\n  escalateToHuman,\n];\n\n// Initialize the model\nconst model = new ChatOpenAI({\n  model: \"gpt-4.1-mini\",\n  temperature: 0.7,\n});\n\n// Create the agent with step-based configuration\nconst agent = createAgent({\n  model,\n  tools: allTools,\n  stateSchema: SupportState,  // [!code highlight]\n  middleware: [applyStepMiddleware],  // [!code highlight]\n  checkpointer: new MemorySaver(),  // [!code highlight]\n});\n```\n:::\n\n<Note>\n**Why a checkpointer?** The checkpointer maintains state across conversation turns. Without it, the `current_step` state would be lost between user messages, breaking the workflow.\n</Note>\n\n## 6. Test the workflow\n\nTest the complete workflow:\n\n:::python\n```python\nfrom langchain.messages import HumanMessage\nimport uuid\n\n# Configuration for this conversation thread\nthread_id = str(uuid.uuid4())\nconfig = {\"configurable\": {\"thread_id\": thread_id}}\n\n# Turn 1: Initial message - starts with warranty_collector step\nprint(\"=== Turn 1: Warranty Collection ===\")\nresult = agent.invoke(\n    {\"messages\": [HumanMessage(\"Hi, my phone screen is cracked\")]},\n    config\n)\nfor msg in result['messages']:\n    msg.pretty_print()\n\n# Turn 2: User responds about warranty\nprint(\"\\n=== Turn 2: Warranty Response ===\")\nresult = agent.invoke(\n    {\"messages\": [HumanMessage(\"Yes, it's still under warranty\")]},\n    config\n)\nfor msg in result['messages']:\n    msg.pretty_print()\nprint(f\"Current step: {result.get('current_step')}\")\n\n# Turn 3: User describes the issue\nprint(\"\\n=== Turn 3: Issue Description ===\")\nresult = agent.invoke(", "metadata": {"source": "multi-agent/handoffs-customer-support.mdx"}}
{"text": " warranty_collector step\nprint(\"=== Turn 1: Warranty Collection ===\")\nresult = agent.invoke(\n    {\"messages\": [HumanMessage(\"Hi, my phone screen is cracked\")]},\n    config\n)\nfor msg in result['messages']:\n    msg.pretty_print()\n\n# Turn 2: User responds about warranty\nprint(\"\\n=== Turn 2: Warranty Response ===\")\nresult = agent.invoke(\n    {\"messages\": [HumanMessage(\"Yes, it's still under warranty\")]},\n    config\n)\nfor msg in result['messages']:\n    msg.pretty_print()\nprint(f\"Current step: {result.get('current_step')}\")\n\n# Turn 3: User describes the issue\nprint(\"\\n=== Turn 3: Issue Description ===\")\nresult = agent.invoke(\n    {\"messages\": [HumanMessage(\"The screen is physically cracked from dropping it\")]},\n    config\n)\nfor msg in result['messages']:\n    msg.pretty_print()\nprint(f\"Current step: {result.get('current_step')}\")\n\n# Turn 4: Resolution\nprint(\"\\n=== Turn 4: Resolution ===\")\nresult = agent.invoke(\n    {\"messages\": [HumanMessage(\"What should I do?\")]},\n    config\n)\nfor msg in result['messages']:\n    msg.pretty_print()\n```\n:::\n\n:::js\n```typescript\nimport { HumanMessage } from \"@langchain/core/messages\";\nimport { v4 as uuidv4 } from \"uuid\";\n\n// Configuration for this conversation thread\nconst threadId = uuidv4();\nconst config = { configurable: { thread_id: threadId } };\n\n// Turn 1: Initial message - starts with warranty_collector step\nconsole.log(\"=== Turn 1: Warranty Collection ===\");\nlet result = await agent.invoke(\n  { messages: [new HumanMessage(\"Hi, my phone screen is cracked\")] },\n  config\n);\nfor (const msg of result.messages) {\n  console.log(msg.content);\n}\n\n// Turn 2: User responds about warranty\nconsole.log(\"\\n=== Turn 2: Warranty Response ===\");\nresult = await agent.invoke(\n  { messages: [new HumanMessage(\"Yes, it's still under warranty\")] },\n  config\n);\nfor (const msg of result.messages) {\n  console.log(msg.content);\n}\nconsole.log(`Current step: ${result.currentStep}`);\n\n// Turn 3: User describes the issue\nconsole.log(\"\\n=== Turn 3: Issue Description ===\");\nresult = await agent.invoke(\n  { messages: [new HumanMessage(\"The screen is physically cracked from dropping it\")] },\n  config\n);\nfor (const msg of result.messages) {\n  console.log(msg.content);\n}\nconsole.log(`Current step: ${result.currentStep}`);\n\n// Turn 4: Resolution\nconsole.log(\"\\n=== Turn 4: Resolution ===\");\nresult = await agent.invoke(\n  { messages: [new HumanMessage(\"What should I do?\")] },\n  config\n);\nfor (const msg of result.messages) {\n  console.log(msg.content);\n}\n```\n:::\n\nExpected flow:\n1", "metadata": {"source": "multi-agent/handoffs-customer-support.mdx"}}
{"text": "Current step: ${result.currentStep}`);\n\n// Turn 3: User describes the issue\nconsole.log(\"\\n=== Turn 3: Issue Description ===\");\nresult = await agent.invoke(\n  { messages: [new HumanMessage(\"The screen is physically cracked from dropping it\")] },\n  config\n);\nfor (const msg of result.messages) {\n  console.log(msg.content);\n}\nconsole.log(`Current step: ${result.currentStep}`);\n\n// Turn 4: Resolution\nconsole.log(\"\\n=== Turn 4: Resolution ===\");\nresult = await agent.invoke(\n  { messages: [new HumanMessage(\"What should I do?\")] },\n  config\n);\nfor (const msg of result.messages) {\n  console.log(msg.content);\n}\n```\n:::\n\nExpected flow:\n1. **Warranty verification step**: Asks about warranty status\n2. **Issue classification step**: Asks about the problem, determines it's hardware\n3. **Resolution step**: Provides warranty repair instructions\n\n## 7. Understanding state transitions\n\nLet's trace what happens at each turn:\n\n### Turn 1: Initial message\n\n:::python\n```python\n{\n    \"messages\": [HumanMessage(\"Hi, my phone screen is cracked\")],\n    \"current_step\": \"warranty_collector\"  # Default value\n}\n```\n:::\n\n:::js\n```typescript\n{\n  messages: [new HumanMessage(\"Hi, my phone screen is cracked\")],\n  currentStep: \"warranty_collector\"  // Default value\n}\n```\n:::\n\nMiddleware applies:\n- System prompt: `WARRANTY_COLLECTOR_PROMPT`\n- Tools: `[record_warranty_status]`\n\n### Turn 2: After warranty recorded\n\n:::python\nTool call: `record_warranty_status(\"in_warranty\")` returns:\n```python\nCommand(update={\n    \"warranty_status\": \"in_warranty\",\n    \"current_step\": \"issue_classifier\"  # State transition!\n})\n```\n:::\n\n:::js\nTool call: `recordWarrantyStatus(\"in_warranty\")` returns:\n```typescript\nnew Command({\n  update: {\n    warrantyStatus: \"in_warranty\",\n    currentStep: \"issue_classifier\"  // State transition!\n  }\n})\n```\n:::\n\nNext turn, middleware applies:\n- System prompt: `ISSUE_CLASSIFIER_PROMPT` (formatted with `warranty_status=\"in_warranty\"`)\n- Tools: `[record_issue_type]`\n\n### Turn 3: After issue classified\n\n:::python\nTool call: `record_issue_type(\"hardware\")` returns:\n```python\nCommand(update={\n    \"issue_type\": \"hardware\",\n    \"current_step\": \"resolution_specialist\"  # State transition!\n})\n```\n:::\n\n:::js\nTool call: `recordIssueType(\"hardware\")` returns:\n```typescript\nnew Command({\n  update: {\n    issueType: \"hardware\",\n    currentStep: \"resolution_specialist", "metadata": {"source": "multi-agent/handoffs-customer-support.mdx"}}
{"text": "\n:::\n\nNext turn, middleware applies:\n- System prompt: `ISSUE_CLASSIFIER_PROMPT` (formatted with `warranty_status=\"in_warranty\"`)\n- Tools: `[record_issue_type]`\n\n### Turn 3: After issue classified\n\n:::python\nTool call: `record_issue_type(\"hardware\")` returns:\n```python\nCommand(update={\n    \"issue_type\": \"hardware\",\n    \"current_step\": \"resolution_specialist\"  # State transition!\n})\n```\n:::\n\n:::js\nTool call: `recordIssueType(\"hardware\")` returns:\n```typescript\nnew Command({\n  update: {\n    issueType: \"hardware\",\n    currentStep: \"resolution_specialist\"  // State transition!\n  }\n})\n```\n:::\n\nNext turn, middleware applies:\n- System prompt: `RESOLUTION_SPECIALIST_PROMPT` (formatted with `warranty_status` and `issue_type`)\n- Tools: `[provide_solution, escalate_to_human]`\n\nThe key insight: **Tools drive the workflow** by updating `current_step`, and **middleware responds** by applying the appropriate configuration on the next turn.\n\n## 8. Manage message history\n\nAs the agent progresses through steps, message history grows. Use [summarization middleware](/oss/langchain/short-term-memory#summarize-messages) to compress earlier messages while preserving conversational context:\n\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import SummarizationMiddleware  # [!code highlight]\nfrom langgraph.checkpoint.memory import InMemorySaver\n\nagent = create_agent(\n    model,\n    tools=all_tools,\n    state_schema=SupportState,\n    middleware=[\n        apply_step_config,\n        SummarizationMiddleware(  # [!code highlight]\n            model=\"gpt-4.1-mini\",\n            trigger=(\"tokens\", 4000),\n            keep=(\"messages\", 10)\n        )\n    ],\n    checkpointer=InMemorySaver(),\n)\n```\n:::\n\n:::js\n```typescript\nimport { createAgent, SummarizationMiddleware } from \"langchain\";  // [!code highlight]\nimport { MemorySaver } from \"@langchain/langgraph\";\n\nconst agent = createAgent({\n  model,\n  tools: allTools,\n  stateSchema: SupportState,\n  middleware: [\n    applyStepMiddleware,\n    new SummarizationMiddleware({  // [!code highlight]\n      model: \"gpt-4.1-mini\",\n      trigger: { tokens: 4000 },\n      keep: { messages: 10 },\n    }),\n  ],\n  checkpointer: new MemorySaver(),\n});\n```\n:::\n\nSee the [short-term memory guide](/", "metadata": {"source": "multi-agent/handoffs-customer-support.mdx"}}
{"text": "\n```\n:::\n\n:::js\n```typescript\nimport { createAgent, SummarizationMiddleware } from \"langchain\";  // [!code highlight]\nimport { MemorySaver } from \"@langchain/langgraph\";\n\nconst agent = createAgent({\n  model,\n  tools: allTools,\n  stateSchema: SupportState,\n  middleware: [\n    applyStepMiddleware,\n    new SummarizationMiddleware({  // [!code highlight]\n      model: \"gpt-4.1-mini\",\n      trigger: { tokens: 4000 },\n      keep: { messages: 10 },\n    }),\n  ],\n  checkpointer: new MemorySaver(),\n});\n```\n:::\n\nSee the [short-term memory guide](/oss/langchain/short-term-memory) for other memory management techniques.\n\n## 9. Add flexibility: Go back\n\nSome workflows need to allow users to return to previous steps to correct information (e.g., changing warranty status or issue classification). However, not all transitions make sense\u2014for example, you typically can't go back once a refund has been processed. For this support workflow, we'll add tools to return to the warranty verification and issue classification steps.\n\n<Tip>\nIf your workflow requires arbitrary transitions between most steps, consider whether you need a structured workflow at all. This pattern works best when steps follow a clear sequential progression with occasional backwards transitions for corrections.\n</Tip>\n\nAdd \"go back\" tools to the resolution step:\n\n:::python\n```python\n@tool\ndef go_back_to_warranty() -> Command:  # [!code highlight]\n    \"\"\"Go back to warranty verification step.\"\"\"\n    return Command(update={\"current_step\": \"warranty_collector\"})  # [!code highlight]\n\n\n@tool\ndef go_back_to_classification() -> Command:  # [!code highlight]\n    \"\"\"Go back to issue classification step.\"\"\"\n    return Command(update={\"current_step\": \"issue_classifier\"})  # [!code highlight]\n\n\n# Update the resolution_specialist configuration to include these tools\nSTEP_CONFIG[\"resolution_specialist\"][\"tools\"].extend([\n    go_back_to_warranty,\n    go_back_to_classification\n])\n```\n:::\n\n:::js\n```typescript\nimport { tool } from \"langchain\";\nimport { Command } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst goBackToWarranty = tool(  // [!code highlight]\n  async () => {\n    return new Command({ update: { currentStep: \"warranty_collector\" } });  // [!code highlight]\n  },\n  {\n    name: \"go_back_to_warranty\",\n    description: \"Go back to warranty verification step.\",\n    schema: z.object({}),\n  }\n);\n\nconst goBackToClassification = tool(  // [!code highlight]\n  async () => {\n    return new Command({ update: { currentStep: \"issue_classifier\" } });  // [!code highlight]\n  },\n  {\n    name: \"go_back_to_classification\",", "metadata": {"source": "multi-agent/handoffs-customer-support.mdx"}}
{"text": "chain/langgraph\";\nimport { z } from \"zod\";\n\nconst goBackToWarranty = tool(  // [!code highlight]\n  async () => {\n    return new Command({ update: { currentStep: \"warranty_collector\" } });  // [!code highlight]\n  },\n  {\n    name: \"go_back_to_warranty\",\n    description: \"Go back to warranty verification step.\",\n    schema: z.object({}),\n  }\n);\n\nconst goBackToClassification = tool(  // [!code highlight]\n  async () => {\n    return new Command({ update: { currentStep: \"issue_classifier\" } });  // [!code highlight]\n  },\n  {\n    name: \"go_back_to_classification\",\n    description: \"Go back to issue classification step.\",\n    schema: z.object({}),\n  }\n);\n\n// Update the resolution_specialist configuration to include these tools\nSTEP_CONFIG.resolution_specialist.tools.push(\n  goBackToWarranty,\n  goBackToClassification\n);\n```\n:::\n\nUpdate the resolution specialist's prompt to mention these tools:\n\n:::python\n```python\nRESOLUTION_SPECIALIST_PROMPT = \"\"\"You are a customer support agent helping with device issues.\n\nCURRENT STAGE: Resolution\nCUSTOMER INFO: Warranty status is {warranty_status}, issue type is {issue_type}\n\nAt this step, you need to:\n1. For SOFTWARE issues: provide troubleshooting steps using provide_solution\n2. For HARDWARE issues:\n   - If IN WARRANTY: explain warranty repair process using provide_solution\n   - If OUT OF WARRANTY: escalate_to_human for paid repair options\n\nIf the customer indicates any information was wrong, use:\n- go_back_to_warranty to correct warranty status\n- go_back_to_classification to correct issue type\n\nBe specific and helpful in your solutions.\"\"\"\n```\n:::\n\n:::js\n```typescript\nconst RESOLUTION_SPECIALIST_PROMPT = `You are a customer support agent helping with device issues.\n\nCURRENT STAGE: Resolution\nCUSTOMER INFO: Warranty status is {warrantyStatus}, issue type is {issueType}\n\nAt this step, you need to:\n1. For SOFTWARE issues: provide troubleshooting steps using provide_solution\n2. For HARDWARE issues:\n   - If IN WARRANTY: explain warranty repair process using provide_solution\n   - If OUT OF WARRANTY: escalate_to_human for paid repair options\n\nIf the customer indicates any information was wrong, use:\n- go_back_to_warranty to correct warranty status\n- go_back_to_classification to correct issue type\n\nBe specific and helpful in your solutions.`;\n```\n:::\n\nNow the agent can handle corrections:\n\n:::python\n```python\nresult = agent.invoke(\n    {\"messages\": [HumanMessage(\"Actually, I made a mistake - my device is out of warranty\")]},\n    config\n)\n# Agent will call go_back_to_warranty and restart the warranty verification step\n```\n:::\n\n:::js\n```typescript\nconst result = await agent.invoke", "metadata": {"source": "multi-agent/handoffs-customer-support.mdx"}}
{"text": "  - If IN WARRANTY: explain warranty repair process using provide_solution\n   - If OUT OF WARRANTY: escalate_to_human for paid repair options\n\nIf the customer indicates any information was wrong, use:\n- go_back_to_warranty to correct warranty status\n- go_back_to_classification to correct issue type\n\nBe specific and helpful in your solutions.`;\n```\n:::\n\nNow the agent can handle corrections:\n\n:::python\n```python\nresult = agent.invoke(\n    {\"messages\": [HumanMessage(\"Actually, I made a mistake - my device is out of warranty\")]},\n    config\n)\n# Agent will call go_back_to_warranty and restart the warranty verification step\n```\n:::\n\n:::js\n```typescript\nconst result = await agent.invoke(\n  { messages: [new HumanMessage(\"Actually, I made a mistake - my device is out of warranty\")] },\n  config\n);\n// Agent will call go_back_to_warranty and restart the warranty verification step\n```\n:::\n\n## Complete example\n\nHere's everything together in a runnable script:\n\n<Expandable title=\"Complete code\" defaultOpen={false}>\n:::python\n```python\n\"\"\"\nCustomer Support State Machine Example\n\nThis example demonstrates the state machine pattern.\nA single agent dynamically changes its behavior based on the current_step state,\ncreating a state machine for sequential information collection.\n\"\"\"\n\nimport uuid\n\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.types import Command\nfrom typing import Callable, Literal\nfrom typing_extensions import NotRequired\n\nfrom langchain.agents import AgentState, create_agent\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse, SummarizationMiddleware\nfrom langchain.chat_models import init_chat_model\nfrom langchain.messages import HumanMessage, ToolMessage\nfrom langchain.tools import tool, ToolRuntime\n\nmodel = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\n\n\n# Define the possible workflow steps\nSupportStep = Literal[\"warranty_collector\", \"issue_classifier\", \"resolution_specialist\"]\n\n\nclass SupportState(AgentState):\n    \"\"\"State for customer support workflow.\"\"\"\n\n    current_step: NotRequired[SupportStep]\n    warranty_status: NotRequired[Literal[\"in_warranty\", \"out_of_warranty\"]]\n    issue_type: NotRequired[Literal[\"hardware\", \"software\"]]\n\n\n@tool\ndef record_warranty_status(\n    status: Literal[\"in_warranty\", \"out_of_warranty\"],\n    runtime: ToolRuntime[None, SupportState],\n) -> Command:\n    \"\"\"Record the customer's warranty status and transition to issue classification.\"\"\"\n    return Command(\n        update={\n            \"messages\": [\n                ToolMessage(\n                    content=f\"Warranty status recorded as: {status}\",\n              ", "metadata": {"source": "multi-agent/handoffs-customer-support.mdx"}}
{"text": "\n    issue_type: NotRequired[Literal[\"hardware\", \"software\"]]\n\n\n@tool\ndef record_warranty_status(\n    status: Literal[\"in_warranty\", \"out_of_warranty\"],\n    runtime: ToolRuntime[None, SupportState],\n) -> Command:\n    \"\"\"Record the customer's warranty status and transition to issue classification.\"\"\"\n    return Command(\n        update={\n            \"messages\": [\n                ToolMessage(\n                    content=f\"Warranty status recorded as: {status}\",\n                    tool_call_id=runtime.tool_call_id,\n                )\n            ],\n            \"warranty_status\": status,\n            \"current_step\": \"issue_classifier\",\n        }\n    )\n\n\n@tool\ndef record_issue_type(\n    issue_type: Literal[\"hardware\", \"software\"],\n    runtime: ToolRuntime[None, SupportState],\n) -> Command:\n    \"\"\"Record the type of issue and transition to resolution specialist.\"\"\"\n    return Command(\n        update={\n            \"messages\": [\n                ToolMessage(\n                    content=f\"Issue type recorded as: {issue_type}\",\n                    tool_call_id=runtime.tool_call_id,\n                )\n            ],\n            \"issue_type\": issue_type,\n            \"current_step\": \"resolution_specialist\",\n        }\n    )\n\n\n@tool\ndef escalate_to_human(reason: str) -> str:\n    \"\"\"Escalate the case to a human support specialist.\"\"\"\n    # In a real system, this would create a ticket, notify staff, etc.\n    return f\"Escalating to human support. Reason: {reason}\"\n\n\n@tool\ndef provide_solution(solution: str) -> str:\n    \"\"\"Provide a solution to the customer's issue.\"\"\"\n    return f\"Solution provided: {solution}\"\n\n\n# Define prompts as constants\nWARRANTY_COLLECTOR_PROMPT = \"\"\"You are a customer support agent helping with device issues.\n\nCURRENT STEP: Warranty verification\n\nAt this step, you need to:\n1. Greet the customer warmly\n2. Ask if their device is under warranty\n3. Use record_warranty_status to record their response and move to the next step\n\n", "metadata": {"source": "multi-agent/handoffs-customer-support.mdx"}}
{"text": ":\n    \"\"\"Escalate the case to a human support specialist.\"\"\"\n    # In a real system, this would create a ticket, notify staff, etc.\n    return f\"Escalating to human support. Reason: {reason}\"\n\n\n@tool\ndef provide_solution(solution: str) -> str:\n    \"\"\"Provide a solution to the customer's issue.\"\"\"\n    return f\"Solution provided: {solution}\"\n\n\n# Define prompts as constants\nWARRANTY_COLLECTOR_PROMPT = \"\"\"You are a customer support agent helping with device issues.\n\nCURRENT STEP: Warranty verification\n\nAt this step, you need to:\n1. Greet the customer warmly\n2. Ask if their device is under warranty\n3. Use record_warranty_status to record their response and move to the next step\n\nBe conversational and friendly. Don't ask multiple questions at once.\"\"\"\n\nISSUE_CLASSIFIER_PROMPT = \"\"\"You are a customer support agent helping with device issues.\n\nCURRENT STEP: Issue classification\nCUSTOMER INFO: Warranty status is {warranty_status}\n\nAt this step, you need to:\n1. Ask the customer to describe their issue\n2. Determine if it's a hardware issue (physical damage, broken parts) or software issue (app crashes, performance)\n3. Use record_issue_type to record the classification and move to the next step\n\nIf unclear, ask clarifying questions before classifying.\"\"\"\n\nRESOLUTION_SPECIALIST_PROMPT = \"\"\"You are a customer support agent helping with device issues.\n\nCURRENT STEP: Resolution\nCUSTOMER INFO: Warranty status is {warranty_status}, issue type is {issue_type}\n\nAt this step, you need to:\n1. For SOFTWARE issues: provide troubleshooting steps using provide_solution\n2. For HARDWARE issues:\n   - If IN WARRANTY: explain warranty repair process using provide_solution\n   - If OUT OF WARRANTY: escalate_to_human for paid repair options\n\nBe specific and helpful in your solutions.\"\"\"\n\n\n# Step configuration: maps step name to (prompt, tools, required_state)\nSTEP_CONFIG = {\n    \"warranty_collector\": {\n        \"prompt\": WARRANTY_COLLECTOR_PROMPT,\n        \"tools\": [record_warranty_status],\n        \"requires\": [],\n    },\n    \"issue_classifier\": {\n        \"prompt\": ISSUE_CLASSIFIER_PROMPT,\n        \"tools\": [record_issue_type],\n        \"requires\": [\"warranty_status\"],\n    },\n    \"resolution_specialist\": {\n        \"prompt\": RESOLUTION_SPECIALIST_PROMPT,\n        \"tools\": [provide_solution, escalate_to_human],\n        \"requires\": [\"warranty_status\", \"issue_type\"],\n    },\n}\n\n\n@wrap_model_call\ndef apply_step_config(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n  ", "metadata": {"source": "multi-agent/handoffs-customer-support.mdx"}}
{"text": "       \"prompt\": ISSUE_CLASSIFIER_PROMPT,\n        \"tools\": [record_issue_type],\n        \"requires\": [\"warranty_status\"],\n    },\n    \"resolution_specialist\": {\n        \"prompt\": RESOLUTION_SPECIALIST_PROMPT,\n        \"tools\": [provide_solution, escalate_to_human],\n        \"requires\": [\"warranty_status\", \"issue_type\"],\n    },\n}\n\n\n@wrap_model_call\ndef apply_step_config(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n    \"\"\"Configure agent behavior based on the current step.\"\"\"\n    # Get current step (defaults to warranty_collector for first interaction)\n    current_step = request.state.get(\"current_step\", \"warranty_collector\")\n\n    # Look up step configuration\n    step_config = STEP_CONFIG[current_step]\n\n    # Validate required state exists\n    for key in step_config[\"requires\"]:\n        if request.state.get(key) is None:\n            raise ValueError(f\"{key} must be set before reaching {current_step}\")\n\n    # Format prompt with state values\n    system_prompt = step_config[\"prompt\"].format(**request.state)\n\n    # Inject system prompt and step-specific tools\n    request = request.override(\n        system_prompt=system_prompt,\n        tools=step_config[\"tools\"],\n    )\n\n    return handler(request)\n\n\n# Collect all tools from all step configurations\nall_tools = [\n    record_warranty_status,\n    record_issue_type,\n    provide_solution,\n    escalate_to_human,\n]\n\n# Create the agent with step-based configuration and summarization\nagent = create_agent(\n    model,\n    tools=all_tools,\n    state_schema=SupportState,\n    middleware=[\n        apply_step_config,\n        SummarizationMiddleware(\n            model=\"gpt-4.1-mini\",\n            trigger=(\"tokens\", 4000),\n            keep=(\"messages\", 10)\n        )\n    ],\n    checkpointer=InMemorySaver(),\n)\n\n\n# ============================================================================\n# Test the workflow\n# ============================================================================\n\nif __name__ == \"__main__\":\n    thread_id = str(uuid.uuid4())\n    config = {\"configurable\": {\"thread_id\": thread_id}}\n\n    result = agent.invoke(\n        {\"messages\": [HumanMessage(\"Hi, my phone screen is cracked", "metadata": {"source": "multi-agent/handoffs-customer-support.mdx"}}
{"text": "      SummarizationMiddleware(\n            model=\"gpt-4.1-mini\",\n            trigger=(\"tokens\", 4000),\n            keep=(\"messages\", 10)\n        )\n    ],\n    checkpointer=InMemorySaver(),\n)\n\n\n# ============================================================================\n# Test the workflow\n# ============================================================================\n\nif __name__ == \"__main__\":\n    thread_id = str(uuid.uuid4())\n    config = {\"configurable\": {\"thread_id\": thread_id}}\n\n    result = agent.invoke(\n        {\"messages\": [HumanMessage(\"Hi, my phone screen is cracked\")]},\n        config\n    )\n\n    result = agent.invoke(\n        {\"messages\": [HumanMessage(\"Yes, it's still under warranty\")]},\n        config\n    )\n\n    result = agent.invoke(\n        {\"messages\": [HumanMessage(\"The screen is physically cracked from dropping it\")]},\n        config\n    )\n\n    result = agent.invoke(\n        {\"messages\": [HumanMessage(\"What should I do?\")]},\n        config\n    )\n    for msg in result['messages']:\n        msg.pretty_print()\n```\n:::\n\n:::js\n```typescript\nimport { createMiddleware, createAgent } from \"langchain\";\n\nimport { z } from \"zod\";\nimport { v4 as uuidv4 } from \"uuid\";\nimport { tool, ToolMessage, type ToolRuntime, HumanMessage } from \"langchain\";\nimport { Command, MemorySaver, StateSchema } from \"@langchain/langgraph\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\n// Define the possible workflow steps\nconst SupportStepSchema = z.enum([\n  \"warranty_collector\",\n  \"issue_classifier\",\n  \"resolution_specialist\",\n]);\nconst WarrantyStatusSchema = z.enum([\"in_warranty\", \"out_of_warranty\"]);\nconst IssueTypeSchema = z.enum([\"hardware\", \"software\"]);\n\n// State for customer support workflow\nconst SupportState = new StateSchema({\n  currentStep: SupportStepSchema.optional(),\n  warrantyStatus: WarrantyStatusSchema.optional(),\n  issueType: IssueTypeSchema.optional(),\n});\n\nconst recordWarrantyStatus = tool(\n  async (input, config: ToolRuntime<typeof SupportState.State>) => {\n    return new Command({\n\n      update: {\n\n        messages: [\n          new ToolMessage({\n            content: `Warranty status recorded as: ${input.status}`,\n            tool_call_id: config.toolCallId,\n          }),\n    ", "metadata": {"source": "multi-agent/handoffs-customer-support.mdx"}}
{"text": "\"]);\n\n// State for customer support workflow\nconst SupportState = new StateSchema({\n  currentStep: SupportStepSchema.optional(),\n  warrantyStatus: WarrantyStatusSchema.optional(),\n  issueType: IssueTypeSchema.optional(),\n});\n\nconst recordWarrantyStatus = tool(\n  async (input, config: ToolRuntime<typeof SupportState.State>) => {\n    return new Command({\n\n      update: {\n\n        messages: [\n          new ToolMessage({\n            content: `Warranty status recorded as: ${input.status}`,\n            tool_call_id: config.toolCallId,\n          }),\n        ],\n        warrantyStatus: input.status,\n        currentStep: \"issue_classifier\",\n      },\n    });\n  },\n  {\n    name: \"record_warranty_status\",\n    description:\n      \"Record the customer's warranty status and transition to issue classification.\",\n    schema: z.object({\n      status: WarrantyStatusSchema,\n    }),\n  }\n);\n\nconst recordIssueType = tool(\n  async (input, config: ToolRuntime<typeof SupportState.State>) => {\n    return new Command({\n\n      update: {\n\n        messages: [\n          new ToolMessage({\n            content: `Issue type recorded as: ${input.issueType}`,\n            tool_call_id: config.toolCallId,\n          }),\n        ],\n        issueType: input.issueType,\n        currentStep: \"resolution_specialist\",\n      },\n    });\n  },\n  {\n    name: \"record_issue_type\",\n    description:\n      \"Record the type of issue and transition to resolution specialist.\",\n    schema: z.object({\n      issueType: IssueTypeSchema,\n    }),\n  }\n);\n\nconst escalateToHuman = tool(\n  async (input) => {\n    // In a real system, this would create a ticket, notify staff, etc.\n    return `Escalating to human support. Reason: ${input.reason}`;\n  },\n  {\n    name: \"escalate_to_human\",\n    description: \"Escalate the case to a human support specialist.\",\n    schema: z.object({\n      reason: z.string(),\n    }),\n  }\n);\n\nconst provideSolution = tool(\n  async (input) => {\n    return `Solution provided: ${input.solution}`;\n  },\n  {\n    name: \"provide_solution\",\n    description: \"Provide a solution to the customer's issue.\",\n    schema:", "metadata": {"source": "multi-agent/handoffs-customer-support.mdx"}}
{"text": " = tool(\n  async (input) => {\n    // In a real system, this would create a ticket, notify staff, etc.\n    return `Escalating to human support. Reason: ${input.reason}`;\n  },\n  {\n    name: \"escalate_to_human\",\n    description: \"Escalate the case to a human support specialist.\",\n    schema: z.object({\n      reason: z.string(),\n    }),\n  }\n);\n\nconst provideSolution = tool(\n  async (input) => {\n    return `Solution provided: ${input.solution}`;\n  },\n  {\n    name: \"provide_solution\",\n    description: \"Provide a solution to the customer's issue.\",\n    schema: z.object({\n      solution: z.string(),\n    }),\n  }\n);\n\n// Define prompts as constants for easy reference\nconst WARRANTY_COLLECTOR_PROMPT = `You are a customer support agent helping with device issues.\n\nCURRENT STAGE: Warranty verification\n\nAt this step, you need to:\n1. Greet the customer warmly\n2. Ask if their device is under warranty\n3. Use record_warranty_status to record their response and move to the next step\n\nBe conversational and friendly. Don't ask multiple questions at once.`;\n\nconst ISSUE_CLASSIFIER_PROMPT = `You are a customer support agent helping with device issues.\n\nCURRENT STAGE: Issue classification\nCUSTOMER INFO: Warranty status is {warranty_status}\n\nAt this step, you need to:\n1. Ask the customer to describe their issue\n2. Determine if it's a hardware issue (physical damage, broken parts) or software issue (app crashes, performance)\n3. Use record_issue_type to record the classification and move to the next step\n\nIf unclear, ask clarifying questions before classifying.`;\n\nconst RESOLUTION_SPECIALIST_PROMPT = `You are a customer support agent helping with device issues.\n\nCURRENT STAGE: Resolution\nCUSTOMER INFO: Warranty status is {warranty_status}, issue type is {issue_type}\n\nAt this step, you need to:\n1. For SOFTWARE issues: provide troubleshooting steps using provide_solution\n2. For HARDWARE issues:\n   - If IN WARRANTY: explain warranty repair process using provide_solution\n   - If OUT OF WARRANTY: escalate_to_human for paid repair options\n\nBe specific and helpful in your solutions.`;\n\n// Step configuration: maps step name to (prompt, tools, required_state)\nconst STEP_CONFIG = {\n  warranty_collector: {\n    prompt: WARRANTY_COLLECTOR_PROMPT,\n    tools: [recordWarrantyStatus],\n    requires: [],\n  },\n  issue_classifier: {\n    prompt: ISSUE_CLASSIFIER_PROMPT,\n    tools: [recordIssueType],\n    requires: [\"warrantyStatus\"],\n  },\n  resolution_specialist: {\n    prompt: RESOLUTION_SPECIALIST_PROMPT,\n    tools: [provideSolution, escalateToHuman],\n    requires: [\"warrantyStatus\", \"issue", "metadata": {"source": "multi-agent/handoffs-customer-support.mdx"}}
{"text": "human for paid repair options\n\nBe specific and helpful in your solutions.`;\n\n// Step configuration: maps step name to (prompt, tools, required_state)\nconst STEP_CONFIG = {\n  warranty_collector: {\n    prompt: WARRANTY_COLLECTOR_PROMPT,\n    tools: [recordWarrantyStatus],\n    requires: [],\n  },\n  issue_classifier: {\n    prompt: ISSUE_CLASSIFIER_PROMPT,\n    tools: [recordIssueType],\n    requires: [\"warrantyStatus\"],\n  },\n  resolution_specialist: {\n    prompt: RESOLUTION_SPECIALIST_PROMPT,\n    tools: [provideSolution, escalateToHuman],\n    requires: [\"warrantyStatus\", \"issueType\"],\n  },\n} as const;\n\nconst applyStepMiddleware = createMiddleware({\n  name: \"applyStep\",\n  stateSchema: SupportState,\n  wrapModelCall: async (request, handler) => {\n    // Get current step (defaults to warranty_collector for first interaction)\n    const currentStep = request.state.currentStep ?? \"warranty_collector\";\n\n    // Look up step configuration\n    const stepConfig = STEP_CONFIG[currentStep];\n\n    // Validate required state exists\n    for (const key of stepConfig.requires) {\n      if (request.state[key] === undefined) {\n        throw new Error(`${key} must be set before reaching ${currentStep}`);\n      }\n    }\n\n    // Format prompt with state values (supports {warrantyStatus}, {issueType}, etc.)\n    let systemPrompt: string = stepConfig.prompt;\n    for (const [key, value] of Object.entries(request.state)) {\n      systemPrompt = systemPrompt.replace(`{${key}}`, String(value ?? \"\"));\n    }\n\n    // Inject system prompt and step-specific tools\n    return handler({\n      ...request,\n      systemPrompt,\n      tools: [...stepConfig.tools],\n    });\n  },\n});\n\n// Collect all tools from all step configurations\nconst allTools = [\n  recordWarrantyStatus,\n  recordIssueType,\n  provideSolution,\n  escalateToHuman,\n];\n\nconst model = new ChatOpenAI({\n  model: \"gpt-4.1-mini\",\n});\n\n// Create the agent with step-based configuration\nconst agent = createAgent({\n  model,\n  tools: allTools,\n  middleware: [applyStepMiddleware],\n  checkpointer: new MemorySaver(),\n});\n\n// Configuration for this conversation thread\nconst threadId = uuidv4();\nconst config = { configurable: { thread_id: threadId } };\n\n// Turn 1: Initial message - starts with warranty_collector step\nconsole.log(\"=== Turn 1: Warranty Collection ===\");\nlet result = await agent.invoke(\n  { messages: [new HumanMessage(\"Hi, my phone screen is cracked\")] },\n  config\n);\nfor (const msg of result.messages) {\n ", "metadata": {"source": "multi-agent/handoffs-customer-support.mdx"}}
{"text": "Solution,\n  escalateToHuman,\n];\n\nconst model = new ChatOpenAI({\n  model: \"gpt-4.1-mini\",\n});\n\n// Create the agent with step-based configuration\nconst agent = createAgent({\n  model,\n  tools: allTools,\n  middleware: [applyStepMiddleware],\n  checkpointer: new MemorySaver(),\n});\n\n// Configuration for this conversation thread\nconst threadId = uuidv4();\nconst config = { configurable: { thread_id: threadId } };\n\n// Turn 1: Initial message - starts with warranty_collector step\nconsole.log(\"=== Turn 1: Warranty Collection ===\");\nlet result = await agent.invoke(\n  { messages: [new HumanMessage(\"Hi, my phone screen is cracked\")] },\n  config\n);\nfor (const msg of result.messages) {\n  console.log(msg.content);\n}\n\n// Turn 2: User responds about warranty\nconsole.log(\"\\n=== Turn 2: Warranty Response ===\");\nresult = await agent.invoke(\n  { messages: [new HumanMessage(\"Yes, it's still under warranty\")] },\n  config\n);\nfor (const msg of result.messages) {\n  console.log(msg.content);\n}\nconsole.log(`Current step: ${result.currentStep}`);\n\n// Turn 3: User describes the issue\nconsole.log(\"\\n=== Turn 3: Issue Description ===\");\nresult = await agent.invoke(\n  {\n    messages: [\n      new HumanMessage(\"The screen is physically cracked from dropping it\"),\n    ],\n  },\n  config\n);\nfor (const msg of result.messages) {\n  console.log(msg.content);\n}\nconsole.log(`Current step: ${result.currentStep}`);\n\n// Turn 4: Resolution\nconsole.log(\"\\n=== Turn 4: Resolution ===\");\nresult = await agent.invoke(\n  { messages: [new HumanMessage(\"What should I do?\")] },\n  config\n);\nfor (const msg of result.messages) {\n  console.log(msg.content);\n}\n```\n:::\n</Expandable>\n\n## Next steps\n\n- Learn about the [subagents pattern](/oss/langchain/multi-agent/subagents-personal-assistant) for centralized orchestration\n- Explore [middleware](/oss/langchain/middleware) for more dynamic behaviors\n- Read the [multi-agent overview](/oss/langchain/multi-agent) to compare patterns\n- Use [LangSmith](https://smith.langchain.com) to debug and monitor your multi-agent system\n", "metadata": {"source": "multi-agent/handoffs-customer-support.mdx"}}
{"text": "---\ntitle: Multi-agent\nsidebarTitle: Overview\n---\n\nMulti-agent systems coordinate specialized components to tackle complex workflows. However, not every complex task requires this approach \u2014 a single agent with the right (sometimes dynamic) tools and prompt can often achieve similar results.\n\n## Why multi-agent?\n\nWhen developers say they need \"multi-agent,\" they're usually looking for one or more of these capabilities:\n\n- <Icon icon=\"brain\" /> **Context management**: Provide specialized knowledge without overwhelming the model's context window. If context were infinite and latency zero, you could dump all knowledge into a single prompt \u2014 but since it's not, you need patterns to selectively surface relevant information.\n- <Icon icon=\"users\" /> **Distributed development**: Allow different teams to develop and maintain capabilities independently, composing them into a larger system with clear boundaries.\n- <Icon icon=\"code-branch\" /> **Parallelization**: Spawn specialized workers for subtasks and execute them concurrently for faster results.\n\nMulti-agent patterns are particularly valuable when a single agent has too many [tools](/oss/langchain/tools) and makes poor decisions about which to use, when tasks require specialized knowledge with extensive context (long prompts and domain-specific tools), or when you need to enforce sequential constraints that unlock capabilities only after certain conditions are met.\n\n<Tip>\nAt the center of multi-agent design is **[context engineering](/oss/langchain/context-engineering)**\u2014deciding what information each agent sees. The quality of your system depends on ensuring each agent has access to the right data for its task.\n</Tip>\n\n## Patterns\n\nHere are the main patterns for building multi-agent systems, each suited to different use cases:\n\n| Pattern | How it works |\n|--------------|--------------|\n| [**Subagents**](/oss/langchain/multi-agent/subagents) | A main agent coordinates subagents as tools. All routing passes through the main agent, which decides when and how to invoke each subagent. |\n| [**Handoffs**](/oss/langchain/multi-agent/handoffs) | Behavior changes dynamically based on state. Tool calls update a state variable that triggers routing or configuration changes, switching agents or adjusting the current agent's tools and prompt. |\n| [**Skills**](/oss/langchain/multi-agent/skills) | Specialized prompts and knowledge loaded on-demand. A single agent stays in control while loading context from skills as needed. |\n| [**Router**](/oss/langchain/multi-agent/router) | A routing step classifies input and directs it to one or more specialized agents. Results are synthesized into a combined response. |\n| [**Custom workflow**](/oss/langchain/multi-agent/custom-workflow) | Build bespoke execution flows with [LangGraph](/oss/langgraph/overview), mixing deterministic logic and agentic behavior. Embed other patterns as nodes in your workflow. |\n\n### Choosing a pattern\n\nUse this table to match your requirements to the right pattern:\n\n<div className=\"compact-first-col\">\n\n| Pattern | Distributed development | Parallelization | Multi-hop | Direct user interaction |\n|---------|:-----------------------:|:---------------:|:----------:|:-----------------------:|\n| [**Subagents**](/oss/langchain/multi-agent/subagents) | \u2b50\u2b50\u2b50\u2b50\u2b50 | \u2b50\u2b50\u2b50\u2b50\u2b50 | \u2b50\u2b50\u2b50\u2b50", "metadata": {"source": "multi-agent/index.mdx"}}
{"text": "](/oss/langchain/multi-agent/custom-workflow) | Build bespoke execution flows with [LangGraph](/oss/langgraph/overview), mixing deterministic logic and agentic behavior. Embed other patterns as nodes in your workflow. |\n\n### Choosing a pattern\n\nUse this table to match your requirements to the right pattern:\n\n<div className=\"compact-first-col\">\n\n| Pattern | Distributed development | Parallelization | Multi-hop | Direct user interaction |\n|---------|:-----------------------:|:---------------:|:----------:|:-----------------------:|\n| [**Subagents**](/oss/langchain/multi-agent/subagents) | \u2b50\u2b50\u2b50\u2b50\u2b50 | \u2b50\u2b50\u2b50\u2b50\u2b50 | \u2b50\u2b50\u2b50\u2b50\u2b50 | \u2b50 |\n| [**Handoffs**](/oss/langchain/multi-agent/handoffs) | \u2014 | \u2014 | \u2b50\u2b50\u2b50\u2b50\u2b50 | \u2b50\u2b50\u2b50\u2b50\u2b50 |\n| [**Skills**](/oss/langchain/multi-agent/skills) | \u2b50\u2b50\u2b50\u2b50\u2b50 | \u2b50\u2b50\u2b50 | \u2b50\u2b50\u2b50\u2b50\u2b50 | \u2b50\u2b50\u2b50\u2b50\u2b50 |\n| [**Router**](/oss/langchain/multi-agent/router) | \u2b50\u2b50\u2b50 | \u2b50\u2b50\u2b50\u2b50\u2b50 | \u2014 | \u2b50\u2b50\u2b50 |\n\n</div>\n\n- **Distributed development**: Can different teams maintain components independently?\n- **Parallelization**: Can multiple agents execute concurrently?\n- **Multi-hop**: Does the pattern support calling multiple subagents in series?\n- **Direct user interaction**: Can subagents converse directly with the user?\n\n<Tip>\n    You can mix patterns! For example, a **subagents** architecture can invoke tools that invoke custom workflows or router agents. Subagents can even use the **skills** pattern to load context on-demand. The possibilities are endless!\n</Tip>\n\n### Visual overview\n\n<Tabs>\n  <Tab title=\"Subagents\">\n    A main agent coordinates subagents as tools. All routing passes through the main agent.\n\n    <Frame>\n      <img src=\"/oss/langchain/multi-agent/images/pattern-subagents.png\" alt=\"Subagents pattern: main agent coordinates subagents as tools\" />\n    </Frame>\n  </Tab>\n\n  <Tab title=\"Handoffs\">\n    Agents transfer control to each other via tool calls. Each agent can hand off to others or respond directly to the user.\n\n    <Frame>\n      <img src=\"/oss/langchain/multi-agent/images/pattern-handoffs.png\" alt=\"Handoffs pattern: agents transfer control via tool calls\" />\n    </Frame>\n  </Tab>\n\n  <Tab title=\"Skills\">\n    A single agent loads specialized prompts and knowledge on-demand while staying in control.\n\n    <Frame>\n      <img src=\"/oss/langchain/multi-agent/images/pattern-", "metadata": {"source": "multi-agent/index.mdx"}}
{"text": "langchain/multi-agent/images/pattern-subagents.png\" alt=\"Subagents pattern: main agent coordinates subagents as tools\" />\n    </Frame>\n  </Tab>\n\n  <Tab title=\"Handoffs\">\n    Agents transfer control to each other via tool calls. Each agent can hand off to others or respond directly to the user.\n\n    <Frame>\n      <img src=\"/oss/langchain/multi-agent/images/pattern-handoffs.png\" alt=\"Handoffs pattern: agents transfer control via tool calls\" />\n    </Frame>\n  </Tab>\n\n  <Tab title=\"Skills\">\n    A single agent loads specialized prompts and knowledge on-demand while staying in control.\n\n    <Frame>\n      <img src=\"/oss/langchain/multi-agent/images/pattern-skills.png\" alt=\"Skills pattern: single agent loads specialized context on-demand\" />\n    </Frame>\n  </Tab>\n\n  <Tab title=\"Router\">\n    A routing step classifies input and directs it to specialized agents. Results are synthesized.\n\n    <Frame>\n      <img src=\"/oss/langchain/multi-agent/images/pattern-router.png\" alt=\"Router pattern: routing step classifies input to specialized agents\" />\n    </Frame>\n  </Tab>\n</Tabs>\n\n## Performance comparison\n\nDifferent patterns have different performance characteristics. Understanding these tradeoffs helps you choose the right pattern for your latency and cost requirements.\n\n**Key metrics:**\n- **Model calls**: Number of LLM invocations. More calls = higher latency (especially if sequential) and higher per-request API costs.\n- **Tokens processed**: Total [context window](/oss/langchain/context-engineering) usage across all calls. More tokens = higher processing costs and potential context limits.\n\n### One-shot request\n\n> **User:** \"Buy coffee\"\n\nA specialized coffee agent/skill can call a `buy_coffee` tool.\n\n| Pattern | Model calls | Best fit |\n|---------|:-----------:|:--------:|\n| [**Subagents**](/oss/langchain/multi-agent/subagents) | 4 | |\n| [**Handoffs**](/oss/langchain/multi-agent/handoffs) | 3 | \u2705 |\n| [**Skills**](/oss/langchain/multi-agent/skills) | 3 | \u2705 |\n| [**Router**](/oss/langchain/multi-agent/router) | 3 | \u2705 |\n\n<Tabs>\n  <Tab title=\"Subagents\">\n    **4 model calls:**\n\n    <Frame>\n      <img src=\"/oss/langchain/multi-agent/images/oneshot-subagents.png\" alt=\"Subagents one-shot: 4 model calls for buy coffee request\" />\n    </Frame>\n  </Tab>\n\n  <Tab title=\"Handoffs\">\n    **3 model calls:**\n\n    <Frame>\n      <img src=\"/oss/langchain/multi-agent/images/oneshot-handoffs.png\" alt=\"Handoffs one-shot: 3 model calls for buy coffee request\" />\n    </Frame>\n  </Tab>\n\n  <Tab title=\"Skills\">\n    **3 model calls:**\n\n", "metadata": {"source": "multi-agent/index.mdx"}}
{"text": " | 3 | \u2705 |\n\n<Tabs>\n  <Tab title=\"Subagents\">\n    **4 model calls:**\n\n    <Frame>\n      <img src=\"/oss/langchain/multi-agent/images/oneshot-subagents.png\" alt=\"Subagents one-shot: 4 model calls for buy coffee request\" />\n    </Frame>\n  </Tab>\n\n  <Tab title=\"Handoffs\">\n    **3 model calls:**\n\n    <Frame>\n      <img src=\"/oss/langchain/multi-agent/images/oneshot-handoffs.png\" alt=\"Handoffs one-shot: 3 model calls for buy coffee request\" />\n    </Frame>\n  </Tab>\n\n  <Tab title=\"Skills\">\n    **3 model calls:**\n\n    <Frame>\n      <img src=\"/oss/langchain/multi-agent/images/oneshot-skills.png\" alt=\"Skills one-shot: 3 model calls for buy coffee request\" />\n    </Frame>\n  </Tab>\n\n  <Tab title=\"Router\">\n    **3 model calls:**\n\n    <Frame>\n      <img src=\"/oss/langchain/multi-agent/images/oneshot-router.png\" alt=\"Router one-shot: 3 model calls for buy coffee request\" />\n    </Frame>\n  </Tab>\n</Tabs>\n\n**Key insight:** Handoffs, Skills, and Router are most efficient for single tasks (3 calls each). Subagents adds one extra call because results flow back through the main agent\u2014this overhead provides centralized control.\n\n### Repeat request\n\n> **Turn 1:** \"Buy coffee\"\n> **Turn 2:** \"Buy coffee again\"\n\nThe user repeats the same request in the same conversation.\n\n<div className=\"compact-first-col\">\n\n| Pattern | Turn 2 calls | Total (both turns) | Best fit |\n|---------|:------------:|:------------------:|:--------:|\n| [**Subagents**](/oss/langchain/multi-agent/subagents) | 4 | 8 | |\n| [**Handoffs**](/oss/langchain/multi-agent/handoffs) | 2 | 5 | \u2705 |\n| [**Skills**](/oss/langchain/multi-agent/skills) | 2 | 5 | \u2705 |\n| [**Router**](/oss/langchain/multi-agent/router) | 3 | 6 | |\n\n</div>\n\n<Tabs>\n  <Tab title=\"Subagents\">\n    **4 calls again \u2192 8 total**\n\n    - Subagents are **stateless by design**\u2014each invocation follows the same flow\n    - The main agent maintains conversation context, but subagents start fresh each time\n    - This provides strong context isolation but repeats the full flow\n  </Tab>\n\n  <Tab title=\"Handoffs\">\n    **2 calls \u2192 5 total**\n\n    - The coffee agent is **still active** from turn 1 (state persists)\n    - No handoff needed\u2014agent directly calls `buy_coffee` tool (call 1)\n    - Agent responds to user (call 2)\n    - **Saves 1 call by skipping the handoff**\n  </Tab>\n\n  <Tab title=\"Skills", "metadata": {"source": "multi-agent/index.mdx"}}
{"text": " | |\n\n</div>\n\n<Tabs>\n  <Tab title=\"Subagents\">\n    **4 calls again \u2192 8 total**\n\n    - Subagents are **stateless by design**\u2014each invocation follows the same flow\n    - The main agent maintains conversation context, but subagents start fresh each time\n    - This provides strong context isolation but repeats the full flow\n  </Tab>\n\n  <Tab title=\"Handoffs\">\n    **2 calls \u2192 5 total**\n\n    - The coffee agent is **still active** from turn 1 (state persists)\n    - No handoff needed\u2014agent directly calls `buy_coffee` tool (call 1)\n    - Agent responds to user (call 2)\n    - **Saves 1 call by skipping the handoff**\n  </Tab>\n\n  <Tab title=\"Skills\">\n    **2 calls \u2192 5 total**\n\n    - The skill context is **already loaded** in conversation history\n    - No need to reload\u2014agent directly calls `buy_coffee` tool (call 1)\n    - Agent responds to user (call 2)\n    - **Saves 1 call by reusing loaded skill**\n  </Tab>\n\n  <Tab title=\"Router\">\n    **3 calls again \u2192 6 total**\n\n    - Routers are **stateless**\u2014each request requires an LLM routing call\n    - Turn 2: Router LLM call (1) \u2192 Milk agent calls buy_coffee (2) \u2192 Milk agent responds (3)\n    - Can be optimized by wrapping as a tool in a stateful agent\n  </Tab>\n</Tabs>\n\n**Key insight:** Stateful patterns (Handoffs, Skills) save 40-50% of calls on repeat requests. Subagents maintain consistent cost per request\u2014this stateless design provides strong context isolation but at the cost of repeated model calls.\n\n### Multi-domain\n\n> **User:** \"Compare Python, JavaScript, and Rust for web development\"\n\nEach language agent/skill contains ~2000 tokens of documentation. All patterns can make parallel tool calls.\n\n| Pattern | Model calls | Total tokens | Best fit |\n|---------|:-----------:|:------------:|:--------:|\n| [**Subagents**](/oss/langchain/multi-agent/subagents) | 5 | ~9K | \u2705 |\n| [**Handoffs**](/oss/langchain/multi-agent/handoffs) | 7+ | ~14K+ | |\n| [**Skills**](/oss/langchain/multi-agent/skills) | 3 | ~15K | |\n| [**Router**](/oss/langchain/multi-agent/router) | 5 | ~9K | \u2705 |\n\n<Tabs>\n  <Tab title=\"Subagents\">\n    **5 calls, ~9K tokens**\n\n    <Frame>\n      <img src=\"/oss/langchain/multi-agent/images/multidomain-subagents.png\" alt=\"Subagents multi-domain: 5 calls with parallel execution\" />\n    </Frame>\n\n    Each subagent works in **isolation** with only its relevant context. Total: **9K tokens**.\n  </Tab>\n\n  <Tab title=\"Handoffs\">\n    **7+ calls, ~14K+ tokens**\n\n    <Frame>\n      <img", "metadata": {"source": "multi-agent/index.mdx"}}
{"text": "ills) | 3 | ~15K | |\n| [**Router**](/oss/langchain/multi-agent/router) | 5 | ~9K | \u2705 |\n\n<Tabs>\n  <Tab title=\"Subagents\">\n    **5 calls, ~9K tokens**\n\n    <Frame>\n      <img src=\"/oss/langchain/multi-agent/images/multidomain-subagents.png\" alt=\"Subagents multi-domain: 5 calls with parallel execution\" />\n    </Frame>\n\n    Each subagent works in **isolation** with only its relevant context. Total: **9K tokens**.\n  </Tab>\n\n  <Tab title=\"Handoffs\">\n    **7+ calls, ~14K+ tokens**\n\n    <Frame>\n      <img src=\"/oss/langchain/multi-agent/images/multidomain-handoffs.png\" alt=\"Handoffs multi-domain: 7+ sequential calls\" />\n    </Frame>\n\n    Handoffs executes **sequentially**\u2014can't research all three languages in parallel. Growing conversation history adds overhead. Total: **~14K+ tokens**.\n  </Tab>\n\n  <Tab title=\"Skills\">\n    **3 calls, ~15K tokens**\n\n    <Frame>\n      <img src=\"/oss/langchain/multi-agent/images/multidomain-skills.png\" alt=\"Skills multi-domain: 3 calls with accumulated context\" />\n    </Frame>\n\n    After loading, **every subsequent call processes all 6K tokens of skill documentation**. Subagents processes 67% fewer tokens overall due to context isolation. Total: **15K tokens**.\n  </Tab>\n\n  <Tab title=\"Router\">\n    **5 calls, ~9K tokens**\n\n    <Frame>\n      <img src=\"/oss/langchain/multi-agent/images/multidomain-router.png\" alt=\"Router multi-domain: 5 calls with parallel execution\" />\n    </Frame>\n\n    Router uses an **LLM for routing**, then invokes agents in parallel. Similar to Subagents but with explicit routing step. Total: **9K tokens**.\n  </Tab>\n</Tabs>\n\n**Key insight:** For multi-domain tasks, patterns with parallel execution (Subagents, Router) are most efficient. Skills has fewer calls but high token usage due to context accumulation. Handoffs is inefficient here\u2014it must execute sequentially and can't leverage parallel tool calling for consulting multiple domains simultaneously.\n\n### Summary\n\nHere's how patterns compare across all three scenarios:\n\n<div className=\"compact-first-col\">\n\n| Pattern | One-shot | Repeat request | Multi-domain |\n|---------|:-----------:|:---------:|:----------------:|\n| [**Subagents**](/oss/langchain/multi-agent/subagents) | 4 calls | 8 calls (4+4) | 5 calls, 9K tokens |\n| [**Handoffs**](/oss/langchain/multi-agent/handoffs) | 3 calls | 5 calls (3+2) | 7+ calls, 14K+ tokens |\n| [**Skills**](/oss/langchain/multi-agent/skills) | 3 calls | 5 calls (3+2) | 3 calls, 15K tokens |\n| [**Router", "metadata": {"source": "multi-agent/index.mdx"}}
{"text": " leverage parallel tool calling for consulting multiple domains simultaneously.\n\n### Summary\n\nHere's how patterns compare across all three scenarios:\n\n<div className=\"compact-first-col\">\n\n| Pattern | One-shot | Repeat request | Multi-domain |\n|---------|:-----------:|:---------:|:----------------:|\n| [**Subagents**](/oss/langchain/multi-agent/subagents) | 4 calls | 8 calls (4+4) | 5 calls, 9K tokens |\n| [**Handoffs**](/oss/langchain/multi-agent/handoffs) | 3 calls | 5 calls (3+2) | 7+ calls, 14K+ tokens |\n| [**Skills**](/oss/langchain/multi-agent/skills) | 3 calls | 5 calls (3+2) | 3 calls, 15K tokens |\n| [**Router**](/oss/langchain/multi-agent/router) | 3 calls | 6 calls (3+3) | 5 calls, 9K tokens |\n\n</div>\n\n**Choosing a pattern:**\n\n<div className=\"compact-first-col\">\n\n| Optimize for | [Subagents](/oss/langchain/multi-agent/subagents) | [Handoffs](/oss/langchain/multi-agent/handoffs) | [Skills](/oss/langchain/multi-agent/skills) | [Router](/oss/langchain/multi-agent/router) |\n|--------------|:---------:|:--------:|:------:|:------:|\n| Single requests | | \u2705 | \u2705 | \u2705 |\n| Repeat requests | | \u2705 | \u2705 | |\n| Parallel execution | \u2705 | | | \u2705 |\n| Large-context domains | \u2705 | | | \u2705 |\n| Simple, focused tasks | | | \u2705 | |\n\n</div>\n", "metadata": {"source": "multi-agent/index.mdx"}}
{"text": "---\ntitle: Build a personal assistant with subagents\nsidebarTitle: \"Subagents: Personal assistant\"\n---\n\nimport ChatModelTabsPy from '/snippets/chat-model-tabs.mdx';\nimport ChatModelTabsJs from '/snippets/chat-model-tabs-js.mdx';\n\n## Overview\n\nThe **supervisor pattern** is a [multi-agent](/oss/langchain/multi-agent) architecture where a central supervisor agent coordinates specialized worker agents. This approach excels when tasks require different types of expertise. Rather than building one agent that manages tool selection across domains, you create focused specialists coordinated by a supervisor who understands the overall workflow.\n\nIn this tutorial, you'll build a personal assistant system that demonstrates these benefits through a realistic workflow. The system will coordinate two specialists with fundamentally different responsibilities:\n\n- A **calendar agent** that handles scheduling, availability checking, and event management.\n- An **email agent** that manages communication, drafts messages, and sends notifications.\n\nWe will also incorporate [human-in-the-loop review](/oss/langchain/human-in-the-loop) to allow users to approve, edit, and reject actions (such as outbound emails) as desired.\n\n### Why use a supervisor?\n\nMulti-agent architectures allow you to partition [tools](/oss/langchain/tools) across workers, each with their own individual prompts or instructions. Consider an agent with direct access to all calendar and email APIs: it must choose from many similar tools, understand exact formats for each API, and handle multiple domains simultaneously. If performance degrades, it may be helpful to separate related tools and associated prompts into logical groups (in part to manage iterative improvements).\n\n### Concepts\n\nWe will cover the following concepts:\n\n- [Multi-agent systems](/oss/langchain/multi-agent)\n- [Human-in-the-loop review](/oss/langchain/human-in-the-loop)\n\n## Setup\n\n### Installation\n\nThis tutorial requires the `langchain` package:\n\n:::python\n<CodeGroup>\n```bash pip\npip install langchain\n```\n```bash conda\nconda install langchain -c conda-forge\n```\n</CodeGroup>\n:::\n\n:::js\n<CodeGroup>\n```bash npm\nnpm install langchain\n```\n```bash yarn\nyarn add langchain\n```\n```bash pnpm\npnpm add langchain\n```\n</CodeGroup>\n:::\n\nFor more details, see our [Installation guide](/oss/langchain/install).\n\n### LangSmith\n\nSet up [LangSmith](https://smith.langchain.com) to inspect what is happening inside your agent. Then set the following environment variables:\n\n:::python\n<CodeGroup>\n```bash bash\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n```\n```python python\nimport getpass\nimport os\n\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```\n</CodeGroup>\n:::\n\n:::js\n<CodeGroup>\n```bash bash\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n```\n```typescript typescript\nprocess.env.LANGSMITH_TR", "metadata": {"source": "multi-agent/subagents-personal-assistant.mdx"}}
{"text": "LangSmith](https://smith.langchain.com) to inspect what is happening inside your agent. Then set the following environment variables:\n\n:::python\n<CodeGroup>\n```bash bash\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n```\n```python python\nimport getpass\nimport os\n\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```\n</CodeGroup>\n:::\n\n:::js\n<CodeGroup>\n```bash bash\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n```\n```typescript typescript\nprocess.env.LANGSMITH_TRACING = \"true\";\nprocess.env.LANGSMITH_API_KEY = \"...\";\n```\n</CodeGroup>\n:::\n\n### Components\n\nWe will need to select a chat model from LangChain's suite of integrations:\n\n:::python\n<ChatModelTabsPy />\n:::\n\n:::js\n<ChatModelTabsJs />\n:::\n\n## 1. Define tools\n\nStart by defining the tools that require structured inputs. In real applications, these would call actual APIs (Google Calendar, SendGrid, etc.). For this tutorial, you'll use stubs to demonstrate the pattern.\n\n:::python\n```python\nfrom langchain.tools import tool\n\n@tool\ndef create_calendar_event(\n    title: str,\n    start_time: str,       # ISO format: \"2024-01-15T14:00:00\"\n    end_time: str,         # ISO format: \"2024-01-15T15:00:00\"\n    attendees: list[str],  # email addresses\n    location: str = \"\"\n) -> str:\n    \"\"\"Create a calendar event. Requires exact ISO datetime format.\"\"\"\n    # Stub: In practice, this would call Google Calendar API, Outlook API, etc.\n    return f\"Event created: {title} from {start_time} to {end_time} with {len(attendees)} attendees\"\n\n\n@tool\ndef send_email(\n    to: list[str],  # email addresses\n    subject: str,\n    body: str,\n    cc: list[str] = []\n) -> str:\n    \"\"\"Send an email via email API. Requires properly formatted addresses.\"\"\"\n    # Stub: In practice, this would call SendGrid, Gmail API, etc.\n    return f\"Email sent to {', '.join(to)} - Subject: {subject}\"\n\n\n@tool\ndef get_available_time_slots(\n    attendees: list[str],\n    date: str,  # ISO format: \"2024-01-15\"\n    duration_minutes: int\n) -> list[str]:\n    \"\"\"Check calendar availability for given attendees on a specific date.\"\"\"\n    # Stub: In practice, this would query calendar APIs\n    return [\"09:00\", \"14:00\", \"16:00\"]\n```\n:::\n\n:::js\n```", "metadata": {"source": "multi-agent/subagents-personal-assistant.mdx"}}
{"text": "str] = []\n) -> str:\n    \"\"\"Send an email via email API. Requires properly formatted addresses.\"\"\"\n    # Stub: In practice, this would call SendGrid, Gmail API, etc.\n    return f\"Email sent to {', '.join(to)} - Subject: {subject}\"\n\n\n@tool\ndef get_available_time_slots(\n    attendees: list[str],\n    date: str,  # ISO format: \"2024-01-15\"\n    duration_minutes: int\n) -> list[str]:\n    \"\"\"Check calendar availability for given attendees on a specific date.\"\"\"\n    # Stub: In practice, this would query calendar APIs\n    return [\"09:00\", \"14:00\", \"16:00\"]\n```\n:::\n\n:::js\n```typescript\nimport { tool } from \"langchain\";\nimport { z } from \"zod\";\n\nconst createCalendarEvent = tool(\n  async ({ title, startTime, endTime, attendees, location }) => {\n    // Stub: In practice, this would call Google Calendar API, Outlook API, etc.\n    return `Event created: ${title} from ${startTime} to ${endTime} with ${attendees.length} attendees`;\n  },\n  {\n    name: \"create_calendar_event\",\n    description: \"Create a calendar event. Requires exact ISO datetime format.\",\n    schema: z.object({\n      title: z.string(),\n      startTime: z.string().describe(\"ISO format: '2024-01-15T14:00:00'\"),\n      endTime: z.string().describe(\"ISO format: '2024-01-15T15:00:00'\"),\n      attendees: z.array(z.string()).describe(\"email addresses\"),\n      location: z.string().optional(),\n    }),\n  }\n);\n\nconst sendEmail = tool(\n  async ({ to, subject, body, cc }) => {\n    // Stub: In practice, this would call SendGrid, Gmail API, etc.\n    return `Email sent to ${to.join(', ')} - Subject: ${subject}`;\n  },\n  {\n    name: \"send_email\",\n    description: \"Send an email via email API. Requires properly formatted addresses.\",\n    schema: z.object({\n      to: z.array(z.string()).describe(\"email addresses\"),\n      subject: z.string(),\n      body: z.string(),\n      cc: z.array(z.string()).optional(),\n    }),\n  }\n);\n\nconst getAvailableTimeSlots = tool(\n  async ({ attendees, date, durationMinutes }) => {\n    // Stub: In practice, this would query calendar APIs\n    return [\"09:00\", \"14:00\", \"16:00\"];\n  },\n  {\n    name: \"get_available_time_slots\",\n    description: \"Check calendar availability for given attendees on a specific date.\",\n    schema: z.object({\n      attendees: z.array(z.string", "metadata": {"source": "multi-agent/subagents-personal-assistant.mdx"}}
{"text": "    to: z.array(z.string()).describe(\"email addresses\"),\n      subject: z.string(),\n      body: z.string(),\n      cc: z.array(z.string()).optional(),\n    }),\n  }\n);\n\nconst getAvailableTimeSlots = tool(\n  async ({ attendees, date, durationMinutes }) => {\n    // Stub: In practice, this would query calendar APIs\n    return [\"09:00\", \"14:00\", \"16:00\"];\n  },\n  {\n    name: \"get_available_time_slots\",\n    description: \"Check calendar availability for given attendees on a specific date.\",\n    schema: z.object({\n      attendees: z.array(z.string()),\n      date: z.string().describe(\"ISO format: '2024-01-15'\"),\n      durationMinutes: z.number(),\n    }),\n  }\n);\n```\n:::\n\n## 2. Create specialized sub-agents\n\nNext, we'll create specialized sub-agents that handle each domain.\n\n### Create a calendar agent\n\nThe calendar agent understands natural language scheduling requests and translates them into precise API calls. It handles date parsing, availability checking, and event creation.\n\n:::python\n```python\nfrom langchain.agents import create_agent\n\n\nCALENDAR_AGENT_PROMPT = (\n    \"You are a calendar scheduling assistant. \"\n    \"Parse natural language scheduling requests (e.g., 'next Tuesday at 2pm') \"\n    \"into proper ISO datetime formats. \"\n    \"Use get_available_time_slots to check availability when needed. \"\n    \"Use create_calendar_event to schedule events. \"\n    \"Always confirm what was scheduled in your final response.\"\n)\n\ncalendar_agent = create_agent(\n    model,\n    tools=[create_calendar_event, get_available_time_slots],\n    system_prompt=CALENDAR_AGENT_PROMPT,\n)\n```\n:::\n\n:::js\n```typescript\nimport { createAgent } from \"langchain\";\n\nconst CALENDAR_AGENT_PROMPT = `\nYou are a calendar scheduling assistant.\nParse natural language scheduling requests (e.g., 'next Tuesday at 2pm')\ninto proper ISO datetime formats.\nUse get_available_time_slots to check availability when needed.\nUse create_calendar_event to schedule events.\nAlways confirm what was scheduled in your final response.\n`.trim();\n\nconst calendarAgent = createAgent({\n  model: llm,\n  tools: [createCalendarEvent, getAvailableTimeSlots],\n  systemPrompt: CALENDAR_AGENT_PROMPT,\n});\n```\n:::\n\nTest the calendar agent to see how it handles natural language scheduling:\n\n:::python\n```python\nquery = \"Schedule a team meeting next Tuesday at 2pm for 1 hour\"\n\nfor step in calendar_agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n):\n    for update in step.values():\n        for message", "metadata": {"source": "multi-agent/subagents-personal-assistant.mdx"}}
{"text": " formats.\nUse get_available_time_slots to check availability when needed.\nUse create_calendar_event to schedule events.\nAlways confirm what was scheduled in your final response.\n`.trim();\n\nconst calendarAgent = createAgent({\n  model: llm,\n  tools: [createCalendarEvent, getAvailableTimeSlots],\n  systemPrompt: CALENDAR_AGENT_PROMPT,\n});\n```\n:::\n\nTest the calendar agent to see how it handles natural language scheduling:\n\n:::python\n```python\nquery = \"Schedule a team meeting next Tuesday at 2pm for 1 hour\"\n\nfor step in calendar_agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n):\n    for update in step.values():\n        for message in update.get(\"messages\", []):\n            message.pretty_print()\n```\n:::\n\n:::js\n```typescript\nconst query = \"Schedule a team meeting next Tuesday at 2pm for 1 hour\";\n\nconst stream = await calendarAgent.stream({\n  messages: [{ role: \"user\", content: query }]\n});\n\nfor await (const step of stream) {\n  for (const update of Object.values(step)) {\n    if (update && typeof update === \"object\" && \"messages\" in update) {\n      for (const message of update.messages) {\n        console.log(message.toFormattedString());\n      }\n    }\n  }\n}\n```\n:::\n```\n================================== Ai Message ==================================\nTool Calls:\n  get_available_time_slots (call_EIeoeIi1hE2VmwZSfHStGmXp)\n Call ID: call_EIeoeIi1hE2VmwZSfHStGmXp\n  Args:\n    attendees: []\n    date: 2024-06-18\n    duration_minutes: 60\n================================= Tool Message =================================\nName: get_available_time_slots\n\n[\"09:00\", \"14:00\", \"16:00\"]\n================================== Ai Message ==================================\nTool Calls:\n  create_calendar_event (call_zgx3iJA66Ut0W8S3NpT93kEB)\n Call ID: call_zgx3iJA66Ut0W8S3NpT93kEB\n  Args:\n    title: Team Meeting\n    start_time: 2024-06-18T14:00:00\n    end_time: 2024-06-18T15:00:00\n    attendees: []\n================================= Tool Message =================================\nName: create_calendar_event\n\nEvent created: Team Meeting from 2024-06-18T14:00:00 to 2024-06-18T15:00:00 with 0 attendees\n================================== Ai Message ==================================\n\nThe team meeting has been scheduled for next Tuesday, June 18th, at 2:00 PM and will last for 1 hour. If you need to add attendees or a location, please let me know!\n```\n\nThe agent parses \"next Tuesday at 2pm\" into ISO format (\"2024-01-16T14:00", "metadata": {"source": "multi-agent/subagents-personal-assistant.mdx"}}
{"text": "66Ut0W8S3NpT93kEB\n  Args:\n    title: Team Meeting\n    start_time: 2024-06-18T14:00:00\n    end_time: 2024-06-18T15:00:00\n    attendees: []\n================================= Tool Message =================================\nName: create_calendar_event\n\nEvent created: Team Meeting from 2024-06-18T14:00:00 to 2024-06-18T15:00:00 with 0 attendees\n================================== Ai Message ==================================\n\nThe team meeting has been scheduled for next Tuesday, June 18th, at 2:00 PM and will last for 1 hour. If you need to add attendees or a location, please let me know!\n```\n\nThe agent parses \"next Tuesday at 2pm\" into ISO format (\"2024-01-16T14:00:00\"), calculates the end time, calls `create_calendar_event`, and returns a natural language confirmation.\n\n### Create an email agent\n\nThe email agent handles message composition and sending. It focuses on extracting recipient information, crafting appropriate subject lines and body text, and managing email communication.\n\n:::python\n```python\nEMAIL_AGENT_PROMPT = (\n    \"You are an email assistant. \"\n    \"Compose professional emails based on natural language requests. \"\n    \"Extract recipient information and craft appropriate subject lines and body text. \"\n    \"Use send_email to send the message. \"\n    \"Always confirm what was sent in your final response.\"\n)\n\nemail_agent = create_agent(\n    model,\n    tools=[send_email],\n    system_prompt=EMAIL_AGENT_PROMPT,\n)\n```\n\nTest the email agent with a natural language request:\n\n```python\nquery = \"Send the design team a reminder about reviewing the new mockups\"\n\nfor step in email_agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n):\n    for update in step.values():\n        for message in update.get(\"messages\", []):\n            message.pretty_print()\n```\n:::\n\n:::js\n```typescript\nconst EMAIL_AGENT_PROMPT = `\nYou are an email assistant.\nCompose professional emails based on natural language requests.\nExtract recipient information and craft appropriate subject lines and body text.\nUse send_email to send the message.\nAlways confirm what was sent in your final response.\n`.trim();\n\nconst emailAgent = createAgent({\n  model: llm,\n  tools: [sendEmail],\n  systemPrompt: EMAIL_AGENT_PROMPT,\n});\n```\n\nTest the email agent with a natural language request:\n\n```typescript\nconst query = \"Send the design team a reminder about reviewing the new mockups\";\n\nconst stream = await emailAgent.stream({\n  messages: [{ role: \"user\", content: query }]\n});\n\nfor await (const step of stream) {\n  for (const update of Object.values(step)) {\n    if (update && typeof update === \"object\" && \"messages\" in update) {\n      for (const message of update.messages) {\n   ", "metadata": {"source": "multi-agent/subagents-personal-assistant.mdx"}}
{"text": " message.\nAlways confirm what was sent in your final response.\n`.trim();\n\nconst emailAgent = createAgent({\n  model: llm,\n  tools: [sendEmail],\n  systemPrompt: EMAIL_AGENT_PROMPT,\n});\n```\n\nTest the email agent with a natural language request:\n\n```typescript\nconst query = \"Send the design team a reminder about reviewing the new mockups\";\n\nconst stream = await emailAgent.stream({\n  messages: [{ role: \"user\", content: query }]\n});\n\nfor await (const step of stream) {\n  for (const update of Object.values(step)) {\n    if (update && typeof update === \"object\" && \"messages\" in update) {\n      for (const message of update.messages) {\n        console.log(message.toFormattedString());\n      }\n    }\n  }\n}\n```\n:::\n```\n================================== Ai Message ==================================\nTool Calls:\n  send_email (call_OMl51FziTVY6CRZvzYfjYOZr)\n Call ID: call_OMl51FziTVY6CRZvzYfjYOZr\n  Args:\n    to: ['design-team@example.com']\n    subject: Reminder: Please Review the New Mockups\n    body: Hi Design Team,\n\nThis is a friendly reminder to review the new mockups at your earliest convenience. Your feedback is important to ensure that we stay on track with our project timeline.\n\nPlease let me know if you have any questions or need additional information.\n\nThank you!\n\nBest regards,\n================================= Tool Message =================================\nName: send_email\n\nEmail sent to design-team@example.com - Subject: Reminder: Please Review the New Mockups\n================================== Ai Message ==================================\n\nI've sent a reminder to the design team asking them to review the new mockups. If you need any further communication on this topic, just let me know!\n```\n\nThe agent infers the recipient from the informal request, crafts a professional subject line and body, calls `send_email`, and returns a confirmation. Each sub-agent has a narrow focus with domain-specific tools and prompts, allowing it to excel at its specific task.\n\n## 3. Wrap sub-agents as tools\n\nNow wrap each sub-agent as a tool that the supervisor can invoke. This is the key architectural step that creates the layered system. The supervisor will see high-level tools like \"schedule_event\", not low-level tools like \"create_calendar_event\".\n\n:::python\n```python\n@tool\ndef schedule_event(request: str) -> str:\n    \"\"\"Schedule calendar events using natural language.\n\n    Use this when the user wants to create, modify, or check calendar appointments.\n    Handles date/time parsing, availability checking, and event creation.\n\n    Input: Natural language scheduling request (e.g., 'meeting with design team\n    next Tuesday at 2pm')\n    \"\"\"\n    result = calendar_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": request}]\n    })\n    return result[\"messages\"][-1].text\n\n\n@tool\ndef manage_email(request: str)", "metadata": {"source": "multi-agent/subagents-personal-assistant.mdx"}}
{"text": "_event\", not low-level tools like \"create_calendar_event\".\n\n:::python\n```python\n@tool\ndef schedule_event(request: str) -> str:\n    \"\"\"Schedule calendar events using natural language.\n\n    Use this when the user wants to create, modify, or check calendar appointments.\n    Handles date/time parsing, availability checking, and event creation.\n\n    Input: Natural language scheduling request (e.g., 'meeting with design team\n    next Tuesday at 2pm')\n    \"\"\"\n    result = calendar_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": request}]\n    })\n    return result[\"messages\"][-1].text\n\n\n@tool\ndef manage_email(request: str) -> str:\n    \"\"\"Send emails using natural language.\n\n    Use this when the user wants to send notifications, reminders, or any email\n    communication. Handles recipient extraction, subject generation, and email\n    composition.\n\n    Input: Natural language email request (e.g., 'send them a reminder about\n    the meeting')\n    \"\"\"\n    result = email_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": request}]\n    })\n    return result[\"messages\"][-1].text\n```\n:::\n\n:::js\n```typescript\nconst scheduleEvent = tool(\n  async ({ request }) => {\n    const result = await calendarAgent.invoke({\n      messages: [{ role: \"user\", content: request }]\n    });\n    const lastMessage = result.messages[result.messages.length - 1];\n    return lastMessage.text;\n  },\n  {\n    name: \"schedule_event\",\n    description: `\nSchedule calendar events using natural language.\n\nUse this when the user wants to create, modify, or check calendar appointments.\nHandles date/time parsing, availability checking, and event creation.\n\nInput: Natural language scheduling request (e.g., 'meeting with design team next Tuesday at 2pm')\n    `.trim(),\n    schema: z.object({\n      request: z.string().describe(\"Natural language scheduling request\"),\n    }),\n  }\n);\n\nconst manageEmail = tool(\n  async ({ request }) => {\n    const result = await emailAgent.invoke({\n      messages: [{ role: \"user\", content: request }]\n    });\n    const lastMessage = result.messages[result.messages.length - 1];\n    return lastMessage.text;\n  },\n  {\n    name: \"manage_email\",\n    description: `\nSend emails using natural language.\n\nUse this when the user wants to send notifications, reminders, or any email communication.\nHandles recipient extraction, subject generation, and email composition.\n\nInput: Natural language email request (e.g., 'send them a reminder about the meeting')\n    `.trim(),\n    schema: z.object({\n      request: z.string().describe(\"Natural language email request\"),", "metadata": {"source": "multi-agent/subagents-personal-assistant.mdx"}}
{"text": " ({ request }) => {\n    const result = await emailAgent.invoke({\n      messages: [{ role: \"user\", content: request }]\n    });\n    const lastMessage = result.messages[result.messages.length - 1];\n    return lastMessage.text;\n  },\n  {\n    name: \"manage_email\",\n    description: `\nSend emails using natural language.\n\nUse this when the user wants to send notifications, reminders, or any email communication.\nHandles recipient extraction, subject generation, and email composition.\n\nInput: Natural language email request (e.g., 'send them a reminder about the meeting')\n    `.trim(),\n    schema: z.object({\n      request: z.string().describe(\"Natural language email request\"),\n    }),\n  }\n);\n```\n:::\n\nThe tool descriptions help the supervisor decide when to use each tool, so make them clear and specific. We return only the sub-agent's final response, as the supervisor doesn't need to see intermediate reasoning or tool calls.\n\n## 4. Create the supervisor agent\n\nNow create the supervisor that orchestrates the sub-agents. The supervisor only sees high-level tools and makes routing decisions at the domain level, not the individual API level.\n\n:::python\n```python\nSUPERVISOR_PROMPT = (\n    \"You are a helpful personal assistant. \"\n    \"You can schedule calendar events and send emails. \"\n    \"Break down user requests into appropriate tool calls and coordinate the results. \"\n    \"When a request involves multiple actions, use multiple tools in sequence.\"\n)\n\nsupervisor_agent = create_agent(\n    model,\n    tools=[schedule_event, manage_email],\n    system_prompt=SUPERVISOR_PROMPT,\n)\n```\n:::\n\n:::js\n```typescript\nconst SUPERVISOR_PROMPT = `\nYou are a helpful personal assistant.\nYou can schedule calendar events and send emails.\nBreak down user requests into appropriate tool calls and coordinate the results.\nWhen a request involves multiple actions, use multiple tools in sequence.\n`.trim();\n\nconst supervisorAgent = createAgent({\n  model: llm,\n  tools: [scheduleEvent, manageEmail],\n  systemPrompt: SUPERVISOR_PROMPT,\n});\n```\n:::\n\n## 5. Use the supervisor\n\nNow test your complete system with complex requests that require coordination across multiple domains:\n\n### Example 1: Simple single-domain request\n\n:::python\n```python\nquery = \"Schedule a team standup for tomorrow at 9am\"\n\nfor step in supervisor_agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n):\n    for update in step.values():\n        for message in update.get(\"messages\", []):\n            message.pretty_print()\n```\n:::\n\n:::js\n```typescript\nconst query = \"Schedule a team standup for tomorrow at 9am\";\n\nconst stream = await supervisorAgent.stream({\n  messages: [{ role: \"user\", content: query }]\n});\n\nfor await (const step of stream) {\n  for (", "metadata": {"source": "multi-agent/subagents-personal-assistant.mdx"}}
{"text": " multiple domains:\n\n### Example 1: Simple single-domain request\n\n:::python\n```python\nquery = \"Schedule a team standup for tomorrow at 9am\"\n\nfor step in supervisor_agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n):\n    for update in step.values():\n        for message in update.get(\"messages\", []):\n            message.pretty_print()\n```\n:::\n\n:::js\n```typescript\nconst query = \"Schedule a team standup for tomorrow at 9am\";\n\nconst stream = await supervisorAgent.stream({\n  messages: [{ role: \"user\", content: query }]\n});\n\nfor await (const step of stream) {\n  for (const update of Object.values(step)) {\n    if (update && typeof update === \"object\" && \"messages\" in update) {\n      for (const message of update.messages) {\n        console.log(message.toFormattedString());\n      }\n    }\n  }\n}\n```\n:::\n```\n================================== Ai Message ==================================\nTool Calls:\n  schedule_event (call_mXFJJDU8bKZadNUZPaag8Lct)\n Call ID: call_mXFJJDU8bKZadNUZPaag8Lct\n  Args:\n    request: Schedule a team standup for tomorrow at 9am with Alice and Bob.\n================================= Tool Message =================================\nName: schedule_event\n\nThe team standup has been scheduled for tomorrow at 9:00 AM with Alice and Bob. If you need to make any changes or add more details, just let me know!\n================================== Ai Message ==================================\n\nThe team standup with Alice and Bob is scheduled for tomorrow at 9:00 AM. If you need any further arrangements or adjustments, please let me know!\n```\n\nThe supervisor identifies this as a calendar task, calls `schedule_event`, and the calendar agent handles date parsing and event creation.\n\n<Tip>\nFor full transparency into the information flow, including prompts and responses for each chat model call, check out the [LangSmith trace](https://smith.langchain.com/public/91a9a95f-fba9-4e84-aff0-371861ad2f4a/r) for the above run.\n</Tip>\n\n### Example 2: Complex multi-domain request\n\n:::python\n```python\nquery = (\n    \"Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, \"\n    \"and send them an email reminder about reviewing the new mockups.\"\n)\n\nfor step in supervisor_agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n):\n    for update in step.values():\n        for message in update.get(\"messages\", []):\n            message.pretty_print()\n```\n:::\n\n:::js\n```typescript\nconst query =\n  \"Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, \" +\n  \"and send them an email reminder about reviewing the new mockups.\"", "metadata": {"source": "multi-agent/subagents-personal-assistant.mdx"}}
{"text": "domain request\n\n:::python\n```python\nquery = (\n    \"Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, \"\n    \"and send them an email reminder about reviewing the new mockups.\"\n)\n\nfor step in supervisor_agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n):\n    for update in step.values():\n        for message in update.get(\"messages\", []):\n            message.pretty_print()\n```\n:::\n\n:::js\n```typescript\nconst query =\n  \"Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, \" +\n  \"and send them an email reminder about reviewing the new mockups.\";\n\nconst stream = await supervisorAgent.stream({\n  messages: [{ role: \"user\", content: query }]\n});\n\nfor await (const step of stream) {\n  for (const update of Object.values(step)) {\n    if (update && typeof update === \"object\" && \"messages\" in update) {\n      for (const message of update.messages) {\n        console.log(message.toFormattedString());\n      }\n    }\n  }\n}\n```\n:::\n```\n================================== Ai Message ==================================\nTool Calls:\n  schedule_event (call_YA68mqF0koZItCFPx0kGQfZi)\n Call ID: call_YA68mqF0koZItCFPx0kGQfZi\n  Args:\n    request: meeting with the design team next Tuesday at 2pm for 1 hour\n  manage_email (call_XxqcJBvVIuKuRK794ZIzlLxx)\n Call ID: call_XxqcJBvVIuKuRK794ZIzlLxx\n  Args:\n    request: send the design team an email reminder about reviewing the new mockups\n================================= Tool Message =================================\nName: schedule_event\n\nYour meeting with the design team is scheduled for next Tuesday, June 18th, from 2:00pm to 3:00pm. Let me know if you need to add more details or make any changes!\n================================= Tool Message =================================\nName: manage_email\n\nI've sent an email reminder to the design team requesting them to review the new mockups. If you need to include more information or recipients, just let me know!\n================================== Ai Message ==================================\n\nYour meeting with the design team is scheduled for next Tuesday, June 18th, from 2:00pm to 3:00pm.\n\nI've also sent an email reminder to the design team, asking them to review the new mockups.\n\nLet me know if you'd like to add more details to the meeting or include additional information in the email!\n```\n\nThe supervisor recognizes this requires both calendar and email actions, calls `schedule_event` for the meeting, then calls `manage_email` for the reminder. Each sub-agent completes its task, and the supervisor synthesizes both results into a coherent response.\n\n<Tip>\nRefer to the [LangSmith trace](https://smith.langchain.com/public/95cd00a3-d1f9-4dba-9731-7bf733fb6a3c/r", "metadata": {"source": "multi-agent/subagents-personal-assistant.mdx"}}
{"text": " let me know!\n================================== Ai Message ==================================\n\nYour meeting with the design team is scheduled for next Tuesday, June 18th, from 2:00pm to 3:00pm.\n\nI've also sent an email reminder to the design team, asking them to review the new mockups.\n\nLet me know if you'd like to add more details to the meeting or include additional information in the email!\n```\n\nThe supervisor recognizes this requires both calendar and email actions, calls `schedule_event` for the meeting, then calls `manage_email` for the reminder. Each sub-agent completes its task, and the supervisor synthesizes both results into a coherent response.\n\n<Tip>\nRefer to the [LangSmith trace](https://smith.langchain.com/public/95cd00a3-d1f9-4dba-9731-7bf733fb6a3c/r) to see the detailed information flow for the above run, including individual chat model prompts and responses.\n</Tip>\n\n### Complete working example\n\nHere's everything together in a runnable script:\n\n<Expandable title=\"View complete code\" defaultOpen={false}>\n\n:::python\n```python\n\"\"\"\nPersonal Assistant Supervisor Example\n\nThis example demonstrates the tool calling pattern for multi-agent systems.\nA supervisor agent coordinates specialized sub-agents (calendar and email)\nthat are wrapped as tools.\n\"\"\"\n\nfrom langchain.tools import tool\nfrom langchain.agents import create_agent\nfrom langchain.chat_models import init_chat_model\n\n# ============================================================================\n# Step 1: Define low-level API tools (stubbed)\n# ============================================================================\n\n@tool\ndef create_calendar_event(\n    title: str,\n    start_time: str,  # ISO format: \"2024-01-15T14:00:00\"\n    end_time: str,    # ISO format: \"2024-01-15T15:00:00\"\n    attendees: list[str],  # email addresses\n    location: str = \"\"\n) -> str:\n    \"\"\"Create a calendar event. Requires exact ISO datetime format.\"\"\"\n    return f\"Event created: {title} from {start_time} to {end_time} with {len(attendees)} attendees\"\n\n\n@tool\ndef send_email(\n    to: list[str],      # email addresses\n    subject: str,\n    body: str,\n    cc: list[str] = []\n) -> str:\n    \"\"\"Send an email via email API. Requires properly formatted addresses.\"\"\"\n    return f\"Email sent to {', '.join(to)} - Subject: {subject}\"\n\n\n@tool\ndef get_available_time_slots(\n    attendees: list[str],\n    date: str,  # ISO format: \"2024-01-15\"\n    duration_minutes: int\n) -> list[str]:\n    \"\"\"Check calendar availability for given attendees on a specific date.\"\"\"\n    return [\"09:00\", \"14:00\", \"16:00\"]\n\n\n# ============================================================================\n# Step 2: Create specialized sub-agents\n# ============================================================================\n\nmodel = init_chat_model(\"claude-haiku-4-5-20251001\")  # for example\n\ncalendar_agent = create_", "metadata": {"source": "multi-agent/subagents-personal-assistant.mdx"}}
{"text": "\n    \"\"\"Send an email via email API. Requires properly formatted addresses.\"\"\"\n    return f\"Email sent to {', '.join(to)} - Subject: {subject}\"\n\n\n@tool\ndef get_available_time_slots(\n    attendees: list[str],\n    date: str,  # ISO format: \"2024-01-15\"\n    duration_minutes: int\n) -> list[str]:\n    \"\"\"Check calendar availability for given attendees on a specific date.\"\"\"\n    return [\"09:00\", \"14:00\", \"16:00\"]\n\n\n# ============================================================================\n# Step 2: Create specialized sub-agents\n# ============================================================================\n\nmodel = init_chat_model(\"claude-haiku-4-5-20251001\")  # for example\n\ncalendar_agent = create_agent(\n    model,\n    tools=[create_calendar_event, get_available_time_slots],\n    system_prompt=(\n        \"You are a calendar scheduling assistant. \"\n        \"Parse natural language scheduling requests (e.g., 'next Tuesday at 2pm') \"\n        \"into proper ISO datetime formats. \"\n        \"Use get_available_time_slots to check availability when needed. \"\n        \"Use create_calendar_event to schedule events. \"\n        \"Always confirm what was scheduled in your final response.\"\n    )\n)\n\nemail_agent = create_agent(\n    model,\n    tools=[send_email],\n    system_prompt=(\n        \"You are an email assistant. \"\n        \"Compose professional emails based on natural language requests. \"\n        \"Extract recipient information and craft appropriate subject lines and body text. \"\n        \"Use send_email to send the message. \"\n        \"Always confirm what was sent in your final response.\"\n    )\n)\n\n# ============================================================================\n# Step 3: Wrap sub-agents as tools for the supervisor\n# ============================================================================\n\n@tool\ndef schedule_event(request: str) -> str:\n    \"\"\"Schedule calendar events using natural language.\n\n    Use this when the user wants to create, modify, or check calendar appointments.\n    Handles date/time parsing, availability checking, and event creation.\n\n    Input: Natural language scheduling request (e.g., 'meeting with design team\n    next Tuesday at 2pm')\n    \"\"\"\n    result = calendar_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": request}]\n    })\n    return result[\"messages\"][-1].text\n\n\n@tool\ndef manage_email(request: str) -> str:\n    \"\"\"Send emails using natural language.\n\n    Use this when the user wants to send notifications, reminders, or any email\n    communication. Handles recipient extraction, subject generation, and email\n    composition.\n\n    Input: Natural language email request (e.g., 'send them a reminder about\n    the meeting')\n    \"\"\"", "metadata": {"source": "multi-agent/subagents-personal-assistant.mdx"}}
{"text": ", and event creation.\n\n    Input: Natural language scheduling request (e.g., 'meeting with design team\n    next Tuesday at 2pm')\n    \"\"\"\n    result = calendar_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": request}]\n    })\n    return result[\"messages\"][-1].text\n\n\n@tool\ndef manage_email(request: str) -> str:\n    \"\"\"Send emails using natural language.\n\n    Use this when the user wants to send notifications, reminders, or any email\n    communication. Handles recipient extraction, subject generation, and email\n    composition.\n\n    Input: Natural language email request (e.g., 'send them a reminder about\n    the meeting')\n    \"\"\"\n    result = email_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": request}]\n    })\n    return result[\"messages\"][-1].text\n\n\n# ============================================================================\n# Step 4: Create the supervisor agent\n# ============================================================================\n\nsupervisor_agent = create_agent(\n    model,\n    tools=[schedule_event, manage_email],\n    system_prompt=(\n        \"You are a helpful personal assistant. \"\n        \"You can schedule calendar events and send emails. \"\n        \"Break down user requests into appropriate tool calls and coordinate the results. \"\n        \"When a request involves multiple actions, use multiple tools in sequence.\"\n    )\n)\n\n# ============================================================================\n# Step 5: Use the supervisor\n# ============================================================================\n\nif __name__ == \"__main__\":\n    # Example: User request requiring both calendar and email coordination\n    user_request = (\n        \"Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, \"\n        \"and send them an email reminder about reviewing the new mockups.\"\n    )\n\n    print(\"User Request:\", user_request)\n    print(\"\\n\" + \"=\"*80 + \"\\n\")\n\n    for step in supervisor_agent.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": user_request}]}\n    ):\n        for update in step.values():\n            for message in update.get(\"messages\", []):\n                message.pretty_print()\n```\n:::\n\n:::js\n```typescript\n/**\n * Personal Assistant Supervisor Example\n *\n * This example demonstrates the tool calling pattern for multi-agent systems.\n * A supervisor agent coordinates specialized sub-agents (calendar and email)\n * that are wrapped as tools.\n */\n\nimport { tool, createAgent } from \"langchain\";\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport { z } from \"zod\";\n\n// ============================================================================\n// Step 1: Define low-level API tools (stubbed)\n// ============================================================================\n\nconst createCalendarEvent = tool(\n", "metadata": {"source": "multi-agent/subagents-personal-assistant.mdx"}}
{"text": " for update in step.values():\n            for message in update.get(\"messages\", []):\n                message.pretty_print()\n```\n:::\n\n:::js\n```typescript\n/**\n * Personal Assistant Supervisor Example\n *\n * This example demonstrates the tool calling pattern for multi-agent systems.\n * A supervisor agent coordinates specialized sub-agents (calendar and email)\n * that are wrapped as tools.\n */\n\nimport { tool, createAgent } from \"langchain\";\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport { z } from \"zod\";\n\n// ============================================================================\n// Step 1: Define low-level API tools (stubbed)\n// ============================================================================\n\nconst createCalendarEvent = tool(\n  async ({ title, startTime, endTime, attendees, location }) => {\n    // Stub: In practice, this would call Google Calendar API, Outlook API, etc.\n    return `Event created: ${title} from ${startTime} to ${endTime} with ${attendees.length} attendees`;\n  },\n  {\n    name: \"create_calendar_event\",\n    description: \"Create a calendar event. Requires exact ISO datetime format.\",\n    schema: z.object({\n      title: z.string(),\n      startTime: z.string().describe(\"ISO format: '2024-01-15T14:00:00'\"),\n      endTime: z.string().describe(\"ISO format: '2024-01-15T15:00:00'\"),\n      attendees: z.array(z.string()).describe(\"email addresses\"),\n      location: z.string().optional().default(\"\"),\n    }),\n  }\n);\n\nconst sendEmail = tool(\n  async ({ to, subject, body, cc }) => {\n    // Stub: In practice, this would call SendGrid, Gmail API, etc.\n    return `Email sent to ${to.join(\", \")} - Subject: ${subject}`;\n  },\n  {\n    name: \"send_email\",\n    description:\n      \"Send an email via email API. Requires properly formatted addresses.\",\n    schema: z.object({\n      to: z.array(z.string()).describe(\"email addresses\"),\n      subject: z.string(),\n      body: z.string(),\n      cc: z.array(z.string()).optional().default([]),\n    }),\n  }\n);\n\nconst getAvailableTimeSlots = tool(\n  async ({ attendees, date, durationMinutes }) => {\n    // Stub: In practice, this would query calendar APIs\n    return [\"09:00\", \"14:00\", \"16:00\"];\n  },\n  {\n    name: \"get_available_time_slots\",\n    description:\n      \"Check calendar availability for given attendees on a specific date.\",\n    schema: z.object({\n      attendees: z.array(z.string()),\n      date: z.string().desc", "metadata": {"source": "multi-agent/subagents-personal-assistant.mdx"}}
{"text": "  subject: z.string(),\n      body: z.string(),\n      cc: z.array(z.string()).optional().default([]),\n    }),\n  }\n);\n\nconst getAvailableTimeSlots = tool(\n  async ({ attendees, date, durationMinutes }) => {\n    // Stub: In practice, this would query calendar APIs\n    return [\"09:00\", \"14:00\", \"16:00\"];\n  },\n  {\n    name: \"get_available_time_slots\",\n    description:\n      \"Check calendar availability for given attendees on a specific date.\",\n    schema: z.object({\n      attendees: z.array(z.string()),\n      date: z.string().describe(\"ISO format: '2024-01-15'\"),\n      durationMinutes: z.number(),\n    }),\n  }\n);\n\n// ============================================================================\n// Step 2: Create specialized sub-agents\n// ============================================================================\n\nconst llm = new ChatAnthropic({\n  model: \"claude-haiku-4-5-20251001\",\n});\n\nconst calendarAgent = createAgent({\n  model: llm,\n  tools: [createCalendarEvent, getAvailableTimeSlots],\n  systemPrompt: `\nYou are a calendar scheduling assistant.\nParse natural language scheduling requests (e.g., 'next Tuesday at 2pm')\ninto proper ISO datetime formats.\nUse get_available_time_slots to check availability when needed.\nUse create_calendar_event to schedule events.\nAlways confirm what was scheduled in your final response.\n  `.trim(),\n});\n\nconst emailAgent = createAgent({\n  model: llm,\n  tools: [sendEmail],\n  systemPrompt: `\nYou are an email assistant.\nCompose professional emails based on natural language requests.\nExtract recipient information and craft appropriate subject lines and body text.\nUse send_email to send the message.\nAlways confirm what was sent in your final response.\n  `.trim(),\n});\n\n// ============================================================================\n// Step 3: Wrap sub-agents as tools for the supervisor\n// ============================================================================\n\nconst scheduleEvent = tool(\n  async ({ request }) => {\n    const result = await calendarAgent.invoke({\n      messages: [{ role: \"user\", content: request }],\n    });\n    const lastMessage = result.messages[result.messages.length - 1];\n    return lastMessage.text;\n  },\n  {\n    name: \"schedule_event\",\n    description: `\nSchedule calendar events using natural language.\n\nUse this when the user wants to create, modify, or check calendar appointments.\nHandles date/time parsing, availability checking, and event creation.\n\nInput: Natural language scheduling request (e.g., 'meeting with design team next Tuesday at 2pm')\n    `.trim(),\n    schema: z.object({\n      request: z.string().describe(\"Natural language scheduling request\"),\n    }),\n  }\n);\n\nconst manageEmail = tool(\n  async ({ request }) => {\n    const result = await emailAgent.invoke({\n      messages: [", "metadata": {"source": "multi-agent/subagents-personal-assistant.mdx"}}
{"text": "[result.messages.length - 1];\n    return lastMessage.text;\n  },\n  {\n    name: \"schedule_event\",\n    description: `\nSchedule calendar events using natural language.\n\nUse this when the user wants to create, modify, or check calendar appointments.\nHandles date/time parsing, availability checking, and event creation.\n\nInput: Natural language scheduling request (e.g., 'meeting with design team next Tuesday at 2pm')\n    `.trim(),\n    schema: z.object({\n      request: z.string().describe(\"Natural language scheduling request\"),\n    }),\n  }\n);\n\nconst manageEmail = tool(\n  async ({ request }) => {\n    const result = await emailAgent.invoke({\n      messages: [{ role: \"user\", content: request }],\n    });\n    const lastMessage = result.messages[result.messages.length - 1];\n    return lastMessage.text;\n  },\n  {\n    name: \"manage_email\",\n    description: `\nSend emails using natural language.\n\nUse this when the user wants to send notifications, reminders, or any email communication.\nHandles recipient extraction, subject generation, and email composition.\n\nInput: Natural language email request (e.g., 'send them a reminder about the meeting')\n    `.trim(),\n    schema: z.object({\n      request: z.string().describe(\"Natural language email request\"),\n    }),\n  }\n);\n\n// ============================================================================\n// Step 4: Create the supervisor agent\n// ============================================================================\n\nconst supervisorAgent = createAgent({\n  model: llm,\n  tools: [scheduleEvent, manageEmail],\n  systemPrompt: `\nYou are a helpful personal assistant.\nYou can schedule calendar events and send emails.\nBreak down user requests into appropriate tool calls and coordinate the results.\nWhen a request involves multiple actions, use multiple tools in sequence.\n  `.trim(),\n});\n\n// ============================================================================\n// Step 5: Use the supervisor\n// ============================================================================\n\n// Example: User request requiring both calendar and email coordination\nconst userRequest =\n  \"Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, \" +\n  \"and send them an email reminder about reviewing the new mockups.\";\n\nconsole.log(\"User Request:\", userRequest);\nconsole.log(`\\n${\"=\".repeat(80)}\\n`);\n\nconst stream = await supervisorAgent.stream({\n  messages: [{ role: \"user\", content: userRequest }],\n});\n\nfor await (const step of stream) {\n  for (const update of Object.values(step)) {\n    if (update && typeof update === \"object\" && \"messages\" in update) {\n      for (const message of update.messages) {\n        console.log(message.toFormattedString());\n      }\n    }\n  }\n}\n```\n:::\n\n</Expandable>\n\n### Understanding the architecture\n\nYour system has three layers. The bottom layer contains rigid API tools that require exact formats. The middle layer contains sub-agents that accept natural language, translate it to structured API calls, and return natural language confirmations.", "metadata": {"source": "multi-agent/subagents-personal-assistant.mdx"}}
{"text": "n`);\n\nconst stream = await supervisorAgent.stream({\n  messages: [{ role: \"user\", content: userRequest }],\n});\n\nfor await (const step of stream) {\n  for (const update of Object.values(step)) {\n    if (update && typeof update === \"object\" && \"messages\" in update) {\n      for (const message of update.messages) {\n        console.log(message.toFormattedString());\n      }\n    }\n  }\n}\n```\n:::\n\n</Expandable>\n\n### Understanding the architecture\n\nYour system has three layers. The bottom layer contains rigid API tools that require exact formats. The middle layer contains sub-agents that accept natural language, translate it to structured API calls, and return natural language confirmations. The top layer contains the supervisor that routes to high-level capabilities and synthesizes results.\n\nThis separation of concerns provides several benefits: each layer has a focused responsibility, you can add new domains without affecting existing ones, and you can test and iterate on each layer independently.\n\n## 6. Add human-in-the-loop review\n\nIt can be prudent to incorporate [human-in-the-loop review](/oss/langchain/human-in-the-loop) of sensitive actions. LangChain includes [built-in middleware](/oss/langchain/human-in-the-loop#configuring-interrupts) to review tool calls, in this case the tools invoked by sub-agents.\n\nLet's add human-in-the-loop review to both sub-agents:\n- We configure the `create_calendar_event` and `send_email` tools to interrupt, permitting all [response types](/oss/langchain/human-in-the-loop) (`approve`, `edit`, `reject`)\n- We add a [checkpointer](/oss/langchain/short-term-memory) **only to the top-level agent**. This is required to pause and resume execution.\n\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware # [!code highlight]\nfrom langgraph.checkpoint.memory import InMemorySaver # [!code highlight]\n\n\ncalendar_agent = create_agent(\n    model,\n    tools=[create_calendar_event, get_available_time_slots],\n    system_prompt=CALENDAR_AGENT_PROMPT,\n    middleware=[ # [!code highlight]\n        HumanInTheLoopMiddleware( # [!code highlight]\n            interrupt_on={\"create_calendar_event\": True}, # [!code highlight]\n            description_prefix=\"Calendar event pending approval\", # [!code highlight]\n        ), # [!code highlight]\n    ], # [!code highlight]\n)\n\nemail_agent = create_agent(\n    model,\n    tools=[send_email],\n    system_prompt=EMAIL_AGENT_PROMPT,\n    middleware=[ # [!code highlight]\n        HumanInTheLoopMiddleware( # [!code highlight]\n        ", "metadata": {"source": "multi-agent/subagents-personal-assistant.mdx"}}
{"text": "code highlight]\n        HumanInTheLoopMiddleware( # [!code highlight]\n            interrupt_on={\"create_calendar_event\": True}, # [!code highlight]\n            description_prefix=\"Calendar event pending approval\", # [!code highlight]\n        ), # [!code highlight]\n    ], # [!code highlight]\n)\n\nemail_agent = create_agent(\n    model,\n    tools=[send_email],\n    system_prompt=EMAIL_AGENT_PROMPT,\n    middleware=[ # [!code highlight]\n        HumanInTheLoopMiddleware( # [!code highlight]\n            interrupt_on={\"send_email\": True}, # [!code highlight]\n            description_prefix=\"Outbound email pending approval\", # [!code highlight]\n        ), # [!code highlight]\n    ], # [!code highlight]\n)\n\nsupervisor_agent = create_agent(\n    model,\n    tools=[schedule_event, manage_email],\n    system_prompt=SUPERVISOR_PROMPT,\n    checkpointer=InMemorySaver(), # [!code highlight]\n)\n```\n:::\n\n:::js\n```typescript\nimport { createAgent, humanInTheLoopMiddleware } from \"langchain\"; // [!code highlight]\nimport { MemorySaver } from \"@langchain/langgraph\"; // [!code highlight]\n\nconst calendarAgent = createAgent({\n  model: llm,\n  tools: [createCalendarEvent, getAvailableTimeSlots],\n  systemPrompt: CALENDAR_AGENT_PROMPT,\n  middleware: [ // [!code highlight]\n    humanInTheLoopMiddleware({ // [!code highlight]\n      interruptOn: { create_calendar_event: true }, // [!code highlight]\n      descriptionPrefix: \"Calendar event pending approval\", // [!code highlight]\n    }), // [!code highlight]\n  ], // [!code highlight]\n});\n\nconst emailAgent = createAgent({\n  model: llm,\n  tools: [sendEmail],\n  systemPrompt: EMAIL_AGENT_PROMPT,\n  middleware: [ // [!code highlight]\n    humanInTheLoopMiddleware({ // [!code highlight]\n      interruptOn: { send_email: true }, // [!code highlight]\n      descriptionPrefix: \"Outbound email pending approval\", // [!code highlight]\n    }), // [!code highlight]\n  ], // [!code highlight]\n});\n\nconst supervisorAgent = createAgent({\n  model: llm,\n  tools: [scheduleEvent, manageEmail],\n  systemPrompt: SUPERVISOR_PROMPT,\n  checkpointer: new MemorySaver(), // [!code highlight]\n});\n```\n:::\n\nLet's repeat the query. Note that we gather interrupt events into a list to access downstream:\n\n:::python\n```python\nquery = (\n    \"", "metadata": {"source": "multi-agent/subagents-personal-assistant.mdx"}}
{"text": " middleware: [ // [!code highlight]\n    humanInTheLoopMiddleware({ // [!code highlight]\n      interruptOn: { send_email: true }, // [!code highlight]\n      descriptionPrefix: \"Outbound email pending approval\", // [!code highlight]\n    }), // [!code highlight]\n  ], // [!code highlight]\n});\n\nconst supervisorAgent = createAgent({\n  model: llm,\n  tools: [scheduleEvent, manageEmail],\n  systemPrompt: SUPERVISOR_PROMPT,\n  checkpointer: new MemorySaver(), // [!code highlight]\n});\n```\n:::\n\nLet's repeat the query. Note that we gather interrupt events into a list to access downstream:\n\n:::python\n```python\nquery = (\n    \"Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, \"\n    \"and send them an email reminder about reviewing the new mockups.\"\n)\n\nconfig = {\"configurable\": {\"thread_id\": \"6\"}}\n\ninterrupts = []\nfor step in supervisor_agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n    config,\n):\n    for update in step.values():\n        if isinstance(update, dict):\n            for message in update.get(\"messages\", []):\n                message.pretty_print()\n        else:\n            interrupt_ = update[0]\n            interrupts.append(interrupt_)\n            print(f\"\\nINTERRUPTED: {interrupt_.id}\")\n```\n:::\n\n:::js\n```typescript\nconst query =\n  \"Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, \" +\n  \"and send them an email reminder about reviewing the new mockups.\";\n\nconst config = { configurable: { thread_id: \"6\" } };\n\nconst interrupts: any[] = [];\nconst stream = await supervisorAgent.stream(\n  { messages: [{ role: \"user\", content: query }] },\n  config\n);\n\nfor await (const step of streamA) {\n  for (const update of Object.values(step)) {\n    for (const message of update.messages) {\n      console.log(message.toFormattedString());\n    }\n    const interrupt = update.__interrupt__?.[0];\n    interrupts.push(interrupt);\n    console.log(`\\nINTERRUPTED: ${interrupt?.id}`);\n  }\n}\n```\n:::\n```\n================================== Ai Message ==================================\nTool Calls:\n  schedule_event (call_t4Wyn32ohaShpEZKuzZbl83z)\n Call ID: call_t4Wyn32ohaShpEZKuzZbl83z\n  Args:\n    request: Schedule a meeting with the design team next Tuesday at 2pm for 1 hour.\n  manage_email (call_JW", "metadata": {"source": "multi-agent/subagents-personal-assistant.mdx"}}
{"text": " {\n    for (const message of update.messages) {\n      console.log(message.toFormattedString());\n    }\n    const interrupt = update.__interrupt__?.[0];\n    interrupts.push(interrupt);\n    console.log(`\\nINTERRUPTED: ${interrupt?.id}`);\n  }\n}\n```\n:::\n```\n================================== Ai Message ==================================\nTool Calls:\n  schedule_event (call_t4Wyn32ohaShpEZKuzZbl83z)\n Call ID: call_t4Wyn32ohaShpEZKuzZbl83z\n  Args:\n    request: Schedule a meeting with the design team next Tuesday at 2pm for 1 hour.\n  manage_email (call_JWj4vDJ5VMnvkySymhCBm4IR)\n Call ID: call_JWj4vDJ5VMnvkySymhCBm4IR\n  Args:\n    request: Send an email reminder to the design team about reviewing the new mockups before our meeting next Tuesday at 2pm.\n\nINTERRUPTED: 4f994c9721682a292af303ec1a46abb7\n\nINTERRUPTED: 2b56f299be313ad8bc689eff02973f16\n```\nThis time we've interrupted execution. Let's inspect the interrupt events:\n\n:::python\n```python\nfor interrupt_ in interrupts:\n    for request in interrupt_.value[\"action_requests\"]:\n        print(f\"INTERRUPTED: {interrupt_.id}\")\n        print(f\"{request['description']}\\n\")\n```\n:::\n\n:::js\n```typescript\nfor (const interrupt of interrupts) {\n  for (const request of interrupt.value.actionRequests) {\n    console.log(`INTERRUPTED: ${interrupt.id}`);\n    console.log(`${request.description}\\n`);\n  }\n}\n```\n:::\n```\nINTERRUPTED: 4f994c9721682a292af303ec1a46abb7\nCalendar event pending approval\n\nTool: create_calendar_event\nArgs: {'title': 'Meeting with the Design Team', 'start_time': '2024-06-18T14:00:00', 'end_time': '2024-06-18T15:00:00', 'attendees': ['design team']}\n\nINTERRUPTED: 2b56f299be313ad8bc689eff02973f16\nOutbound email pending approval\n\nTool: send_email\nArgs: {'to': ['designteam@example.com'], 'subject': 'Reminder: Review New Mockups Before Meeting Next Tuesday at 2pm', 'body': \"Hello Team,\\n\\nThis is a reminder to review the new mockups ahead of our meeting scheduled for next Tuesday at 2pm. Your feedback and insights will be valuable for our discussion and next steps.\\n\\nPlease ensure you've gone through the designs and are ready to share your thoughts during the meeting.\\n\\nThank you!\\n\\nBest regards,\\n[Your Name]\"}\n```\n\nWe can specify decisions for each interrupt by referring to its ID using a @[`Command`", "metadata": {"source": "multi-agent/subagents-personal-assistant.mdx"}}
{"text": "00', 'attendees': ['design team']}\n\nINTERRUPTED: 2b56f299be313ad8bc689eff02973f16\nOutbound email pending approval\n\nTool: send_email\nArgs: {'to': ['designteam@example.com'], 'subject': 'Reminder: Review New Mockups Before Meeting Next Tuesday at 2pm', 'body': \"Hello Team,\\n\\nThis is a reminder to review the new mockups ahead of our meeting scheduled for next Tuesday at 2pm. Your feedback and insights will be valuable for our discussion and next steps.\\n\\nPlease ensure you've gone through the designs and are ready to share your thoughts during the meeting.\\n\\nThank you!\\n\\nBest regards,\\n[Your Name]\"}\n```\n\nWe can specify decisions for each interrupt by referring to its ID using a @[`Command`]. Refer to the [human-in-the-loop guide](/oss/langchain/human-in-the-loop) for additional details. For demonstration purposes, here we will accept the calendar event, but edit the subject of the outbound email:\n\n:::python\n```python\nfrom langgraph.types import Command # [!code highlight]\n\nresume = {}\nfor interrupt_ in interrupts:\n    if interrupt_.id == \"2b56f299be313ad8bc689eff02973f16\":\n        # Edit email\n        edited_action = interrupt_.value[\"action_requests\"][0].copy()\n        edited_action[\"args\"][\"subject\"] = \"Mockups reminder\"\n        resume[interrupt_.id] = {\n            \"decisions\": [{\"type\": \"edit\", \"edited_action\": edited_action}]\n        }\n    else:\n        resume[interrupt_.id] = {\"decisions\": [{\"type\": \"approve\"}]}\n\ninterrupts = []\nfor step in supervisor_agent.stream(\n    Command(resume=resume), # [!code highlight]\n    config,\n):\n    for update in step.values():\n        if isinstance(update, dict):\n            for message in update.get(\"messages\", []):\n                message.pretty_print()\n        else:\n            interrupt_ = update[0]\n            interrupts.append(interrupt_)\n            print(f\"\\nINTERRUPTED: {interrupt_.id}\")\n```\n:::\n\n:::js\n```typescript\nimport { Command } from \"@langchain/langgraph\"; // [!code highlight]\n\nconst resume: Record<string, any> = {};\nfor (const interrupt of interrupts) {\n  const actionRequest = interrupt.value.actionRequests[0];\n  if (actionRequest.name === \"send_email\") {\n    // Edit email\n    const editedAction = { ...actionRequest };\n    editedAction.args.subject = \"Mockups reminder\";\n    resume", "metadata": {"source": "multi-agent/subagents-personal-assistant.mdx"}}
{"text": "            interrupt_ = update[0]\n            interrupts.append(interrupt_)\n            print(f\"\\nINTERRUPTED: {interrupt_.id}\")\n```\n:::\n\n:::js\n```typescript\nimport { Command } from \"@langchain/langgraph\"; // [!code highlight]\n\nconst resume: Record<string, any> = {};\nfor (const interrupt of interrupts) {\n  const actionRequest = interrupt.value.actionRequests[0];\n  if (actionRequest.name === \"send_email\") {\n    // Edit email\n    const editedAction = { ...actionRequest };\n    editedAction.args.subject = \"Mockups reminder\";\n    resume[interrupt.id] = {\n      decisions: [{ type: \"edit\", editedAction }]\n    };\n  } else {\n    resume[interrupt.id] = { decisions: [{ type: \"approve\" }] };\n  }\n}\n\nconst resumeStream = await supervisorAgent.stream(\n  new Command({ resume }), // [!code highlight]\n  config\n);\n\nfor await (const step of resumeStream) {\n  for (const update of Object.values(step)) {\n    if (update && typeof update === \"object\" && \"messages\" in update) {\n      for (const message of update.messages) {\n        console.log(message.toFormattedString());\n      }\n    }\n  }\n}\n```\n:::\n```\n================================= Tool Message =================================\nName: schedule_event\n\nYour meeting with the design team has been scheduled for next Tuesday, June 18th, from 2:00 pm to 3:00 pm.\n================================= Tool Message =================================\nName: manage_email\n\nYour email reminder to the design team has been sent. Here\u2019s what was sent:\n\n- Recipient: designteam@example.com\n- Subject: Mockups reminder\n- Body: A reminder to review the new mockups before the meeting next Tuesday at 2pm, with a request for feedback and readiness for discussion.\n\nLet me know if you need any further assistance!\n================================== Ai Message ==================================\n\n- Your meeting with the design team has been scheduled for next Tuesday, June 18th, from 2:00 pm to 3:00 pm.\n- An email reminder has been sent to the design team about reviewing the new mockups before the meeting.\n\nLet me know if you need any further assistance!\n```\nThe run proceeds with our input.\n\n## 7. Advanced: Control information flow\n\nBy default, sub-agents receive only the request string from the supervisor. You might want to pass additional context, such as conversation history or user preferences.\n\n### Pass additional conversational context to sub-agents\n\n:::python\n```python\nfrom langchain.tools import tool, ToolRuntime\n\n@tool\ndef schedule_event(\n    request: str,\n    runtime: ToolRuntime\n) -> str:\n    \"\"\"Schedule calendar events using natural language.\"\"\"\n    # Customize context received by sub-agent\n    original_user_message = next(\n        message for message in runtime.state[\"messages\"]\n  ", "metadata": {"source": "multi-agent/subagents-personal-assistant.mdx"}}
{"text": " about reviewing the new mockups before the meeting.\n\nLet me know if you need any further assistance!\n```\nThe run proceeds with our input.\n\n## 7. Advanced: Control information flow\n\nBy default, sub-agents receive only the request string from the supervisor. You might want to pass additional context, such as conversation history or user preferences.\n\n### Pass additional conversational context to sub-agents\n\n:::python\n```python\nfrom langchain.tools import tool, ToolRuntime\n\n@tool\ndef schedule_event(\n    request: str,\n    runtime: ToolRuntime\n) -> str:\n    \"\"\"Schedule calendar events using natural language.\"\"\"\n    # Customize context received by sub-agent\n    original_user_message = next(\n        message for message in runtime.state[\"messages\"]\n        if message.type == \"human\"\n    )\n    prompt = (\n        \"You are assisting with the following user inquiry:\\n\\n\"\n        f\"{original_user_message.text}\\n\\n\"\n        \"You are tasked with the following sub-request:\\n\\n\"\n        f\"{request}\"\n    )\n    result = calendar_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n    })\n    return result[\"messages\"][-1].text\n```\n:::\n\n:::js\n```typescript\nimport { getCurrentTaskInput } from \"@langchain/langgraph\";\nimport { type BuiltInState, HumanMessage } from \"langchain\";\n\nconst scheduleEvent = tool(\n  async ({ request }, config) => {\n    // Customize context received by sub-agent\n    // Access full thread messages from the config\n    const currentMessages = getCurrentTaskInput<BuiltInState>(config).messages;\n    const originalUserMessage = currentMessages.find(HumanMessage.isInstance);\n    const prompt = `\nYou are assisting with the following user inquiry:\n\n${originalUserMessage?.content || \"No context available\"}\n\nYou are tasked with the following sub-request:\n\n${request}\n    `.trim();\n\n    const result = await calendarAgent.invoke({\n      messages: [{ role: \"user\", content: prompt }],\n    });\n    const lastMessage = result.messages[result.messages.length - 1];\n    return lastMessage.text;\n  },\n  {\n    name: \"schedule_event\",\n    description: \"Schedule calendar events using natural language.\",\n    schema: z.object({\n      request: z.string().describe(\"Natural language scheduling request\"),\n    }),\n  }\n);\n```\n:::\n\nThis allows sub-agents to see the full conversation context, which can be useful for resolving ambiguities like \"schedule it for the same time tomorrow\" (referencing a previous conversation).\n\n<Tip>\nYou can see the full context received by the sub agent in the [chat model call](https://smith.langchain.com/public/c7d54882-afb8-", "metadata": {"source": "multi-agent/subagents-personal-assistant.mdx"}}
{"text": " });\n    const lastMessage = result.messages[result.messages.length - 1];\n    return lastMessage.text;\n  },\n  {\n    name: \"schedule_event\",\n    description: \"Schedule calendar events using natural language.\",\n    schema: z.object({\n      request: z.string().describe(\"Natural language scheduling request\"),\n    }),\n  }\n);\n```\n:::\n\nThis allows sub-agents to see the full conversation context, which can be useful for resolving ambiguities like \"schedule it for the same time tomorrow\" (referencing a previous conversation).\n\n<Tip>\nYou can see the full context received by the sub agent in the [chat model call](https://smith.langchain.com/public/c7d54882-afb8-4039-9c5a-4112d0f458b0/r/6803571e-af78-4c68-904a-ecf55771084d) of the LangSmith trace.\n</Tip>\n\n### Control what supervisor receives\n\nYou can also customize what information flows back to the supervisor:\n\n:::python\n```python\nimport json\n\n@tool\ndef schedule_event(request: str) -> str:\n    \"\"\"Schedule calendar events using natural language.\"\"\"\n    result = calendar_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": request}]\n    })\n\n    # Option 1: Return just the confirmation message\n    return result[\"messages\"][-1].text\n\n    # Option 2: Return structured data\n    # return json.dumps({\n    #     \"status\": \"success\",\n    #     \"event_id\": \"evt_123\",\n    #     \"summary\": result[\"messages\"][-1].text\n    # })\n```\n:::\n\n:::js\n```typescript\nconst scheduleEvent = tool(\n  async ({ request }) => {\n    const result = await calendarAgent.invoke({\n      messages: [{ role: \"user\", content: request }]\n    });\n\n    const lastMessage = result.messages[result.messages.length - 1];\n\n    // Option 1: Return just the confirmation message\n    return lastMessage.text;\n\n    // Option 2: Return structured data\n    // return JSON.stringify({\n    //   status: \"success\",\n    //   event_id: \"evt_123\",\n    //   summary: lastMessage.text\n    // });\n  },\n  {\n    name: \"schedule_event\",\n    description: \"Schedule calendar events using natural language.\",\n    schema: z.object({\n      request: z.string().describe(\"Natural language scheduling request\"),\n    }),\n  }\n);\n```\n:::\n\n**Important:** Make sure sub-agent prompts emphasize that their final message should contain all relevant information. A common failure mode is sub-agents that perform tool calls but don't include the results in their final response.\n\n:::js\n<Tip>\nFor a complete working example that demonstrates", "metadata": {"source": "multi-agent/subagents-personal-assistant.mdx"}}
{"text": "   // return JSON.stringify({\n    //   status: \"success\",\n    //   event_id: \"evt_123\",\n    //   summary: lastMessage.text\n    // });\n  },\n  {\n    name: \"schedule_event\",\n    description: \"Schedule calendar events using natural language.\",\n    schema: z.object({\n      request: z.string().describe(\"Natural language scheduling request\"),\n    }),\n  }\n);\n```\n:::\n\n**Important:** Make sure sub-agent prompts emphasize that their final message should contain all relevant information. A common failure mode is sub-agents that perform tool calls but don't include the results in their final response.\n\n:::js\n<Tip>\nFor a complete working example that demonstrates the full supervisor pattern with human-in-the-loop review and advanced information flow control, check out [`supervisor_complete.ts`](https://github.com/langchain-ai/langchainjs/blob/main/examples/src/createAgent/supervisor.ts) in the LangChain.js examples.\n</Tip>\n:::\n\n## 8. Key takeaways\n\nThe supervisor pattern creates layers of abstraction where each layer has a clear responsibility. When designing a supervisor system, start with clear domain boundaries and give each sub-agent focused tools and prompts. Write clear tool descriptions for the supervisor, test each layer independently before integration, and control information flow based on your specific needs.\n\n<Tip>\n**When to use the supervisor pattern**\n\nUse the supervisor pattern when you have multiple distinct domains (calendar, email, CRM, database), each domain has multiple tools or complex logic, you want centralized workflow control, and sub-agents don't need to converse directly with users.\n\nFor simpler cases with just a few tools, use a single agent. When agents need to have conversations with users, use [handoffs](/oss/langchain/multi-agent/handoffs) instead. For peer-to-peer collaboration between agents, consider other multi-agent patterns.\n</Tip>\n\n## Next steps\n\nLearn about [handoffs](/oss/langchain/multi-agent/handoffs) for agent-to-agent conversations, explore [context engineering](/oss/langchain/context-engineering) to fine-tune information flow, read the [multi-agent overview](/oss/langchain/multi-agent) to compare different patterns, and use [LangSmith](https://smith.langchain.com) to debug and monitor your multi-agent system.\n", "metadata": {"source": "multi-agent/subagents-personal-assistant.mdx"}}
{"text": "---\ntitle: Handoffs\n---\n\nIn the **handoffs** architecture, behavior changes dynamically based on state. The core mechanism: [tools](/oss/langchain/tools) update a state variable (e.g., `current_step` or `active_agent`) that persists across turns, and the system reads this variable to adjust behavior\u2014either applying different configuration (system prompt, tools) or routing to a different [agent](/oss/langchain/agents). This pattern supports both handoffs between distinct agents and dynamic configuration changes within a single agent.\n\n<Tip>\nThe term **handoffs** was coined by [OpenAI](https://openai.github.io/openai-agents-python/handoffs/) for using tool calls (e.g., `transfer_to_sales_agent`) to transfer control between agents or states.\n</Tip>\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Agent\n    participant Workflow State\n\n    User->>Agent: \"My phone is broken\"\n    Note over Agent,Workflow State: Step: Get warranty status<br/>Tools: record_warranty_status\n    Agent-->>User: \"Is your device under warranty?\"\n\n    User->>Agent: \"Yes, it's still under warranty\"\n    Agent->>Workflow State: record_warranty_status(\"in_warranty\")\n    Note over Agent,Workflow State: Step: Classify issue<br/>Tools: record_issue_type\n    Agent-->>User: \"Can you describe the issue?\"\n\n    User->>Agent: \"The screen is cracked\"\n    Agent->>Workflow State: record_issue_type(\"hardware\")\n    Note over Agent,Workflow State: Step: Provide resolution<br/>Tools: provide_solution, escalate_to_human\n    Agent-->>User: \"Here's the warranty repair process...\"\n```\n\n## Key characteristics\n\n* State-driven behavior: Behavior changes based on a state variable (e.g., `current_step` or `active_agent`)\n* Tool-based transitions: Tools update the state variable to move between states\n* Direct user interaction: Each state's configuration handles user messages directly\n* Persistent state: State survives across conversation turns\n\n## When to use\n\nUse the handoffs pattern when you need to enforce sequential constraints (unlock capabilities only after preconditions are met), the agent needs to converse directly with the user across different states, or you're building multi-stage conversational flows. This pattern is particularly valuable for customer support scenarios where you need to collect information in a specific sequence \u2014 for example, collecting a warranty ID before processing a refund.\n\n## Basic implementation\n\nThe core mechanism is a [tool](/oss/langchain/tools) that returns a [`Command`](/oss/langgraph/graph-api#command) to update state, triggering a transition to a new step or agent:\n\n:::python\n```python\nfrom langchain.tools import tool\nfrom langchain.messages import ToolMessage\nfrom langgraph.types import Command\n\n@tool\ndef transfer_to_specialist(runtime) -> Command:\n    \"\"\"Transfer to the specialist agent.\"\"\"\n    return Command(\n        update={\n            \"messages\": [\n                ToolMessage(  # [!code highlight", "metadata": {"source": "multi-agent/handoffs.mdx"}}
{"text": " collect information in a specific sequence \u2014 for example, collecting a warranty ID before processing a refund.\n\n## Basic implementation\n\nThe core mechanism is a [tool](/oss/langchain/tools) that returns a [`Command`](/oss/langgraph/graph-api#command) to update state, triggering a transition to a new step or agent:\n\n:::python\n```python\nfrom langchain.tools import tool\nfrom langchain.messages import ToolMessage\nfrom langgraph.types import Command\n\n@tool\ndef transfer_to_specialist(runtime) -> Command:\n    \"\"\"Transfer to the specialist agent.\"\"\"\n    return Command(\n        update={\n            \"messages\": [\n                ToolMessage(  # [!code highlight]\n                    content=\"Transferred to specialist\",\n                    tool_call_id=runtime.tool_call_id  # [!code highlight]\n                )\n            ],\n            \"current_step\": \"specialist\"  # Triggers behavior change\n        }\n    )\n```\n:::\n:::js\n```typescript\nimport { tool, ToolMessage, type ToolRuntime } from \"langchain\";\nimport { Command } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst transferToSpecialist = tool(\n  async (_, config: ToolRuntime<typeof StateSchema>) => {\n    return new Command({\n      update: {\n        messages: [\n          new ToolMessage({  // [!code highlight]\n            content: \"Transferred to specialist\",\n            tool_call_id: config.toolCallId  // [!code highlight]\n          })\n        ],\n        currentStep: \"specialist\"  // Triggers behavior change\n      }\n    });\n  },\n  {\n    name: \"transfer_to_specialist\",\n    description: \"Transfer to the specialist agent.\",\n    schema: z.object({})\n  }\n);\n```\n:::\n\n<Note>\n**Why include a `ToolMessage`?** When an LLM calls a tool, it expects a response. The `ToolMessage` with matching `tool_call_id` completes this request-response cycle\u2014without it, the conversation history becomes malformed. This is required whenever your handoff tool updates messages.\n</Note>\n\nFor a complete implementation, see the tutorial below.\n\n<Card\n    title=\"Tutorial: Build customer support with handoffs\"\n    icon=\"people-arrows\"\n    href=\"/oss/langchain/multi-agent/handoffs-customer-support\"\n    arrow cta=\"Learn more\"\n>\n    Learn how to build a customer support agent using the", "metadata": {"source": "multi-agent/handoffs.mdx"}}
{"text": "  description: \"Transfer to the specialist agent.\",\n    schema: z.object({})\n  }\n);\n```\n:::\n\n<Note>\n**Why include a `ToolMessage`?** When an LLM calls a tool, it expects a response. The `ToolMessage` with matching `tool_call_id` completes this request-response cycle\u2014without it, the conversation history becomes malformed. This is required whenever your handoff tool updates messages.\n</Note>\n\nFor a complete implementation, see the tutorial below.\n\n<Card\n    title=\"Tutorial: Build customer support with handoffs\"\n    icon=\"people-arrows\"\n    href=\"/oss/langchain/multi-agent/handoffs-customer-support\"\n    arrow cta=\"Learn more\"\n>\n    Learn how to build a customer support agent using the handoffs pattern, where a single agent transitions between different configurations.\n</Card>\n\n## Implementation approaches\n\nThere are two ways to implement handoffs: **[single agent with middleware](#single-agent-with-middleware)** (one agent with dynamic configuration) or **[multiple agent subgraphs](#multiple-agent-subgraphs)** (distinct agents as graph nodes).\n\n### Single agent with middleware\n\nA single agent changes its behavior based on state. Middleware intercepts each model call and dynamically adjusts the system prompt and available tools. Tools update the state variable to trigger transitions:\n\n:::python\n```python\nfrom langchain.tools import ToolRuntime, tool\nfrom langchain.messages import ToolMessage\nfrom langgraph.types import Command\n\n@tool\ndef record_warranty_status(\n    status: str,\n    runtime: ToolRuntime[None, SupportState]\n) -> Command:\n    \"\"\"Record warranty status and transition to next step.\"\"\"\n    return Command(\n        update={\n            \"messages\": [\n                ToolMessage(\n                    content=f\"Warranty status recorded: {status}\",\n                    tool_call_id=runtime.tool_call_id\n                )\n            ],\n            \"warranty_status\": status,\n            \"current_step\": \"specialist\"  # Update state to trigger transition\n        }\n    )\n```\n:::\n:::js\n```typescript\nimport { tool, ToolMessage, type ToolRuntime } from \"langchain\";\nimport { Command } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst recordWarrantyStatus = tool(\n  async ({ status }, config: ToolRuntime<typeof StateSchema>) => {\n    return new Command({\n      update: {\n        messages: [\n          new ToolMessage({\n            content: `Warranty status recorded: ${status}`,\n       ", "metadata": {"source": "multi-agent/handoffs.mdx"}}
{"text": "          \"current_step\": \"specialist\"  # Update state to trigger transition\n        }\n    )\n```\n:::\n:::js\n```typescript\nimport { tool, ToolMessage, type ToolRuntime } from \"langchain\";\nimport { Command } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst recordWarrantyStatus = tool(\n  async ({ status }, config: ToolRuntime<typeof StateSchema>) => {\n    return new Command({\n      update: {\n        messages: [\n          new ToolMessage({\n            content: `Warranty status recorded: ${status}`,\n            tool_call_id: config.toolCallId,\n          }),\n        ],\n        warrantyStatus: status,\n        currentStep: \"specialist\", // Update state to trigger transition\n      },\n    });\n  },\n  {\n    name: \"record_warranty_status\",\n    description: \"Record warranty status and transition to next step.\",\n    schema: z.object({\n      status: z.string(),\n    }),\n  }\n);\n```\n:::\n\n<Accordion title=\"Complete example: Customer support with middleware\">\n\n:::python\n```python\nfrom langchain.agents import AgentState, create_agent\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\nfrom langchain.tools import tool, ToolRuntime\nfrom langchain.messages import ToolMessage\nfrom langgraph.types import Command\nfrom typing import Callable\n\n# 1. Define state with current_step tracker\nclass SupportState(AgentState):  # [!code highlight]\n    \"\"\"Track which step is currently active.\"\"\"\n    current_step: str = \"triage\"  # [!code highlight]\n    warranty_status: str | None = None\n\n# 2. Tools update current_step via Command\n@tool\ndef record_warranty_status(\n    status: str,\n    runtime: ToolRuntime[None, SupportState]\n) -> Command:  # [!code highlight]\n    \"\"\"Record warranty status and transition to next step.\"\"\"\n    return Command(update={  # [!code highlight]\n        \"messages\": [  # [!code highlight]\n            ToolMessage(\n                content=f\"Warranty status recorded: {status}\",\n                tool_call_id=runtime.tool_call_id\n            )\n        ],\n        \"warranty_status\": status,\n        # Transition to next step\n        \"current_step\": \"specialist\"    # [!code highlight]\n    })\n\n# 3", "metadata": {"source": "multi-agent/handoffs.mdx"}}
{"text": " step.\"\"\"\n    return Command(update={  # [!code highlight]\n        \"messages\": [  # [!code highlight]\n            ToolMessage(\n                content=f\"Warranty status recorded: {status}\",\n                tool_call_id=runtime.tool_call_id\n            )\n        ],\n        \"warranty_status\": status,\n        # Transition to next step\n        \"current_step\": \"specialist\"    # [!code highlight]\n    })\n\n# 3. Middleware applies dynamic configuration based on current_step\n@wrap_model_call  # [!code highlight]\ndef apply_step_config(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse]\n) -> ModelResponse:\n    \"\"\"Configure agent behavior based on current_step.\"\"\"\n    step = request.state.get(\"current_step\", \"triage\")  # [!code highlight]\n\n    # Map steps to their configurations\n    configs = {\n        \"triage\": {\n            \"prompt\": \"Collect warranty information...\",\n            \"tools\": [record_warranty_status]\n        },\n        \"specialist\": {\n            \"prompt\": \"Provide solutions based on warranty: {warranty_status}\",\n            \"tools\": [provide_solution, escalate]\n        }\n    }\n\n    config = configs[step]\n    request = request.override(  # [!code highlight]\n        system_prompt=config[\"prompt\"].format(**request.state),  # [!code highlight]\n        tools=config[\"tools\"]  # [!code highlight]\n    )\n    return handler(request)\n\n# 4. Create agent with middleware\nagent = create_agent(\n    model,\n    tools=[record_warranty_status, provide_solution, escalate],\n    state_schema=SupportState,\n    middleware=[apply_step_config],  # [!code highlight]\n    checkpointer=InMemorySaver()  # Persist state across turns  # [!code highlight]\n)\n```\n:::\n:::js\n```typescript\nimport {\n  createAgent,\n  createMiddleware,\n  tool,\n  ToolMessage,\n  type ToolRuntime,\n} from \"langchain\";\nimport { Command, MemorySaver, StateSchema } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\n// 1. Define state with current_step tracker\nconst SupportState = new StateSchema({ // [!code highlight]\n  currentStep: z.string().default(\"t", "metadata": {"source": "multi-agent/handoffs.mdx"}}
{"text": "=[record_warranty_status, provide_solution, escalate],\n    state_schema=SupportState,\n    middleware=[apply_step_config],  # [!code highlight]\n    checkpointer=InMemorySaver()  # Persist state across turns  # [!code highlight]\n)\n```\n:::\n:::js\n```typescript\nimport {\n  createAgent,\n  createMiddleware,\n  tool,\n  ToolMessage,\n  type ToolRuntime,\n} from \"langchain\";\nimport { Command, MemorySaver, StateSchema } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\n// 1. Define state with current_step tracker\nconst SupportState = new StateSchema({ // [!code highlight]\n  currentStep: z.string().default(\"triage\"), // [!code highlight]\n  warrantyStatus: z.string().optional(),\n});\n\n// 2. Tools update currentStep via Command\nconst recordWarrantyStatus = tool(\n  async ({ status }, config: ToolRuntime<typeof SupportState.State>) => {\n    return new Command({ // [!code highlight]\n      update: { // [!code highlight]\n        messages: [ // [!code highlight]\n          new ToolMessage({\n            content: `Warranty status recorded: ${status}`,\n            tool_call_id: config.toolCallId,\n          }),\n        ],\n        warrantyStatus: status,\n        // Transition to next step\n        currentStep: \"specialist\", // [!code highlight]\n      },\n    });\n  },\n  {\n    name: \"record_warranty_status\",\n    description: \"Record warranty status and transition\",\n    schema: z.object({ status: z.string() }),\n  }\n);\n\n// 3. Middleware applies dynamic configuration based on currentStep\nconst applyStepConfig = createMiddleware({\n  name: \"applyStepConfig\",\n  stateSchema: SupportState, // [!code highlight]\n  wrapModelCall: async (request, handler) => {\n    const step = request.state.currentStep || \"triage\"; // [!code highlight]\n\n    // Map steps to their configurations\n    const configs = {\n      triage: {\n        prompt: \"Collect warranty information...\",\n        tools: [recordWarrantyStatus],\n      },\n      specialist: {\n        prompt: `Provide solutions based on warranty: ${request.state.warrantyStatus}`,\n        tools: [provideSolution, escalate],\n      },\n    };\n\n    const config = configs[step as keyof typeof configs];\n    return handler({\n      ...request,\n      systemPrompt: config.prompt,\n      tools: config.tools,\n    });\n  },", "metadata": {"source": "multi-agent/handoffs.mdx"}}
{"text": " steps to their configurations\n    const configs = {\n      triage: {\n        prompt: \"Collect warranty information...\",\n        tools: [recordWarrantyStatus],\n      },\n      specialist: {\n        prompt: `Provide solutions based on warranty: ${request.state.warrantyStatus}`,\n        tools: [provideSolution, escalate],\n      },\n    };\n\n    const config = configs[step as keyof typeof configs];\n    return handler({\n      ...request,\n      systemPrompt: config.prompt,\n      tools: config.tools,\n    });\n  },\n});\n\n// 4. Create agent with middleware\nconst agent = createAgent({\n  model,\n  tools: [recordWarrantyStatus, provideSolution, escalate],\n  middleware: [applyStepConfig], // [!code highlight]\n  checkpointer: new MemorySaver(), // Persist state across turns  // [!code highlight]\n});\n```\n:::\n\n</Accordion>\n\n### Multiple agent subgraphs\n\nMultiple distinct agents exist as separate nodes in a graph. Handoff tools navigate between agent nodes using `Command.PARENT` to specify which node to execute next.\n\n<Warning>\nSubgraph handoffs require careful **[context engineering](/oss/langchain/context-engineering)**. Unlike single-agent middleware (where message history flows naturally), you must explicitly decide what messages pass between agents. Get this wrong and agents receive malformed conversation history or bloated context. See [Context engineering](#context-engineering) below.\n</Warning>\n\n:::python\n```python\nfrom langchain.messages import AIMessage, ToolMessage\nfrom langchain.tools import tool, ToolRuntime\nfrom langgraph.types import Command\n\n@tool\ndef transfer_to_sales(\n    runtime: ToolRuntime,\n) -> Command:\n    \"\"\"Transfer to the sales agent.\"\"\"\n    last_ai_message = next(  # [!code highlight]\n        msg for msg in reversed(runtime.state[\"messages\"]) if isinstance(msg, AIMessage)  # [!code highlight]\n    )  # [!code highlight]\n    transfer_message = ToolMessage(  # [!code highlight]\n        content=\"Transferred to sales agent\",  # [!code highlight]\n        tool_call_id=runtime.tool_call_id,  # [!code highlight]\n    )  # [!code highlight]\n    return Command(\n        goto=\"sales_agent\",\n        update={\n            \"active_agent\": \"sales_agent\",\n            \"messages\": [last_ai_message, transfer_message],  # [!code highlight]\n        },\n        graph=Command.PARENT\n    )\n```\n:::\n:::js\n```typescript\nimport {\n  tool,\n  ToolMessage,\n  AIM", "metadata": {"source": "multi-agent/handoffs.mdx"}}
{"text": " sales agent\",  # [!code highlight]\n        tool_call_id=runtime.tool_call_id,  # [!code highlight]\n    )  # [!code highlight]\n    return Command(\n        goto=\"sales_agent\",\n        update={\n            \"active_agent\": \"sales_agent\",\n            \"messages\": [last_ai_message, transfer_message],  # [!code highlight]\n        },\n        graph=Command.PARENT\n    )\n```\n:::\n:::js\n```typescript\nimport {\n  tool,\n  ToolMessage,\n  AIMessage,\n  type ToolRuntime,\n} from \"langchain\";\nimport { Command, StateSchema, MessagesValue } from \"@langchain/langgraph\";\n\nconst CustomState = new StateSchema({\n  messages: MessagesValue,\n});\n\nconst transferToSales = tool(\n  async (_, runtime: ToolRuntime<typeof CustomState.State>) => {\n    const lastAiMessage = runtime.state.messages // [!code highlight]\n      .reverse() // [!code highlight]\n      .find(AIMessage.isInstance); // [!code highlight]\n\n    const transferMessage = new ToolMessage({ // [!code highlight]\n      content: \"Transferred to sales agent\", // [!code highlight]\n      tool_call_id: runtime.toolCallId, // [!code highlight]\n    }); // [!code highlight]\n    return new Command({\n      goto: \"sales_agent\",\n      update: {\n        activeAgent: \"sales_agent\",\n        messages: [lastAiMessage, transferMessage].filter(Boolean), // [!code highlight]\n      },\n      graph: Command.PARENT,\n    });\n  },\n  {\n    name: \"transfer_to_sales\",\n    description: \"Transfer to the sales agent.\",\n    schema: z.object({}),\n  }\n);\n```\n:::\n\n<Accordion title=\"Complete example: Sales and support with handoffs\">\n\nThis example shows a multi-agent system with separate sales and support agents. Each agent is a separate graph node, and handoff tools allow agents to transfer conversations to each other.\n\n:::python\n```python\nfrom typing import Literal\n\nfrom langchain.agents import AgentState, create_agent\nfrom langchain.messages import AIMessage, ToolMessage\nfrom langchain.tools import tool, ToolRuntime\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.types import Command\nfrom typing_extensions import NotRequired\n\n\n# 1. Define state with active_agent tracker\nclass MultiAgentState(AgentState):\n    active_agent: NotRequired[str]\n\n\n# 2. Create handoff tools\n@tool\ndef transfer_to_sales(\n    runtime: ToolRuntime,\n) -> Command:\n    \"\"\"Transfer to the sales agent.\"\"\"\n  ", "metadata": {"source": "multi-agent/handoffs.mdx"}}
{"text": "-agent system with separate sales and support agents. Each agent is a separate graph node, and handoff tools allow agents to transfer conversations to each other.\n\n:::python\n```python\nfrom typing import Literal\n\nfrom langchain.agents import AgentState, create_agent\nfrom langchain.messages import AIMessage, ToolMessage\nfrom langchain.tools import tool, ToolRuntime\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.types import Command\nfrom typing_extensions import NotRequired\n\n\n# 1. Define state with active_agent tracker\nclass MultiAgentState(AgentState):\n    active_agent: NotRequired[str]\n\n\n# 2. Create handoff tools\n@tool\ndef transfer_to_sales(\n    runtime: ToolRuntime,\n) -> Command:\n    \"\"\"Transfer to the sales agent.\"\"\"\n    last_ai_message = next(  # [!code highlight]\n        msg for msg in reversed(runtime.state[\"messages\"]) if isinstance(msg, AIMessage)  # [!code highlight]\n    )  # [!code highlight]\n    transfer_message = ToolMessage(  # [!code highlight]\n        content=\"Transferred to sales agent from support agent\",  # [!code highlight]\n        tool_call_id=runtime.tool_call_id,  # [!code highlight]\n    )  # [!code highlight]\n    return Command(\n        goto=\"sales_agent\",\n        update={\n            \"active_agent\": \"sales_agent\",\n            \"messages\": [last_ai_message, transfer_message],  # [!code highlight]\n        },\n        graph=Command.PARENT,\n    )\n\n\n@tool\ndef transfer_to_support(\n    runtime: ToolRuntime,\n) -> Command:\n    \"\"\"Transfer to the support agent.\"\"\"\n    last_ai_message = next(  # [!code highlight]\n        msg for msg in reversed(runtime.state[\"messages\"]) if isinstance(msg, AIMessage)  # [!code highlight]\n    )  # [!code highlight]\n    transfer_message = ToolMessage(  # [!code highlight]\n        content=\"Transferred to support agent from sales agent\",  # [!code highlight]\n        tool_call_id=runtime.tool_call_id,  # [!code highlight]\n    )  # [!code highlight]\n    return Command(\n        goto=\"support_agent\",\n        update={\n            \"active_agent\": \"support_agent\",\n            \"messages\": [last_ai_message, transfer_message],  # [!code highlight]\n        },\n        graph=Command.PARENT,\n    )\n\n\n# 3. Create agents with handoff tools\nsales_agent = create_agent(", "metadata": {"source": "multi-agent/handoffs.mdx"}}
{"text": "     content=\"Transferred to support agent from sales agent\",  # [!code highlight]\n        tool_call_id=runtime.tool_call_id,  # [!code highlight]\n    )  # [!code highlight]\n    return Command(\n        goto=\"support_agent\",\n        update={\n            \"active_agent\": \"support_agent\",\n            \"messages\": [last_ai_message, transfer_message],  # [!code highlight]\n        },\n        graph=Command.PARENT,\n    )\n\n\n# 3. Create agents with handoff tools\nsales_agent = create_agent(\n    model=\"anthropic:claude-sonnet-4-20250514\",\n    tools=[transfer_to_support],\n    system_prompt=\"You are a sales agent. Help with sales inquiries. If asked about technical issues or support, transfer to the support agent.\",\n)\n\nsupport_agent = create_agent(\n    model=\"anthropic:claude-sonnet-4-20250514\",\n    tools=[transfer_to_sales],\n    system_prompt=\"You are a support agent. Help with technical issues. If asked about pricing or purchasing, transfer to the sales agent.\",\n)\n\n\n# 4. Create agent nodes that invoke the agents\ndef call_sales_agent(state: MultiAgentState) -> Command:\n    \"\"\"Node that calls the sales agent.\"\"\"\n    response = sales_agent.invoke(state)\n    return response\n\n\ndef call_support_agent(state: MultiAgentState) -> Command:\n    \"\"\"Node that calls the support agent.\"\"\"\n    response = support_agent.invoke(state)\n    return response\n\n\n# 5. Create router that checks if we should end or continue\ndef route_after_agent(\n    state: MultiAgentState,\n) -> Literal[\"sales_agent\", \"support_agent\", \"__end__\"]:\n    \"\"\"Route based on active_agent, or END if the agent finished without handoff.\"\"\"\n    messages = state.get(\"messages\", [])\n\n    # Check the last message - if it's an AIMessage without tool calls, we're done\n    if messages:\n        last_msg = messages[-1]\n        if isinstance(last_msg, AIMessage) and not last_msg.tool_calls:  # [!code highlight]\n            return \"__end__\"  # [!code highlight]\n\n    # Otherwise route to the active agent\n    active = state.get(\"active_agent\", \"sales_agent\")\n    return active if active else \"sales_agent\"\n\n\ndef route_initial(\n    state: MultiAgentState,\n) -> Literal[\"sales_agent\", \"support_agent\"]:\n    \"\"\"Route to the active agent based on state, default to sales agent.\"\"\"\n    return state.get(\"active_agent\") or \"sales_agent\"\n\n\n# 6. Build the graph\nbuilder = State", "metadata": {"source": "multi-agent/handoffs.mdx"}}
{"text": " messages[-1]\n        if isinstance(last_msg, AIMessage) and not last_msg.tool_calls:  # [!code highlight]\n            return \"__end__\"  # [!code highlight]\n\n    # Otherwise route to the active agent\n    active = state.get(\"active_agent\", \"sales_agent\")\n    return active if active else \"sales_agent\"\n\n\ndef route_initial(\n    state: MultiAgentState,\n) -> Literal[\"sales_agent\", \"support_agent\"]:\n    \"\"\"Route to the active agent based on state, default to sales agent.\"\"\"\n    return state.get(\"active_agent\") or \"sales_agent\"\n\n\n# 6. Build the graph\nbuilder = StateGraph(MultiAgentState)\nbuilder.add_node(\"sales_agent\", call_sales_agent)\nbuilder.add_node(\"support_agent\", call_support_agent)\n\n# Start with conditional routing based on initial active_agent\nbuilder.add_conditional_edges(START, route_initial, [\"sales_agent\", \"support_agent\"])\n\n# After each agent, check if we should end or route to another agent\nbuilder.add_conditional_edges(\n    \"sales_agent\", route_after_agent, [\"sales_agent\", \"support_agent\", END]\n)\nbuilder.add_conditional_edges(\n    \"support_agent\", route_after_agent, [\"sales_agent\", \"support_agent\", END]\n)\n\ngraph = builder.compile()\nresult = graph.invoke(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"Hi, I'm having trouble with my account login. Can you help?\",\n            }\n        ]\n    }\n)\n\nfor msg in result[\"messages\"]:\n    msg.pretty_print()\n```\n:::\n\n:::js\n```typescript\nimport {\n  StateGraph,\n  START,\n  END,\n  StateSchema,\n  MessagesValue,\n  Command,\n  ConditionalEdgeRouter,\n  GraphNode,\n} from \"@langchain/langgraph\";\nimport { createAgent, AIMessage, ToolMessage } from \"langchain\";\nimport { tool, ToolRuntime } from \"@langchain/core/tools\";\nimport { z } from \"zod/v4\";\n\n// 1. Define state with active_agent tracker\nconst MultiAgentState = new StateSchema({\n  messages: MessagesValue,\n  activeAgent: z.string().optional(),\n});\n\n// 2. Create handoff tools\nconst transferToSales = tool(\n  async (_, runtime: ToolRuntime<typeof MultiAgentState.State>) => {\n    const lastAiMessage = [...runtime.state.messages] // [!code highlight]\n      .reverse() // [!code highlight]\n      .find(AIMessage.isInstance); // [!code highlight", "metadata": {"source": "multi-agent/handoffs.mdx"}}
{"text": "\n} from \"@langchain/langgraph\";\nimport { createAgent, AIMessage, ToolMessage } from \"langchain\";\nimport { tool, ToolRuntime } from \"@langchain/core/tools\";\nimport { z } from \"zod/v4\";\n\n// 1. Define state with active_agent tracker\nconst MultiAgentState = new StateSchema({\n  messages: MessagesValue,\n  activeAgent: z.string().optional(),\n});\n\n// 2. Create handoff tools\nconst transferToSales = tool(\n  async (_, runtime: ToolRuntime<typeof MultiAgentState.State>) => {\n    const lastAiMessage = [...runtime.state.messages] // [!code highlight]\n      .reverse() // [!code highlight]\n      .find(AIMessage.isInstance); // [!code highlight]\n    const transferMessage = new ToolMessage({ // [!code highlight]\n      content: \"Transferred to sales agent from support agent\", // [!code highlight]\n      tool_call_id: runtime.toolCallId, // [!code highlight]\n    }); // [!code highlight]\n    return new Command({\n      goto: \"sales_agent\",\n      update: {\n        activeAgent: \"sales_agent\",\n        messages: [lastAiMessage, transferMessage].filter(Boolean), // [!code highlight]\n      },\n      graph: Command.PARENT,\n    });\n  },\n  {\n    name: \"transfer_to_sales\",\n    description: \"Transfer to the sales agent.\",\n    schema: z.object({}),\n  }\n);\n\nconst transferToSupport = tool(\n  async (_, runtime: ToolRuntime<typeof MultiAgentState.State>) => {\n    const lastAiMessage = [...runtime.state.messages] // [!code highlight]\n      .reverse() // [!code highlight]\n      .find(AIMessage.isInstance); // [!code highlight]\n    const transferMessage = new ToolMessage({ // [!code highlight]\n      content: \"Transferred to support agent from sales agent\", // [!code highlight]\n      tool_call_id: runtime.toolCallId, // [!code highlight]\n    }); // [!code highlight]\n    return new Command({\n      goto: \"support_agent\",\n      update: {\n        activeAgent: \"support_agent\",\n        messages: [lastAiMessage, transferMessage].filter(Boolean), // [!code highlight]\n      },\n      graph: Command.PARENT,\n    });\n  },\n  {\n    name: \"transfer_to_support\",\n    description: \"Transfer to the support agent.\",\n    schema: z.object({}),\n  }\n);\n\n// 3. Create agents with handoff tools\nconst salesAgent = createAgent({\n  model: \"anthropic:claude-sonnet-4-20250514\",\n  tools: [transferToSupport],\n ", "metadata": {"source": "multi-agent/handoffs.mdx"}}
{"text": "    return new Command({\n      goto: \"support_agent\",\n      update: {\n        activeAgent: \"support_agent\",\n        messages: [lastAiMessage, transferMessage].filter(Boolean), // [!code highlight]\n      },\n      graph: Command.PARENT,\n    });\n  },\n  {\n    name: \"transfer_to_support\",\n    description: \"Transfer to the support agent.\",\n    schema: z.object({}),\n  }\n);\n\n// 3. Create agents with handoff tools\nconst salesAgent = createAgent({\n  model: \"anthropic:claude-sonnet-4-20250514\",\n  tools: [transferToSupport],\n  systemPrompt:\n    \"You are a sales agent. Help with sales inquiries. If asked about technical issues or support, transfer to the support agent.\",\n});\n\nconst supportAgent = createAgent({\n  model: \"anthropic:claude-sonnet-4-20250514\",\n  tools: [transferToSales],\n  systemPrompt:\n    \"You are a support agent. Help with technical issues. If asked about pricing or purchasing, transfer to the sales agent.\",\n});\n\n// 4. Create agent nodes that invoke the agents\nconst callSalesAgent: GraphNode<typeof MultiAgentState.State> = async (state) => {\n  const response = await salesAgent.invoke(state);\n  return response;\n};\n\nconst callSupportAgent: GraphNode<typeof MultiAgentState.State> = async (state) => {\n  const response = await supportAgent.invoke(state);\n  return response;\n};\n\n// 5. Create router that checks if we should end or continue\nconst routeAfterAgent: ConditionalEdgeRouter<\n  typeof MultiAgentState.State,\n  \"sales_agent\" | \"support_agent\"\n> = (state) => {\n  const messages = state.messages ?? [];\n\n  // Check the last message - if it's an AIMessage without tool calls, we're done\n  if (messages.length > 0) {\n    const lastMsg = messages[messages.length - 1];\n    if (lastMsg instanceof AIMessage && !lastMsg.tool_calls?.length) { // [!code highlight]\n      return END; // [!code highlight]\n    } // [!code highlight]\n  }\n\n  // Otherwise route to the active agent\n  const active = state.activeAgent ?? \"sales_agent\";\n  return active as \"sales_agent\" | \"support_agent\";\n};\n\nconst routeInitial: ConditionalEdgeRouter<\n  typeof MultiAgentState.State,\n  \"sales_agent\" | \"support_agent\"\n> = (state) => {\n  // Route to the active agent based on state, default to sales agent\n  return (state.activeAgent ?? \"sales_agent\") as\n    | \"sales_agent\"\n    | \"support_agent\";\n};\n\n// 6. Build the graph\nconst builder = new StateGraph(MultiAgentState)\n  .addNode(\"sales_agent\", callSalesAgent)\n  .addNode(\"support_agent\", callSupportAgent);\n  // Start with conditional routing based on initial activeAgent", "metadata": {"source": "multi-agent/handoffs.mdx"}}
{"text": " route to the active agent\n  const active = state.activeAgent ?? \"sales_agent\";\n  return active as \"sales_agent\" | \"support_agent\";\n};\n\nconst routeInitial: ConditionalEdgeRouter<\n  typeof MultiAgentState.State,\n  \"sales_agent\" | \"support_agent\"\n> = (state) => {\n  // Route to the active agent based on state, default to sales agent\n  return (state.activeAgent ?? \"sales_agent\") as\n    | \"sales_agent\"\n    | \"support_agent\";\n};\n\n// 6. Build the graph\nconst builder = new StateGraph(MultiAgentState)\n  .addNode(\"sales_agent\", callSalesAgent)\n  .addNode(\"support_agent\", callSupportAgent);\n  // Start with conditional routing based on initial activeAgent\n  .addConditionalEdges(START, routeInitial, [\n    \"sales_agent\",\n    \"support_agent\",\n  ])\n  // After each agent, check if we should end or route to another agent\n  .addConditionalEdges(\"sales_agent\", routeAfterAgent, [\n    \"sales_agent\",\n    \"support_agent\",\n    END,\n  ]);\n  builder.addConditionalEdges(\"support_agent\", routeAfterAgent, [\n    \"sales_agent\",\n    \"support_agent\",\n    END,\n  ]);\n\nconst graph = builder.compile();\nconst result = await graph.invoke({\n  messages: [\n    {\n      role: \"user\",\n      content: \"Hi, I'm having trouble with my account login. Can you help?\",\n    },\n  ],\n});\n\nfor (const msg of result.messages) {\n  console.log(msg.content);\n}\n```\n:::\n\n</Accordion>\n\n<Tip>\nUse **single agent with middleware** for most handoffs use cases\u2014it's simpler. Only use **multiple agent subgraphs** when you need bespoke agent implementations (e.g., a node that's itself a complex graph with reflection or retrieval steps).\n</Tip>\n\n#### Context engineering\n\nWith subgraph handoffs, you control exactly what messages flow between agents. This precision is essential for maintaining valid conversation history and avoiding context bloat that could confuse downstream agents. For more on this topic, see [context engineering](/oss/langchain/context-engineering).\n\n**Handling context during handoffs**\n\nWhen handing off between agents, you need to ensure the conversation history remains valid. LLMs expect tool calls to be paired with their responses, so when using `Command.PARENT` to hand off to another agent, you must include both:\n\n1. **The `AIMessage` containing the tool call** (the message that triggered the handoff)\n2. **A `ToolMessage` acknowledging the handoff** (the artificial response to that tool call)\n\nWithout this pairing, the receiving agent will see an incomplete conversation and may produce errors or unexpected behavior.\n\nThe example below assumes only the handoff tool was called (no parallel tool calls):\n\n:::python\n```python\n@tool\ndef transfer_to_sales(runtime: ToolRuntime) -> Command:\n    # Get the AI message that triggered this handoff\n    last_ai_message = runtime.state[\"messages\"", "metadata": {"source": "multi-agent/handoffs.mdx"}}
{"text": "\n\nWhen handing off between agents, you need to ensure the conversation history remains valid. LLMs expect tool calls to be paired with their responses, so when using `Command.PARENT` to hand off to another agent, you must include both:\n\n1. **The `AIMessage` containing the tool call** (the message that triggered the handoff)\n2. **A `ToolMessage` acknowledging the handoff** (the artificial response to that tool call)\n\nWithout this pairing, the receiving agent will see an incomplete conversation and may produce errors or unexpected behavior.\n\nThe example below assumes only the handoff tool was called (no parallel tool calls):\n\n:::python\n```python\n@tool\ndef transfer_to_sales(runtime: ToolRuntime) -> Command:\n    # Get the AI message that triggered this handoff\n    last_ai_message = runtime.state[\"messages\"][-1]\n\n    # Create an artificial tool response to complete the pair\n    transfer_message = ToolMessage(\n        content=\"Transferred to sales agent\",\n        tool_call_id=runtime.tool_call_id,\n    )\n\n    return Command(\n        goto=\"sales_agent\",\n        update={\n            \"active_agent\": \"sales_agent\",\n            # Pass only these two messages, not the full subagent history\n            \"messages\": [last_ai_message, transfer_message],\n        },\n        graph=Command.PARENT,\n    )\n```\n:::\n\n:::js\n```typescript\nconst transferToSales = tool(\n  async (_, runtime: ToolRuntime<typeof MultiAgentState.State>) => {\n    // Get the AI message that triggered this handoff\n    const lastAiMessage = runtime.state.messages.at(-1);\n\n    // Create an artificial tool response to complete the pair\n    const transferMessage = new ToolMessage({\n      content: \"Transferred to sales agent\",\n      tool_call_id: runtime.toolCallId,\n    });\n\n    return new Command({\n      goto: \"sales_agent\",\n      update: {\n        activeAgent: \"sales_agent\",\n        // Pass only these two messages, not the full subagent history\n        messages: [lastAiMessage, transferMessage],\n      },\n      graph: Command.PARENT,\n    });\n  },\n  {\n    name: \"transfer_to_sales\",\n    description: \"Transfer to the sales agent.\",\n    schema: z.object({}),\n  }\n);\n```\n:::\n\n<Note>\n**Why not pass all subagent messages?** While you could include the full subagent conversation in the handoff, this often creates problems. The receiving agent may become confused by irrelevant internal reasoning, and token costs increase unnecessarily. By passing only the handoff pair, you keep the parent graph's context focused on high-level coordination. If the receiving agent needs", "metadata": {"source": "multi-agent/handoffs.mdx"}}
{"text": "\n        // Pass only these two messages, not the full subagent history\n        messages: [lastAiMessage, transferMessage],\n      },\n      graph: Command.PARENT,\n    });\n  },\n  {\n    name: \"transfer_to_sales\",\n    description: \"Transfer to the sales agent.\",\n    schema: z.object({}),\n  }\n);\n```\n:::\n\n<Note>\n**Why not pass all subagent messages?** While you could include the full subagent conversation in the handoff, this often creates problems. The receiving agent may become confused by irrelevant internal reasoning, and token costs increase unnecessarily. By passing only the handoff pair, you keep the parent graph's context focused on high-level coordination. If the receiving agent needs additional context, consider summarizing the subagent's work in the ToolMessage content instead of passing raw message history.\n</Note>\n\n**Returning control to the user**\n\nWhen returning control to the user (ending the agent's turn), ensure the final message is an `AIMessage`. This maintains valid conversation history and signals to the user interface that the agent has finished its work.\n\n## Implementation considerations\n\nAs you design your multi-agent system, consider:\n\n* **Context filtering strategy**: Will each agent receive full conversation history, filtered portions, or summaries? Different agents may need different context depending on their role.\n* **Tool semantics**: Clarify whether handoff tools only update routing state or also perform side effects. For example, should `transfer_to_sales()` also create a support ticket, or should that be a separate action?\n* **Token efficiency**: Balance context completeness against token costs. Summarization and selective context passing become more important as conversations grow longer.\n", "metadata": {"source": "multi-agent/handoffs.mdx"}}
{"text": "---\ntitle: Skills\n---\n\nIn the **skills** architecture, specialized capabilities are packaged as invokable \"skills\" that augment an [agent's](/oss/langchain/agents) behavior. Skills are primarily prompt-driven specializations that an agent can invoke on-demand.\nFor built-in skill support, see [Deep Agents](/oss/deepagents/skills).\n\n<Tip>\nThis pattern is conceptually identical to [Agent Skills](https://agentskills.io/) and [llms.txt](https://llmstxt.org/) (introduced by Jeremy Howard), which uses tool calling for progressive disclosure of documentation. The skills pattern applies progressive disclosure to specialized prompts and domain knowledge rather than just documentation pages.\n</Tip>\n\n```mermaid\ngraph LR\n    A[User] --> B[Agent]\n    B --> C[Skill A]\n    B --> D[Skill B]\n    B --> E[Skill C]\n    B --> A\n```\n\n## Key characteristics\n\n* Prompt-driven specialization: Skills are primarily defined by specialized prompts\n* Progressive disclosure: Skills become available based on context or user needs\n* Team distribution: Different teams can develop and maintain skills independently\n* Lightweight composition: Skills are simpler than full sub-agents\n* Reference awareness: Skills can reference scripts, templates, and other resources\n\n## When to use\n\nUse the skills pattern when you want a single [agent](/oss/langchain/agents) with many possible specializations, you don't need to enforce specific constraints between skills, or different teams need to develop capabilities independently. Common examples include coding assistants (skills for different languages or tasks), knowledge bases (skills for different domains), and creative assistants (skills for different formats).\n\n## Basic implementation\n\n:::python\n```python\nfrom langchain.tools import tool\nfrom langchain.agents import create_agent\n\n@tool\ndef load_skill(skill_name: str) -> str:\n    \"\"\"Load a specialized skill prompt.\n\n    Available skills:\n    - write_sql: SQL query writing expert\n    - review_legal_doc: Legal document reviewer\n\n    Returns the skill's prompt and context.\n    \"\"\"\n    # Load skill content from file/database\n    ...\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[load_skill],\n    system_prompt=(\n        \"You are a helpful assistant. \"\n        \"You have access to two skills: \"\n        \"write_sql and review_legal_doc. \"\n        \"Use load_skill to access them.\"\n    ),\n)\n```\n:::\n:::js\n```typescript\nimport { tool, createAgent } from \"langchain\";\nimport * as z from \"zod\";\n\nconst loadSkill = tool(\n  async ({ skillName }) => {\n    // Load skill content from file/database\n    return \"\";\n  },\n  {\n    name: \"load_skill\",\n    description: `Load a specialized skill.\n\nAvailable skills:\n- write_sql: SQL query writing expert\n- review_legal_doc: Legal document reviewer\n\nReturns the skill's prompt and context.`,\n    schema: z.object({\n      skillName:", "metadata": {"source": "multi-agent/skills.mdx"}}
{"text": "     \"write_sql and review_legal_doc. \"\n        \"Use load_skill to access them.\"\n    ),\n)\n```\n:::\n:::js\n```typescript\nimport { tool, createAgent } from \"langchain\";\nimport * as z from \"zod\";\n\nconst loadSkill = tool(\n  async ({ skillName }) => {\n    // Load skill content from file/database\n    return \"\";\n  },\n  {\n    name: \"load_skill\",\n    description: `Load a specialized skill.\n\nAvailable skills:\n- write_sql: SQL query writing expert\n- review_legal_doc: Legal document reviewer\n\nReturns the skill's prompt and context.`,\n    schema: z.object({\n      skillName: z\n        .string()\n        .describe(\"Name of skill to load\")\n    })\n  }\n);\n\nconst agent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [loadSkill],\n  systemPrompt: (\n    \"You are a helpful assistant. \" +\n    \"You have access to two skills: \" +\n    \"write_sql and review_legal_doc. \" +\n    \"Use load_skill to access them.\"\n  ),\n});\n```\n:::\n\nFor a complete implementation, see the tutorial below.\n\n<Card\n    title=\"Tutorial: Build a SQL assistant with on-demand skills\"\n    icon=\"wand-magic-sparkles\"\n    href=\"/oss/langchain/multi-agent/skills-sql-assistant\"\n    arrow cta=\"Learn more\"\n>\n    Learn how to implement skills with progressive disclosure, where the agent loads specialized prompts and schemas on-demand rather than upfront.\n</Card>\n\n## Extending the pattern\n\nWhen writing custom implementations, you can extend the basic skills pattern in several ways:\n\n- **Dynamic tool registration**: Combine progressive disclosure with state management to register new [tools](/oss/langchain/tools) as skills load. For example, loading a \"database_admin\" skill could both add specialized context and register database-specific tools (backup, restore, migrate). This uses the same tool-and-state mechanisms used across multi-agent patterns\u2014tools updating state to dynamically change agent capabilities.\n\n- **Hierarchical skills**: Skills can define other skills in a tree structure, creating nested specializations. For instance, loading a \"data_science\" skill might make available sub-skills like \"pandas_expert\", \"visualization\", and \"statistical_analysis\". Each sub-skill can be loaded independently as needed, allowing for fine-grained progressive disclosure of domain knowledge. This hierarchical approach helps manage large knowledge bases by organizing capabilities into logical groupings that can be discovered and loaded on-demand.\n\n- **Reference awareness**: While each skill only has one prompt, this prompt can reference the location of other assets and provide information on when the agent should use those assets.\nWhen those assets become relevant, the agent will know that those files exist and read them into memory as needed to complete tasks.\nThis also follows the progressive disclosure pattern and limits the information in the context window.\n", "metadata": {"source": "multi-agent/skills.mdx"}}
{"text": "---\ntitle: Router\n---\n\nIn the **router** architecture, a routing step classifies input and directs it to specialized [agents](/oss/langchain/agents). This is useful when you have distinct **verticals**\u2014separate knowledge domains that each require their own agent.\n\n```mermaid\ngraph LR\n    A([Query]) --> B[Router]\n    B --> C[Agent A]\n    B --> D[Agent B]\n    B --> E[Agent C]\n    C --> F[Synthesize]\n    D --> F\n    E --> F\n    F --> G([Combined answer])\n```\n\n## Key characteristics\n\n* Router decomposes the query\n* Zero or more specialized agents are invoked in parallel\n* Results are synthesized into a coherent response\n\n## When to use\n\nUse the router pattern when you have distinct verticals (separate knowledge domains that each require their own agent), need to query multiple sources in parallel, and want to synthesize results into a combined response.\n\n## Basic implementation\n\nThe router classifies the query and directs it to the appropriate agent(s). Use [`Command`](/oss/langgraph/graph-api#command) for single-agent routing or [`Send`](/oss/langgraph/graph-api#send) for parallel fan-out to multiple agents.\n\n<Tabs>\n<Tab title=\"Single agent\">\n\nUse `Command` to route to a single specialized agent:\n\n:::python\n```python\nfrom langgraph.types import Command\n\ndef classify_query(query: str) -> str:\n    \"\"\"Use LLM to classify query and determine the appropriate agent.\"\"\"\n    # Classification logic here\n    ...\n\ndef route_query(state: State) -> Command:\n    \"\"\"Route to the appropriate agent based on query classification.\"\"\"\n    active_agent = classify_query(state[\"query\"])\n\n    # Route to the selected agent\n    return Command(goto=active_agent)\n```\n:::\n:::js\n```typescript\nimport { z } from \"zod\";\nimport { Command } from \"@langchain/langgraph\";\n\nconst ClassificationResult = z.object({\n  query: z.string(),\n  agent: z.string(),\n});\n\nfunction classifyQuery(query: string): z.infer<typeof ClassificationResult> {\n  // Use LLM to classify query and determine the appropriate agent\n  // Classification logic here\n  ...\n}\n\nfunction routeQuery(state: z.infer<typeof ClassificationResult>) {\n  const classification = classifyQuery(state.query);\n\n  // Route to the selected agent\n  return new Command({ goto: classification.agent });\n}\n```\n:::\n\n</Tab>\n<Tab title=\"Multiple agents (parallel)\">\n\nUse `Send` to fan out to multiple specialized agents in parallel:\n\n:::python\n```python\nfrom typing import TypedDict\nfrom langgraph.types import Send\n\nclass ClassificationResult(TypedDict):\n    query: str\n    agent: str\n\ndef classify_query(query: str) -> list[ClassificationResult]:\n    \"\"\"Use LLM to classify query and determine which agents to invoke.\"\"\"\n    # Classification logic here\n    ...\n\ndef route_query(state: State):\n    \"\"\"Route to", "metadata": {"source": "multi-agent/router.mdx"}}
{"text": "infer<typeof ClassificationResult>) {\n  const classification = classifyQuery(state.query);\n\n  // Route to the selected agent\n  return new Command({ goto: classification.agent });\n}\n```\n:::\n\n</Tab>\n<Tab title=\"Multiple agents (parallel)\">\n\nUse `Send` to fan out to multiple specialized agents in parallel:\n\n:::python\n```python\nfrom typing import TypedDict\nfrom langgraph.types import Send\n\nclass ClassificationResult(TypedDict):\n    query: str\n    agent: str\n\ndef classify_query(query: str) -> list[ClassificationResult]:\n    \"\"\"Use LLM to classify query and determine which agents to invoke.\"\"\"\n    # Classification logic here\n    ...\n\ndef route_query(state: State):\n    \"\"\"Route to relevant agents based on query classification.\"\"\"\n    classifications = classify_query(state[\"query\"])\n\n    # Fan out to selected agents in parallel\n    return [\n        Send(c[\"agent\"], {\"query\": c[\"query\"]})\n        for c in classifications\n    ]\n```\n:::\n:::js\n```typescript\nimport { z } from \"zod\";\nimport { Command } from \"@langchain/langgraph\";\n\nconst ClassificationResult = z.object({\n  query: z.string(),\n  agent: z.string(),\n});\n\nfunction classifyQuery(query: string): z.infer<typeof ClassificationResult>[] {\n  // Use LLM to classify query and determine the appropriate agent\n  // Classification logic here\n  ...\n}\n\nfunction routeQuery(state: typeof State.State) {\n  const classifications = classifyQuery(state.query);\n\n  // Fan out to selected agents in parallel\n  return classifications.map(\n    (c) => new Send(c.agent, { query: c.query })\n  );\n}\n```\n:::\n\n</Tab>\n</Tabs>\n\nFor a complete implementation, see the tutorial below.\n\n<Card title=\"Tutorial: Build a multi-source knowledge base with routing\" icon=\"book\" href=\"/oss/langchain/multi-agent/router-knowledge-base\">\nBuild a router that queries GitHub, Notion, and Slack in parallel, then synthesizes results into a coherent answer. Covers state definition, specialized agents, parallel execution with `Send`, and result synthesis.\n</Card>\n\n## Stateless vs. stateful\n\nTwo approaches:\n* [**Stateless routers**](#stateless) address each request independently\n* [**Stateful routers**](#stateful) maintain conversation history across requests\n\n## Stateless\n\nEach request is routed independently\u2014no memory between calls. For multi-turn conversations, see [Stateful routers](#stateful).\n\n<Tip>\n**Router vs. Subagents**: Both patterns can dispatch work to multiple agents, but they differ in how routing decisions are made:\n\n- **Router**: A dedicated routing step (often a single LLM call or rule-based logic) that classifies the input and dispatches to agents. The router itself typically doesn't maintain conversation history or perform multi-turn orchestration\u2014it's a preprocessing step.\n- **Subagents**: An main supervisor agent dynamically decides which [subagents](/oss/langchain/multi-agent/subagents) to call as part of an ongoing", "metadata": {"source": "multi-agent/router.mdx"}}
{"text": "* [**Stateless routers**](#stateless) address each request independently\n* [**Stateful routers**](#stateful) maintain conversation history across requests\n\n## Stateless\n\nEach request is routed independently\u2014no memory between calls. For multi-turn conversations, see [Stateful routers](#stateful).\n\n<Tip>\n**Router vs. Subagents**: Both patterns can dispatch work to multiple agents, but they differ in how routing decisions are made:\n\n- **Router**: A dedicated routing step (often a single LLM call or rule-based logic) that classifies the input and dispatches to agents. The router itself typically doesn't maintain conversation history or perform multi-turn orchestration\u2014it's a preprocessing step.\n- **Subagents**: An main supervisor agent dynamically decides which [subagents](/oss/langchain/multi-agent/subagents) to call as part of an ongoing conversation. The main agent maintains context, can call multiple subagents across turns, and orchestrates complex multi-step workflows.\n\nUse a **router** when you have clear input categories and want deterministic or lightweight classification. Use a **supervisor** when you need flexible, conversation-aware orchestration where the LLM decides what to do next based on evolving context.\n</Tip>\n\n\n## Stateful\n\nFor multi-turn conversations, you need to maintain context across invocations.\n\n### Tool wrapper\n\nThe simplest approach: wrap the stateless router as a tool that a conversational agent can call. The conversational agent handles memory and context; the router stays stateless. This avoids the complexity of managing conversation history across multiple parallel agents.\n\n:::python\n```python\n@tool\ndef search_docs(query: str) -> str:\n    \"\"\"Search across multiple documentation sources.\"\"\"\n    result = workflow.invoke({\"query\": query})  # [!code highlight]\n    return result[\"final_answer\"]\n\n# Conversational agent uses the router as a tool\nconversational_agent = create_agent(\n    model,\n    tools=[search_docs],\n    prompt=\"You are a helpful assistant. Use search_docs to answer questions.\"\n)\n```\n:::\n:::js\n```typescript\nconst searchDocs = tool(\n  async ({ query }) => {\n    const result = await workflow.invoke({ query }); // [!code highlight]\n    return result.finalAnswer;\n  },\n  {\n    name: \"search_docs\",\n    description: \"Search across multiple documentation sources\",\n    schema: z.object({\n      query: z.string().describe(\"The search query\"),\n    }),\n  }\n);\n\n// Conversational agent uses the router as a tool\nconst conversationalAgent = createAgent({\n  model,\n  tools: [searchDocs],\n  systemPrompt: \"You are a helpful assistant. Use search_docs to answer questions.\",\n});\n```\n:::\n\n### Full persistence\n\nIf you need the router itself to maintain state, use [persistence](/oss/langchain/short-term-memory) to store message history. When routing to an agent, fetch previous messages from state and selectively include them in the agent's context\u2014this is a lever for [context engineering](/oss/langchain/context-engineering).\n\n<Warning>\n**Stateful routers require custom history management.** If the router switches between agents across turns, conversations may not feel fluid to end users when agents have", "metadata": {"source": "multi-agent/router.mdx"}}
{"text": " z.string().describe(\"The search query\"),\n    }),\n  }\n);\n\n// Conversational agent uses the router as a tool\nconst conversationalAgent = createAgent({\n  model,\n  tools: [searchDocs],\n  systemPrompt: \"You are a helpful assistant. Use search_docs to answer questions.\",\n});\n```\n:::\n\n### Full persistence\n\nIf you need the router itself to maintain state, use [persistence](/oss/langchain/short-term-memory) to store message history. When routing to an agent, fetch previous messages from state and selectively include them in the agent's context\u2014this is a lever for [context engineering](/oss/langchain/context-engineering).\n\n<Warning>\n**Stateful routers require custom history management.** If the router switches between agents across turns, conversations may not feel fluid to end users when agents have different tones or prompts. With parallel invocation, you'll need to maintain history at the router level (inputs and synthesized outputs) and leverage this history in routing logic. Consider the [handoffs pattern](/oss/langchain/multi-agent/handoffs) or [subagents pattern](/oss/langchain/multi-agent/subagents) instead\u2014both provide clearer semantics for multi-turn conversations.\n</Warning>\n", "metadata": {"source": "multi-agent/router.mdx"}}
{"text": "---\ntitle: Subagents\n---\n\nIn the **subagents** architecture, a central main [agent](/oss/langchain/agents) (often referred to as a **supervisor**) coordinates subagents by calling them as [tools](/oss/langchain/tools). The main agent decides which subagent to invoke, what input to provide, and how to combine results. Subagents are stateless\u2014they don't remember past interactions, with all conversation memory maintained by the main agent. This provides [context](/oss/langchain/context-engineering) isolation: each subagent invocation works in a clean context window, preventing context bloat in the main conversation.\n\n```mermaid\ngraph LR\n    A[User] --> B[Main Agent]\n    B --> C[Subagent A]\n    B --> D[Subagent B]\n    B --> E[Subagent C]\n    C --> B\n    D --> B\n    E --> B\n    B --> F[User response]\n```\n\n## Key characteristics\n\n* Centralized control: All routing passes through the main agent\n* No direct user interaction: Subagents return results to the main agent, not the user (though you can use [interrupts](/oss/langgraph/human-in-the-loop#interrupt) within a subagent to allow user interaction)\n* Subagents via tools: Subagents are invoked via tools\n* Parallel execution: The main agent can invoke multiple subagents in a single turn\n\n<Note>\n**Supervisor vs. Router**: A supervisor agent (this pattern) is different from a [router](/oss/langchain/multi-agent/router). The supervisor is a full agent that maintains conversation context and dynamically decides which subagents to call across multiple turns. A router is typically a single classification step that dispatches to agents without maintaining ongoing conversation state.\n</Note>\n\n## When to use\n\nUse the subagents pattern when you have multiple distinct domains (e.g., calendar, email, CRM, database), subagents don't need to converse directly with users, or you want centralized workflow control. For simpler cases with just a few [tools](/oss/langchain/tools), use a [single agent](/oss/langchain/agents).\n\n<Tip>\n**Need user interaction within a subagent?** While subagents typically return results to the main agent rather than conversing directly with users, you can use [interrupts](/oss/langgraph/human-in-the-loop#interrupt) within a subagent to pause execution and gather user input. This is useful when a subagent needs clarification or approval before proceeding. The main agent remains the orchestrator, but the subagent can collect information from the user mid-task.\n</Tip>\n\n## Basic implementation\n\nThe core mechanism wraps a subagent as a tool that the main agent can call:\n\n:::python\n```python\nfrom langchain.tools import tool\nfrom langchain.agents import create_agent\n\n# Create a subagent\nsubagent = create_agent(model=\"anthropic:claude-sonnet-4-20250514\", tools=[...])\n\n# Wrap it as a tool\n@tool(\"research\", description=\"Research a topic and return findings\")\ndef call_research_agent(query: str):\n    result = subagent.invoke({\"messages\": [{\"role\": \"user\", \"content\": query}]})\n    return result[\"messages\"][-1].content\n\n# Main agent with subagent as a tool", "metadata": {"source": "multi-agent/subagents.mdx"}}
{"text": " the orchestrator, but the subagent can collect information from the user mid-task.\n</Tip>\n\n## Basic implementation\n\nThe core mechanism wraps a subagent as a tool that the main agent can call:\n\n:::python\n```python\nfrom langchain.tools import tool\nfrom langchain.agents import create_agent\n\n# Create a subagent\nsubagent = create_agent(model=\"anthropic:claude-sonnet-4-20250514\", tools=[...])\n\n# Wrap it as a tool\n@tool(\"research\", description=\"Research a topic and return findings\")\ndef call_research_agent(query: str):\n    result = subagent.invoke({\"messages\": [{\"role\": \"user\", \"content\": query}]})\n    return result[\"messages\"][-1].content\n\n# Main agent with subagent as a tool\nmain_agent = create_agent(model=\"anthropic:claude-sonnet-4-20250514\", tools=[call_research_agent])\n```\n:::\n:::js\n```typescript\nimport { createAgent, tool } from \"langchain\";\nimport { z } from \"zod\";\n\n// Create a subagent\nconst subagent = createAgent({ model: \"anthropic:claude-sonnet-4-20250514\", tools: [...] });\n\n// Wrap it as a tool\nconst callResearchAgent = tool(\n  async ({ query }) => {\n    const result = await subagent.invoke({\n      messages: [{ role: \"user\", content: query }]\n    });\n    return result.messages.at(-1)?.content;\n  },\n  {\n    name: \"research\",\n    description: \"Research a topic and return findings\",\n    schema: z.object({ query: z.string() })\n  }\n);\n\n// Main agent with subagent as a tool\nconst mainAgent = createAgent({ model: \"anthropic:claude-sonnet-4-20250514\", tools: [callResearchAgent] });\n```\n:::\n\n<Card\n    title=\"Tutorial: Build a personal assistant with subagents\"\n    icon=\"sitemap\"\n    href=\"/oss/langchain/multi-agent/subagents-personal-assistant\"\n    arrow cta=\"Learn more\"\n>\n    Learn how to build a personal assistant using the subagents pattern, where a central main agent (supervisor) coordinates specialized worker agents.\n</Card>\n\n## Design decisions\n\nWhen implementing the subagents pattern, you'll make several key design choices. This table summarizes the options\u2014each is covered in detail in the sections below.\n\n| Decision | Options |\n|----------|---------|\n| [**Sync vs. async**](#sync-vs-async) | Sync (blocking) vs. async (background) |\n| [**Tool patterns**](#tool-patterns) | Tool per agent vs. single dispatch tool |\n| [**Subagent specs**](#subagent-specs) | System prompt vs. enum constraint vs. tool-based discovery (single dispatch tool only) |\n| [**Subagent inputs**](#subagent-inputs) | Query only vs. full context |\n| [**Subagent outputs**](#subagent-outputs) | Subagent result vs full conversation history |\n\n## Sync vs. async\n\nSubagent execution can be", "metadata": {"source": "multi-agent/subagents.mdx"}}
{"text": " decisions\n\nWhen implementing the subagents pattern, you'll make several key design choices. This table summarizes the options\u2014each is covered in detail in the sections below.\n\n| Decision | Options |\n|----------|---------|\n| [**Sync vs. async**](#sync-vs-async) | Sync (blocking) vs. async (background) |\n| [**Tool patterns**](#tool-patterns) | Tool per agent vs. single dispatch tool |\n| [**Subagent specs**](#subagent-specs) | System prompt vs. enum constraint vs. tool-based discovery (single dispatch tool only) |\n| [**Subagent inputs**](#subagent-inputs) | Query only vs. full context |\n| [**Subagent outputs**](#subagent-outputs) | Subagent result vs full conversation history |\n\n## Sync vs. async\n\nSubagent execution can be **synchronous** (blocking) or **asynchronous** (background). Your choice depends on whether the main agent needs the result to continue.\n\n| Mode | Main agent behavior | Best for | Tradeoff |\n|------|---------------------|----------|----------|\n| **Sync** | Waits for subagent to complete | Main agent needs result to continue | Simple, but blocks the conversation |\n| **Async** | Continues while subagent runs in background | Independent tasks, user shouldn't wait | Responsive, but more complex |\n\n<Tip>\nNot to be confused with Python's `async`/`await`. Here, \"async\" means the main agent kicks off a background job (typically in a separate process or service) and continues without blocking.\n</Tip>\n\n### Synchronous (default)\n\nBy default, subagent calls are **synchronous**\u2014the main agent waits for each subagent to complete before continuing. Use sync when the main agent's next action depends on the subagent's result.\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Main Agent\n    participant Research Subagent\n\n    User->>Main Agent: \"What's the weather in Tokyo?\"\n    Main Agent->>Research Subagent: research(\"Tokyo weather\")\n    Note over Main Agent: Waiting for result...\n    Research Subagent-->>Main Agent: \"Currently 72\u00b0F, sunny\"\n    Main Agent-->>User: \"It's 72\u00b0F and sunny in Tokyo\"\n```\n\n**When to use sync:**\n- Main agent needs the subagent's result to formulate its response\n- Tasks have order dependencies (e.g., fetch data \u2192 analyze \u2192 respond)\n- Subagent failures should block the main agent's response\n\n**Tradeoffs:**\n- Simple implementation\u2014just call and wait\n- User sees no response until all subagents complete\n- Long-running tasks freeze the conversation\n\n### Asynchronous\n\nUse **asynchronous execution** when the subagent's work is independent\u2014the main agent doesn't need the result to continue conversing with the user. The main agent kicks off a background job and remains responsive.\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Main Agent\n    participant Job System\n    participant Contract Reviewer\n\n    User->>Main Agent: \"Review this M&A contract\"\n    Main Agent->>Job System: run_agent(\"legal_reviewer\", task)\n    Job System->>Contract Reviewer: Start agent\n    Job System-->>Main Agent: job_id: \"job_123\"\n ", "metadata": {"source": "multi-agent/subagents.mdx"}}
{"text": "\n\n**Tradeoffs:**\n- Simple implementation\u2014just call and wait\n- User sees no response until all subagents complete\n- Long-running tasks freeze the conversation\n\n### Asynchronous\n\nUse **asynchronous execution** when the subagent's work is independent\u2014the main agent doesn't need the result to continue conversing with the user. The main agent kicks off a background job and remains responsive.\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Main Agent\n    participant Job System\n    participant Contract Reviewer\n\n    User->>Main Agent: \"Review this M&A contract\"\n    Main Agent->>Job System: run_agent(\"legal_reviewer\", task)\n    Job System->>Contract Reviewer: Start agent\n    Job System-->>Main Agent: job_id: \"job_123\"\n    Main Agent-->>User: \"Started review (job_123)\"\n\n    Note over Contract Reviewer: Reviewing 150+ pages...\n\n    User->>Main Agent: \"What's the status?\"\n    Main Agent->>Job System: check_status(job_id)\n    Job System-->>Main Agent: \"running\"\n    Main Agent-->>User: \"Still reviewing contract...\"\n\n    Note over Contract Reviewer: Review completes\n\n    User->>Main Agent: \"Is it done yet?\"\n    Main Agent->>Job System: check_status(job_id)\n    Job System-->>Main Agent: \"completed\"\n    Main Agent->>Job System: get_result(job_id)\n    Job System-->>Main Agent: Contract analysis\n    Main Agent-->>User: \"Review complete: [findings]\"\n```\n\n**When to use async:**\n- Subagent work is independent of the main conversation flow\n- Users should be able to continue chatting while work happens\n- You want to run multiple independent tasks in parallel\n\n**Three-tool pattern:**\n1. **Start job**: Kicks off the background task, returns a job ID\n2. **Check status**: Returns current state (pending, running, completed, failed)\n3. **Get result**: Retrieves the completed result\n\n**Handling job completion:** When a job finishes, your application needs to notify the user. One approach: surface a notification that, when clicked, sends a `HumanMessage` like \"Check job_123 and summarize the results.\"\n\n## Tool patterns\n\nThere are two main ways to expose subagents as tools:\n\n| Pattern | Best for | Trade-off |\n|---------|----------|-----------|\n| [**Tool per agent**](#tool-per-agent) | Fine-grained control over each subagent's input/output | More setup, but more customization |\n| [**Single dispatch tool**](#single-dispatch-tool) | Many agents, distributed teams, convention over configuration | Simpler composition, less per-agent customization |\n\n### Tool per agent\n\n```mermaid\ngraph LR\n    A[User] --> B[Main Agent]\n    B --> C[Subagent A]\n    B --> D[Subagent B]\n    B --> E[Subagent C]\n    C --> B\n    D --> B\n    E --> B\n    B --> F[User response]\n```\n\nThe key idea is wrapping subagents as tools that the main agent can", "metadata": {"source": "multi-agent/subagents.mdx"}}
{"text": "\n|---------|----------|-----------|\n| [**Tool per agent**](#tool-per-agent) | Fine-grained control over each subagent's input/output | More setup, but more customization |\n| [**Single dispatch tool**](#single-dispatch-tool) | Many agents, distributed teams, convention over configuration | Simpler composition, less per-agent customization |\n\n### Tool per agent\n\n```mermaid\ngraph LR\n    A[User] --> B[Main Agent]\n    B --> C[Subagent A]\n    B --> D[Subagent B]\n    B --> E[Subagent C]\n    C --> B\n    D --> B\n    E --> B\n    B --> F[User response]\n```\n\nThe key idea is wrapping subagents as tools that the main agent can call:\n\n:::python\n```python\nfrom langchain.tools import tool\nfrom langchain.agents import create_agent\n\n# Create a sub-agent\nsubagent = create_agent(model=\"...\", tools=[...])  # [!code highlight]\n\n# Wrap it as a tool  # [!code highlight]\n@tool(\"subagent_name\", description=\"subagent_description\")  # [!code highlight]\ndef call_subagent(query: str):  # [!code highlight]\n    result = subagent.invoke({\"messages\": [{\"role\": \"user\", \"content\": query}]})\n    return result[\"messages\"][-1].content\n\n# Main agent with subagent as a tool  # [!code highlight]\nmain_agent = create_agent(model=\"...\", tools=[call_subagent])  # [!code highlight]\n```\n:::\n:::js\n```typescript\nimport { createAgent, tool } from \"langchain\";\nimport * as z from \"zod\";\n\n// Create a sub-agent\nconst subagent = createAgent({...});  // [!code highlight]\n\n// Wrap it as a tool  // [!code highlight]\nconst callSubagent = tool(  // [!code highlight]\n  async ({ query }) => {  // [!code highlight]\n    const result = await subagent.invoke({\n      messages: [{ role: \"user\", content: query }]\n    });\n    return result.messages.at(-1)?.text;\n  },\n  {\n    name: \"subagent_name\",\n    description: \"subagent_description\",\n    schema: z.object({\n      query: z.string().describe(\"The query to send to subagent\")\n    })\n  }\n);\n\n// Main agent with subagent as a tool  // [!code highlight]\nconst mainAgent = createAgent({ model, tools: [callSubagent] });  // [!code highlight]\n```\n:::\n\nThe main agent invokes the subagent tool when it decides the task matches the subagent's description, receives the result, and continues orchestration. See [Context engineering](#context-engineering) for fine-grained control.\n\n### Single dispatch tool\n\nAn alternative approach uses a single parameterized tool to invoke ephemeral sub-agents for independent tasks. Unlike the [tool per agent](#tool-per-agent) approach where each sub-agent is wrapped as a separate tool, this uses a convention", "metadata": {"source": "multi-agent/subagents.mdx"}}
{"text": "    schema: z.object({\n      query: z.string().describe(\"The query to send to subagent\")\n    })\n  }\n);\n\n// Main agent with subagent as a tool  // [!code highlight]\nconst mainAgent = createAgent({ model, tools: [callSubagent] });  // [!code highlight]\n```\n:::\n\nThe main agent invokes the subagent tool when it decides the task matches the subagent's description, receives the result, and continues orchestration. See [Context engineering](#context-engineering) for fine-grained control.\n\n### Single dispatch tool\n\nAn alternative approach uses a single parameterized tool to invoke ephemeral sub-agents for independent tasks. Unlike the [tool per agent](#tool-per-agent) approach where each sub-agent is wrapped as a separate tool, this uses a convention-based approach with a single `task` tool: the task description is passed as a human message to the sub-agent, and the sub-agent's final message is returned as the tool result.\n\nUse this approach when you want to distribute agent development across multiple teams, need to isolate complex tasks into separate context windows, need a scalable way to add new agents without modifying the coordinator, or prefer convention over customization. This approach trades flexibility in context engineering for simplicity in agent composition and strong context isolation.\n\n```mermaid\ngraph LR\n    A[User] --> B[Main Agent]\n    B --> C{task<br/>agent_name, description}\n    C -->|research| D[Research Agent]\n    C -->|writer| E[Writer Agent]\n    C -->|reviewer| F[Reviewer Agent]\n    D --> C\n    E --> C\n    F --> C\n    C --> B\n    B --> G[User response]\n```\n\n**Key characteristics:**\n\n* Single task tool: One parameterized tool that can invoke any registered sub-agent by name\n* Convention-based invocation: Agent selected by name, task passed as human message, final message returned as tool result\n* Team distribution: Different teams can develop and deploy agents independently\n* Agent discovery: Sub-agents can be discovered via system prompt (listing available agents) or through [progressive disclosure](/oss/langchain/multi-agent/skills-sql-assistant) (loading agent information on-demand via tools)\n\n<Tip>\nAn interesting aspect of this approach is that sub-agents may have the exact same capabilities as the main agent. In such cases, invoking a sub-agent is **really about context isolation** as the primary reason\u2014allowing complex, multi-step tasks to run in isolated context windows without bloating the main agent's conversation history. The sub-agent completes its work autonomously and returns only a concise summary, keeping the main thread focused and efficient.\n</Tip>\n\n<Accordion title=\"Agent registry with task dispatcher\">\n\n:::python\n```python\nfrom langchain.tools import tool\nfrom langchain.agents import create_agent\n\n# Sub-agents developed by different teams\nresearch_agent = create_agent(\n    model=\"gpt-4.1\",\n    prompt=\"You are a research specialist...\"\n)\n\nwriter_agent = create_agent(\n    model=\"gpt-4.1\",\n    prompt=\"You are a writing specialist...\"\n)\n\n# Registry of available sub-agents\nSUBAGENTS = {\n    \"research\": research_agent", "metadata": {"source": "multi-agent/subagents.mdx"}}
{"text": " complex, multi-step tasks to run in isolated context windows without bloating the main agent's conversation history. The sub-agent completes its work autonomously and returns only a concise summary, keeping the main thread focused and efficient.\n</Tip>\n\n<Accordion title=\"Agent registry with task dispatcher\">\n\n:::python\n```python\nfrom langchain.tools import tool\nfrom langchain.agents import create_agent\n\n# Sub-agents developed by different teams\nresearch_agent = create_agent(\n    model=\"gpt-4.1\",\n    prompt=\"You are a research specialist...\"\n)\n\nwriter_agent = create_agent(\n    model=\"gpt-4.1\",\n    prompt=\"You are a writing specialist...\"\n)\n\n# Registry of available sub-agents\nSUBAGENTS = {\n    \"research\": research_agent,\n    \"writer\": writer_agent,\n}\n\n@tool\ndef task(\n    agent_name: str,\n    description: str\n) -> str:\n    \"\"\"Launch an ephemeral subagent for a task.\n\n    Available agents:\n    - research: Research and fact-finding\n    - writer: Content creation and editing\n    \"\"\"\n    agent = SUBAGENTS[agent_name]\n    result = agent.invoke({\n        \"messages\": [\n            {\"role\": \"user\", \"content\": description}\n        ]\n    })\n    return result[\"messages\"][-1].content\n\n# Main coordinator agent\nmain_agent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[task],\n    system_prompt=(\n        \"You coordinate specialized sub-agents. \"\n        \"Available: research (fact-finding), \"\n        \"writer (content creation). \"\n        \"Use the task tool to delegate work.\"\n    ),\n)\n```\n:::\n:::js\n```typescript\nimport { tool, createAgent } from \"langchain\";\nimport * as z from \"zod\";\n\n// Sub-agents developed by different teams\nconst researchAgent = createAgent({\n  model: \"gpt-4.1\",\n  prompt: \"You are a research specialist...\",\n});\n\nconst writerAgent = createAgent({\n  model: \"gpt-4.1\",\n  prompt: \"You are a writing specialist...\",\n});\n\n// Registry of available sub-agents\nconst SUBAGENTS = {\n  research: researchAgent,\n  writer: writerAgent,\n};\n\nconst task = tool(\n  async ({ agentName, description }) => {\n    const agent = SUBAGENTS[agentName];\n    const result = await agent.invoke({\n      messages: [\n        { role: \"user\", content: description }\n      ],\n    });\n    return result.messages.at(-1)?.content;\n  },\n  {\n    name: \"task\",\n    description: `Launch an ephemeral subagent.\n\nAvailable agents:\n- research: Research and fact-finding", "metadata": {"source": "multi-agent/subagents.mdx"}}
{"text": " \"gpt-4.1\",\n  prompt: \"You are a writing specialist...\",\n});\n\n// Registry of available sub-agents\nconst SUBAGENTS = {\n  research: researchAgent,\n  writer: writerAgent,\n};\n\nconst task = tool(\n  async ({ agentName, description }) => {\n    const agent = SUBAGENTS[agentName];\n    const result = await agent.invoke({\n      messages: [\n        { role: \"user\", content: description }\n      ],\n    });\n    return result.messages.at(-1)?.content;\n  },\n  {\n    name: \"task\",\n    description: `Launch an ephemeral subagent.\n\nAvailable agents:\n- research: Research and fact-finding\n- writer: Content creation and editing`,\n    schema: z.object({\n      agentName: z\n        .string()\n        .describe(\"Name of agent to invoke\"),\n      description: z\n        .string()\n        .describe(\"Task description\"),\n    }),\n  }\n);\n\n// Main coordinator agent\nconst mainAgent = createAgent({\n  model: \"gpt-4.1\",\n  tools: [task],\n  prompt: (\n    \"You coordinate specialized sub-agents. \" +\n    \"Available: research (fact-finding), \" +\n    \"writer (content creation). \" +\n    \"Use the task tool to delegate work.\"\n  ),\n});\n```\n:::\n\n</Accordion>\n\n## Context engineering\n\nControl how context flows between the main agent and its subagents:\n\n| Category | Purpose | Impacts |\n|----------|---------|---------|\n| [**Subagent specs**](#subagent-specs) | Ensure subagents are invoked when they should be | Main agent routing decisions |\n| [**Subagent inputs**](#subagent-inputs) | Ensure subagents can execute well with optimized context | Subagent performance |\n| [**Subagent outputs**](#subagent-outputs) | Ensure the supervisor can act on subagent results | Main agent performance |\n\nSee also our comprehensive guide on [context engineering](/oss/langchain/context-engineering) for agents.\n\n### Subagent specs\n\nThe **names** and **descriptions** associated with subagents are the primary way the main agent knows which subagents to invoke. These are prompting levers\u2014choose them carefully.\n\n* **Name**: How the main agent refers to the sub-agent. Keep it clear and action-oriented (e.g., `research_agent`, `code_reviewer`).\n* **Description**: What the main agent knows about the sub-agent's capabilities. Be specific about what tasks it handles and when to use it.\n\nFor the [single dispatch tool](#single-dispatch-tool) design, you must additionally provide the main agent with information about the subagents it can invoke.\nYou can provide this information in different ways based on the number of agents and whether your registry is static or dynamic:\n\n| Method | Best for | Tradeoff |\n|--------|----------|----------|\n| **System prompt enumeration** | Small, static agent lists (< 10 agents) | Simple, but requires prompt updates when agents change |\n", "metadata": {"source": "multi-agent/subagents.mdx"}}
{"text": " knows which subagents to invoke. These are prompting levers\u2014choose them carefully.\n\n* **Name**: How the main agent refers to the sub-agent. Keep it clear and action-oriented (e.g., `research_agent`, `code_reviewer`).\n* **Description**: What the main agent knows about the sub-agent's capabilities. Be specific about what tasks it handles and when to use it.\n\nFor the [single dispatch tool](#single-dispatch-tool) design, you must additionally provide the main agent with information about the subagents it can invoke.\nYou can provide this information in different ways based on the number of agents and whether your registry is static or dynamic:\n\n| Method | Best for | Tradeoff |\n|--------|----------|----------|\n| **System prompt enumeration** | Small, static agent lists (< 10 agents) | Simple, but requires prompt updates when agents change |\n| **Enum constraint** | Small, static agent lists (< 10 agents) | Type-safe and explicit, but requires code changes when agents change |\n| **Tool-based discovery** | Large or dynamic agent registries | Flexible and scalable, but adds complexity |\n\n#### System prompt enumeration\n\nList available agents directly in the main agent's system prompt. The main agent sees the list of agents and their descriptions as part of its instructions.\n\n**When to use:**\n- You have a small, fixed set of agents (< 10)\n- Agent registry rarely changes\n- You want the simplest implementation\n\n**Example:**\n```python\nmain_agent = create_agent(\n    model=\"...\",\n    tools=[task],\n    system_prompt=(\n        \"You coordinate specialized sub-agents. \"\n        \"Available agents:\\n\"\n        \"- research: Research and fact-finding\\n\"\n        \"- writer: Content creation and editing\\n\"\n        \"- reviewer: Code and document review\\n\"\n        \"Use the task tool to delegate work.\"\n    ),\n)\n```\n\n#### Enum constraint on dispatch tool\n\nAdd an enum constraint to the `agent_name` parameter in your dispatch tool. This provides type safety and makes available agents explicit in the tool schema.\n\n**When to use:**\n- You have a small, fixed set of agents (< 10)\n- You want type safety and explicit agent names\n- You prefer schema-based validation over prompt-based guidance\n\n**Example:**\n```python\nfrom enum import Enum\n\nclass AgentName(str, Enum):\n    RESEARCH = \"research\"\n    WRITER = \"writer\"\n    REVIEWER = \"reviewer\"\n\n@tool\ndef task(\n    agent_name: AgentName,  # Enum constraint\n    description: str\n) -> str:\n    \"\"\"Launch an ephemeral subagent for a task.\"\"\"\n    # ...\n```\n\n#### Tool-based discovery\n\nProvide a separate tool (e.g., `list_agents` or `search_agents`) that the main agent can call to discover available agents on-demand. This enables progressive disclosure and supports dynamic registries.\n\n**When to use:**\n- You have many agents (> 10) or a growing registry\n- Agent registry changes frequently or is dynamic\n- You want to reduce prompt size and token usage\n- Different teams manage different agents independently", "metadata": {"source": "multi-agent/subagents.mdx"}}
{"text": "\n    RESEARCH = \"research\"\n    WRITER = \"writer\"\n    REVIEWER = \"reviewer\"\n\n@tool\ndef task(\n    agent_name: AgentName,  # Enum constraint\n    description: str\n) -> str:\n    \"\"\"Launch an ephemeral subagent for a task.\"\"\"\n    # ...\n```\n\n#### Tool-based discovery\n\nProvide a separate tool (e.g., `list_agents` or `search_agents`) that the main agent can call to discover available agents on-demand. This enables progressive disclosure and supports dynamic registries.\n\n**When to use:**\n- You have many agents (> 10) or a growing registry\n- Agent registry changes frequently or is dynamic\n- You want to reduce prompt size and token usage\n- Different teams manage different agents independently\n\n**Example:**\n```python\n@tool\ndef list_agents(query: str = \"\") -> str:\n    \"\"\"List available subagents, optionally filtered by query.\"\"\"\n    agents = search_agent_registry(query)\n    return format_agent_list(agents)\n\n@tool\ndef task(agent_name: str, description: str) -> str:\n    \"\"\"Launch an ephemeral subagent for a task.\"\"\"\n    # ...\n\nmain_agent = create_agent(\n    model=\"...\",\n    tools=[task, list_agents],\n    system_prompt=\"Use list_agents to discover available subagents, then use task to invoke them.\"\n)\n```\n\n### Subagent inputs\n\nCustomize what context the subagent receives to execute its task. Add input that isn't practical to capture in a static prompt\u2014full message history, prior results, or task metadata\u2014by pulling from the agent's state.\n\n:::python\n```python Subagent inputs example expandable\nfrom langchain.agents import AgentState\nfrom langchain.tools import tool, ToolRuntime\n\nclass CustomState(AgentState):\n    example_state_key: str\n\n@tool(\n    \"subagent1_name\",\n    description=\"subagent1_description\"\n)\ndef call_subagent1(query: str, runtime: ToolRuntime[None, CustomState]):\n    # Apply any logic needed to transform the messages into a suitable input\n    subagent_input = some_logic(query, runtime.state[\"messages\"])\n    result = subagent1.invoke({\n        \"messages\": subagent_input,\n        # You could also pass other state keys here as needed.\n        # Make sure to define these in both the main and subagent's\n        # state schemas.\n        \"example_state_key\": runtime.state[\"example_state_key\"]\n    })\n    return result[\"messages\"][-1].content\n```\n:::\n:::js\n```typescript Subagent inputs example expandable\nimport { createAgent, tool, AgentState, ToolMessage } from \"langchain\";\nimport { Command } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\n// Example of passing the full conversation history to the sub agent via the state.\nconst callSubagent1 = tool(\n", "metadata": {"source": "multi-agent/subagents.mdx"}}
{"text": "messages\": subagent_input,\n        # You could also pass other state keys here as needed.\n        # Make sure to define these in both the main and subagent's\n        # state schemas.\n        \"example_state_key\": runtime.state[\"example_state_key\"]\n    })\n    return result[\"messages\"][-1].content\n```\n:::\n:::js\n```typescript Subagent inputs example expandable\nimport { createAgent, tool, AgentState, ToolMessage } from \"langchain\";\nimport { Command } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\n// Example of passing the full conversation history to the sub agent via the state.\nconst callSubagent1 = tool(\n  async ({query}) => {\n    const state = getCurrentTaskInput<AgentState>();\n    // Apply any logic needed to transform the messages into a suitable input\n    const subAgentInput = someLogic(query, state.messages);\n    const result = await subagent1.invoke({\n      messages: subAgentInput,\n      // You could also pass other state keys here as needed.\n      // Make sure to define these in both the main and subagent's\n      // state schemas.\n      exampleStateKey: state.exampleStateKey\n    });\n    return result.messages.at(-1)?.content;\n  },\n  {\n    name: \"subagent1_name\",\n    description: \"subagent1_description\",\n  }\n);\n```\n:::\n\n### Subagent outputs\n\nCustomize what the main agent receives back so it can make good decisions. Two strategies:\n\n1. **Prompt the sub-agent**: Specify exactly what should be returned. A common failure mode is that the sub-agent performs tool calls or reasoning but doesn't include results in its final message\u2014remind it that the supervisor only sees the final output.\n2. **Format in code**: Adjust or enrich the response before returning it. For example, pass specific state keys back in addition to the final text using a [`Command`](/oss/langgraph/graph-api#command).\n\n:::python\n```python Subagent outputs example expandable\nfrom typing import Annotated\nfrom langchain.agents import AgentState\nfrom langchain.tools import InjectedToolCallId\nfrom langgraph.types import Command\n\n\n@tool(\n    \"subagent1_name\",\n    description=\"subagent1_description\"\n)\ndef call_subagent1(\n    query: str,\n    tool_call_id: Annotated[str, InjectedToolCallId],\n) -> Command:\n    result = subagent1.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": query}]\n    })\n    return Command(update={\n        # Pass back additional state from the subagent\n        \"example_state_key\": result[\"example_state_key\"],\n        \"messages\": [\n            ToolMessage(\n      ", "metadata": {"source": "multi-agent/subagents.mdx"}}
{"text": " Command\n\n\n@tool(\n    \"subagent1_name\",\n    description=\"subagent1_description\"\n)\ndef call_subagent1(\n    query: str,\n    tool_call_id: Annotated[str, InjectedToolCallId],\n) -> Command:\n    result = subagent1.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": query}]\n    })\n    return Command(update={\n        # Pass back additional state from the subagent\n        \"example_state_key\": result[\"example_state_key\"],\n        \"messages\": [\n            ToolMessage(\n                content=result[\"messages\"][-1].content,\n                tool_call_id=tool_call_id\n            )\n        ]\n    })\n```\n:::\n:::js\n```typescript Subagent outputs example expandable\nimport { tool, ToolMessage } from \"langchain\";\nimport { Command } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\nconst callSubagent1 = tool(\n  async ({ query }, config) => {\n    const result = await subagent1.invoke({\n      messages: [{ role: \"user\", content: query }]\n    });\n\n    // Return a Command to update multiple state keys\n    return new Command({\n      update: {\n        // Pass back additional state from the subagent\n        exampleStateKey: result.exampleStateKey,\n        messages: [\n          new ToolMessage({\n            content: result.messages.at(-1)?.text,\n            tool_call_id: config.toolCall?.id!\n          })\n        ]\n      }\n    });\n  },\n  {\n    name: \"subagent1_name\",\n    description: \"subagent1_description\",\n    schema: z.object({\n      query: z.string().describe(\"The query to send to subagent1\")\n    })\n  }\n);\n```\n:::\n", "metadata": {"source": "multi-agent/subagents.mdx"}}
{"text": "---\ntitle: Build a SQL assistant with on-demand skills\nsidebarTitle: \"Skills: SQL assistant\"\n---\n\nimport ChatModelTabsPy from '/snippets/chat-model-tabs.mdx';\nimport ChatModelTabsJs from '/snippets/chat-model-tabs-js.mdx';\n\nThis tutorial shows how to use **progressive disclosure** - a context management technique where the agent loads information on-demand rather than upfront - to implement **skills** (specialized prompt-based instructions). The agent loads skills via tool calls, rather than dynamically changing the system prompt, discovering and loading only the skills it needs for each task.\n\n**Use case:** Imagine building an agent to help write SQL queries across different business verticals in a large enterprise. Your organization might have separate datastores for each vertical, or a single monolithic database with thousands of tables. Either way, loading all schemas upfront would overwhelm the context window. Progressive disclosure solves this by loading only the relevant schema when needed. This architecture also enables different product owners and stakeholders to independently contribute and maintain skills for their specific business verticals.\n\n**What you'll build:** A SQL query assistant with two skills (sales analytics and inventory management). The agent sees lightweight skill descriptions in its system prompt, then loads full database schemas and business logic through tool calls only when relevant to the user's query.\n\n<Note>\nFor a more complete example of a SQL agent with query execution, error correction, and validation, see our [SQL Agent tutorial](/oss/langchain/sql-agent). This tutorial focuses on the progressive disclosure pattern which can be applied to any domain.\n</Note>\n\n<Tip>\nProgressive disclosure was popularized by Anthropic as a technique for building scalable agent skills systems. This approach uses a three-level architecture (metadata \u2192 core content \u2192 detailed resources) where agents load information only as needed. For more on this technique, see [Equipping agents for the real world with Agent Skills](https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills).\n</Tip>\n\n## How it works\n\nHere's the flow when a user asks for a SQL query:\n\n```mermaid\n%%{init: {'theme':'base', 'themeVariables': { 'primaryColor':'#4CAF50','primaryTextColor':'#fff','primaryBorderColor':'#2E7D32','lineColor':'#666','secondaryColor':'#FF9800','tertiaryColor':'#2196F3','tertiaryBorderColor':'#1565C0','tertiaryTextColor':'#fff'}}}%%\nflowchart TD\n    Start([\ud83d\udcac User: Write SQL query<br/>for high-value customers]) --> SystemPrompt[\ud83d\udccb Agent sees skill descriptions:<br/>\u2022 sales_analytics<br/>\u2022 inventory_management]\n\n    SystemPrompt --> Decide{\ud83e\udd14 Need sales schema}\n\n    Decide --> LoadSkill[\ud83d\udd27 load_skill<br/>'sales_analytics']\n\n    LoadSkill --> Schema[\ud83d\udcca Schema loaded:<br/>customers, orders tables<br/>+ business logic]\n\n    Schema --> WriteQuery[\u270d\ufe0f Agent writes SQL query<br/>using schema knowledge]\n\n    WriteQuery --> Response([\u2705 Returns valid SQL<br/>following business rules])\n\n    %% Styling for light and dark modes\n    classDef startEnd fill:#4CAF50,", "metadata": {"source": "multi-agent/skills-sql-assistant.mdx"}}
{"text": "    Start([\ud83d\udcac User: Write SQL query<br/>for high-value customers]) --> SystemPrompt[\ud83d\udccb Agent sees skill descriptions:<br/>\u2022 sales_analytics<br/>\u2022 inventory_management]\n\n    SystemPrompt --> Decide{\ud83e\udd14 Need sales schema}\n\n    Decide --> LoadSkill[\ud83d\udd27 load_skill<br/>'sales_analytics']\n\n    LoadSkill --> Schema[\ud83d\udcca Schema loaded:<br/>customers, orders tables<br/>+ business logic]\n\n    Schema --> WriteQuery[\u270d\ufe0f Agent writes SQL query<br/>using schema knowledge]\n\n    WriteQuery --> Response([\u2705 Returns valid SQL<br/>following business rules])\n\n    %% Styling for light and dark modes\n    classDef startEnd fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff\n    classDef process fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff\n    classDef decision fill:#2196F3,stroke:#1565C0,stroke-width:2px,color:#fff\n    classDef enrichment fill:#9C27B0,stroke:#6A1B9A,stroke-width:2px,color:#fff\n\n    class Start,Response startEnd\n    class SystemPrompt,LoadSkill,WriteQuery process\n    class Decide decision\n    class Schema enrichment\n```\n\n**Why progressive disclosure:**\n- **Reduces context usage** - load only the 2-3 skills needed for a task, not all available skills\n- **Enables team autonomy** - different teams can develop specialized skills independently (similar to other multi-agent architectures)\n- **Scales efficiently** - add dozens or hundreds of skills without overwhelming context\n- **Simplifies conversation history** - single agent with one conversation thread\n\n**What are skills:** Skills, as popularized by Claude Code, are primarily prompt-based: self-contained units of specialized instructions for specific business tasks. In Claude Code, skills are exposed as directories with files on the file system, discovered through file operations. Skills guide behavior through prompts and can provide information about tool usage or include sample code for a coding agent to execute.\n\n<Tip>\nSkills with progressive disclosure can be viewed as a form of [RAG (Retrieval-Augmented Generation)](/oss/langchain/rag), where each skill is a retrieval unit\u2014though not necessarily backed by embeddings or keyword search, but by tools for browsing content (like file operations or, in this tutorial, direct lookup).\n</Tip>\n\n**Trade-offs:**\n- **Latency**: Loading skills on-demand requires additional tool calls, which adds latency to the first request that needs each skill\n- **Workflow control**: Basic implementations rely on prompting to guide skill usage - you cannot enforce hard constraints like \"always try skill A before skill B\" without custom logic\n\n<Tip>\n**Implementing your own skills system**\n\nWhen building your own skills implementation (as we do in this tutorial), the core concept is progressive disclosure - loading information on-demand. Beyond that, you have full flexibility in implementation:\n\n- **Storage**: databases, S3, in-memory data structures, or any backend\n- **Discovery**: direct lookup (this tutorial), RAG for large skill collections, file system scanning, or API calls\n- **Loading logic**: customize latency characteristics and add logic to search through skill content or", "metadata": {"source": "multi-agent/skills-sql-assistant.mdx"}}
{"text": " direct lookup).\n</Tip>\n\n**Trade-offs:**\n- **Latency**: Loading skills on-demand requires additional tool calls, which adds latency to the first request that needs each skill\n- **Workflow control**: Basic implementations rely on prompting to guide skill usage - you cannot enforce hard constraints like \"always try skill A before skill B\" without custom logic\n\n<Tip>\n**Implementing your own skills system**\n\nWhen building your own skills implementation (as we do in this tutorial), the core concept is progressive disclosure - loading information on-demand. Beyond that, you have full flexibility in implementation:\n\n- **Storage**: databases, S3, in-memory data structures, or any backend\n- **Discovery**: direct lookup (this tutorial), RAG for large skill collections, file system scanning, or API calls\n- **Loading logic**: customize latency characteristics and add logic to search through skill content or rank relevance\n- **Side effects**: define what happens when a skill loads, such as exposing tools associated with that skill (covered in section 8)\n\nThis flexibility lets you optimize for your specific requirements around performance, storage, and workflow control.\n</Tip>\n\n## Setup\n\n### Installation\n\nThis tutorial requires the `langchain` package:\n\n:::python\n<CodeGroup>\n```bash pip\npip install langchain\n```\n```bash uv\nuv add langchain\n```\n```bash conda\nconda install langchain -c conda-forge\n```\n</CodeGroup>\n:::\n\n:::js\n<CodeGroup>\n```bash npm\nnpm install langchain\n```\n```bash yarn\nyarn add langchain\n```\n```bash pnpm\npnpm add langchain\n```\n</CodeGroup>\n:::\n\nFor more details, see our [Installation guide](/oss/langchain/install).\n\n### LangSmith\n\nSet up [LangSmith](https://smith.langchain.com) to inspect what is happening inside your agent. Then set the following environment variables:\n\n:::python\n<CodeGroup>\n```bash bash\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n```\n```python python\nimport getpass\nimport os\n\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n```\n</CodeGroup>\n:::\n\n:::js\n<CodeGroup>\n```bash bash\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n```\n```typescript typescript\nprocess.env.LANGSMITH_TRACING = \"true\";\nprocess.env.LANGSMITH_API_KEY = \"...\";\n```\n</CodeGroup>\n:::\n\n### Select an LLM\n\nSelect a chat model from LangChain's suite of integrations:\n\n:::python\n<ChatModelTabsPy />\n:::\n\n:::js\n<ChatModelTabsJs />\n:::\n\n## 1. Define skills\n\nFirst, define the structure for skills. Each skill has a name, a brief description (shown in the system prompt), and full content (loaded on-demand):\n\n:::python\n```python\nfrom typing import TypedDict\n\nclass Skill(TypedDict):  # [!code highlight]\n", "metadata": {"source": "multi-agent/skills-sql-assistant.mdx"}}
{"text": "export LANGSMITH_API_KEY=\"...\"\n```\n```typescript typescript\nprocess.env.LANGSMITH_TRACING = \"true\";\nprocess.env.LANGSMITH_API_KEY = \"...\";\n```\n</CodeGroup>\n:::\n\n### Select an LLM\n\nSelect a chat model from LangChain's suite of integrations:\n\n:::python\n<ChatModelTabsPy />\n:::\n\n:::js\n<ChatModelTabsJs />\n:::\n\n## 1. Define skills\n\nFirst, define the structure for skills. Each skill has a name, a brief description (shown in the system prompt), and full content (loaded on-demand):\n\n:::python\n```python\nfrom typing import TypedDict\n\nclass Skill(TypedDict):  # [!code highlight]\n    \"\"\"A skill that can be progressively disclosed to the agent.\"\"\"\n    name: str  # Unique identifier for the skill\n    description: str  # 1-2 sentence description to show in system prompt\n    content: str  # Full skill content with detailed instructions\n```\n:::\n\n:::js\n```typescript\nimport { z } from \"zod\";\n\n// A skill that can be progressively disclosed to the agent\nconst SkillSchema = z.object({  // [!code highlight]\n  name: z.string(),  // Unique identifier for the skill\n  description: z.string(),  // 1-2 sentence description to show in system prompt\n  content: z.string(),  // Full skill content with detailed instructions\n});\n\ntype Skill = z.infer<typeof SkillSchema>;\n```\n:::\n\nNow define example skills for a SQL query assistant. The skills are designed to be **lightweight in description** (shown to the agent upfront) but **detailed in content** (loaded only when needed):\n\n<Accordion title=\"View complete skill definitions\">\n\n:::python\n```python\nSKILLS: list[Skill] = [\n    {\n        \"name\": \"sales_analytics\",\n        \"description\": \"Database schema and business logic for sales data analysis including customers, orders, and revenue.\",\n        \"content\": \"\"\"# Sales Analytics Schema\n\n## Tables\n\n### customers\n- customer_id (PRIMARY KEY)\n- name\n- email\n- signup_date\n- status (active/inactive)\n- customer_tier (bronze/silver/gold/platinum)\n\n### orders\n- order_id (PRIMARY KEY)\n- customer_id (FOREIGN KEY -> customers)\n- order_date\n- status (pending/completed/cancelled/refunded)\n- total_amount\n- sales_region (north/south/east/west)\n\n### order_items\n- item_id (PRIMARY KEY)\n- order_id (FOREIGN KEY -> orders)\n- product_id\n- quantity\n- unit_price\n- discount_percent\n\n## Business Logic\n\n**Active customers**: status = 'active' AND signup_date <= CURRENT_DATE - INTERVAL '90 days'\n\n**Revenue calculation**: Only count orders with status = 'completed'. Use total_amount from orders table, which already accounts for discounts.\n\n**Customer lifetime value (CLV)**: Sum of all completed order", "metadata": {"source": "multi-agent/skills-sql-assistant.mdx"}}
{"text": "\n\n### orders\n- order_id (PRIMARY KEY)\n- customer_id (FOREIGN KEY -> customers)\n- order_date\n- status (pending/completed/cancelled/refunded)\n- total_amount\n- sales_region (north/south/east/west)\n\n### order_items\n- item_id (PRIMARY KEY)\n- order_id (FOREIGN KEY -> orders)\n- product_id\n- quantity\n- unit_price\n- discount_percent\n\n## Business Logic\n\n**Active customers**: status = 'active' AND signup_date <= CURRENT_DATE - INTERVAL '90 days'\n\n**Revenue calculation**: Only count orders with status = 'completed'. Use total_amount from orders table, which already accounts for discounts.\n\n**Customer lifetime value (CLV)**: Sum of all completed order amounts for a customer.\n\n**High-value orders**: Orders with total_amount > 1000\n\n## Example Query\n\n-- Get top 10 customers by revenue in the last quarter\nSELECT\n    c.customer_id,\n    c.name,\n    c.customer_tier,\n    SUM(o.total_amount) as total_revenue\nFROM customers c\nJOIN orders o ON c.customer_id = o.customer_id\nWHERE o.status = 'completed'\n  AND o.order_date >= CURRENT_DATE - INTERVAL '3 months'\nGROUP BY c.customer_id, c.name, c.customer_tier\nORDER BY total_revenue DESC\nLIMIT 10;\n\"\"\",\n    },\n    {\n        \"name\": \"inventory_management\",\n        \"description\": \"Database schema and business logic for inventory tracking including products, warehouses, and stock levels.\",\n        \"content\": \"\"\"# Inventory Management Schema\n\n## Tables\n\n### products\n- product_id (PRIMARY KEY)\n- product_name\n- sku\n- category\n- unit_cost\n- reorder_point (minimum stock level before reordering)\n- discontinued (boolean)\n\n### warehouses\n- warehouse_id (PRIMARY KEY)\n- warehouse_name\n- location\n- capacity\n\n### inventory\n- inventory_id (PRIMARY KEY)\n- product_id (FOREIGN KEY -> products)\n- warehouse_id (FOREIGN KEY -> warehouses)\n- quantity_on_hand\n- last_updated\n\n### stock_movements\n- movement_id (PRIMARY KEY)\n- product_id (FOREIGN KEY -> products)\n- warehouse_id (FOREIGN KEY -> warehouses)\n- movement_type (inbound/outbound/transfer/adjustment)\n- quantity (positive for inbound, negative for outbound)\n- movement_date\n- reference_number\n\n## Business Logic\n\n**Available stock**: quantity_on_hand from inventory table where quantity_on_hand > 0\n\n**Products needing reorder**: Products where total quantity_on_hand across all warehouses is less than or equal to the product's reorder_point\n\n**Active products only**: Exclude products where discontinued = true unless specifically analyzing discontinued items\n\n**Stock valuation**: quantity_on_hand * unit_cost for each product\n\n## Example Query\n\n-- Find products below reorder point across all warehouses\nSELECT\n    p.product_id,", "metadata": {"source": "multi-agent/skills-sql-assistant.mdx"}}
{"text": " (PRIMARY KEY)\n- product_id (FOREIGN KEY -> products)\n- warehouse_id (FOREIGN KEY -> warehouses)\n- movement_type (inbound/outbound/transfer/adjustment)\n- quantity (positive for inbound, negative for outbound)\n- movement_date\n- reference_number\n\n## Business Logic\n\n**Available stock**: quantity_on_hand from inventory table where quantity_on_hand > 0\n\n**Products needing reorder**: Products where total quantity_on_hand across all warehouses is less than or equal to the product's reorder_point\n\n**Active products only**: Exclude products where discontinued = true unless specifically analyzing discontinued items\n\n**Stock valuation**: quantity_on_hand * unit_cost for each product\n\n## Example Query\n\n-- Find products below reorder point across all warehouses\nSELECT\n    p.product_id,\n    p.product_name,\n    p.reorder_point,\n    SUM(i.quantity_on_hand) as total_stock,\n    p.unit_cost,\n    (p.reorder_point - SUM(i.quantity_on_hand)) as units_to_reorder\nFROM products p\nJOIN inventory i ON p.product_id = i.product_id\nWHERE p.discontinued = false\nGROUP BY p.product_id, p.product_name, p.reorder_point, p.unit_cost\nHAVING SUM(i.quantity_on_hand) <= p.reorder_point\nORDER BY units_to_reorder DESC;\n\"\"\",\n    },\n]\n```\n:::\n\n:::js\n```typescript\nimport { context } from \"langchain\";\n\nconst SKILLS: Skill[] = [\n  {\n    name: \"sales_analytics\",\n    description:\n      \"Database schema and business logic for sales data analysis including customers, orders, and revenue.\",\n    content: context`\n    # Sales Analytics Schema\n\n    ## Tables\n\n    ### customers\n    - customer_id (PRIMARY KEY)\n    - name\n    - email\n    - signup_date\n    - status (active/inactive)\n    - customer_tier (bronze/silver/gold/platinum)\n\n    ### orders\n    - order_id (PRIMARY KEY)\n    - customer_id (FOREIGN KEY -> customers)\n    - order_date\n    - status (pending/completed/cancelled/refunded)\n    - total_amount\n    - sales_region (north/south/east/west)\n\n    ### order_items\n    - item_id (PRIMARY KEY)\n    - order_id (FOREIGN KEY -> orders)\n    - product_id\n    - quantity\n    - unit_price\n    - discount_percent\n\n    ## Business Logic\n\n    **Active customers**:\n    status = 'active' AND signup_date <= CURRENT_DATE - INTERVAL '90 days'\n\n    **Revenue calculation**:\n    Only count orders with status = 'completed'.\n    Use total_amount from orders table,", "metadata": {"source": "multi-agent/skills-sql-assistant.mdx"}}
{"text": "\n    - order_date\n    - status (pending/completed/cancelled/refunded)\n    - total_amount\n    - sales_region (north/south/east/west)\n\n    ### order_items\n    - item_id (PRIMARY KEY)\n    - order_id (FOREIGN KEY -> orders)\n    - product_id\n    - quantity\n    - unit_price\n    - discount_percent\n\n    ## Business Logic\n\n    **Active customers**:\n    status = 'active' AND signup_date <= CURRENT_DATE - INTERVAL '90 days'\n\n    **Revenue calculation**:\n    Only count orders with status = 'completed'.\n    Use total_amount from orders table, which already accounts for discounts.\n\n    **Customer lifetime value (CLV)**:\n    Sum of all completed order amounts for a customer.\n\n    **High-value orders**:\n    Orders with total_amount > 1000\n\n    ## Example Query\n\n    -- Get top 10 customers by revenue in the last quarter\n    SELECT\n        c.customer_id,\n        c.name,\n        c.customer_tier,\n        SUM(o.total_amount) as total_revenue\n    FROM customers c\n    JOIN orders o ON c.customer_id = o.customer_id\n    WHERE o.status = 'completed'\n    AND o.order_date >= CURRENT_DATE - INTERVAL '3 months'\n    GROUP BY c.customer_id, c.name, c.customer_tier\n    ORDER BY total_revenue DESC\n    LIMIT 10;`,\n  },\n  {\n    name: \"inventory_management\",\n    description:\n      \"Database schema and business logic for inventory tracking including products, warehouses, and stock levels.\",\n    content: context`\n    # Inventory Management Schema\n\n    ## Tables\n\n    ### products\n    - product_id (PRIMARY KEY)\n    - product_name\n    - sku\n    - category\n    - unit_cost\n    - reorder_point (minimum stock level before reordering)\n    - discontinued (boolean)\n\n    ### warehouses\n    - warehouse_id (PRIMARY KEY)\n    - warehouse_name\n    - location\n    - capacity\n\n    ### inventory\n    - inventory_id (PRIMARY KEY)\n    - product_id (FOREIGN KEY -> products)\n    - warehouse_id (FOREIGN KEY -> warehouses)\n    - quantity_on_hand\n    - last_updated\n\n    ### stock_movements\n    - movement_id (PRIMARY KEY)\n    - product_id (FOREIGN KEY -> products)\n    - warehouse_id (FOREIGN KEY -> warehouses)\n    - movement_type (inbound/outbound/transfer/adjustment)\n    - quantity (positive for inbound, negative for outbound)\n", "metadata": {"source": "multi-agent/skills-sql-assistant.mdx"}}
{"text": ")\n\n    ### warehouses\n    - warehouse_id (PRIMARY KEY)\n    - warehouse_name\n    - location\n    - capacity\n\n    ### inventory\n    - inventory_id (PRIMARY KEY)\n    - product_id (FOREIGN KEY -> products)\n    - warehouse_id (FOREIGN KEY -> warehouses)\n    - quantity_on_hand\n    - last_updated\n\n    ### stock_movements\n    - movement_id (PRIMARY KEY)\n    - product_id (FOREIGN KEY -> products)\n    - warehouse_id (FOREIGN KEY -> warehouses)\n    - movement_type (inbound/outbound/transfer/adjustment)\n    - quantity (positive for inbound, negative for outbound)\n    - movement_date\n    - reference_number\n\n    ## Business Logic\n\n    **Available stock**:\n    quantity_on_hand from inventory table where quantity_on_hand > 0\n\n    **Products needing reorder**:\n    Products where total quantity_on_hand across all warehouses is less\n    than or equal to the product's reorder_point\n\n    **Active products only**:\n    Exclude products where discontinued = true unless specifically analyzing discontinued items\n\n    **Stock valuation**:\n    quantity_on_hand * unit_cost for each product\n\n    ## Example Query\n\n    -- Find products below reorder point across all warehouses\n    SELECT\n        p.product_id,\n        p.product_name,\n        p.reorder_point,\n        SUM(i.quantity_on_hand) as total_stock,\n        p.unit_cost,\n        (p.reorder_point - SUM(i.quantity_on_hand)) as units_to_reorder\n    FROM products p\n    JOIN inventory i ON p.product_id = i.product_id\n    WHERE p.discontinued = false\n    GROUP BY p.product_id, p.product_name, p.reorder_point, p.unit_cost\n    HAVING SUM(i.quantity_on_hand) <= p.reorder_point\n    ORDER BY units_to_reorder DESC;`,\n  },\n];\n```\n:::\n\n</Accordion>\n\n## 2. Create skill loading tool\n\nCreate a tool to load full skill content on-demand:\n\n:::python\n```python\nfrom langchain.tools import tool\n\n@tool  # [!code highlight]\ndef load_skill(skill_name: str) -> str:\n    \"\"\"Load the full content of a skill into the agent's context.\n\n    Use this when you need detailed information about how to handle a specific\n    type of request. This will provide you with comprehensive instructions,\n    policies, and guidelines for the skill area.\n\n    Args:\n        skill_name: The name of the skill to load (e.g., \"expense_reporting\", \"travel_booking\")\n    \"\"\"\n    # Find and", "metadata": {"source": "multi-agent/skills-sql-assistant.mdx"}}
{"text": "\n];\n```\n:::\n\n</Accordion>\n\n## 2. Create skill loading tool\n\nCreate a tool to load full skill content on-demand:\n\n:::python\n```python\nfrom langchain.tools import tool\n\n@tool  # [!code highlight]\ndef load_skill(skill_name: str) -> str:\n    \"\"\"Load the full content of a skill into the agent's context.\n\n    Use this when you need detailed information about how to handle a specific\n    type of request. This will provide you with comprehensive instructions,\n    policies, and guidelines for the skill area.\n\n    Args:\n        skill_name: The name of the skill to load (e.g., \"expense_reporting\", \"travel_booking\")\n    \"\"\"\n    # Find and return the requested skill\n    for skill in SKILLS:\n        if skill[\"name\"] == skill_name:\n            return f\"Loaded skill: {skill_name}\\n\\n{skill['content']}\"  # [!code highlight]\n\n    # Skill not found\n    available = \", \".join(s[\"name\"] for s in SKILLS)\n    return f\"Skill '{skill_name}' not found. Available skills: {available}\"\n```\n:::\n\n:::js\n```typescript\nimport { tool } from \"langchain\";\nimport { z } from \"zod\";\n\nconst loadSkill = tool(  // [!code highlight]\n  async ({ skillName }) => {\n    // Find and return the requested skill\n    const skill = SKILLS.find((s) => s.name === skillName);\n    if (skill) {\n      return `Loaded skill: ${skillName}\\n\\n${skill.content}`;  // [!code highlight]\n    }\n\n    // Skill not found\n    const available = SKILLS.map((s) => s.name).join(\", \");\n    return `Skill '${skillName}' not found. Available skills: ${available}`;\n  },\n  {\n    name: \"load_skill\",\n    description: `Load the full content of a skill into the agent's context.\n\nUse this when you need detailed information about how to handle a specific\ntype of request. This will provide you with comprehensive instructions,\npolicies, and guidelines for the skill area.`,\n    schema: z.object({\n      skillName: z.string().describe(\"The name of the skill to load\"),\n    }),\n  }\n);\n```\n:::\n\nThe `load_skill` tool returns the full skill content as a string, which becomes part of the conversation as a ToolMessage. For more details on creating and using tools, see the [Tools guide](/oss/langchain/tools).\n\n## 3. Build skill middleware\n\nCreate custom middleware that injects skill descriptions into the system prompt. This middleware makes skills discoverable without loading their full content upfront.\n\n<Note>\nThis guide demonstrates creating custom middleware. For a comprehensive guide on middleware concepts and patterns, see the [custom middleware documentation](/oss/langchain/middleware/custom).\n</Note>\n\n:::python\n``", "metadata": {"source": "multi-agent/skills-sql-assistant.mdx"}}
{"text": " area.`,\n    schema: z.object({\n      skillName: z.string().describe(\"The name of the skill to load\"),\n    }),\n  }\n);\n```\n:::\n\nThe `load_skill` tool returns the full skill content as a string, which becomes part of the conversation as a ToolMessage. For more details on creating and using tools, see the [Tools guide](/oss/langchain/tools).\n\n## 3. Build skill middleware\n\nCreate custom middleware that injects skill descriptions into the system prompt. This middleware makes skills discoverable without loading their full content upfront.\n\n<Note>\nThis guide demonstrates creating custom middleware. For a comprehensive guide on middleware concepts and patterns, see the [custom middleware documentation](/oss/langchain/middleware/custom).\n</Note>\n\n:::python\n```python\nfrom langchain.agents.middleware import ModelRequest, ModelResponse, AgentMiddleware\nfrom langchain.messages import SystemMessage\nfrom typing import Callable\n\nclass SkillMiddleware(AgentMiddleware):  # [!code highlight]\n    \"\"\"Middleware that injects skill descriptions into the system prompt.\"\"\"\n\n    # Register the load_skill tool as a class variable\n    tools = [load_skill]  # [!code highlight]\n\n    def __init__(self):\n        \"\"\"Initialize and generate the skills prompt from SKILLS.\"\"\"\n        # Build skills prompt from the SKILLS list\n        skills_list = []\n        for skill in SKILLS:\n            skills_list.append(\n                f\"- **{skill['name']}**: {skill['description']}\"\n            )\n        self.skills_prompt = \"\\n\".join(skills_list)\n\n    def wrap_model_call(\n        self,\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse],\n    ) -> ModelResponse:\n        \"\"\"Sync: Inject skill descriptions into system prompt.\"\"\"\n        # Build the skills addendum\n        skills_addendum = ( # [!code highlight]\n            f\"\\n\\n## Available Skills\\n\\n{self.skills_prompt}\\n\\n\" # [!code highlight]\n            \"Use the load_skill tool when you need detailed information \" # [!code highlight]\n            \"about handling a specific type of request.\" # [!code highlight]\n        )\n\n        # Append to system message content blocks\n        new_content = list(request.system_message.content_blocks) + [\n            {\"type\": \"text\", \"text\": skills_addendum}\n        ]\n        new_system_message = SystemMessage(content=new_", "metadata": {"source": "multi-agent/skills-sql-assistant.mdx"}}
{"text": "    f\"\\n\\n## Available Skills\\n\\n{self.skills_prompt}\\n\\n\" # [!code highlight]\n            \"Use the load_skill tool when you need detailed information \" # [!code highlight]\n            \"about handling a specific type of request.\" # [!code highlight]\n        )\n\n        # Append to system message content blocks\n        new_content = list(request.system_message.content_blocks) + [\n            {\"type\": \"text\", \"text\": skills_addendum}\n        ]\n        new_system_message = SystemMessage(content=new_content)\n        modified_request = request.override(system_message=new_system_message)\n        return handler(modified_request)\n```\n:::\n\n:::js\n```typescript\nimport { createMiddleware } from \"langchain\";\n\n// Build skills prompt from the SKILLS list\nconst skillsPrompt = SKILLS.map(\n  (skill) => `- **${skill.name}**: ${skill.description}`\n).join(\"\\n\");\n\nconst skillMiddleware = createMiddleware({  // [!code highlight]\n  name: \"skillMiddleware\",\n  tools: [loadSkill],  // [!code highlight]\n  wrapModelCall: async (request, handler) => {\n    // Build the skills addendum\n    const skillsAddendum =  // [!code highlight]\n      `\\n\\n## Available Skills\\n\\n${skillsPrompt}\\n\\n` +  // [!code highlight]\n      \"Use the load_skill tool when you need detailed information \" +  // [!code highlight]\n      \"about handling a specific type of request.\";  // [!code highlight]\n\n    // Append to system prompt\n    const newSystemPrompt = request.systemPrompt + skillsAddendum;\n\n    return handler({\n      ...request,\n      systemPrompt: newSystemPrompt,\n    });\n  },\n});\n```\n:::\n\nThe middleware appends skill descriptions to the system prompt, making the agent aware of available skills without loading their full content. The `load_skill` tool is registered as a class variable, making it available to the agent.\n\n<Note>\n**Production consideration**: This tutorial loads the skill list in `__init__` for simplicity. In a production system, you may want to load skills in the `before_agent` hook instead, allowing them to be refreshed periodically to reflect up-to-date changes (e.g., when new skills are added or existing ones are modified). See the [before_agent hook documentation](/oss/langchain/middleware/custom#before_agent) for details.\n</Note>\n\n## 4. Create the agent with skill support\n\nNow create the agent with the skill middleware and a checkpointer for state persistence:\n\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n# Create the agent with skill support\n", "metadata": {"source": "multi-agent/skills-sql-assistant.mdx"}}
{"text": "load_skill` tool is registered as a class variable, making it available to the agent.\n\n<Note>\n**Production consideration**: This tutorial loads the skill list in `__init__` for simplicity. In a production system, you may want to load skills in the `before_agent` hook instead, allowing them to be refreshed periodically to reflect up-to-date changes (e.g., when new skills are added or existing ones are modified). See the [before_agent hook documentation](/oss/langchain/middleware/custom#before_agent) for details.\n</Note>\n\n## 4. Create the agent with skill support\n\nNow create the agent with the skill middleware and a checkpointer for state persistence:\n\n:::python\n```python\nfrom langchain.agents import create_agent\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n# Create the agent with skill support\nagent = create_agent(\n    model,\n    system_prompt=(\n        \"You are a SQL query assistant that helps users \"\n        \"write queries against business databases.\"\n    ),\n    middleware=[SkillMiddleware()],  # [!code highlight]\n    checkpointer=InMemorySaver(),\n)\n```\n:::\n\n:::js\n```typescript\nimport { createAgent } from \"langchain\";\nimport { MemorySaver } from \"@langchain/langgraph\";\n\n// Create the agent with skill support\nconst agent = createAgent({\n  model,\n  systemPrompt:\n    \"You are a SQL query assistant that helps users \" +\n    \"write queries against business databases.\",\n  middleware: [skillMiddleware],  // [!code highlight]\n  checkpointer: new MemorySaver(),\n});\n```\n:::\n\nThe agent now has access to skill descriptions in its system prompt and can call `load_skill` to retrieve full skill content when needed. The checkpointer maintains conversation history across turns.\n\n## 5. Test progressive disclosure\n\nTest the agent with a question that requires skill-specific knowledge:\n\n:::python\n```python\nimport uuid\n\n# Configuration for this conversation thread\nthread_id = str(uuid.uuid4())\nconfig = {\"configurable\": {\"thread_id\": thread_id}}\n\n# Ask for a SQL query\nresult = agent.invoke(  # [!code highlight]\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": (\n                    \"Write a SQL query to find all customers \"\n                    \"who made orders over $1000 in the last month\"\n                ),\n            }\n        ]\n    },\n    config\n)\n\n# Print the conversation\nfor message in result[\"messages\"]:\n    if hasattr(message, 'pretty_print'):\n        message.pretty_print()\n    else:\n  ", "metadata": {"source": "multi-agent/skills-sql-assistant.mdx"}}
{"text": "       \"role\": \"user\",\n                \"content\": (\n                    \"Write a SQL query to find all customers \"\n                    \"who made orders over $1000 in the last month\"\n                ),\n            }\n        ]\n    },\n    config\n)\n\n# Print the conversation\nfor message in result[\"messages\"]:\n    if hasattr(message, 'pretty_print'):\n        message.pretty_print()\n    else:\n        print(f\"{message.type}: {message.content}\")\n```\n:::\n\n:::js\n```typescript\nimport { v4 as uuidv4 } from \"uuid\";\n\n// Configuration for this conversation thread\nconst threadId = uuidv4();\nconst config = { configurable: { thread_id: threadId } };\n\n// Ask for a SQL query\nconst result = await agent.invoke(  // [!code highlight]\n  {\n    messages: [\n      {\n        role: \"user\",\n        content:\n          \"Write a SQL query to find all customers \" +\n          \"who made orders over $1000 in the last month\",\n      },\n    ],\n  },\n  config\n);\n\n// Print the conversation\nfor (const message of result.messages) {\n  console.log(`${message._getType()}: ${message.content}`);\n}\n```\n:::\n\nExpected output:\n\n```\n================================ Human Message =================================\n\nWrite a SQL query to find all customers who made orders over $1000 in the last month\n================================== Ai Message ==================================\nTool Calls:\n  load_skill (call_abc123)\n Call ID: call_abc123\n  Args:\n    skill_name: sales_analytics\n================================= Tool Message =================================\nName: load_skill\n\nLoaded skill: sales_analytics\n\n# Sales Analytics Schema\n\n## Tables\n\n### customers\n- customer_id (PRIMARY KEY)\n- name\n- email\n- signup_date\n- status (active/inactive)\n- customer_tier (bronze/silver/gold/platinum)\n\n### orders\n- order_id (PRIMARY KEY)\n- customer_id (FOREIGN KEY -> customers)\n- order_date\n- status (pending/completed/cancelled/refunded)\n- total_amount\n- sales_region (north/south/east/west)\n\n[... rest of schema ...]\n\n## Business Logic\n\n**High-value orders**: Orders with `total_amount > 1000`\n**Revenue calculation**: Only count orders with `status = 'completed'`\n\n================================== Ai Message ==================================\n\nHere's a SQL query to find all customers who made orders over $1000 in the last month:\n\n\\`\\`\\`sql\nSELECT DISTINCT\n    c", "metadata": {"source": "multi-agent/skills-sql-assistant.mdx"}}
{"text": " signup_date\n- status (active/inactive)\n- customer_tier (bronze/silver/gold/platinum)\n\n### orders\n- order_id (PRIMARY KEY)\n- customer_id (FOREIGN KEY -> customers)\n- order_date\n- status (pending/completed/cancelled/refunded)\n- total_amount\n- sales_region (north/south/east/west)\n\n[... rest of schema ...]\n\n## Business Logic\n\n**High-value orders**: Orders with `total_amount > 1000`\n**Revenue calculation**: Only count orders with `status = 'completed'`\n\n================================== Ai Message ==================================\n\nHere's a SQL query to find all customers who made orders over $1000 in the last month:\n\n\\`\\`\\`sql\nSELECT DISTINCT\n    c.customer_id,\n    c.name,\n    c.email,\n    c.customer_tier\nFROM customers c\nJOIN orders o ON c.customer_id = o.customer_id\nWHERE o.total_amount > 1000\n  AND o.status = 'completed'\n  AND o.order_date >= CURRENT_DATE - INTERVAL '1 month'\nORDER BY c.customer_id;\n\\`\\`\\`\n\nThis query:\n- Joins customers with their orders\n- Filters for high-value orders (>$1000) using the total_amount field\n- Only includes completed orders (as per the business logic)\n- Restricts to orders from the last month\n- Returns distinct customers to avoid duplicates if they made multiple qualifying orders\n```\n\nThe agent saw the lightweight skill description in its system prompt, recognized the question required sales database knowledge, called `load_skill(\"sales_analytics\")` to get the full schema and business logic, and then used that information to write a correct query following the database conventions.\n\n## 6. Advanced: Add constraints with custom state\n\n<Accordion title=\"Optional: Track loaded skills and enforce tool constraints\">\n\nYou can add constraints to enforce that certain tools are only available after specific skills have been loaded. This requires tracking which skills have been loaded in custom agent state.\n\n### Define custom state\n\nFirst, extend the agent state to track loaded skills:\n\n:::python\n```python\nfrom langchain.agents.middleware import AgentState\n\nclass CustomState(AgentState):  # [!code highlight]\n    skills_loaded: NotRequired[list[str]]  # Track which skills have been loaded  # [!code highlight]\n```\n:::\n\n:::js\n```typescript\nimport { StateSchema } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst CustomState = new StateSchema({\n  skillsLoaded: z.array(z.string()).optional(),  // Track which skills have been loaded  // [!code highlight]\n});\n```\n:::\n\n### Update load_skill to modify state\n\nModify the `load_skill` tool to update state when a skill is loaded:\n\n:::python\n```python\nfrom langgraph.types import Command  # [!code highlight]\nfrom langchain.tools import tool, ToolRuntime\nfrom langchain.messages import ToolMessage  # [!code highlight]\n\n@tool\ndef load_skill(skill_name: str, runtime: ToolRuntime) -> Command:  # [", "metadata": {"source": "multi-agent/skills-sql-assistant.mdx"}}
{"text": "`\n:::\n\n:::js\n```typescript\nimport { StateSchema } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst CustomState = new StateSchema({\n  skillsLoaded: z.array(z.string()).optional(),  // Track which skills have been loaded  // [!code highlight]\n});\n```\n:::\n\n### Update load_skill to modify state\n\nModify the `load_skill` tool to update state when a skill is loaded:\n\n:::python\n```python\nfrom langgraph.types import Command  # [!code highlight]\nfrom langchain.tools import tool, ToolRuntime\nfrom langchain.messages import ToolMessage  # [!code highlight]\n\n@tool\ndef load_skill(skill_name: str, runtime: ToolRuntime) -> Command:  # [!code highlight]\n    \"\"\"Load the full content of a skill into the agent's context.\n\n    Use this when you need detailed information about how to handle a specific\n    type of request. This will provide you with comprehensive instructions,\n    policies, and guidelines for the skill area.\n\n    Args:\n        skill_name: The name of the skill to load\n    \"\"\"\n    # Find and return the requested skill\n    for skill in SKILLS:\n        if skill[\"name\"] == skill_name:\n            skill_content = f\"Loaded skill: {skill_name}\\n\\n{skill['content']}\"\n\n            # Update state to track loaded skill\n            return Command(  # [!code highlight]\n                update={  # [!code highlight]\n                    \"messages\": [  # [!code highlight]\n                        ToolMessage(  # [!code highlight]\n                            content=skill_content,  # [!code highlight]\n                            tool_call_id=runtime.tool_call_id,  # [!code highlight]\n                        )  # [!code highlight]\n                    ],  # [!code highlight]\n                    \"skills_loaded\": [skill_name],  # [!code highlight]\n                }  # [!code highlight]\n            )  # [!code highlight]\n\n    # Skill not found\n    available = \", \".join(s[\"name\"] for s in SKILLS)\n    return Command(\n        update={\n            \"messages", "metadata": {"source": "multi-agent/skills-sql-assistant.mdx"}}
{"text": "                      )  # [!code highlight]\n                    ],  # [!code highlight]\n                    \"skills_loaded\": [skill_name],  # [!code highlight]\n                }  # [!code highlight]\n            )  # [!code highlight]\n\n    # Skill not found\n    available = \", \".join(s[\"name\"] for s in SKILLS)\n    return Command(\n        update={\n            \"messages\": [\n                ToolMessage(\n                    content=f\"Skill '{skill_name}' not found. Available skills: {available}\",\n                    tool_call_id=runtime.tool_call_id,\n                )\n            ]\n        }\n    )\n```\n:::\n\n:::js\n```typescript\nimport { tool, ToolMessage, type ToolRuntime } from \"langchain\";\nimport { Command } from \"@langchain/langgraph\";  // [!code highlight]\nimport { z } from \"zod\";\n\nconst loadSkill = tool(  // [!code highlight]\n  async ({ skillName }, runtime: ToolRuntime<typeof CustomState.State>) => {\n    // Find and return the requested skill\n    const skill = SKILLS.find((s) => s.name === skillName);\n\n    if (skill) {\n      const skillContent = `Loaded skill: ${skillName}\\n\\n${skill.content}`;\n\n      // Update state to track loaded skill\n      return new Command({  // [!code highlight]\n        update: {  // [!code highlight]\n          messages: [  // [!code highlight]\n            new ToolMessage({  // [!code highlight]\n              content: skillContent,  // [!code highlight]\n              tool_call_id: runtime.toolCallId,  // [!code highlight]\n            }),  // [!code highlight]\n          ],  // [!code highlight]\n          skillsLoaded: [skillName],  // [!code highlight]\n        },  // [!code highlight]\n      });  // [!code highlight]\n    }\n\n    // Skill not found\n    const available = SKILLS.map((s) => s.name).join(\", \");\n    return", "metadata": {"source": "multi-agent/skills-sql-assistant.mdx"}}
{"text": "!code highlight]\n              content: skillContent,  // [!code highlight]\n              tool_call_id: runtime.toolCallId,  // [!code highlight]\n            }),  // [!code highlight]\n          ],  // [!code highlight]\n          skillsLoaded: [skillName],  // [!code highlight]\n        },  // [!code highlight]\n      });  // [!code highlight]\n    }\n\n    // Skill not found\n    const available = SKILLS.map((s) => s.name).join(\", \");\n    return new Command({\n      update: {\n        messages: [\n          new ToolMessage({\n            content: `Skill '${skillName}' not found. Available skills: ${available}`,\n            tool_call_id: runtime.toolCallId,\n          }),\n        ],\n      },\n    });\n  },\n  {\n    name: \"load_skill\",\n    description: `Load the full content of a skill into the agent's context.`,\n    schema: z.object({\n      skillName: z.string().describe(\"The name of the skill to load\"),\n    }),\n  }\n);\n```\n:::\n\n### Create constrained tool\n\nCreate a tool that's only usable after a specific skill has been loaded:\n\n:::python\n```python\n@tool\ndef write_sql_query(  # [!code highlight]\n    query: str,\n    vertical: str,\n    runtime: ToolRuntime,\n) -> str:\n    \"\"\"Write and validate a SQL query for a specific business vertical.\n\n    This tool helps format and validate SQL queries. You must load the\n    appropriate skill first to understand the database schema.\n\n    Args:\n        query: The SQL query to write\n        vertical: The business vertical (sales_analytics or inventory_management)\n    \"\"\"\n    # Check if the required skill has been loaded\n    skills_loaded = runtime.state.get(\"skills_loaded\", [])  # [!code highlight]\n\n    if vertical not in skills_loaded:  # [!code highlight]\n        return (  # [!code highlight]\n            f\"Error: You must load the '{vertical}' skill first \"  # [!code highlight]\n            f\"to understand the database schema before writing queries. \"  # [!code highlight]\n            f\"Use load_skill('{vertical}') to load the schema.\"  # [!code highlight]\n        )  # [!code highlight]\n\n    # Validate and format the query", "metadata": {"source": "multi-agent/skills-sql-assistant.mdx"}}
{"text": " the required skill has been loaded\n    skills_loaded = runtime.state.get(\"skills_loaded\", [])  # [!code highlight]\n\n    if vertical not in skills_loaded:  # [!code highlight]\n        return (  # [!code highlight]\n            f\"Error: You must load the '{vertical}' skill first \"  # [!code highlight]\n            f\"to understand the database schema before writing queries. \"  # [!code highlight]\n            f\"Use load_skill('{vertical}') to load the schema.\"  # [!code highlight]\n        )  # [!code highlight]\n\n    # Validate and format the query\n    return (\n        f\"SQL Query for {vertical}:\\n\\n\"\n        f\"```sql\\n{query}\\n```\\n\\n\"\n        f\"\u2713 Query validated against {vertical} schema\\n\"\n        f\"Ready to execute against the database.\"\n    )\n```\n:::\n\n:::js\n```typescript\nconst writeSqlQuery = tool(  // [!code highlight]\n  async ({ query, vertical }, runtime: ToolRuntime<typeof CustomState.State>) => {\n    // Check if the required skill has been loaded\n    const skillsLoaded = runtime.state.skillsLoaded ?? [];  // [!code highlight]\n\n    if (!skillsLoaded.includes(vertical)) {  // [!code highlight]\n      return (  // [!code highlight]\n        `Error: You must load the '${vertical}' skill first ` +  // [!code highlight]\n        `to understand the database schema before writing queries. ` +  // [!code highlight]\n        `Use load_skill('${vertical}') to load the schema.`  // [!code highlight]\n      );  // [!code highlight]\n    }\n\n    // Validate and format the query\n    return (\n      `SQL Query for ${vertical}:\\n\\n` +\n      `\\`\\`\\`sql\\n${query}\\n\\`\\`\\`\\n\\n` +\n      `\u2713 Query validated against ${vertical} schema\\n` +\n      `Ready to execute against the database.`\n    );\n  },\n  {\n    name: \"write_sql_query\",\n    description: `Write and validate a SQL query for a specific business vertical.\n\nThis tool helps format and validate SQL queries. You must load the\nappropriate skill first to understand the database schema.`,\n    schema: z.object({\n      query: z.string().describe(\"The SQL query to write\"),\n      vertical: z.string().describe(\"The business vertical (sales_analytics or inventory_management)\"),\n    }),\n  }\n);\n```\n:::\n\n### Update middleware", "metadata": {"source": "multi-agent/skills-sql-assistant.mdx"}}
{"text": "`\\`\\`\\n\\n` +\n      `\u2713 Query validated against ${vertical} schema\\n` +\n      `Ready to execute against the database.`\n    );\n  },\n  {\n    name: \"write_sql_query\",\n    description: `Write and validate a SQL query for a specific business vertical.\n\nThis tool helps format and validate SQL queries. You must load the\nappropriate skill first to understand the database schema.`,\n    schema: z.object({\n      query: z.string().describe(\"The SQL query to write\"),\n      vertical: z.string().describe(\"The business vertical (sales_analytics or inventory_management)\"),\n    }),\n  }\n);\n```\n:::\n\n### Update middleware and agent\n\nUpdate the middleware to use the custom state schema:\n\n:::python\n```python\nclass SkillMiddleware(AgentMiddleware[CustomState]):  # [!code highlight]\n    \"\"\"Middleware that injects skill descriptions into the system prompt.\"\"\"\n\n    state_schema = CustomState  # [!code highlight]\n    tools = [load_skill, write_sql_query]  # [!code highlight]\n\n    # ... rest of the middleware implementation stays the same\n```\n:::\n\n:::js\n```typescript\nconst skillMiddleware = createMiddleware({  // [!code highlight]\n  name: \"skillMiddleware\",\n  stateSchema: CustomState,  // [!code highlight]\n  tools: [loadSkill, writeSqlQuery],  // [!code highlight]\n  // ... rest of the middleware implementation stays the same\n});\n```\n:::\n\nCreate the agent with the middleware that registers the constrained tool:\n\n:::python\n```python\nagent = create_agent(\n    model,\n    system_prompt=(\n        \"You are a SQL query assistant that helps users \"\n        \"write queries against business databases.\"\n    ),\n    middleware=[SkillMiddleware()],  # [!code highlight]\n    checkpointer=InMemorySaver(),\n)\n```\n:::\n\n:::js\n```typescript\nconst agent = createAgent({\n  model,\n  systemPrompt:\n    \"You are a SQL query assistant that helps users \" +\n    \"write queries against business databases.\",\n  middleware: [skillMiddleware],  // [!code highlight]\n  checkpointer: new MemorySaver(),\n});\n```\n:::\n\nNow if the agent tries to use `write_sql_query` before loading the required skill, it will receive an error message prompting it to load the appropriate skill (e.g., `sales_analytics` or `inventory_management`) first. This ensures the agent has the necessary schema knowledge before attempting to validate queries.\n\n</Accordion>\n\n## Complete example\n\n<Accordion title=\"View complete runnable script\">\n\nHere's a complete, runnable implementation combining all the pieces from this tutorial:\n\n:::python\n```python\nimport uuid\nfrom typing import TypedDict, NotRequired\nfrom langchain.tools import tool\nfrom langchain.agents import create_agent\nfrom langchain.agents.", "metadata": {"source": "multi-agent/skills-sql-assistant.mdx"}}
{"text": "  middleware: [skillMiddleware],  // [!code highlight]\n  checkpointer: new MemorySaver(),\n});\n```\n:::\n\nNow if the agent tries to use `write_sql_query` before loading the required skill, it will receive an error message prompting it to load the appropriate skill (e.g., `sales_analytics` or `inventory_management`) first. This ensures the agent has the necessary schema knowledge before attempting to validate queries.\n\n</Accordion>\n\n## Complete example\n\n<Accordion title=\"View complete runnable script\">\n\nHere's a complete, runnable implementation combining all the pieces from this tutorial:\n\n:::python\n```python\nimport uuid\nfrom typing import TypedDict, NotRequired\nfrom langchain.tools import tool\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import ModelRequest, ModelResponse, AgentMiddleware\nfrom langchain.messages import SystemMessage\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom typing import Callable\n\n# Define skill structure\nclass Skill(TypedDict):\n    \"\"\"A skill that can be progressively disclosed to the agent.\"\"\"\n    name: str\n    description: str\n    content: str\n\n# Define skills with schemas and business logic\nSKILLS: list[Skill] = [\n    {\n        \"name\": \"sales_analytics\",\n        \"description\": \"Database schema and business logic for sales data analysis including customers, orders, and revenue.\",\n        \"content\": \"\"\"# Sales Analytics Schema\n\n## Tables\n\n### customers\n- customer_id (PRIMARY KEY)\n- name\n- email\n- signup_date\n- status (active/inactive)\n- customer_tier (bronze/silver/gold/platinum)\n\n### orders\n- order_id (PRIMARY KEY)\n- customer_id (FOREIGN KEY -> customers)\n- order_date\n- status (pending/completed/cancelled/refunded)\n- total_amount\n- sales_region (north/south/east/west)\n\n### order_items\n- item_id (PRIMARY KEY)\n- order_id (FOREIGN KEY -> orders)\n- product_id\n- quantity\n- unit_price\n- discount_percent\n\n## Business Logic\n\n**Active customers**: status = 'active' AND signup_date <= CURRENT_DATE - INTERVAL '90 days'\n\n**Revenue calculation**: Only count orders with status = 'completed'. Use total_amount from orders table, which already accounts for discounts.\n\n**Customer lifetime value (CLV)**: Sum of all completed order amounts for a customer.\n\n**High-value orders**: Orders with total_amount > 1000\n\n## Example Query\n\n-- Get top 10 customers by revenue in the last quarter\nSELECT\n    c.customer_id,\n    c.name,\n    c.customer_tier,\n    SUM(o.total_amount) as total_revenue\nFROM customers c\nJOIN orders o ON c.customer_id = o.customer_id\nWHERE o.status = 'completed'\n  AND o.order_date >= CURRENT_DATE - INTERVAL '3 months'\nGROUP BY c.customer_id, c.", "metadata": {"source": "multi-agent/skills-sql-assistant.mdx"}}
{"text": "venue calculation**: Only count orders with status = 'completed'. Use total_amount from orders table, which already accounts for discounts.\n\n**Customer lifetime value (CLV)**: Sum of all completed order amounts for a customer.\n\n**High-value orders**: Orders with total_amount > 1000\n\n## Example Query\n\n-- Get top 10 customers by revenue in the last quarter\nSELECT\n    c.customer_id,\n    c.name,\n    c.customer_tier,\n    SUM(o.total_amount) as total_revenue\nFROM customers c\nJOIN orders o ON c.customer_id = o.customer_id\nWHERE o.status = 'completed'\n  AND o.order_date >= CURRENT_DATE - INTERVAL '3 months'\nGROUP BY c.customer_id, c.name, c.customer_tier\nORDER BY total_revenue DESC\nLIMIT 10;\n\"\"\",\n    },\n    {\n        \"name\": \"inventory_management\",\n        \"description\": \"Database schema and business logic for inventory tracking including products, warehouses, and stock levels.\",\n        \"content\": \"\"\"# Inventory Management Schema\n\n## Tables\n\n### products\n- product_id (PRIMARY KEY)\n- product_name\n- sku\n- category\n- unit_cost\n- reorder_point (minimum stock level before reordering)\n- discontinued (boolean)\n\n### warehouses\n- warehouse_id (PRIMARY KEY)\n- warehouse_name\n- location\n- capacity\n\n### inventory\n- inventory_id (PRIMARY KEY)\n- product_id (FOREIGN KEY -> products)\n- warehouse_id (FOREIGN KEY -> warehouses)\n- quantity_on_hand\n- last_updated\n\n### stock_movements\n- movement_id (PRIMARY KEY)\n- product_id (FOREIGN KEY -> products)\n- warehouse_id (FOREIGN KEY -> warehouses)\n- movement_type (inbound/outbound/transfer/adjustment)\n- quantity (positive for inbound, negative for outbound)\n- movement_date\n- reference_number\n\n## Business Logic\n\n**Available stock**: quantity_on_hand from inventory table where quantity_on_hand > 0\n\n**Products needing reorder**: Products where total quantity_on_hand across all warehouses is less than or equal to the product's reorder_point\n\n**Active products only**: Exclude products where discontinued = true unless specifically analyzing discontinued items\n\n**Stock valuation**: quantity_on_hand * unit_cost for each product\n\n## Example Query\n\n-- Find products below reorder point across all warehouses\nSELECT\n    p.product_id,\n    p.product_name,\n    p.reorder_point,\n    SUM(i.quantity_on_hand) as total_stock,\n    p.unit_cost,\n    (p.reorder_point - SUM(i.quantity_on_hand)) as units_to_reorder\nFROM products p\nJOIN inventory i ON p.product_id = i.product_id\nWHERE p.discontinued = false\nGROUP BY p.product_id, p.product_name, p.reorder_point, p.unit_cost\nHAVING SUM(i.quantity_on_hand)", "metadata": {"source": "multi-agent/skills-sql-assistant.mdx"}}
{"text": "Stock valuation**: quantity_on_hand * unit_cost for each product\n\n## Example Query\n\n-- Find products below reorder point across all warehouses\nSELECT\n    p.product_id,\n    p.product_name,\n    p.reorder_point,\n    SUM(i.quantity_on_hand) as total_stock,\n    p.unit_cost,\n    (p.reorder_point - SUM(i.quantity_on_hand)) as units_to_reorder\nFROM products p\nJOIN inventory i ON p.product_id = i.product_id\nWHERE p.discontinued = false\nGROUP BY p.product_id, p.product_name, p.reorder_point, p.unit_cost\nHAVING SUM(i.quantity_on_hand) <= p.reorder_point\nORDER BY units_to_reorder DESC;\n\"\"\",\n    },\n]\n\n# Create skill loading tool\n@tool\ndef load_skill(skill_name: str) -> str:\n    \"\"\"Load the full content of a skill into the agent's context.\n\n    Use this when you need detailed information about how to handle a specific\n    type of request. This will provide you with comprehensive instructions,\n    policies, and guidelines for the skill area.\n\n    Args:\n        skill_name: The name of the skill to load (e.g., \"sales_analytics\", \"inventory_management\")\n    \"\"\"\n    # Find and return the requested skill\n    for skill in SKILLS:\n        if skill[\"name\"] == skill_name:\n            return f\"Loaded skill: {skill_name}\\n\\n{skill['content']}\"\n\n    # Skill not found\n    available = \", \".join(s[\"name\"] for s in SKILLS)\n    return f\"Skill '{skill_name}' not found. Available skills: {available}\"\n\n# Create skill middleware\nclass SkillMiddleware(AgentMiddleware):\n    \"\"\"Middleware that injects skill descriptions into the system prompt.\"\"\"\n\n    # Register the load_skill tool as a class variable\n    tools = [load_skill]\n\n    def __init__(self):\n        \"\"\"Initialize and generate the skills prompt from SKILLS.\"\"\"\n        # Build skills prompt from the SKILLS list\n        skills_list = []\n        for skill in SKILLS:\n            skills_list.append(\n                f\"- **{skill['name']}**: {skill['description']}\"\n            )\n        self.skills_prompt = \"\\n\".join(skills_list)\n\n    def wrap_model_call(\n        self,\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse],\n    ) -> ModelResponse:\n        \"\"\"Sync: Inject skill descriptions", "metadata": {"source": "multi-agent/skills-sql-assistant.mdx"}}
{"text": " the SKILLS list\n        skills_list = []\n        for skill in SKILLS:\n            skills_list.append(\n                f\"- **{skill['name']}**: {skill['description']}\"\n            )\n        self.skills_prompt = \"\\n\".join(skills_list)\n\n    def wrap_model_call(\n        self,\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse],\n    ) -> ModelResponse:\n        \"\"\"Sync: Inject skill descriptions into system prompt.\"\"\"\n        # Build the skills addendum\n        skills_addendum = (\n            f\"\\n\\n## Available Skills\\n\\n{self.skills_prompt}\\n\\n\"\n            \"Use the load_skill tool when you need detailed information \"\n            \"about handling a specific type of request.\"\n        )\n\n        # Append to system message content blocks\n        new_content = list(request.system_message.content_blocks) + [\n            {\"type\": \"text\", \"text\": skills_addendum}\n        ]\n        new_system_message = SystemMessage(content=new_content)\n        modified_request = request.override(system_message=new_system_message)\n        return handler(modified_request)\n\n# Initialize your chat model (replace with your model)\n# Example: from langchain_anthropic import ChatAnthropic\n# model = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\")\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model=\"gpt-4\")\n\n# Create the agent with skill support\nagent = create_agent(\n    model,\n    system_prompt=(\n        \"You are a SQL query assistant that helps users \"\n        \"write queries against business databases.\"\n    ),\n    middleware=[SkillMiddleware()],\n    checkpointer=InMemorySaver(),\n)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Configuration for this conversation thread\n    thread_id = str(uuid.uuid4())\n    config = {\"configurable\": {\"thread_id\": thread_id}}\n\n    # Ask for a SQL query\n    result = agent.invoke(\n        {\n            \"messages\": [\n                {\n                    \"role\": \"user\",\n   ", "metadata": {"source": "multi-agent/skills-sql-assistant.mdx"}}
{"text": "        \"write queries against business databases.\"\n    ),\n    middleware=[SkillMiddleware()],\n    checkpointer=InMemorySaver(),\n)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Configuration for this conversation thread\n    thread_id = str(uuid.uuid4())\n    config = {\"configurable\": {\"thread_id\": thread_id}}\n\n    # Ask for a SQL query\n    result = agent.invoke(\n        {\n            \"messages\": [\n                {\n                    \"role\": \"user\",\n                    \"content\": (\n                        \"Write a SQL query to find all customers \"\n                        \"who made orders over $1000 in the last month\"\n                    ),\n                }\n            ]\n        },\n        config\n    )\n\n    # Print the conversation\n    for message in result[\"messages\"]:\n        if hasattr(message, 'pretty_print'):\n            message.pretty_print()\n        else:\n            print(f\"{message.type}: {message.content}\")\n```\n:::\n\n:::js\n```typescript\nimport {\n  tool,\n  createAgent,\n  createMiddleware,\n  ToolMessage,\n  context,\n  type ToolRuntime,\n} from \"langchain\";\nimport { MemorySaver, Command } from \"@langchain/langgraph\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { v4 as uuidv4 } from \"uuid\";\nimport { z } from \"zod\";\n\n// A skill that can be progressively disclosed to the agent\nconst SkillSchema = z.object({\n  name: z.string(), // Unique identifier for the skill\n  description: z.string(), // 1-2 sentence description to show in system prompt\n  content: z.string(), // Full skill content with detailed instructions\n});\n\ntype Skill = z.infer<typeof SkillSchema>;\n\nconst SKILLS: Skill[] = [\n  {\n    name: \"sales_analytics\",\n    description:\n      \"Database schema and business logic for sales data analysis including customers, orders, and revenue.\",\n    content: context`\n    # Sales Analytics Schema\n\n    ## Tables\n\n    ### customers\n    - customer_id (PRIMARY KEY)\n    - name\n    - email\n    - signup_date\n    - status (active/inactive)\n ", "metadata": {"source": "multi-agent/skills-sql-assistant.mdx"}}
{"text": " name: z.string(), // Unique identifier for the skill\n  description: z.string(), // 1-2 sentence description to show in system prompt\n  content: z.string(), // Full skill content with detailed instructions\n});\n\ntype Skill = z.infer<typeof SkillSchema>;\n\nconst SKILLS: Skill[] = [\n  {\n    name: \"sales_analytics\",\n    description:\n      \"Database schema and business logic for sales data analysis including customers, orders, and revenue.\",\n    content: context`\n    # Sales Analytics Schema\n\n    ## Tables\n\n    ### customers\n    - customer_id (PRIMARY KEY)\n    - name\n    - email\n    - signup_date\n    - status (active/inactive)\n    - customer_tier (bronze/silver/gold/platinum)\n\n    ### orders\n    - order_id (PRIMARY KEY)\n    - customer_id (FOREIGN KEY -> customers)\n    - order_date\n    - status (pending/completed/cancelled/refunded)\n    - total_amount\n    - sales_region (north/south/east/west)\n\n    ### order_items\n    - item_id (PRIMARY KEY)\n    - order_id (FOREIGN KEY -> orders)\n    - product_id\n    - quantity\n    - unit_price\n    - discount_percent\n\n    ## Business Logic\n\n    **Active customers**: status = 'active' AND signup_date <= CURRENT_DATE - INTERVAL '90 days'\n\n    **Revenue calculation**:\n    Only count orders with status = 'completed'. Use total_amount from orders table,\n    which already accounts for discounts.\n\n    **Customer lifetime value (CLV)**:\n    Sum of all completed order amounts for a customer.\n\n    **High-value orders**:\n    Orders with total_amount > 1000\n\n    ## Example Query\n    -- Get top 10 customers by revenue in the last quarter\n    SELECT\n        c.customer_id,\n        c.name,\n        c.customer_tier,\n        SUM(o.total_amount) as total_revenue\n    FROM customers c\n    JOIN orders o ON c.customer_id = o.customer_id\n    WHERE o.status = 'completed'\n    AND o.order_date >= CURRENT_DATE - INTERVAL '3 months'\n    GROUP BY c.customer_id, c.name, c.customer_tier\n    ORDER BY total_revenue DESC\n    LIMIT 10;`,\n  },\n  {\n    name: \"inventory_management\",\n    description:\n      \"Database schema and business logic for inventory tracking including products, warehouses, and stock levels.\",\n    content: context`\n    # Inventory Management Schema\n\n    ## Tables\n\n    ### products\n    - product_id (PRIMARY KEY)\n    - product", "metadata": {"source": "multi-agent/skills-sql-assistant.mdx"}}
{"text": " customers c\n    JOIN orders o ON c.customer_id = o.customer_id\n    WHERE o.status = 'completed'\n    AND o.order_date >= CURRENT_DATE - INTERVAL '3 months'\n    GROUP BY c.customer_id, c.name, c.customer_tier\n    ORDER BY total_revenue DESC\n    LIMIT 10;`,\n  },\n  {\n    name: \"inventory_management\",\n    description:\n      \"Database schema and business logic for inventory tracking including products, warehouses, and stock levels.\",\n    content: context`\n    # Inventory Management Schema\n\n    ## Tables\n\n    ### products\n    - product_id (PRIMARY KEY)\n    - product_name\n    - sku\n    - category\n    - unit_cost\n    - reorder_point (minimum stock level before reordering)\n    - discontinued (boolean)\n\n    ### warehouses\n    - warehouse_id (PRIMARY KEY)\n    - warehouse_name\n    - location\n    - capacity\n\n    ### inventory\n    - inventory_id (PRIMARY KEY)\n    - product_id (FOREIGN KEY -> products)\n    - warehouse_id (FOREIGN KEY -> warehouses)\n    - quantity_on_hand\n    - last_updated\n\n    ### stock_movements\n    - movement_id (PRIMARY KEY)\n    - product_id (FOREIGN KEY -> products)\n    - warehouse_id (FOREIGN KEY -> warehouses)\n    - movement_type (inbound/outbound/transfer/adjustment)\n    - quantity (positive for inbound, negative for outbound)\n    - movement_date\n    - reference_number\n\n    ## Business Logic\n\n    **Available stock**:\n    quantity_on_hand from inventory table where quantity_on_hand > 0\n\n    **Products needing reorder**:\n    Products where total quantity_on_hand across all warehouses is\n    less than or equal to the product's reorder_point\n\n    **Active products only**:\n    Exclude products where discontinued = true unless specifically\n    analyzing discontinued items\n\n    **Stock valuation**:\n    quantity_on_hand * unit_cost for each product\n\n    ## Example Query\n\n    -- Find products below reorder point across all warehouses\n    SELECT\n        p.product_id,\n        p.product_name,\n        p.reorder_point,\n        SUM(i.quantity_on_hand) as total_stock,\n        p.unit_cost,\n        (p.reorder_point - SUM(i.quantity_on_hand)) as units_to_reorder\n    FROM products p\n    JOIN inventory i ON p.product_id = i.product_id\n    WHERE p.discontinued = false\n    GROUP BY p.product_id, p.product_name, p.re", "metadata": {"source": "multi-agent/skills-sql-assistant.mdx"}}
{"text": "   ## Example Query\n\n    -- Find products below reorder point across all warehouses\n    SELECT\n        p.product_id,\n        p.product_name,\n        p.reorder_point,\n        SUM(i.quantity_on_hand) as total_stock,\n        p.unit_cost,\n        (p.reorder_point - SUM(i.quantity_on_hand)) as units_to_reorder\n    FROM products p\n    JOIN inventory i ON p.product_id = i.product_id\n    WHERE p.discontinued = false\n    GROUP BY p.product_id, p.product_name, p.reorder_point, p.unit_cost\n    HAVING SUM(i.quantity_on_hand) <= p.reorder_point\n    ORDER BY units_to_reorder DESC;`,\n  },\n];\n\n// const loadSkill = tool(\n//   async ({ skillName }) => {\n//     // Find and return the requested skill\n//     const skill = SKILLS.find((s) => s.name === skillName);\n//     if (skill) {\n//       return `Loaded skill: ${skillName}\\n\\n${skill.content}`;\n//     }\n\n//     // Skill not found\n//     const available = SKILLS.map((s) => s.name).join(\", \");\n//     return `Skill '${skillName}' not found. Available skills: ${available}`;\n//   },\n//   {\n//     name: \"load_skill\",\n//     description: `Load the full content of a skill into the agent's context.\n\n// Use this when you need detailed information about how to handle a specific\n// type of request. This will provide you with comprehensive instructions,\n// policies, and guidelines for the skill area.`,\n//     schema: z.object({\n//       skillName: z.string().describe(\"The name of the skill to load\"),\n//     }),\n//   }\n// );\n\n// Build skills prompt from the SKILLS list\nconst skillsPrompt = SKILLS.map(\n  (skill) => `- **${skill.name}**: ${skill.description}`\n).join(\"\\n\");\n\nconst skillMiddleware = createMiddleware({\n  name: \"skillMiddleware\",\n  tools: [loadSkill],\n  wrapModelCall: async (request, handler) => {\n    // Build the skills addendum\n    const skillsAddendum =\n      `\\n\\n## Available Skills\\n\\n${skillsPrompt}\\n\\n` +\n      \"Use the load_skill tool when you need detailed information \" +\n      \"about handling a specific type of request.\";\n\n    // Append to system prompt\n    const newSystemPrompt = request.systemPrompt + skillsAddendum;\n\n    return handler({\n      ...request,\n      systemPrompt: new", "metadata": {"source": "multi-agent/skills-sql-assistant.mdx"}}
{"text": "name}**: ${skill.description}`\n).join(\"\\n\");\n\nconst skillMiddleware = createMiddleware({\n  name: \"skillMiddleware\",\n  tools: [loadSkill],\n  wrapModelCall: async (request, handler) => {\n    // Build the skills addendum\n    const skillsAddendum =\n      `\\n\\n## Available Skills\\n\\n${skillsPrompt}\\n\\n` +\n      \"Use the load_skill tool when you need detailed information \" +\n      \"about handling a specific type of request.\";\n\n    // Append to system prompt\n    const newSystemPrompt = request.systemPrompt + skillsAddendum;\n\n    return handler({\n      ...request,\n      systemPrompt: newSystemPrompt,\n    });\n  },\n});\n\nconst model = new ChatOpenAI({\n  model: \"gpt-4.1-mini\",\n  temperature: 0,\n});\n\n// Create the agent with skill support\nconst agent = createAgent({\n  model,\n  systemPrompt:\n    \"You are a SQL query assistant that helps users \" +\n    \"write queries against business databases.\",\n  middleware: [skillMiddleware],\n  checkpointer: new MemorySaver(),\n});\n\n// Configuration for this conversation thread\nconst threadId = uuidv4();\nconst config = { configurable: { thread_id: threadId } };\n\n// Ask for a SQL query\nconst result = await agent.invoke(\n  {\n    messages: [\n      {\n        role: \"user\",\n        content:\n          \"Write a SQL query to find all customers \" +\n          \"who made orders over $1000 in the last month\",\n      },\n    ],\n  },\n  config\n);\n\n// Print the conversation\nfor (const message of result.messages) {\n  console.log(`${message.type}: ${message.content}`);\n}\n```\n:::\n\nThis complete example includes:\n- Skill definitions with full database schemas\n- The `load_skill` tool for on-demand loading\n- `SkillMiddleware` that injects skill descriptions into the system prompt\n- Agent creation with middleware and checkpointer\n- Example usage showing how the agent loads skills and writes SQL queries\n\nTo run this, you'll need to:\n1. Install required packages: `pip install langchain langchain-openai langgraph`\n2. Set your API key (e.g., `export OPENAI_API_KEY=...`)\n3. Replace the model initialization with your preferred LLM provider\n\n</Accordion>\n\n## Implementation variations\n\n<Accordion title=\"View implementation options and trade-offs\">\n\nThis tutorial implemented skills as in-memory Python dictionaries loaded through tool calls. However, there are several ways to implement progressive disclosure with skills:\n\n**Storage backends:**\n- **In-memory** (this tutorial): Skills defined as Python data structures, fast access, no I/O overhead\n- **File system** (Claude Code approach): Skills as directories with files, discovered via file operations like `read_file`\n- **Remote storage**: Skills in S3, databases, Notion, or APIs", "metadata": {"source": "multi-agent/skills-sql-assistant.mdx"}}
{"text": " to:\n1. Install required packages: `pip install langchain langchain-openai langgraph`\n2. Set your API key (e.g., `export OPENAI_API_KEY=...`)\n3. Replace the model initialization with your preferred LLM provider\n\n</Accordion>\n\n## Implementation variations\n\n<Accordion title=\"View implementation options and trade-offs\">\n\nThis tutorial implemented skills as in-memory Python dictionaries loaded through tool calls. However, there are several ways to implement progressive disclosure with skills:\n\n**Storage backends:**\n- **In-memory** (this tutorial): Skills defined as Python data structures, fast access, no I/O overhead\n- **File system** (Claude Code approach): Skills as directories with files, discovered via file operations like `read_file`\n- **Remote storage**: Skills in S3, databases, Notion, or APIs, fetched on-demand\n\n**Skill discovery** (how the agent learns which skills exist):\n- **System prompt listing**: Skill descriptions in system prompt (used in this tutorial)\n- **File-based**: Discover skills by scanning directories (Claude Code approach)\n- **Registry-based**: Query a skill registry service or API for available skills\n- **Dynamic lookup**: List available skills via a tool call\n\n**Progressive disclosure strategies** (how skill content is loaded):\n- **Single load**: Load entire skill content in one tool call (used in this tutorial)\n- **Paginated**: Load skill content in multiple pages/chunks for large skills\n- **Search-based**: Search within a specific skill's content for relevant sections (e.g., using grep/read operations on skill files)\n- **Hierarchical**: Load skill overview first, then drill into specific subsections\n\n**Size considerations** (uncalibrated mental model - optimize for your system):\n- **Small skills** (< 1K tokens / ~750 words): Can be included directly in system prompt and cached with prompt caching for cost savings and faster responses\n- **Medium skills** (1-10K tokens / ~750-7.5K words): Benefit from on-demand loading to avoid context overhead (this tutorial)\n- **Large skills** (> 10K tokens / ~7.5K words, or > 5-10% of context window): Should use progressive disclosure techniques like pagination, search-based loading, or hierarchical exploration to avoid consuming excessive context\n\nThe choice depends on your requirements: in-memory is fastest but requires redeployment for skill updates, while file-based or remote storage enables dynamic skill management without code changes.\n\n</Accordion>\n\n## Progressive disclosure and context engineering\n\n<Accordion title=\"Combining with few-shot prompting and other techniques\">\n\nProgressive disclosure is fundamentally a **[context engineering](/oss/langchain/context-engineering) technique** - you're managing what information is available to the agent and when. This tutorial focused on loading database schemas, but the same principles apply to other types of context.\n\n### Combining with few-shot prompting\n\nFor the SQL query use case, you could extend progressive disclosure to dynamically load **few-shot examples** that match the user's query:\n\n**Example approach:**\n1. User asks: \"Find customers who haven't ordered in 6 months\"\n2. Agent loads `sales_analytics` schema (as shown in this tutorial)\n3. Agent also loads 2-3 relevant example queries (via semantic search or tag-based lookup):\n   - Query for finding inactive customers\n   - Query with date-based filtering\n   - Query joining customers", "metadata": {"source": "multi-agent/skills-sql-assistant.mdx"}}
{"text": " with few-shot prompting and other techniques\">\n\nProgressive disclosure is fundamentally a **[context engineering](/oss/langchain/context-engineering) technique** - you're managing what information is available to the agent and when. This tutorial focused on loading database schemas, but the same principles apply to other types of context.\n\n### Combining with few-shot prompting\n\nFor the SQL query use case, you could extend progressive disclosure to dynamically load **few-shot examples** that match the user's query:\n\n**Example approach:**\n1. User asks: \"Find customers who haven't ordered in 6 months\"\n2. Agent loads `sales_analytics` schema (as shown in this tutorial)\n3. Agent also loads 2-3 relevant example queries (via semantic search or tag-based lookup):\n   - Query for finding inactive customers\n   - Query with date-based filtering\n   - Query joining customers and orders tables\n4. Agent writes query using both schema knowledge AND example patterns\n\nThis combination of progressive disclosure (loading schemas on-demand) and dynamic few-shot prompting (loading relevant examples) creates a powerful context engineering pattern that scales to large knowledge bases while providing high-quality, grounded outputs.\n\n</Accordion>\n\n## Next steps\n\n- Learn about [middleware](/oss/langchain/middleware) for more dynamic agent behaviors\n- Explore [context engineering](/oss/langchain/context-engineering) techniques for managing agent context\n- Explore the [handoffs pattern](/oss/langchain/multi-agent/handoffs-customer-support) for sequential workflows\n- Read the [subagents pattern](/oss/langchain/multi-agent/subagents-personal-assistant) for parallel task routing\n- See [multi-agent patterns](/oss/langchain/multi-agent) for other approaches to specialized agents\n- Use [LangSmith](https://smith.langchain.com) to debug and monitor skill loading\n", "metadata": {"source": "multi-agent/skills-sql-assistant.mdx"}}
{"text": "---\ntitle: Overview\ndescription: Stream real-time updates from agent runs\n---\n\nLangChain implements a streaming system to surface real-time updates.\n\nStreaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\n\n## Overview\n\nLangChain's streaming system lets you surface live feedback from agent runs to your application.\n\nWhat's possible with LangChain streaming:\n\n* <Icon icon=\"brain\" size={16} /> [**Stream agent progress**](#agent-progress) \u2014 get state updates after each agent step.\n* <Icon icon=\"square-binary\" size={16} /> [**Stream LLM tokens**](#llm-tokens) \u2014 stream language model tokens as they're generated.\n* <Icon icon=\"table\" size={16} /> [**Stream custom updates**](#custom-updates) \u2014 emit user-defined signals (e.g., `\"Fetched 10/100 records\"`).\n* <Icon icon=\"layer-plus\" size={16} /> [**Stream multiple modes**](#stream-multiple-modes) \u2014 choose from `updates` (agent progress), `messages` (LLM tokens + metadata), or `custom` (arbitrary user data).\n\nSee the [common patterns](#common-patterns) section below for additional end-to-end examples.\n\n## Supported stream modes\n\n:::python\nPass one or more of the following stream modes as a list to the @[`stream`][CompiledStateGraph.stream] or @[`astream`][CompiledStateGraph.astream] methods:\n:::\n\n:::js\nPass one or more of the following stream modes as a list to the @[`stream`][CompiledStateGraph.stream] method:\n:::\n\n| Mode       | Description                                                                                                 |\n| ---------- | ----------------------------------------------------------------------------------------------------------- |\n| `updates`  | Streams state updates after each agent step. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately. |\n| `messages` | Streams tuples of `(token, metadata)` from any graph nodes where an LLM is invoked. |\n| `custom`   | Streams custom data from inside your graph nodes using the stream writer. |\n\n## Agent progress\n\n:::python\nTo stream agent progress, use the @[`stream`][CompiledStateGraph.stream] or @[`astream`][CompiledStateGraph.astream] methods with `stream_mode=\"updates\"`. This emits an event after every agent step.\n:::\n\n:::js\nTo stream agent progress, use the @[`stream`][CompiledStateGraph.stream] method with `streamMode: \"updates\"`. This emits an event after every agent step.\n:::\n\nFor example, if you have an agent that calls a tool once, you should see the following updates:\n\n* **LLM node**: @[`AIMessage`", "metadata": {"source": "streaming/overview.mdx"}}
{"text": " `(token, metadata)` from any graph nodes where an LLM is invoked. |\n| `custom`   | Streams custom data from inside your graph nodes using the stream writer. |\n\n## Agent progress\n\n:::python\nTo stream agent progress, use the @[`stream`][CompiledStateGraph.stream] or @[`astream`][CompiledStateGraph.astream] methods with `stream_mode=\"updates\"`. This emits an event after every agent step.\n:::\n\n:::js\nTo stream agent progress, use the @[`stream`][CompiledStateGraph.stream] method with `streamMode: \"updates\"`. This emits an event after every agent step.\n:::\n\nFor example, if you have an agent that calls a tool once, you should see the following updates:\n\n* **LLM node**: @[`AIMessage`] with tool call requests\n* **Tool node**: @[`ToolMessage`] with execution result\n* **LLM node**: Final AI response\n\n:::python\n\n```python title=\"Streaming agent progress\"\nfrom langchain.agents import create_agent\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[get_weather],\n)\nfor chunk in agent.stream(  # [!code highlight]\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n    stream_mode=\"updates\",\n):\n    for step, data in chunk.items():\n        print(f\"step: {step}\")\n        print(f\"content: {data['messages'][-1].content_blocks}\")\n```\n\n```shell title=\"Output\"\nstep: model\ncontent: [{'type': 'tool_call', 'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_OW2NYNsNSKhRZpjW0wm2Aszd'}]\n\nstep: tools\ncontent: [{'type': 'text', 'text': \"It's always sunny in San Francisco!\"}]\n\nstep: model\ncontent: [{'type': 'text', 'text': 'It's always sunny in San Francisco!'}]\n```\n:::\n\n:::js\n```typescript\nimport z from \"zod\";\nimport { createAgent, tool } from \"langchain\";\n\nconst getWeather = tool(\n    async ({ city }) => {\n        return `The weather in ${city} is always sunny!`;\n    },\n    {\n        name: \"get_weather\",\n        description: \"Get weather for a given city.\",\n        schema: z.object({\n        city: z.string(),\n        }),\n    }\n);\n\nconst agent = createAgent({\n    model: \"gpt-5-nano\",\n    tools: [getWeather],\n});\n\nfor await (const chunk of await agent", "metadata": {"source": "streaming/overview.mdx"}}
{"text": "\n```typescript\nimport z from \"zod\";\nimport { createAgent, tool } from \"langchain\";\n\nconst getWeather = tool(\n    async ({ city }) => {\n        return `The weather in ${city} is always sunny!`;\n    },\n    {\n        name: \"get_weather\",\n        description: \"Get weather for a given city.\",\n        schema: z.object({\n        city: z.string(),\n        }),\n    }\n);\n\nconst agent = createAgent({\n    model: \"gpt-5-nano\",\n    tools: [getWeather],\n});\n\nfor await (const chunk of await agent.stream(\n    { messages: [{ role: \"user\", content: \"what is the weather in sf\" }] },\n    { streamMode: \"updates\" }\n)) {\n    const [step, content] = Object.entries(chunk)[0];\n    console.log(`step: ${step}`);\n    console.log(`content: ${JSON.stringify(content, null, 2)}`);\n}\n/**\n * step: model\n * content: {\n *   \"messages\": [\n *     {\n *       \"kwargs\": {\n *         // ...\n *         \"tool_calls\": [\n *           {\n *             \"name\": \"get_weather\",\n *             \"args\": {\n *               \"city\": \"San Francisco\"\n *             },\n *             \"type\": \"tool_call\",\n *             \"id\": \"call_0qLS2Jp3MCmaKJ5MAYtr4jJd\"\n *           }\n *         ],\n *         // ...\n *       }\n *     }\n *   ]\n * }\n * step: tools\n * content: {\n *   \"messages\": [\n *     {\n *       \"kwargs\": {\n *         \"content\": \"The weather in San Francisco is always sunny!\",\n *         \"name\": \"get_weather\",\n *         // ...\n *       }\n *     }\n *   ]\n * }\n * step: model\n * content: {\n *   \"messages\": [\n *     {\n *       \"kwargs\": {\n *         \"content\": \"The latest update says: The weather in San Francisco is always sunny!\\n\\nIf you'd like real-time details (current temperature, humidity, wind, and today's", "metadata": {"source": "streaming/overview.mdx"}}
{"text": " * content: {\n *   \"messages\": [\n *     {\n *       \"kwargs\": {\n *         \"content\": \"The weather in San Francisco is always sunny!\",\n *         \"name\": \"get_weather\",\n *         // ...\n *       }\n *     }\n *   ]\n * }\n * step: model\n * content: {\n *   \"messages\": [\n *     {\n *       \"kwargs\": {\n *         \"content\": \"The latest update says: The weather in San Francisco is always sunny!\\n\\nIf you'd like real-time details (current temperature, humidity, wind, and today's forecast), I can pull the latest data for you. Want me to fetch that?\",\n *         // ...\n *       }\n *     }\n *   ]\n * }\n */\n```\n:::\n\n## LLM tokens\n\n:::python\nTo stream tokens as they are produced by the LLM, use `stream_mode=\"messages\"`. Below you can see the output of the agent streaming tool calls and the final response.\n\n```python title=\"Streaming LLM tokens\"\nfrom langchain.agents import create_agent\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[get_weather],\n)\nfor token, metadata in agent.stream(  # [!code highlight]\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n    stream_mode=\"messages\",\n):\n    print(f\"node: {metadata['langgraph_node']}\")\n    print(f\"content: {token.content_blocks}\")\n    print(\"\\n\")\n```\n\n```shell title=\"Output\" expandable\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': 'call_vbCyBcP8VuneUzyYlSBZZsVa', 'name': 'get_weather', 'args': '', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': '{\"', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': 'city', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': '\":\"', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': 'San', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args", "metadata": {"source": "streaming/overview.mdx"}}
{"text": "\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': '{\"', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': 'city', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': '\":\"', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': 'San', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': ' Francisco', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': '\"}', 'index': 0}]\n\n\nnode: model\ncontent: []\n\n\nnode: tools\ncontent: [{'type': 'text', 'text': \"It's always sunny in San Francisco!\"}]\n\n\nnode: model\ncontent: []\n\n\nnode: model\ncontent: [{'type': 'text', 'text': 'Here'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ''s'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' what'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' I'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' got'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ':'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' \"'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': \"It's\"}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' always'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' sunny'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' in'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' San'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' Francisco'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': '!\"\\n\\n'}]\n```\n:::\n\n:::js\nTo stream tokens as they are produced by the LLM, use `streamMode: \"messages\"`:\n\n```typescript\nimport z from \"zod\";\nimport { createAgent, tool } from \"langchain\";\n\nconst getWeather = tool(\n    async ({ city }) => {\n        return `The weather in ${city} is always sunny!`;\n    },\n    {\n        name: \"get_weather\",\n        description: \"Get weather for a given city.\",\n        schema: z.object({\n", "metadata": {"source": "streaming/overview.mdx"}}
{"text": " 'text', 'text': ' Francisco'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': '!\"\\n\\n'}]\n```\n:::\n\n:::js\nTo stream tokens as they are produced by the LLM, use `streamMode: \"messages\"`:\n\n```typescript\nimport z from \"zod\";\nimport { createAgent, tool } from \"langchain\";\n\nconst getWeather = tool(\n    async ({ city }) => {\n        return `The weather in ${city} is always sunny!`;\n    },\n    {\n        name: \"get_weather\",\n        description: \"Get weather for a given city.\",\n        schema: z.object({\n        city: z.string(),\n        }),\n    }\n);\n\nconst agent = createAgent({\n    model: \"gpt-4.1-mini\",\n    tools: [getWeather],\n});\n\nfor await (const [token, metadata] of await agent.stream(\n    { messages: [{ role: \"user\", content: \"what is the weather in sf\" }] },\n    { streamMode: \"messages\" }\n)) {\n    console.log(`node: ${metadata.langgraph_node}`);\n    console.log(`content: ${JSON.stringify(token.contentBlocks, null, 2)}`);\n}\n```\n:::\n\n## Custom updates\n\n:::python\nTo stream updates from tools as they are executed, you can use @[`get_stream_writer`].\n\n```python title=\"Streaming custom updates\"\nfrom langchain.agents import create_agent\nfrom langgraph.config import get_stream_writer  # [!code highlight]\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    writer = get_stream_writer()  # [!code highlight]\n    # stream any arbitrary data\n    writer(f\"Looking up data for city: {city}\")\n    writer(f\"Acquired data for city: {city}\")\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[get_weather],\n)\n\nfor chunk in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n    stream_mode=\"custom\"  # [!code highlight]\n):\n    print(chunk)\n```\n\n```shell title=\"Output\"\nLooking up data for city: San Francisco\nAcquired data for city: San Francisco\n```\n\n<Note>\n    If you add @[`get_stream_writer`] inside your tool, you won't be able to invoke the tool outside of a LangGraph execution context.\n</Note>\n:::\n\n:::js\nTo stream updates from tools as they are executed, you can use the `writer` parameter from the configuration.\n\n```typescript\nimport z from \"z", "metadata": {"source": "streaming/overview.mdx"}}
{"text": "  tools=[get_weather],\n)\n\nfor chunk in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n    stream_mode=\"custom\"  # [!code highlight]\n):\n    print(chunk)\n```\n\n```shell title=\"Output\"\nLooking up data for city: San Francisco\nAcquired data for city: San Francisco\n```\n\n<Note>\n    If you add @[`get_stream_writer`] inside your tool, you won't be able to invoke the tool outside of a LangGraph execution context.\n</Note>\n:::\n\n:::js\nTo stream updates from tools as they are executed, you can use the `writer` parameter from the configuration.\n\n```typescript\nimport z from \"zod\";\nimport { tool, createAgent } from \"langchain\";\nimport { LangGraphRunnableConfig } from \"@langchain/langgraph\";\n\nconst getWeather = tool(\n    async (input, config: LangGraphRunnableConfig) => {\n        // Stream any arbitrary data\n        config.writer?.(`Looking up data for city: ${input.city}`);\n        // ... fetch city data\n        config.writer?.(`Acquired data for city: ${input.city}`);\n        return `It's always sunny in ${input.city}!`;\n    },\n    {\n        name: \"get_weather\",\n        description: \"Get weather for a given city.\",\n        schema: z.object({\n        city: z.string().describe(\"The city to get weather for.\"),\n        }),\n    }\n);\n\nconst agent = createAgent({\n    model: \"gpt-4.1-mini\",\n    tools: [getWeather],\n});\n\nfor await (const chunk of await agent.stream(\n    { messages: [{ role: \"user\", content: \"what is the weather in sf\" }] },\n    { streamMode: \"custom\" }\n)) {\n    console.log(chunk);\n}\n```\n\n```shell title=\"Output\"\nLooking up data for city: San Francisco\nAcquired data for city: San Francisco\n```\n\n<Note>\n    If you add the `writer` parameter to your tool, you won't be able to invoke the tool outside of a LangGraph execution context without providing a writer function.\n</Note>\n:::\n\n## Stream multiple modes\n\n:::python\nYou can specify multiple streaming modes by passing stream mode as a list: `stream_mode=[\"updates\", \"custom\"]`.\n\nThe streamed outputs will be tuples of `(mode, chunk)` where `mode` is the name of the stream mode and `chunk` is the data streamed by that mode.\n\n```python title=\"Streaming multiple modes\"\nfrom langchain.agents import create_agent\nfrom langgraph.config import get_stream_writer\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    writer = get", "metadata": {"source": "streaming/overview.mdx"}}
{"text": "```\n\n<Note>\n    If you add the `writer` parameter to your tool, you won't be able to invoke the tool outside of a LangGraph execution context without providing a writer function.\n</Note>\n:::\n\n## Stream multiple modes\n\n:::python\nYou can specify multiple streaming modes by passing stream mode as a list: `stream_mode=[\"updates\", \"custom\"]`.\n\nThe streamed outputs will be tuples of `(mode, chunk)` where `mode` is the name of the stream mode and `chunk` is the data streamed by that mode.\n\n```python title=\"Streaming multiple modes\"\nfrom langchain.agents import create_agent\nfrom langgraph.config import get_stream_writer\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    writer = get_stream_writer()\n    writer(f\"Looking up data for city: {city}\")\n    writer(f\"Acquired data for city: {city}\")\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[get_weather],\n)\n\nfor stream_mode, chunk in agent.stream(  # [!code highlight]\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n    stream_mode=[\"updates\", \"custom\"]\n):\n    print(f\"stream_mode: {stream_mode}\")\n    print(f\"content: {chunk}\")\n    print(\"\\n\")\n```\n\n```shell title=\"Output\"\nstream_mode: updates\ncontent: {'model': {'messages': [AIMessage(content='', response_metadata={'token_usage': {'completion_tokens': 280, 'prompt_tokens': 132, 'total_tokens': 412, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C9tlgBzGEbedGYxZ0rTCz5F7OXpL7', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--480c07cb-e405-4411-aa7f-0520fddeed66-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_KTNQIftMrl9vgNwEfAJMVu7r', 'type': 'tool_call'}], usage_metadata={'input_tokens': 132, 'output_tokens': 280, 'total_tokens': 412, 'input_token_details': {'audio': 0, 'cache_", "metadata": {"source": "streaming/overview.mdx"}}
{"text": "': None, 'id': 'chatcmpl-C9tlgBzGEbedGYxZ0rTCz5F7OXpL7', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--480c07cb-e405-4411-aa7f-0520fddeed66-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_KTNQIftMrl9vgNwEfAJMVu7r', 'type': 'tool_call'}], usage_metadata={'input_tokens': 132, 'output_tokens': 280, 'total_tokens': 412, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 256}})]}}\n\n\nstream_mode: custom\ncontent: Looking up data for city: San Francisco\n\n\nstream_mode: custom\ncontent: Acquired data for city: San Francisco\n\n\nstream_mode: updates\ncontent: {'tools': {'messages': [ToolMessage(content=\"It's always sunny in San Francisco!\", name='get_weather', tool_call_id='call_KTNQIftMrl9vgNwEfAJMVu7r')]}}\n\n\nstream_mode: updates\ncontent: {'model': {'messages': [AIMessage(content='San Francisco weather: It's always sunny in San Francisco!\\n\\n', response_metadata={'token_usage': {'completion_tokens': 764, 'prompt_tokens': 168, 'total_tokens': 932, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 704, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C9tljDFVki1e1haCyikBptAuXuHYG', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--acbc740a-18fe-4a14-8619-da92a0d0ee90-0', usage_metadata={'input_tokens': 168, 'output_tokens': 764, 'total_tokens': 932, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 704}})]}}\n```\n:::\n\n:::js\nYou can specify multiple streaming modes by passing streamMode as an array: `streamMode: [\"updates\", \"messages\", \"custom\"]`.\n\nThe streamed outputs will be tuples of `[mode, chunk]` where `mode` is the name of the stream mode and `chunk` is the data streamed by that mode.\n\n```typescript\nimport z from \"zod\";", "metadata": {"source": "streaming/overview.mdx"}}
{"text": "--acbc740a-18fe-4a14-8619-da92a0d0ee90-0', usage_metadata={'input_tokens': 168, 'output_tokens': 764, 'total_tokens': 932, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 704}})]}}\n```\n:::\n\n:::js\nYou can specify multiple streaming modes by passing streamMode as an array: `streamMode: [\"updates\", \"messages\", \"custom\"]`.\n\nThe streamed outputs will be tuples of `[mode, chunk]` where `mode` is the name of the stream mode and `chunk` is the data streamed by that mode.\n\n```typescript\nimport z from \"zod\";\nimport { tool, createAgent } from \"langchain\";\nimport { LangGraphRunnableConfig } from \"@langchain/langgraph\";\n\nconst getWeather = tool(\n    async (input, config: LangGraphRunnableConfig) => {\n        // Stream any arbitrary data\n        config.writer?.(`Looking up data for city: ${input.city}`);\n        // ... fetch city data\n        config.writer?.(`Acquired data for city: ${input.city}`);\n        return `It's always sunny in ${input.city}!`;\n    },\n    {\n        name: \"get_weather\",\n        description: \"Get weather for a given city.\",\n        schema: z.object({\n        city: z.string().describe(\"The city to get weather for.\"),\n        }),\n    }\n);\n\nconst agent = createAgent({\n    model: \"gpt-4.1-mini\",\n    tools: [getWeather],\n});\n\nfor await (const [streamMode, chunk] of await agent.stream(\n    { messages: [{ role: \"user\", content: \"what is the weather in sf\" }] },\n    { streamMode: [\"updates\", \"messages\", \"custom\"] }\n)) {\n    console.log(`${streamMode}: ${JSON.stringify(chunk, null, 2)}`);\n}\n```\n:::\n\n:::python\n## Common patterns\n\nBelow are examples showing common use cases for streaming.\n\n### Streaming tool calls\n\nYou may want to stream both:\n\n1. Partial JSON as [tool calls](/oss/langchain/models#tool-calling) are generated\n2. The completed, parsed tool calls that are executed\n\nSpecifying [`stream_mode=\"messages\"`](#llm-tokens) will stream incremental [message chunks](/oss/langchain/messages#streaming-and-chunks) generated by all LLM calls in the agent. To access the completed messages with parsed tool calls:\n\n1. If those messages are tracked in the [state](/oss/langchain/agents#memory) (as in the model node of [`create_agent`](/oss/langchain/agents)), use `stream_mode=[\"messages\", \"up", "metadata": {"source": "streaming/overview.mdx"}}
{"text": ":::\n\n:::python\n## Common patterns\n\nBelow are examples showing common use cases for streaming.\n\n### Streaming tool calls\n\nYou may want to stream both:\n\n1. Partial JSON as [tool calls](/oss/langchain/models#tool-calling) are generated\n2. The completed, parsed tool calls that are executed\n\nSpecifying [`stream_mode=\"messages\"`](#llm-tokens) will stream incremental [message chunks](/oss/langchain/messages#streaming-and-chunks) generated by all LLM calls in the agent. To access the completed messages with parsed tool calls:\n\n1. If those messages are tracked in the [state](/oss/langchain/agents#memory) (as in the model node of [`create_agent`](/oss/langchain/agents)), use `stream_mode=[\"messages\", \"updates\"]` to access completed messages through [state updates](#agent-progress) (demonstrated below).\n2. If those messages are not tracked in the state, use [custom updates](#custom-updates) or aggregate the chunks during the streaming loop ([next section](#accessing-completed-messages)).\n\n<Note>\nRefer to the section below on [streaming from sub-agents](#streaming-from-sub-agents) if your agent includes multiple LLMs.\n</Note>\n\n```python\nfrom typing import Any\n\nfrom langchain.agents import create_agent\nfrom langchain.messages import AIMessage, AIMessageChunk, AnyMessage, ToolMessage\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n\n    return f\"It's always sunny in {city}!\"\n\n\nagent = create_agent(\"openai:gpt-5.2\", tools=[get_weather])\n\n\ndef _render_message_chunk(token: AIMessageChunk) -> None:\n    if token.text:\n        print(token.text, end=\"|\")\n    if token.tool_call_chunks:\n        print(token.tool_call_chunks)\n    # N.B. all content is available through token.content_blocks\n\n\ndef _render_completed_message(message: AnyMessage) -> None:\n    if isinstance(message, AIMessage) and message.tool_calls:\n        print(f\"Tool calls: {message.tool_calls}\")\n    if isinstance(message, ToolMessage):\n        print(f\"Tool response: {message.content_blocks}\")\n\n\ninput_message = {\"role\": \"user\", \"content\": \"What is the weather in Boston?\"}\nfor stream_mode, data in agent.stream(\n    {\"messages\": [input_message]},\n    stream_mode=[\"messages\", \"updates\"],  # [!code highlight]\n):\n    if stream_mode == \"messages\":\n        token, metadata = data\n        if isinstance(token, AIMessageChunk):\n            _render_message_chunk(token)  # [!code highlight]\n    if stream_mode == \"updates\":\n        for source, update in", "metadata": {"source": "streaming/overview.mdx"}}
{"text": "):\n        print(f\"Tool response: {message.content_blocks}\")\n\n\ninput_message = {\"role\": \"user\", \"content\": \"What is the weather in Boston?\"}\nfor stream_mode, data in agent.stream(\n    {\"messages\": [input_message]},\n    stream_mode=[\"messages\", \"updates\"],  # [!code highlight]\n):\n    if stream_mode == \"messages\":\n        token, metadata = data\n        if isinstance(token, AIMessageChunk):\n            _render_message_chunk(token)  # [!code highlight]\n    if stream_mode == \"updates\":\n        for source, update in data.items():\n            if source in (\"model\", \"tools\"):  # `source` captures node name\n                _render_completed_message(update[\"messages\"][-1])  # [!code highlight]\n```\n```shell title=\"Output\" expandable\n[{'name': 'get_weather', 'args': '', 'id': 'call_D3Orjr89KgsLTZ9hTzYv7Hpf', 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '{\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'city', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '\":\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'Boston', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '\"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\nTool calls: [{'name': 'get_weather', 'args': {'city': 'Boston'}, 'id': 'call_D3Orjr89KgsLTZ9hTzYv7Hpf', 'type': 'tool_call'}]\nTool response: [{'type': 'text', 'text': \"It's always sunny in Boston!\"}]\nThe| weather| in| Boston| is| **|sun|ny|**|.|\n```\n\n#### Accessing completed messages\n\n<Note>\nIf completed messages are tracked in an agent's [state](/oss/langchain/agents#memory), you can use `stream_mode=[\"messages\", \"updates\"]` as demonstrated [above](#streaming-tool-calls) to access completed messages during streaming.\n</Note>\n\nIn some cases, completed messages are not reflected in [state updates](#agent-progress). If you have access to the agent internals, you can use [custom updates](#custom-updates) to access these messages during streaming. Otherwise, you can aggregate message chunks in the streaming loop (see below).\n\nConsider the below example, where we incorporate a [stream", "metadata": {"source": "streaming/overview.mdx"}}
{"text": "{'type': 'text', 'text': \"It's always sunny in Boston!\"}]\nThe| weather| in| Boston| is| **|sun|ny|**|.|\n```\n\n#### Accessing completed messages\n\n<Note>\nIf completed messages are tracked in an agent's [state](/oss/langchain/agents#memory), you can use `stream_mode=[\"messages\", \"updates\"]` as demonstrated [above](#streaming-tool-calls) to access completed messages during streaming.\n</Note>\n\nIn some cases, completed messages are not reflected in [state updates](#agent-progress). If you have access to the agent internals, you can use [custom updates](#custom-updates) to access these messages during streaming. Otherwise, you can aggregate message chunks in the streaming loop (see below).\n\nConsider the below example, where we incorporate a [stream writer](#custom-updates) into a simplified [guardrail middleware](/oss/langchain/guardrails#after-agent-guardrails). This middleware demonstrates tool calling to generate a structured \"safe / unsafe\" evaluation (one could also use [structured outputs](/oss/langchain/models#structured-output) for this):\n```python\nfrom typing import Any, Literal\n\nfrom langchain.agents.middleware import after_agent, AgentState\nfrom langgraph.runtime import Runtime\nfrom langchain.messages import AIMessage\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.config import get_stream_writer  # [!code highlight]\nfrom pydantic import BaseModel\n\n\nclass ResponseSafety(BaseModel):\n    \"\"\"Evaluate a response as safe or unsafe.\"\"\"\n    evaluation: Literal[\"safe\", \"unsafe\"]\n\n\nsafety_model = init_chat_model(\"openai:gpt-5.2\")\n\n@after_agent(can_jump_to=[\"end\"])\ndef safety_guardrail(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n    \"\"\"Model-based guardrail: Use an LLM to evaluate response safety.\"\"\"\n    stream_writer = get_stream_writer()  # [!code highlight]\n    # Get the model response\n    if not state[\"messages\"]:\n        return None\n\n    last_message = state[\"messages\"][-1]\n    if not isinstance(last_message, AIMessage):\n        return None\n\n    # Use another model to evaluate safety\n    model_with_tools = safety_model.bind_tools([ResponseSafety], tool_choice=\"any\")\n    result = model_with_tools.invoke(\n        [\n            {\n                \"role\": \"system\",\n                \"content\": \"Evaluate this AI response as generally safe or unsafe.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"AI response: {last_message.text}\"\n    ", "metadata": {"source": "streaming/overview.mdx"}}
{"text": "    model_with_tools = safety_model.bind_tools([ResponseSafety], tool_choice=\"any\")\n    result = model_with_tools.invoke(\n        [\n            {\n                \"role\": \"system\",\n                \"content\": \"Evaluate this AI response as generally safe or unsafe.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"AI response: {last_message.text}\"\n            }\n        ]\n    )\n    stream_writer(result)  # [!code highlight]\n\n    tool_call = result.tool_calls[0]\n    if tool_call[\"args\"][\"evaluation\"] == \"unsafe\":\n        last_message.content = \"I cannot provide that response. Please rephrase your request.\"\n\n    return None\n```\nWe can then incorporate this middleware into our agent and include its custom stream events:\n```python\nfrom typing import Any\n\nfrom langchain.agents import create_agent\nfrom langchain.messages import AIMessageChunk, AIMessage, AnyMessage\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n\n    return f\"It's always sunny in {city}!\"\n\n\nagent = create_agent(\n    model=\"openai:gpt-5.2\",\n    tools=[get_weather],\n    middleware=[safety_guardrail],  # [!code highlight]\n)\n\ndef _render_message_chunk(token: AIMessageChunk) -> None:\n    if token.text:\n        print(token.text, end=\"|\")\n    if token.tool_call_chunks:\n        print(token.tool_call_chunks)\n\n\ndef _render_completed_message(message: AnyMessage) -> None:\n    if isinstance(message, AIMessage) and message.tool_calls:\n        print(f\"Tool calls: {message.tool_calls}\")\n    if isinstance(message, ToolMessage):\n        print(f\"Tool response: {message.content_blocks}\")\n\n\ninput_message = {\"role\": \"user\", \"content\": \"What is the weather in Boston?\"}\nfor stream_mode, data in agent.stream(\n    {\"messages\": [input_message]},\n    stream_mode=[\"messages\", \"updates\", \"custom\"],  # [!code highlight]\n):\n    if stream_mode == \"messages\":\n        token, metadata = data\n        if isinstance(token, AIMessageChunk):\n            _render_message_chunk(token)\n ", "metadata": {"source": "streaming/overview.mdx"}}
{"text": " print(f\"Tool calls: {message.tool_calls}\")\n    if isinstance(message, ToolMessage):\n        print(f\"Tool response: {message.content_blocks}\")\n\n\ninput_message = {\"role\": \"user\", \"content\": \"What is the weather in Boston?\"}\nfor stream_mode, data in agent.stream(\n    {\"messages\": [input_message]},\n    stream_mode=[\"messages\", \"updates\", \"custom\"],  # [!code highlight]\n):\n    if stream_mode == \"messages\":\n        token, metadata = data\n        if isinstance(token, AIMessageChunk):\n            _render_message_chunk(token)\n    if stream_mode == \"updates\":\n        for source, update in data.items():\n            if source in (\"model\", \"tools\"):\n                _render_completed_message(update[\"messages\"][-1])\n    if stream_mode == \"custom\":  # [!code highlight]\n        # access completed message in stream\n        print(f\"Tool calls: {data.tool_calls}\")  # [!code highlight]\n```\n```shell title=\"Output\" expandable\n[{'name': 'get_weather', 'args': '', 'id': 'call_je6LWgxYzuZ84mmoDalTYMJC', 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '{\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'city', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '\":\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'Boston', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '\"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\nTool calls: [{'name': 'get_weather', 'args': {'city': 'Boston'}, 'id': 'call_je6LWgxYzuZ84mmoDalTYMJC', 'type': 'tool_call'}]\nTool response: [{'type': 'text', 'text': \"It's always sunny in Boston!\"}]\nThe| weather| in| **|Boston|**| is| **|sun|ny|**|.|[{'name': 'ResponseSafety', 'args': '', 'id': 'call_O8VJIbOG4Q9nQF0T8ltVi58O', 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '{\"', 'id': None, 'index': 0, 'type': 'tool", "metadata": {"source": "streaming/overview.mdx"}}
{"text": "'}]\nTool calls: [{'name': 'get_weather', 'args': {'city': 'Boston'}, 'id': 'call_je6LWgxYzuZ84mmoDalTYMJC', 'type': 'tool_call'}]\nTool response: [{'type': 'text', 'text': \"It's always sunny in Boston!\"}]\nThe| weather| in| **|Boston|**| is| **|sun|ny|**|.|[{'name': 'ResponseSafety', 'args': '', 'id': 'call_O8VJIbOG4Q9nQF0T8ltVi58O', 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '{\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'evaluation', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '\":\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'safe', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '\"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\nTool calls: [{'name': 'ResponseSafety', 'args': {'evaluation': 'safe'}, 'id': 'call_O8VJIbOG4Q9nQF0T8ltVi58O', 'type': 'tool_call'}]\n```\n\nAlternatively, if you aren't able to add custom events to the stream, you can aggregate message chunks within the streaming loop:\n```python\ninput_message = {\"role\": \"user\", \"content\": \"What is the weather in Boston?\"}\nfull_message = None  # [!code highlight]\nfor stream_mode, data in agent.stream(\n    {\"messages\": [input_message]},\n    stream_mode=[\"messages\", \"updates\"],\n):\n    if stream_mode == \"messages\":\n        token, metadata = data\n        if isinstance(token, AIMessageChunk):\n            _render_message_chunk(token)\n            full_message = token if full_message is None else full_message + token  # [!code highlight]\n            if token.chunk_position == \"last\":  # [!code highlight]\n                if full_message.tool_calls:  # [!code highlight]\n                    print(f\"Tool calls: {full_message.tool_calls}\")  # [!code highlight]\n                full_message = None  # [!code highlight]\n    if stream_mode == \"updates\":\n        for source, update in data", "metadata": {"source": "streaming/overview.mdx"}}
{"text": ")\n            full_message = token if full_message is None else full_message + token  # [!code highlight]\n            if token.chunk_position == \"last\":  # [!code highlight]\n                if full_message.tool_calls:  # [!code highlight]\n                    print(f\"Tool calls: {full_message.tool_calls}\")  # [!code highlight]\n                full_message = None  # [!code highlight]\n    if stream_mode == \"updates\":\n        for source, update in data.items():\n            if source == \"tools\":\n                _render_completed_message(update[\"messages\"][-1])\n```\n\n### Streaming with human-in-the-loop\n\nTo handle human-in-the-loop [interrupts](/oss/langchain/human-in-the-loop), we build on the [above example](#streaming-tool-calls):\n\n1. We configure the agent with [human-in-the-loop middleware and a checkpointer](/oss/langchain/human-in-the-loop#configuring-interrupts)\n2. We collect interrupts generated during the `\"updates\"` stream mode\n3. We respond to those interrupts with a [command](/oss/langchain/human-in-the-loop#responding-to-interrupts)\n\n```python\nfrom typing import Any\n\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware\nfrom langchain.messages import AIMessage, AIMessageChunk, AnyMessage, ToolMessage\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.types import Command, Interrupt\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n\n    return f\"It's always sunny in {city}!\"\n\n\ncheckpointer = InMemorySaver()\n\nagent = create_agent(\n    \"openai:gpt-5.2\",\n    tools=[get_weather],\n    middleware=[  # [!code highlight]\n        HumanInTheLoopMiddleware(interrupt_on={\"get_weather\": True}),  # [!code highlight]\n    ],  # [!code highlight]\n    checkpointer=checkpointer,  # [!code highlight]\n)\n\n\ndef _render_message_chunk(token: AIMessageChunk) -> None:\n    if token.text:\n        print(token.text, end=\"|\")\n    if token.tool_call_chunks:\n        print(token.tool_call_chunks)\n\n\ndef _render_completed_message(message: AnyMessage) -> None:\n    if isinstance(message, AIMessage) and message.tool_calls:\n        print(f\"", "metadata": {"source": "streaming/overview.mdx"}}
{"text": "        HumanInTheLoopMiddleware(interrupt_on={\"get_weather\": True}),  # [!code highlight]\n    ],  # [!code highlight]\n    checkpointer=checkpointer,  # [!code highlight]\n)\n\n\ndef _render_message_chunk(token: AIMessageChunk) -> None:\n    if token.text:\n        print(token.text, end=\"|\")\n    if token.tool_call_chunks:\n        print(token.tool_call_chunks)\n\n\ndef _render_completed_message(message: AnyMessage) -> None:\n    if isinstance(message, AIMessage) and message.tool_calls:\n        print(f\"Tool calls: {message.tool_calls}\")\n    if isinstance(message, ToolMessage):\n        print(f\"Tool response: {message.content_blocks}\")\n\n\ndef _render_interrupt(interrupt: Interrupt) -> None:  # [!code highlight]\n    interrupts = interrupt.value  # [!code highlight]\n    for request in interrupts[\"action_requests\"]:  # [!code highlight]\n        print(request[\"description\"])  # [!code highlight]\n\n\ninput_message = {\n    \"role\": \"user\",\n    \"content\": (\n        \"Can you look up the weather in Boston and San Francisco?\"\n    ),\n}\nconfig = {\"configurable\": {\"thread_id\": \"some_id\"}}  # [!code highlight]\ninterrupts = []  # [!code highlight]\nfor stream_mode, data in agent.stream(\n    {\"messages\": [input_message]},\n    config=config,  # [!code highlight]\n    stream_mode=[\"messages\", \"updates\"],\n):\n    if stream_mode == \"messages\":\n        token, metadata = data\n        if isinstance(token, AIMessageChunk):\n            _render_message_chunk(token)\n    if stream_mode == \"updates\":\n        for source, update in data.items():\n            if source in (\"model\", \"tools\"):\n                _render_completed_message(update[\"messages\"][-1])\n            if source == \"__interrupt__\":  # [!code highlight]\n                interrupts.extend(update)  # [!code highlight]\n                _render_interrupt(update[0])  # [!code highlight]\n```\n```shell title=\"Output\" expandable\n[{'name': 'get_weather', 'args': '', 'id': 'call_GOwNaQHeqMixay2qy80padfE', 'index': 0, 'type': 'tool_call_chunk'}]\n[{'", "metadata": {"source": "streaming/overview.mdx"}}
{"text": "tools\"):\n                _render_completed_message(update[\"messages\"][-1])\n            if source == \"__interrupt__\":  # [!code highlight]\n                interrupts.extend(update)  # [!code highlight]\n                _render_interrupt(update[0])  # [!code highlight]\n```\n```shell title=\"Output\" expandable\n[{'name': 'get_weather', 'args': '', 'id': 'call_GOwNaQHeqMixay2qy80padfE', 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '{\"ci', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'ty\": ', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '\"Bosto', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'n\"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': 'get_weather', 'args': '', 'id': 'call_Ndb4jvWm2uMA0JDQXu37wDH6', 'index': 1, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '{\"ci', 'id': None, 'index': 1, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'ty\": ', 'id': None, 'index': 1, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '\"San F', 'id': None, 'index': 1, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'ranc', 'id': None, 'index': 1, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'isco\"', 'id': None, 'index': 1, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '}', 'id': None, 'index': 1, 'type': 'tool_call_chunk'}]\nTool calls: [{'name': 'get_weather', 'args': {'city': 'Boston'}, 'id': 'call_GOwNaQHeqMixay2qy80padfE', 'type': 'tool_call'}, {'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_Ndb4jvWm2uMA0JDQXu37wDH6', 'type': 'tool_call'}]\nTool execution requires approval\n\nTool: get_weather\nArgs: {'city': 'Boston'}\nTool execution requires approval\n\nTool: get_weather\nArgs: {'city': 'San Francisco'}\n``", "metadata": {"source": "streaming/overview.mdx"}}
{"text": "]\n[{'name': None, 'args': '}', 'id': None, 'index': 1, 'type': 'tool_call_chunk'}]\nTool calls: [{'name': 'get_weather', 'args': {'city': 'Boston'}, 'id': 'call_GOwNaQHeqMixay2qy80padfE', 'type': 'tool_call'}, {'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_Ndb4jvWm2uMA0JDQXu37wDH6', 'type': 'tool_call'}]\nTool execution requires approval\n\nTool: get_weather\nArgs: {'city': 'Boston'}\nTool execution requires approval\n\nTool: get_weather\nArgs: {'city': 'San Francisco'}\n```\n\nWe next collect a [decision](/oss/langchain/human-in-the-loop#interrupt-decision-types) for each interrupt. Importantly, the order of decisions must match the order of actions we collected.\n\nTo illustrate, we will edit one tool call and accept the other:\n```python\ndef _get_interrupt_decisions(interrupt: Interrupt) -> list[dict]:\n    return [\n        {\n            \"type\": \"edit\",\n            \"edited_action\": {\n                \"name\": \"get_weather\",\n                \"args\": {\"city\": \"Boston, U.K.\"},\n            },\n        }\n        if \"boston\" in request[\"description\"].lower()\n        else {\"type\": \"approve\"}\n        for request in interrupt.value[\"action_requests\"]\n    ]\n\ndecisions = {}\nfor interrupt in interrupts:\n    decisions[interrupt.id] = {\n        \"decisions\": _get_interrupt_decisions(interrupt)\n    }\n\ndecisions\n```\n```shell title=\"Output\"\n{\n    'a96c40474e429d661b5b32a8d86f0f3e': {\n        'decisions': [\n            {\n                'type': 'edit',\n                 'edited_action': {\n                     'name': 'get_weather',\n                     'args': {'city': 'Boston, U.K.'}\n                 }\n            },\n            {'type': 'approve'},\n        ]\n    }\n}\n```\n\nWe can then resume by passing a", "metadata": {"source": "streaming/overview.mdx"}}
{"text": "          {\n                'type': 'edit',\n                 'edited_action': {\n                     'name': 'get_weather',\n                     'args': {'city': 'Boston, U.K.'}\n                 }\n            },\n            {'type': 'approve'},\n        ]\n    }\n}\n```\n\nWe can then resume by passing a [command](/oss/langchain/human-in-the-loop#responding-to-interrupts) into the same streaming loop:\n```python\ninterrupts = []\nfor stream_mode, data in agent.stream(\n    Command(resume=decisions),  # [!code highlight]\n    config=config,\n    stream_mode=[\"messages\", \"updates\"],\n):\n    # Streaming loop is unchanged\n    if stream_mode == \"messages\":\n        token, metadata = data\n        if isinstance(token, AIMessageChunk):\n            _render_message_chunk(token)\n    if stream_mode == \"updates\":\n        for source, update in data.items():\n            if source in (\"model\", \"tools\"):\n                _render_completed_message(update[\"messages\"][-1])\n            if source == \"__interrupt__\":\n                interrupts.extend(update)\n                _render_interrupt(update[0])\n```\n```shell title=\"Output\"\nTool response: [{'type': 'text', 'text': \"It's always sunny in Boston, U.K.!\"}]\nTool response: [{'type': 'text', 'text': \"It's always sunny in San Francisco!\"}]\n-| **|Boston|**|:| It|'s| always| sunny| in| Boston|,| U|.K|.|\n|-| **|San| Francisco|**|:| It|'s| always| sunny| in| San| Francisco|!|\n```\n\n### Streaming from sub-agents\n\nWhen there are multiple LLMs at any point in an agent, it's often necessary to disambiguate the source of messages as they are generated.\n\nTo do this, pass a [`name`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent(name)) to each agent when creating it. This name is then available in metadata via the `lc_agent_name` key when streaming in `\"messages\"` mode.\n\nBelow, we update the [streaming tool calls](#streaming-tool-calls", "metadata": {"source": "streaming/overview.mdx"}}
{"text": " **|Boston|**|:| It|'s| always| sunny| in| Boston|,| U|.K|.|\n|-| **|San| Francisco|**|:| It|'s| always| sunny| in| San| Francisco|!|\n```\n\n### Streaming from sub-agents\n\nWhen there are multiple LLMs at any point in an agent, it's often necessary to disambiguate the source of messages as they are generated.\n\nTo do this, pass a [`name`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent(name)) to each agent when creating it. This name is then available in metadata via the `lc_agent_name` key when streaming in `\"messages\"` mode.\n\nBelow, we update the [streaming tool calls](#streaming-tool-calls) example:\n\n1. We replace our tool with a `call_weather_agent` tool that invokes an agent internally\n2. We add a `name` to each agent\n3. We specify [`subgraphs=True`](/oss/langgraph/use-subgraphs#stream-subgraph-outputs) when creating the stream\n4. Our stream processing is identical to before, but we add logic to keep track of what agent is active using `create_agent`'s `name` parameter\n\n<Tip>\nWhen you set a `name` on an agent, that name is also attached to any `AIMessage`s generated by that agent.\n</Tip>\n\n\n\nFirst we construct the agent:\n```python\nfrom typing import Any\n\nfrom langchain.agents import create_agent\nfrom langchain.chat_models import init_chat_model\nfrom langchain.messages import AIMessage, AnyMessage\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n\n    return f\"It's always sunny in {city}!\"\n\n\nweather_model = init_chat_model(\"openai:gpt-5.2\")\nweather_agent = create_agent(\n    model=weather_model,\n    tools=[get_weather],\n    name=\"weather_agent\",  # [!code highlight]\n)\n\n\ndef call_weather_agent(query: str) -> str:\n    \"\"\"Query the weather agent.\"\"\"\n    result = weather_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": query}]\n    })\n    return result[\"messages\"][-1].text\n\n\nsupervisor_model = init_chat_model(\"openai:gpt-5.2\")\nagent = create_agent(\n    model=supervisor_model,\n    tools=[call_weather_agent],\n    name=\"supervisor\",  # [!code highlight]\n)\n```\nNext, we add logic to the streaming loop to report which agent is emitting tokens:\n```python\ndef _render_message_chunk(token: AIMessageChunk) -> None:\n    if token.text:\n        print(token.text, end=\"|\")\n    if token.tool_call_chunks:\n        print(token.tool_call_chunks)\n\n\ndef _render_completed_message(message: AnyMessage) -> None", "metadata": {"source": "streaming/overview.mdx"}}
{"text": "][-1].text\n\n\nsupervisor_model = init_chat_model(\"openai:gpt-5.2\")\nagent = create_agent(\n    model=supervisor_model,\n    tools=[call_weather_agent],\n    name=\"supervisor\",  # [!code highlight]\n)\n```\nNext, we add logic to the streaming loop to report which agent is emitting tokens:\n```python\ndef _render_message_chunk(token: AIMessageChunk) -> None:\n    if token.text:\n        print(token.text, end=\"|\")\n    if token.tool_call_chunks:\n        print(token.tool_call_chunks)\n\n\ndef _render_completed_message(message: AnyMessage) -> None:\n    if isinstance(message, AIMessage) and message.tool_calls:\n        print(f\"Tool calls: {message.tool_calls}\")\n    if isinstance(message, ToolMessage):\n        print(f\"Tool response: {message.content_blocks}\")\n\n\ninput_message = {\"role\": \"user\", \"content\": \"What is the weather in Boston?\"}\ncurrent_agent = None  # [!code highlight]\nfor _, stream_mode, data in agent.stream(\n    {\"messages\": [input_message]},\n    stream_mode=[\"messages\", \"updates\"],\n    subgraphs=True,  # [!code highlight]\n):\n    if stream_mode == \"messages\":\n        token, metadata = data\n        if agent_name := metadata.get(\"lc_agent_name\"):  # [!code highlight]\n            if agent_name != current_agent:  # [!code highlight]\n                print(f\"\ud83e\udd16 {agent_name}: \")  # [!code highlight]\n                current_agent = agent_name  # [!code highlight]\n        if isinstance(token, AIMessage):\n            _render_message_chunk(token)\n    if stream_mode == \"updates\":\n        for source, update in data.items():\n            if source in (\"model\", \"tools\"):\n                _render_completed_message(update[\"messages\"][-1])\n```\n```shell title=\"Output\" expandable\n\ud83e\udd16 supervisor:\n[{'name': 'call_weather_agent', 'args': '', 'id': 'call_asorzUf0mB6sb7MiKfgojp7I', 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '{\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'query', 'id': None, 'index': 0", "metadata": {"source": "streaming/overview.mdx"}}
{"text": " in data.items():\n            if source in (\"model\", \"tools\"):\n                _render_completed_message(update[\"messages\"][-1])\n```\n```shell title=\"Output\" expandable\n\ud83e\udd16 supervisor:\n[{'name': 'call_weather_agent', 'args': '', 'id': 'call_asorzUf0mB6sb7MiKfgojp7I', 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '{\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'query', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '\":\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'Boston', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': ' weather', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': ' right', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': ' now', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': ' and', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': \" today's\", 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': ' forecast', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '\"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\nTool calls: [{'name': 'call_weather_agent', 'args': {'query': \"Boston weather right now and today's forecast\"}, 'id': 'call_asorzUf0mB6sb7MiKfgojp7I', 'type': 'tool_call'}]\n\ud83e\udd16 weather_agent:\n[{'name': 'get_weather', 'args': '', 'id': 'call_LZ89lT8fW6w8vqck5pZeaDIx', 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '{\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'city', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '\":\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'", "metadata": {"source": "streaming/overview.mdx"}}
{"text": "fgojp7I', 'type': 'tool_call'}]\n\ud83e\udd16 weather_agent:\n[{'name': 'get_weather', 'args': '', 'id': 'call_LZ89lT8fW6w8vqck5pZeaDIx', 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '{\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'city', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '\":\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'Boston', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '\"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\nTool calls: [{'name': 'get_weather', 'args': {'city': 'Boston'}, 'id': 'call_LZ89lT8fW6w8vqck5pZeaDIx', 'type': 'tool_call'}]\nTool response: [{'type': 'text', 'text': \"It's always sunny in Boston!\"}]\nBoston| weather| right| now|:| **|Sunny|**|.\n\n|Today|'s| forecast| for| Boston|:| **|Sunny| all| day|**|.|Tool response: [{'type': 'text', 'text': 'Boston weather right now: **Sunny**.\\n\\nToday's forecast for Boston: **Sunny all day**.'}]\n\ud83e\udd16 supervisor:\nBoston| weather| right| now|:| **|Sunny|**|.\n\n|Today|'s| forecast| for| Boston|:| **|Sunny| all| day|**|.|\n```\n:::\n\n## Disable streaming\n\nIn some applications you might need to disable streaming of individual tokens for a given model. This is useful when:\n\n- Working with [multi-agent](/oss/langchain/multi-agent) systems to control which agents stream their output\n- Mixing models that support streaming with those that do not\n- Deploying to [LangSmith](/langsmith/home) and wanting to prevent certain model outputs from being streamed to the client\n\n:::python\nSet `streaming=False` when initializing the model.\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(\n    model=\"gpt-4.1\",\n    streaming=False  # [!code highlight]\n)\n```\n:::\n\n:::js\nSet `streaming: false` when initializing the model.\n\n```typescript\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst model = new ChatOpenAI({\n  model: \"gpt-4.1\",\n  streaming: false,  // [!code highlight]\n});\n```\n:::\n\n<Tip>\nWhen deploying to LangSmith, set `streaming=False` on any models whose output you don't want streamed to", "metadata": {"source": "streaming/overview.mdx"}}
{"text": " model outputs from being streamed to the client\n\n:::python\nSet `streaming=False` when initializing the model.\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(\n    model=\"gpt-4.1\",\n    streaming=False  # [!code highlight]\n)\n```\n:::\n\n:::js\nSet `streaming: false` when initializing the model.\n\n```typescript\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst model = new ChatOpenAI({\n  model: \"gpt-4.1\",\n  streaming: false,  // [!code highlight]\n});\n```\n:::\n\n<Tip>\nWhen deploying to LangSmith, set `streaming=False` on any models whose output you don't want streamed to the client. This is configured in your graph code before deployment.\n</Tip>\n\n:::python\n<Note>\nNot all chat model integrations support the `streaming` parameter. If your model doesn't support it, use `disable_streaming=True` instead. This parameter is available on all chat models via the base class.\n</Note>\n:::\n\n:::js\n<Note>\nNot all chat model integrations support the `streaming` parameter. If your model doesn't support it, use `disableStreaming: true` instead. This parameter is available on all chat models via the base class.\n</Note>\n:::\n\nSee the [LangGraph streaming guide](/oss/langgraph/streaming#disable-streaming-for-specific-chat-models) for more details.\n\n## Related\n\n- [Frontend streaming](/oss/langchain/streaming/frontend) \u2014 Build React UIs with `useStream` for real-time agent interactions\n- [Streaming with chat models](/oss/langchain/models#stream) \u2014 Stream tokens directly from a chat model without using an agent or graph\n- [Streaming with human-in-the-loop](/oss/langchain/human-in-the-loop#streaming-with-hil) \u2014 Stream agent progress while handling interrupts for human review\n- [LangGraph streaming](/oss/langgraph/streaming) \u2014 Advanced streaming options including `values`, `debug` modes, and subgraph streaming\n\n", "metadata": {"source": "streaming/overview.mdx"}}
{"text": "---\ntitle: Frontend\ndescription: Build generative UIs with real-time streaming from LangChain agents, LangGraph graphs, and custom APIs\n---\n\nThe `useStream` React hook provides seamless integration with LangGraph streaming capabilities. It handles all the complexities of streaming, state management, and branching logic, letting you focus on building great generative UI experiences.\n\nKey features:\n\n* <Icon icon=\"messages\" size={16} /> **Messages streaming** \u2014 Handle a stream of message chunks to form a complete message\n* <Icon icon=\"arrows-rotate\" size={16} /> **Automatic state management** \u2014 for messages, interrupts, loading states, and errors\n* <Icon icon=\"code-branch\" size={16} /> **Conversation branching** \u2014 Create alternate conversation paths from any point in the chat history\n* <Icon icon=\"palette\" size={16} /> **UI-agnostic design** \u2014 Bring your own components and styling\n\n## Installation\n\nInstall the LangGraph SDK to use the `useStream` hook in your React application:\n\n:::js\n```bash\nnpm install @langchain/langgraph-sdk\n```\n:::\n\n## Basic usage\n\nThe `useStream` hook connects to any LangGraph graph, whether that's running on from your own endpoint, or deployed using [LangSmith deployments](/langsmith/deployments).\n\n```tsx\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\n\nfunction Chat() {\n  const stream = useStream({\n    assistantId: \"agent\",\n    // Local development\n    apiUrl: \"http://localhost:2024\",\n    // Production deployment (LangSmith hosted)\n    // apiUrl: \"https://your-deployment.us.langgraph.app\"\n  });\n\n  const handleSubmit = (message: string) => {\n    stream.submit({\n      messages: [\n        { content: message, type: \"human\" }\n      ],\n    });\n  };\n\n  return (\n    <div>\n      {stream.messages.map((message, idx) => (\n        <div key={message.id ?? idx}>\n          {message.type}: {message.content}\n        </div>\n      ))}\n\n      {stream.isLoading && <div>Loading...</div>}\n      {stream.error && <div>Error: {stream.error.message}</div>}\n    </div>\n  );\n}\n```\n\n<Tip>\nLearn how to [deploy your agents to LangSmith](/oss/langchain/deploy) for production-ready hosting with built-in observability, authentication, and scaling.\n</Tip>\n\n<Accordion title=\"`useStream` parameters\">\n\n<ParamField body=\"assistantId\" type=\"string\" required>\n    The ID of the agent to connect to. When using LangSmith deployments, this must match the agent ID shown in your deployment dashboard. For custom API deployments or local development, this can be any string that your server uses to identify the agent.\n</ParamField>\n\n<ParamField body=\"apiUrl\" type=\"string\">\n    The URL of the LangGraph server", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "   {stream.error && <div>Error: {stream.error.message}</div>}\n    </div>\n  );\n}\n```\n\n<Tip>\nLearn how to [deploy your agents to LangSmith](/oss/langchain/deploy) for production-ready hosting with built-in observability, authentication, and scaling.\n</Tip>\n\n<Accordion title=\"`useStream` parameters\">\n\n<ParamField body=\"assistantId\" type=\"string\" required>\n    The ID of the agent to connect to. When using LangSmith deployments, this must match the agent ID shown in your deployment dashboard. For custom API deployments or local development, this can be any string that your server uses to identify the agent.\n</ParamField>\n\n<ParamField body=\"apiUrl\" type=\"string\">\n    The URL of the LangGraph server. Defaults to `http://localhost:2024` for local development.\n</ParamField>\n\n<ParamField body=\"apiKey\" type=\"string\">\n    API key for authentication. Required when connecting to deployed agents on LangSmith.\n</ParamField>\n\n<ParamField body=\"threadId\" type=\"string\">\n    Connect to an existing thread instead of creating a new one. Useful for resuming conversations.\n</ParamField>\n\n<ParamField body=\"onThreadId\" type=\"(id: string) => void\">\n    Callback invoked when a new thread is created. Use this to persist the thread ID for later use.\n</ParamField>\n\n<ParamField body=\"reconnectOnMount\" type=\"boolean | (() => Storage)\">\n    Automatically resume an ongoing run when the component mounts. Set to `true` to use session storage, or provide a custom storage function.\n</ParamField>\n\n<ParamField body=\"onCreated\" type=\"(run: Run) => void\">\n    Callback invoked when a new run is created. Useful for persisting run metadata for resumption.\n</ParamField>\n\n<ParamField body=\"onError\" type=\"(error: Error) => void\">\n    Callback invoked when an error occurs during streaming.\n</ParamField>\n\n<ParamField body=\"onFinish\" type=\"(state: StateType, run?: Run) => void\">\n    Callback invoked when the stream completes successfully with the final state.\n</ParamField>\n\n<ParamField body=\"onCustomEvent\" type=\"(data: unknown, context: { mutate }) => void\">\n    Handle custom events emitted from your agent using the `writer`. See [Custom streaming events](#custom-streaming-events).\n</ParamField>\n\n<ParamField body=\"onUpdateEvent\" type=\"(data: unknown, context: { mutate }) => void\">\n    Handle state update events after each graph step.\n</ParamField>\n\n<ParamField body=\"onMetadataEvent\" type=\"(metadata: { run_id, thread_id }) => void\">\n    Handle metadata events with run and thread information.\n</ParamField>\n\n<ParamField body=\"messagesKey\" type=\"string\" default=\"messages\">\n    The key in the graph state that contains the messages array.\n</ParamField>\n\n<ParamField body=\"throttle\" type=\"boolean\" default=\"true\">\n    Batch state updates for better rendering performance. Disable for immediate updates.\n</ParamField>\n\n<ParamField body=\"initialValues\" type=\"StateType | null\">", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "-events).\n</ParamField>\n\n<ParamField body=\"onUpdateEvent\" type=\"(data: unknown, context: { mutate }) => void\">\n    Handle state update events after each graph step.\n</ParamField>\n\n<ParamField body=\"onMetadataEvent\" type=\"(metadata: { run_id, thread_id }) => void\">\n    Handle metadata events with run and thread information.\n</ParamField>\n\n<ParamField body=\"messagesKey\" type=\"string\" default=\"messages\">\n    The key in the graph state that contains the messages array.\n</ParamField>\n\n<ParamField body=\"throttle\" type=\"boolean\" default=\"true\">\n    Batch state updates for better rendering performance. Disable for immediate updates.\n</ParamField>\n\n<ParamField body=\"initialValues\" type=\"StateType | null\">\n    Initial state values to display while the first stream is loading. Useful for showing cached thread data immediately.\n</ParamField>\n\n</Accordion>\n\n<Accordion title=\"`useStream` return values\">\n\n<ParamField body=\"messages\" type=\"Message[]\">\n    All messages in the current thread, including both human and AI messages.\n</ParamField>\n\n<ParamField body=\"values\" type=\"StateType\">\n    The current graph state values. Type is inferred from the agent or graph type parameter.\n</ParamField>\n\n<ParamField body=\"isLoading\" type=\"boolean\">\n    Whether a stream is currently in progress. Use this to show loading indicators.\n</ParamField>\n\n<ParamField body=\"error\" type=\"Error | null\">\n    Any error that occurred during streaming. `null` when no error.\n</ParamField>\n\n<ParamField body=\"interrupt\" type=\"Interrupt | undefined\">\n    Current interrupt requiring user input, such as human-in-the-loop approval requests.\n</ParamField>\n\n<ParamField body=\"toolCalls\" type=\"ToolCallWithResult[]\">\n    All tool calls across all messages, with their results and state (`pending`, `completed`, or `error`).\n</ParamField>\n\n<ParamField body=\"submit\" type=\"(input, options?) => Promise<void>\">\n    Submit new input to the agent. Pass `null` as input when resuming from an interrupt with a command. Options include `checkpoint` for branching, `optimisticValues` for optimistic updates, and `threadId` for optimistic thread creation.\n</ParamField>\n\n<ParamField body=\"stop\" type=\"() => void\">\n    Stop the current stream immediately.\n</ParamField>\n\n<ParamField body=\"joinStream\" type=\"(runId: string) => void\">\n    Resume an existing stream by run ID. Use with `onCreated` for manual stream resumption.\n</ParamField>\n\n<ParamField body=\"setBranch\" type=\"(branch: string) => void\">\n    Switch to a different branch in the conversation history.\n</ParamField>\n\n<ParamField body=\"getToolCalls\" type=\"(message) => ToolCall[]\">\n    Get all tool calls for a specific AI message.\n</ParamField>\n\n<ParamField body=\"getMessagesMetadata\" type=\"(message) => MessageMetadata\">\n    Get metadata for a message, including streaming info like `langgraph_node` for identifying the source node, and `", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "\">\n    Stop the current stream immediately.\n</ParamField>\n\n<ParamField body=\"joinStream\" type=\"(runId: string) => void\">\n    Resume an existing stream by run ID. Use with `onCreated` for manual stream resumption.\n</ParamField>\n\n<ParamField body=\"setBranch\" type=\"(branch: string) => void\">\n    Switch to a different branch in the conversation history.\n</ParamField>\n\n<ParamField body=\"getToolCalls\" type=\"(message) => ToolCall[]\">\n    Get all tool calls for a specific AI message.\n</ParamField>\n\n<ParamField body=\"getMessagesMetadata\" type=\"(message) => MessageMetadata\">\n    Get metadata for a message, including streaming info like `langgraph_node` for identifying the source node, and `firstSeenState` for branching.\n</ParamField>\n\n<ParamField body=\"experimental_branchTree\" type=\"BranchTree\">\n    Tree representation of the thread for advanced branching controls in non-message based graphs.\n</ParamField>\n\n</Accordion>\n\n## Thread management\n\nKeep track of conversations with built-in thread management. You can access the current thread ID and get notified when new threads are created:\n\n```tsx\nimport { useState } from \"react\";\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\n\nfunction Chat() {\n  const [threadId, setThreadId] = useState<string | null>(null);\n\n  const stream = useStream({\n    apiUrl: \"http://localhost:2024\",\n    assistantId: \"agent\",\n    threadId: threadId,\n    onThreadId: setThreadId,\n  });\n\n  // threadId is updated when a new thread is created\n  // Store it in URL params or localStorage for persistence\n}\n```\n\nWe recommend storing the `threadId` to let users resume conversations after page refreshes.\n\n### Resume after page refresh\n\nThe `useStream` hook can automatically resume an ongoing run upon mounting by setting `reconnectOnMount: true`. This is useful for continuing a stream after a page refresh, ensuring no messages and events generated during the downtime are lost.\n\n```tsx\nconst stream = useStream({\n  apiUrl: \"http://localhost:2024\",\n  assistantId: \"agent\",\n  reconnectOnMount: true,\n});\n```\n\nBy default the ID of the created run is stored in `window.sessionStorage`, which can be swapped by passing a custom storage function:\n\n```tsx\nconst stream = useStream({\n  apiUrl: \"http://localhost:2024\",\n  assistantId: \"agent\",\n  reconnectOnMount: () => window.localStorage,\n});\n```\n\nFor manual control over the resumption process, use the run callbacks to persist metadata and `joinStream` to resume:\n\n```tsx\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\nimport { useEffect, useRef } from \"react\";\n\nfunction Chat({ threadId }: { threadId: string | null }) {\n  const stream = useStream({\n    apiUrl: \"http://localhost:2024\",\n    assistantId: \"agent\",\n    threadId,\n    onCreated: (run) => {\n      // Pers", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": " a custom storage function:\n\n```tsx\nconst stream = useStream({\n  apiUrl: \"http://localhost:2024\",\n  assistantId: \"agent\",\n  reconnectOnMount: () => window.localStorage,\n});\n```\n\nFor manual control over the resumption process, use the run callbacks to persist metadata and `joinStream` to resume:\n\n```tsx\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\nimport { useEffect, useRef } from \"react\";\n\nfunction Chat({ threadId }: { threadId: string | null }) {\n  const stream = useStream({\n    apiUrl: \"http://localhost:2024\",\n    assistantId: \"agent\",\n    threadId,\n    onCreated: (run) => {\n      // Persist run ID when stream starts\n      window.sessionStorage.setItem(`resume:${run.thread_id}`, run.run_id);\n    },\n    onFinish: (_, run) => {\n      // Clean up when stream completes\n      window.sessionStorage.removeItem(`resume:${run?.thread_id}`);\n    },\n  });\n\n  // Resume stream on mount if there's a stored run ID\n  const joinedThreadId = useRef<string | null>(null);\n  useEffect(() => {\n    if (!threadId) return;\n    const runId = window.sessionStorage.getItem(`resume:${threadId}`);\n    if (runId && joinedThreadId.current !== threadId) {\n      stream.joinStream(runId);\n      joinedThreadId.current = threadId;\n    }\n  }, [threadId]);\n\n  const handleSubmit = (text: string) => {\n    // Use streamResumable to ensure events aren't lost\n    stream.submit(\n      { messages: [{ type: \"human\", content: text }] },\n      { streamResumable: true }\n    );\n  };\n}\n```\n\n<Card title=\"Try the session persistence example\" icon=\"rotate\" href=\"https://github.com/langchain-ai/langgraphjs/tree/main/examples/ui-react/src/examples/session-persistence\">\n  See a complete implementation of stream resumption with `reconnectOnMount` and thread persistence in the `session-persistence` example.\n</Card>\n\n## Optimistic updates\n\nYou can optimistically update the client state before performing a network request, providing immediate feedback to the user:\n\n```tsx\nconst stream = useStream({\n  apiUrl: \"http://localhost:2024\",\n  assistantId: \"agent\",\n});\n\nconst handleSubmit = (text: string) => {\n  const newMessage = { type: \"human\" as const, content: text };\n\n  stream.submit(\n    { messages: [newMessage] },\n    {\n      optimisticValues(prev) {\n        const prevMessages = prev.messages ?? [];\n        return { ...prev, messages: [...prevMessages, newMessage] };\n      },\n", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": " thread persistence in the `session-persistence` example.\n</Card>\n\n## Optimistic updates\n\nYou can optimistically update the client state before performing a network request, providing immediate feedback to the user:\n\n```tsx\nconst stream = useStream({\n  apiUrl: \"http://localhost:2024\",\n  assistantId: \"agent\",\n});\n\nconst handleSubmit = (text: string) => {\n  const newMessage = { type: \"human\" as const, content: text };\n\n  stream.submit(\n    { messages: [newMessage] },\n    {\n      optimisticValues(prev) {\n        const prevMessages = prev.messages ?? [];\n        return { ...prev, messages: [...prevMessages, newMessage] };\n      },\n    }\n  );\n};\n```\n\n### Optimistic thread creation\n\nUse the `threadId` option in `submit` to enable optimistic UI patterns where you need to know the thread ID before the thread is created:\n\n```tsx\nimport { useState } from \"react\";\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\n\nfunction Chat() {\n  const [threadId, setThreadId] = useState<string | null>(null);\n  const [optimisticThreadId] = useState(() => crypto.randomUUID());\n\n  const stream = useStream({\n    apiUrl: \"http://localhost:2024\",\n    assistantId: \"agent\",\n    threadId,\n    onThreadId: setThreadId,\n  });\n\n  const handleSubmit = (text: string) => {\n    // Navigate immediately without waiting for thread creation\n    window.history.pushState({}, \"\", `/threads/${optimisticThreadId}`);\n\n    // Create thread with the predetermined ID\n    stream.submit(\n      { messages: [{ type: \"human\", content: text }] },\n      { threadId: optimisticThreadId }\n    );\n  };\n}\n```\n\n### Cached thread display\n\nUse the `initialValues` option to display cached thread data immediately while the history is being loaded from the server:\n\n```tsx\nfunction Chat({ threadId, cachedData }) {\n  const stream = useStream({\n    apiUrl: \"http://localhost:2024\",\n    assistantId: \"agent\",\n    threadId,\n    initialValues: cachedData?.values,\n  });\n\n  // Shows cached messages instantly, then updates when server responds\n}\n```\n\n## Branching\n\nCreate alternate conversation paths by editing previous messages or regenerating AI responses. Use `getMessagesMetadata()` to access checkpoint information for branching:\n\n<CodeGroup>\n\n```tsx Chat.tsx\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\nimport { BranchSwitcher } from \"./BranchSwitcher\";\n\nfunction Chat() {\n  const stream = useStream({\n    apiUrl: \"http://localhost:2024\",\n    assistantId: \"agent\",\n  });\n\n  return (\n    <div>\n      {stream.messages.map((message) => {\n        const", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "    initialValues: cachedData?.values,\n  });\n\n  // Shows cached messages instantly, then updates when server responds\n}\n```\n\n## Branching\n\nCreate alternate conversation paths by editing previous messages or regenerating AI responses. Use `getMessagesMetadata()` to access checkpoint information for branching:\n\n<CodeGroup>\n\n```tsx Chat.tsx\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\nimport { BranchSwitcher } from \"./BranchSwitcher\";\n\nfunction Chat() {\n  const stream = useStream({\n    apiUrl: \"http://localhost:2024\",\n    assistantId: \"agent\",\n  });\n\n  return (\n    <div>\n      {stream.messages.map((message) => {\n        const meta = stream.getMessagesMetadata(message);\n        const parentCheckpoint = meta?.firstSeenState?.parent_checkpoint;\n\n        return (\n          <div key={message.id}>\n            <div>{message.content as string}</div>\n\n            {/* Edit human messages */}\n            {message.type === \"human\" && (\n              <button\n                onClick={() => {\n                  const newContent = prompt(\"Edit message:\", message.content as string);\n                  if (newContent) {\n                    stream.submit(\n                      { messages: [{ type: \"human\", content: newContent }] },\n                      { checkpoint: parentCheckpoint }\n                    );\n                  }\n                }}\n              >\n                Edit\n              </button>\n            )}\n\n            {/* Regenerate AI messages */}\n            {message.type === \"ai\" && (\n              <button\n                onClick={() => stream.submit(undefined, { checkpoint: parentCheckpoint })}\n              >\n                Regenerate\n              </button>\n            )}", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "           Edit\n              </button>\n            )}\n\n            {/* Regenerate AI messages */}\n            {message.type === \"ai\" && (\n              <button\n                onClick={() => stream.submit(undefined, { checkpoint: parentCheckpoint })}\n              >\n                Regenerate\n              </button>\n            )}\n\n            {/* Switch between branches */}\n            <BranchSwitcher\n              branch={meta?.branch}\n              branchOptions={meta?.branchOptions}\n              onSelect={(branch) => stream.setBranch(branch)}\n            />\n          </div>\n        );\n      })}\n    </div>\n  );\n}\n```\n\n```tsx BranchSwitcher.tsx\n/**\n * Component for navigating between conversation branches.\n * Shows the current branch position and allows switching between alternatives.\n */\nexport function BranchSwitcher({\n  branch,\n  branchOptions,\n  onSelect,\n}: {\n  branch: string | undefined;\n  branchOptions: string[] | undefined;\n  onSelect: (branch: string) => void;\n}) {\n  if (!branchOptions || !branch) return null;\n  const index = branchOptions.indexOf(branch);\n\n  return (\n    <div className=\"flex items-center gap-2\">\n      <button\n        type=\"button\"\n        disabled={index <= 0}\n        onClick={() => onSelect(branchOptions[index - 1])}\n      >\n        \u2190\n      </button>\n      <span>{index + 1} / {branchOptions.length}</span>\n      <button\n        type=\"button\"\n        disabled={index >= branchOptions.length - 1}\n        onClick={() => onSelect(branchOptions[index + 1])}\n      >\n        \u2192\n      </button>\n    </div>\n  );\n}\n```\n\n</CodeGroup>\n\nFor advanced use cases, use the `experimental_branchTree` property to get the tree representation of the thread for non-message based graphs.\n\n<Card title=\"Try the branching example\" icon=\"code-branch\" href=\"https://github.com/langchain-", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": " </button>\n      <span>{index + 1} / {branchOptions.length}</span>\n      <button\n        type=\"button\"\n        disabled={index >= branchOptions.length - 1}\n        onClick={() => onSelect(branchOptions[index + 1])}\n      >\n        \u2192\n      </button>\n    </div>\n  );\n}\n```\n\n</CodeGroup>\n\nFor advanced use cases, use the `experimental_branchTree` property to get the tree representation of the thread for non-message based graphs.\n\n<Card title=\"Try the branching example\" icon=\"code-branch\" href=\"https://github.com/langchain-ai/langgraphjs/tree/main/examples/ui-react/src/examples/branching-chat\">\n  See a complete implementation of conversation branching with edit, regenerate, and branch switching in the `branching-chat` example.\n</Card>\n\n## Type-safe streaming\n\nThe `useStream` hook supports full type inference when used with agents created via @[`createAgent`] or graphs created with @[`StateGraph`]. Pass `typeof agent` or `typeof graph` as the type parameter to automatically infer tool call types.\n\n### With `createAgent`\n\nWhen using @[`createAgent`], tool call types are automatically inferred from the tools you register to your agent:\n\n:::python\n<CodeGroup>\n\n```python agent.py\nfrom langchain import create_agent, tool\n\n@tool\ndef get_weather(location: str) -> str:\n    \"\"\"Get weather for a location.\"\"\"\n    return f\"Weather in {location}: Sunny, 72\u00b0F\"\n\nagent = create_agent(\n    model=\"openai:gpt-4.1-mini\",\n    tools=[get_weather],\n)\n```\n\n```tsx Chat.tsx\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\nimport type { AgentState } from \"./types\";\n\nfunction Chat() {\n  // Use the manually defined state type\n  const stream = useStream<AgentState>({\n    assistantId: \"agent\",\n    apiUrl: \"http://localhost:2024\",\n  });\n\n  // stream.toolCalls[0].call.name is typed as \"get_weather\"\n  // stream.toolCalls[0].call.args is typed as { location: string }\n}\n```\n\n```typescript types.ts\nimport type { Message } from \"@langchain/langgraph-sdk\";\n\n// Define tool call types to match your Python agent\nexport type GetWeatherToolCall = {\n  name: \"get_weather\";\n  args: { location: string };\n  id?: string;\n};\n\nexport type AgentToolCalls = GetWeatherToolCall;\n\nexport interface AgentState {\n  messages: Message<AgentToolCalls>[];\n}\n```\n\n</CodeGroup>\n:::\n\n:::js\n<CodeGroup>\n\n```typescript agent.ts\nimport { createAgent, tool } from \"langchain\";\nimport { z } from \"zod\";\n\nconst getWeather = tool(\n  async ({ location })", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "Calls[0].call.args is typed as { location: string }\n}\n```\n\n```typescript types.ts\nimport type { Message } from \"@langchain/langgraph-sdk\";\n\n// Define tool call types to match your Python agent\nexport type GetWeatherToolCall = {\n  name: \"get_weather\";\n  args: { location: string };\n  id?: string;\n};\n\nexport type AgentToolCalls = GetWeatherToolCall;\n\nexport interface AgentState {\n  messages: Message<AgentToolCalls>[];\n}\n```\n\n</CodeGroup>\n:::\n\n:::js\n<CodeGroup>\n\n```typescript agent.ts\nimport { createAgent, tool } from \"langchain\";\nimport { z } from \"zod\";\n\nconst getWeather = tool(\n  async ({ location }) => `Weather in ${location}: Sunny, 72\u00b0F`,\n  {\n    name: \"get_weather\",\n    description: \"Get weather for a location\",\n    schema: z.object({\n      location: z.string().describe(\"The city to get weather for\"),\n    }),\n  }\n);\n\nexport const agent = createAgent({\n  model: \"openai:gpt-4.1-mini\",\n  tools: [getWeather],\n});\n```\n\n```tsx Chat.tsx\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\nimport type { agent } from \"./agent\";\n\nfunction Chat() {\n  // Tool calls are automatically typed from the agent's tools\n  const stream = useStream<typeof agent>({\n    assistantId: \"agent\",\n    apiUrl: \"http://localhost:2024\",\n  });\n\n  // stream.toolCalls[0].call.name is typed as \"get_weather\"\n  // stream.toolCalls[0].call.args is typed as { location: string }\n}\n```\n\n</CodeGroup>\n:::\n\n### With `StateGraph`\n\nFor custom @[`StateGraph`] applications, the state types are inferred from the graph's annotation:\n\n:::python\n<CodeGroup>\n\n```python graph.py\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\nfrom langchain_openai import ChatOpenAI\nfrom typing import TypedDict, Annotated\n\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n\nmodel = ChatOpenAI(model=\"gpt-4.1-mini\")\n\nasync def agent(state: State) -> dict:\n    response = await model.ainvoke(state[\"messages\"])\n    return {\"messages\": [response]}\n\nworkflow = StateGraph(State)\nworkflow.add_node(\"agent\", agent)\nworkflow.add_edge(START, \"agent\")\nworkflow.add_edge(\"agent\", END)\n\ngraph = workflow.compile()\n```\n\n```tsx Chat.tsx\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\nimport type { GraphState } from \"./types\";\n\nfunction Chat() {\n  // Use the manually defined state type\n  const stream = useStream<GraphState>({\n ", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": ", add_messages]\n\nmodel = ChatOpenAI(model=\"gpt-4.1-mini\")\n\nasync def agent(state: State) -> dict:\n    response = await model.ainvoke(state[\"messages\"])\n    return {\"messages\": [response]}\n\nworkflow = StateGraph(State)\nworkflow.add_node(\"agent\", agent)\nworkflow.add_edge(START, \"agent\")\nworkflow.add_edge(\"agent\", END)\n\ngraph = workflow.compile()\n```\n\n```tsx Chat.tsx\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\nimport type { GraphState } from \"./types\";\n\nfunction Chat() {\n  // Use the manually defined state type\n  const stream = useStream<GraphState>({\n    assistantId: \"my-graph\",\n    apiUrl: \"http://localhost:2024\",\n  });\n\n  // stream.values is typed based on your defined state\n}\n```\n\n```typescript types.ts\nimport type { Message } from \"@langchain/langgraph-sdk\";\n\n// Define state to match your Python graph's State TypedDict\nexport interface GraphState {\n  messages: Message[];\n}\n```\n\n</CodeGroup>\n:::\n\n:::js\n<CodeGroup>\n\n```typescript graph.ts\nimport { StateGraph, MessagesAnnotation, START, END } from \"@langchain/langgraph\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst model = new ChatOpenAI({ model: \"gpt-4.1-mini\" });\n\nconst workflow = new StateGraph(MessagesAnnotation)\n  .addNode(\"agent\", async (state) => {\n    const response = await model.invoke(state.messages);\n    return { messages: [response] };\n  })\n  .addEdge(START, \"agent\")\n  .addEdge(\"agent\", END);\n\nexport const graph = workflow.compile();\n```\n\n```tsx Chat.tsx\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\nimport type { graph } from \"./graph\";\n\nfunction Chat() {\n  // State types are automatically inferred from the graph\n  const stream = useStream<typeof graph>({\n    assistantId: \"my-graph\",\n    apiUrl: \"http://localhost:2024\",\n  });\n\n  // stream.values is typed based on the graph's state annotation\n}\n```\n\n</CodeGroup>\n:::\n\n### With Annotation types\n\nIf you're using LangGraph.js, you can reuse your graph's annotation types. Make sure to only import types to avoid importing the entire LangGraph.js runtime:\n\n:::js\n```tsx\nimport {\n  Annotation,\n  MessagesAnnotation,\n  type StateType,\n  type UpdateType,\n} from \"@langchain/langgraph/web\";\n\nconst AgentState = Annotation.Root({\n  ...MessagesAnnotation.spec,\n  context: Annotation<string>(),\n});\n\nconst stream = useStream<\n  StateType<typeof AgentState.spec>,\n  { UpdateType: UpdateType<typeof AgentState.spec> }\n>({\n  apiUrl: \"http://localhost:2024\",", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": " typed based on the graph's state annotation\n}\n```\n\n</CodeGroup>\n:::\n\n### With Annotation types\n\nIf you're using LangGraph.js, you can reuse your graph's annotation types. Make sure to only import types to avoid importing the entire LangGraph.js runtime:\n\n:::js\n```tsx\nimport {\n  Annotation,\n  MessagesAnnotation,\n  type StateType,\n  type UpdateType,\n} from \"@langchain/langgraph/web\";\n\nconst AgentState = Annotation.Root({\n  ...MessagesAnnotation.spec,\n  context: Annotation<string>(),\n});\n\nconst stream = useStream<\n  StateType<typeof AgentState.spec>,\n  { UpdateType: UpdateType<typeof AgentState.spec> }\n>({\n  apiUrl: \"http://localhost:2024\",\n  assistantId: \"agent\",\n});\n```\n:::\n\n### Advanced type configuration\n\nYou can specify additional type parameters for interrupts, custom events, and configurable options:\n\n:::js\n```tsx\nimport type { Message } from \"@langchain/langgraph-sdk\";\n\ntype State = { messages: Message[]; context?: string };\n\nconst stream = useStream<\n  State,\n  {\n    UpdateType: { messages: Message[] | Message; context?: string };\n    InterruptType: string;\n    CustomEventType: { type: \"progress\" | \"debug\"; payload: unknown };\n    ConfigurableType: { model: string };\n  }\n>({\n  apiUrl: \"http://localhost:2024\",\n  assistantId: \"agent\",\n});\n\n// stream.interrupt is typed as string | undefined\n// onCustomEvent receives typed events\n```\n:::\n\n## Rendering tool calls\n\nUse `getToolCalls` to extract and render tool calls from AI messages. Tool calls include the call details, result (if completed), and state.\n\n:::python\n<CodeGroup>\n\n```python agent.py\nfrom langchain import create_agent, tool\n\n@tool\ndef get_weather(location: str) -> str:\n    \"\"\"Get the current weather for a location.\"\"\"\n    return f'{{\"status\": \"success\", \"content\": \"Weather in {location}: Sunny, 72\u00b0F\"}}'\n\nagent = create_agent(\n    model=\"openai:gpt-4.1-mini\",\n    tools=[get_weather],\n)\n```\n\n```tsx Chat.tsx\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\nimport type { AgentState, AgentToolCalls } from \"./types\";\nimport { ToolCallCard } from \"./ToolCallCard\";\nimport { MessageBubble } from \"./MessageBubble\";\n\nfunction Chat() {\n  const stream = useStream<AgentState>({\n    assistantId: \"agent\",\n    apiUrl: \"http://localhost:2024\",\n  });\n\n  return (\n    <div className=\"flex flex-col gap-4\">\n      {stream.messages.map((message, idx) => {\n        if (message.type === \"ai\") {\n          const toolCalls = stream.getToolCalls(message);\n\n   ", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\nimport type { AgentState, AgentToolCalls } from \"./types\";\nimport { ToolCallCard } from \"./ToolCallCard\";\nimport { MessageBubble } from \"./MessageBubble\";\n\nfunction Chat() {\n  const stream = useStream<AgentState>({\n    assistantId: \"agent\",\n    apiUrl: \"http://localhost:2024\",\n  });\n\n  return (\n    <div className=\"flex flex-col gap-4\">\n      {stream.messages.map((message, idx) => {\n        if (message.type === \"ai\") {\n          const toolCalls = stream.getToolCalls(message);\n\n          if (toolCalls.length > 0) {\n            return (\n              <div key={message.id ?? idx} className=\"flex flex-col gap-2\">\n                {toolCalls.map((toolCall) => (\n                  <ToolCallCard key={toolCall.id} toolCall={toolCall} />\n                ))}\n              </div>\n            );\n          }\n        }\n\n        return <MessageBubble key={message.id ?? idx} message={message} />;\n      })}\n    </div>\n  );\n}\n```\n\n```tsx ToolCallCard.tsx\nimport type { ToolCallWithResult, ToolCallState } from \"@langchain/langgraph-sdk/react\";\nimport type { ToolMessage } from \"@langchain/langgraph-sdk\";\nimport type { AgentToolCalls, GetWeatherToolCall } from \"./types\";\nimport { parseToolResult } from \"./utils\";\nimport { WeatherCard } from \"./WeatherCard\";\nimport { GenericToolCallCard } from \"./GenericToolCallCard\";\n\nexport function ToolCallCard({\n  toolCall,\n}: {\n  toolCall: ToolCallWithResult<AgentToolCalls>;\n}) {\n  const { call, result, state } = toolCall;\n\n  if (call.name === \"get_weather\") {\n    return <WeatherCard call={call} result={result} state={state} />;\n  }\n\n  return <GenericToolCallCard call={call} result={result} state={state} />;\n}\n```\n\n```tsx WeatherCard.tsx\nimport type { ToolCallState } from \"@langchain/langgraph-sdk/react\";\nimport type { ToolMessage } from \"@langchain/langgraph-sdk\";\nimport type { GetWeatherToolCall } from \"./types\";\nimport { parseToolResult } from \"./utils\";\n\nexport function WeatherCard({\n  call,\n  result,\n  state,\n}: {\n  call: GetWeatherToolCall;\n  result?: ToolMessage;\n  state: Tool", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": " {\n  const { call, result, state } = toolCall;\n\n  if (call.name === \"get_weather\") {\n    return <WeatherCard call={call} result={result} state={state} />;\n  }\n\n  return <GenericToolCallCard call={call} result={result} state={state} />;\n}\n```\n\n```tsx WeatherCard.tsx\nimport type { ToolCallState } from \"@langchain/langgraph-sdk/react\";\nimport type { ToolMessage } from \"@langchain/langgraph-sdk\";\nimport type { GetWeatherToolCall } from \"./types\";\nimport { parseToolResult } from \"./utils\";\n\nexport function WeatherCard({\n  call,\n  result,\n  state,\n}: {\n  call: GetWeatherToolCall;\n  result?: ToolMessage;\n  state: ToolCallState;\n}) {\n  const isLoading = state === \"pending\";\n  const parsedResult = parseToolResult(result);\n\n  return (\n    <div className=\"relative overflow-hidden rounded-xl\">\n      <div className=\"absolute inset-0 bg-gradient-to-br from-sky-600 to-indigo-600\" />\n      <div className=\"relative p-4\">\n        <div className=\"flex items-center gap-2 text-white/80 text-xs mb-3\">\n          <span className=\"font-medium\">{call.args.location}</span>\n          {isLoading && <span className=\"ml-auto\">Loading...</span>}\n        </div>\n        {parsedResult.status === \"error\" ? (\n          <div className=\"bg-red-500/20 rounded-lg p-3 text-red-200 text-sm\">\n            {parsedResult.content}\n          </div>\n        ) : (\n          <div className=\"text-white text-lg font-medium\">\n            {parsedResult.content || \"Fetching weather...\"}\n          </div>\n        )}\n      </div>\n    </div>\n  );\n}\n```\n\n```typescript types.ts\nimport type { Message } from \"@langchain/langgraph-sdk\";\n\n// Define tool call types to match your Python agent's tools\nexport type GetWeatherToolCall = {\n  name: \"get_weather\";\n  args: { location: string };\n  id?: string;\n};\n\n// Union of all tool calls in your agent\nexport type AgentToolCalls = GetWeatherToolCall;\n\n// Define state type with your tool calls\nexport interface AgentState {\n  messages: Message<AgentToolCalls>[];\n}\n```\n\n```typescript utils.ts\nimport type { ToolMessage } from \"@langchain/langgraph-sdk\";\n\nexport function parseToolResult(result?: ToolMessage): {\n  status: string;\n  content: string;", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "div>\n  );\n}\n```\n\n```typescript types.ts\nimport type { Message } from \"@langchain/langgraph-sdk\";\n\n// Define tool call types to match your Python agent's tools\nexport type GetWeatherToolCall = {\n  name: \"get_weather\";\n  args: { location: string };\n  id?: string;\n};\n\n// Union of all tool calls in your agent\nexport type AgentToolCalls = GetWeatherToolCall;\n\n// Define state type with your tool calls\nexport interface AgentState {\n  messages: Message<AgentToolCalls>[];\n}\n```\n\n```typescript utils.ts\nimport type { ToolMessage } from \"@langchain/langgraph-sdk\";\n\nexport function parseToolResult(result?: ToolMessage): {\n  status: string;\n  content: string;\n} {\n  if (!result) return { status: \"pending\", content: \"\" };\n  try {\n    return JSON.parse(result.content as string);\n  } catch {\n    return { status: \"success\", content: result.content as string };\n  }\n}\n```\n\n</CodeGroup>\n:::\n\n:::js\n<CodeGroup>\n\n```tsx Chat.tsx\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\nimport type { agent } from \"./agent\";\nimport { ToolCallCard } from \"./ToolCallCard\";\nimport { MessageBubble } from \"./MessageBubble\";\n\nfunction Chat() {\n  const stream = useStream<typeof agent>({\n    assistantId: \"agent\",\n    apiUrl: \"http://localhost:2024\",\n  });\n\n  return (\n    <div className=\"flex flex-col gap-4\">\n      {stream.messages.map((message, idx) => {\n        if (message.type === \"ai\") {\n          const toolCalls = stream.getToolCalls(message);\n\n          if (toolCalls.length > 0) {\n            return (\n              <div key={message.id ?? idx} className=\"flex flex-col gap-2\">\n                {toolCalls.map((toolCall) => (\n                  <ToolCallCard key={toolCall.id} toolCall={toolCall} />\n                ))}\n              </div>\n            );\n          }\n        }\n\n        return <MessageBubble key={message.id ?? idx} message={message} />;\n      })}\n    </div>\n  );\n}\n```\n\n```tsx ToolCallCard.tsx\nimport type {\n  ToolCallWithResult,\n  ToolCallFromTool,\n  ToolCallState,\n  InferAgentToolCalls,\n} from \"@langchain/lang", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "          <ToolCallCard key={toolCall.id} toolCall={toolCall} />\n                ))}\n              </div>\n            );\n          }\n        }\n\n        return <MessageBubble key={message.id ?? idx} message={message} />;\n      })}\n    </div>\n  );\n}\n```\n\n```tsx ToolCallCard.tsx\nimport type {\n  ToolCallWithResult,\n  ToolCallFromTool,\n  ToolCallState,\n  InferAgentToolCalls,\n} from \"@langchain/langgraph-sdk/react\";\nimport type { ToolMessage } from \"@langchain/langgraph-sdk\";\nimport type { agent } from \"./agent\";\nimport type { getWeather } from \"./tools\";\nimport { parseToolResult } from \"./utils\";\nimport { WeatherCard } from \"./WeatherCard\";\n\n/**\n * Define tool call types for this component.\n * Use InferAgentToolCalls for agents or ToolCallFromTool for individual tools.\n */\ntype AgentToolCalls = InferAgentToolCalls<typeof agent>;\n\n/**\n * Component that renders a tool call with its result.\n * Uses typed ToolCallWithResult for discriminated union narrowing.\n */\nexport function ToolCallCard({\n  toolCall,\n}: {\n  toolCall: ToolCallWithResult<AgentToolCalls>;\n}) {\n  const { call, result, state } = toolCall;\n\n  // Type narrowing works when call.name is a literal type\n  if (call.name === \"get_weather\") {\n    return <WeatherCard call={call} result={result} state={state} />;\n  }\n\n  // Fallback for other tools\n  return <GenericToolCallCard call={call} result={result} state={state} />;\n}\n```\n\n```tsx GenericToolCallCard.tsx\nimport type { ToolCallState } from \"@langchain/langgraph-sdk/react\";\nimport type { ToolMessage } from \"@langchain/langgraph-sdk\";\nimport { parseToolResult } from \"./utils\";\n\n/**\n * Generic fallback for unknown or unhandled tools.\n * Uses a simple type that works with any tool call.\n */\nexport function GenericToolCallCard({\n  call,\n  result,\n  state,\n}: {\n  call: { name: string; args: Record<string, unknown> };\n  result?: ToolMessage;\n  state: ToolCallState;\n}) {\n  const isLoading = state === \"pending\";\n  const parsedResult = parseToolResult(result);\n\n  return (\n    <div className=\"bg-neutral-900 rounded-lg p-4 border border-neutral-800\">\n      <div className=\"flex items-center gap-3 mb-3\">\n        <div className=\"flex-1\">\n          <div className=\"text-sm font-medium text-white font-mono\">\n            {call.name}\n          </div", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "  result,\n  state,\n}: {\n  call: { name: string; args: Record<string, unknown> };\n  result?: ToolMessage;\n  state: ToolCallState;\n}) {\n  const isLoading = state === \"pending\";\n  const parsedResult = parseToolResult(result);\n\n  return (\n    <div className=\"bg-neutral-900 rounded-lg p-4 border border-neutral-800\">\n      <div className=\"flex items-center gap-3 mb-3\">\n        <div className=\"flex-1\">\n          <div className=\"text-sm font-medium text-white font-mono\">\n            {call.name}\n          </div>\n          <div className=\"text-xs text-neutral-500\">\n            {isLoading ? \"Processing...\" : \"Completed\"}\n          </div>\n        </div>\n      </div>\n      <pre className=\"text-xs bg-black rounded p-2 mb-2 overflow-x-auto\">\n        {JSON.stringify(call.args, null, 2)}\n      </pre>\n      {result && (\n        <div className=\"text-sm rounded-lg p-3 bg-black text-neutral-300\">\n          {parsedResult.content}\n        </div>\n      )}\n    </div>\n  );\n}\n```\n\n```tsx WeatherCard.tsx\nimport type { ToolCallFromTool, ToolCallState } from \"@langchain/langgraph-sdk/react\";\nimport type { ToolMessage } from \"@langchain/langgraph-sdk\";\nimport type { getWeather } from \"./tools\";\nimport { parseToolResult } from \"./utils\";\n\n// Infer tool call type directly from the tool definition\ntype GetWeatherToolCall = ToolCallFromTool<typeof getWeather>;\n\n/**\n * Weather-specific tool card with rich UI.\n * Uses ToolCallFromTool to infer args type from the tool schema.\n */\nexport function WeatherCard({\n  call,\n  result,\n  state,\n}: {\n  call: GetWeatherToolCall;\n  result?: ToolMessage;\n  state: ToolCallState;\n}) {\n  const isLoading = state === \"pending\";\n  const parsedResult = parseToolResult(result);\n\n  return (\n    <div className=\"relative overflow-hidden rounded-xl\">\n      {/* Sky gradient background */}\n      <div className=\"absolute inset-0 bg-gradient-to-br from-sky-600 to-indigo-600\" />\n\n      <div className=\"relative p-4\">\n        <div className=\"flex items-center gap-2 text-white/80 text-xs mb-3\">\n          {/* call.args is typed as { location: string } from the", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "\n}: {\n  call: GetWeatherToolCall;\n  result?: ToolMessage;\n  state: ToolCallState;\n}) {\n  const isLoading = state === \"pending\";\n  const parsedResult = parseToolResult(result);\n\n  return (\n    <div className=\"relative overflow-hidden rounded-xl\">\n      {/* Sky gradient background */}\n      <div className=\"absolute inset-0 bg-gradient-to-br from-sky-600 to-indigo-600\" />\n\n      <div className=\"relative p-4\">\n        <div className=\"flex items-center gap-2 text-white/80 text-xs mb-3\">\n          {/* call.args is typed as { location: string } from the tool schema */}\n          <span className=\"font-medium\">{call.args.location}</span>\n          {isLoading && <span className=\"ml-auto\">Loading...</span>}\n        </div>\n\n        {parsedResult.status === \"error\" ? (\n          <div className=\"bg-red-500/20 rounded-lg p-3 text-red-200 text-sm\">\n            {parsedResult.content}\n          </div>\n        ) : (\n          <div className=\"text-white text-lg font-medium\">\n            {parsedResult.content || \"Fetching weather...\"}\n          </div>\n        )}\n      </div>\n    </div>\n  );\n}\n```\n\n```typescript tools.ts\nimport { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n\n// Define the weather tool with a Zod schema\nexport const getWeather = tool(\n  async ({ location }) => {\n    // Tool implementation\n    return JSON.stringify({ status: \"success\", content: `Weather in ${location}: Sunny, 72\u00b0F` });\n  },\n  {\n    name: \"get_weather\",\n    description: \"Get the current weather for a location\",\n    schema: z.object({\n      location: z.string().describe(\"The city and state, e.g. San Francisco, CA\"),\n    }),\n  }\n);\n```\n\n```typescript utils.ts\nimport type { ToolMessage } from \"@langchain/langgraph-sdk\";\n\n/**\n * Helper to parse tool result safely.\n * Tool results may be JSON strings or plain text.\n */\nexport function parseToolResult(result?: ToolMessage): {\n  status: string;\n  content: string;\n} {\n  if (!result) return { status: \"pending\", content: \"\" };\n  try {\n    return JSON.parse(result.content as string);\n  } catch {\n    return { status: \"success\", content:", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "Get the current weather for a location\",\n    schema: z.object({\n      location: z.string().describe(\"The city and state, e.g. San Francisco, CA\"),\n    }),\n  }\n);\n```\n\n```typescript utils.ts\nimport type { ToolMessage } from \"@langchain/langgraph-sdk\";\n\n/**\n * Helper to parse tool result safely.\n * Tool results may be JSON strings or plain text.\n */\nexport function parseToolResult(result?: ToolMessage): {\n  status: string;\n  content: string;\n} {\n  if (!result) return { status: \"pending\", content: \"\" };\n  try {\n    return JSON.parse(result.content as string);\n  } catch {\n    return { status: \"success\", content: result.content as string };\n  }\n}\n```\n\n</CodeGroup>\n:::\n\n<Card title=\"Try the tool calling example\" icon=\"hammer\" href=\"https://github.com/langchain-ai/langgraphjs/tree/main/examples/ui-react/src/examples/tool-calling-agent\">\n  See a complete implementation of tool call rendering with weather, calculator, and note-taking tools in the `tool-calling-agent` example.\n</Card>\n\n## Custom streaming events\n\nStream custom data from your agent using the `writer` in your tools or nodes. Handle these events in the UI with the `onCustomEvent` callback.\n\n:::python\n<CodeGroup>\n\n```python agent.py\nimport asyncio\nimport time\nfrom langchain import create_agent, tool\nfrom langchain.types import ToolRuntime\n\n@tool\nasync def analyze_data(data_source: str, *, config: ToolRuntime) -> str:\n    \"\"\"Analyze data with progress updates.\"\"\"\n    steps = [\"Connecting...\", \"Fetching...\", \"Processing...\", \"Done!\"]\n\n    for i, step in enumerate(steps):\n        # Emit progress events during execution\n        if config.writer:\n            config.writer({\n                \"type\": \"progress\",\n                \"id\": f\"analysis-{int(time.time() * 1000)}\",\n                \"message\": step,\n                \"progress\": ((i + 1) / len(steps)) * 100,\n            })\n        await asyncio.sleep(0.5)\n\n    return '{\"result\": \"Analysis complete\"}'\n\nagent = create_agent(\n    model=\"openai:gpt-4.1-mini\",\n    tools=[analyze_data],\n)\n```\n\n```tsx Chat.tsx\nimport { useState, useCallback } from \"react\";\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\nimport type { AgentState } from \"./types\";\n\ninterface ProgressData {\n  type: \"progress\";\n  id: string;\n  message: string;\n  progress", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "                \"progress\": ((i + 1) / len(steps)) * 100,\n            })\n        await asyncio.sleep(0.5)\n\n    return '{\"result\": \"Analysis complete\"}'\n\nagent = create_agent(\n    model=\"openai:gpt-4.1-mini\",\n    tools=[analyze_data],\n)\n```\n\n```tsx Chat.tsx\nimport { useState, useCallback } from \"react\";\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\nimport type { AgentState } from \"./types\";\n\ninterface ProgressData {\n  type: \"progress\";\n  id: string;\n  message: string;\n  progress: number;\n}\n\nfunction isProgressData(data: unknown): data is ProgressData {\n  return (\n    typeof data === \"object\" &&\n    data !== null &&\n    \"type\" in data &&\n    (data as ProgressData).type === \"progress\"\n  );\n}\n\nfunction CustomStreamingUI() {\n  const [progressData, setProgressData] = useState<Map<string, ProgressData>>(\n    new Map()\n  );\n\n  const handleCustomEvent = useCallback((data: unknown) => {\n    if (isProgressData(data)) {\n      setProgressData((prev) => {\n        const updated = new Map(prev);\n        updated.set(data.id, data);\n        return updated;\n      });\n    }\n  }, []);\n\n  const stream = useStream<AgentState>({\n    assistantId: \"custom-streaming\",\n    apiUrl: \"http://localhost:2024\",\n    onCustomEvent: handleCustomEvent,\n  });\n\n  return (\n    <div>\n      {Array.from(progressData.values()).map((data) => (\n        <div key={data.id} className=\"bg-neutral-800 rounded-lg p-4 mb-4\">\n          <div className=\"flex justify-between mb-2\">\n            <span className=\"text-sm text-white\">{data.message}</span>\n            <span className=\"text-xs text-neutral-400\">{data.progress}%</span>\n          </div>\n          <div className=\"w-full bg-neutral-700 rounded-full h-2\">\n            <div\n              className=\"bg-blue-500 h-2 rounded-full transition-all\"\n              style={{ width: `${data.progress}%` }}\n            />\n          </div>\n        </div>\n     ", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "}</span>\n            <span className=\"text-xs text-neutral-400\">{data.progress}%</span>\n          </div>\n          <div className=\"w-full bg-neutral-700 rounded-full h-2\">\n            <div\n              className=\"bg-blue-500 h-2 rounded-full transition-all\"\n              style={{ width: `${data.progress}%` }}\n            />\n          </div>\n        </div>\n      ))}\n    </div>\n  );\n}\n```\n\n```typescript types.ts\nimport type { Message } from \"@langchain/langgraph-sdk\";\n\n// Define tool calls to match your Python agent\nexport type AnalyzeDataToolCall = {\n  name: \"analyze_data\";\n  args: { data_source: string };\n  id?: string;\n};\n\nexport type AgentToolCalls = AnalyzeDataToolCall;\n\nexport interface AgentState {\n  messages: Message<AgentToolCalls>[];\n}\n```\n\n</CodeGroup>\n:::\n\n:::js\n<CodeGroup>\n\n```typescript agent.ts\nimport { tool, type ToolRuntime } from \"langchain\";\nimport { z } from \"zod\";\n\n// Define your custom event types\ninterface ProgressData {\n  type: \"progress\";\n  id: string;\n  message: string;\n  progress: number;\n}\n\nconst analyzeDataTool = tool(\n  async ({ dataSource }, config: ToolRuntime) => {\n    const steps = [\"Connecting...\", \"Fetching...\", \"Processing...\", \"Done!\"];\n\n    for (let i = 0; i < steps.length; i++) {\n      // Emit progress events during execution\n      config.writer?.({\n        type: \"progress\",\n        id: `analysis-${Date.now()}`,\n        message: steps[i],\n        progress: ((i + 1) / steps.length) * 100,\n      } satisfies ProgressData);\n\n      await new Promise((resolve) => setTimeout(resolve, 500));\n    }\n\n    return JSON.stringify({ result: \"Analysis complete\" });\n  },\n  {\n    name: \"analyze_data\",\n    description: \"Analyze data with progress updates\",\n    schema: z.object({\n      dataSource: z.string().describe(\"Data source to analyze\"),\n    }),\n  }\n);\n```\n\n```tsx Chat.tsx\nimport { useState, useCallback } from \"react\";\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\nimport type { agent } from \"./agent\";\n\ninterface ProgressData {\n  type: \"progress\";\n  id: string;\n  message", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": " satisfies ProgressData);\n\n      await new Promise((resolve) => setTimeout(resolve, 500));\n    }\n\n    return JSON.stringify({ result: \"Analysis complete\" });\n  },\n  {\n    name: \"analyze_data\",\n    description: \"Analyze data with progress updates\",\n    schema: z.object({\n      dataSource: z.string().describe(\"Data source to analyze\"),\n    }),\n  }\n);\n```\n\n```tsx Chat.tsx\nimport { useState, useCallback } from \"react\";\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\nimport type { agent } from \"./agent\";\n\ninterface ProgressData {\n  type: \"progress\";\n  id: string;\n  message: string;\n  progress: number;\n}\n\nfunction isProgressData(data: unknown): data is ProgressData {\n  return (\n    typeof data === \"object\" &&\n    data !== null &&\n    \"type\" in data &&\n    (data as ProgressData).type === \"progress\"\n  );\n}\n\nfunction CustomStreamingUI() {\n  const [progressData, setProgressData] = useState<Map<string, ProgressData>>(\n    new Map()\n  );\n\n  const handleCustomEvent = useCallback((data: unknown) => {\n    if (isProgressData(data)) {\n      setProgressData((prev) => {\n        const updated = new Map(prev);\n        updated.set(data.id, data);\n        return updated;\n      });\n    }\n  }, []);\n\n  const stream = useStream<typeof agent>({\n    assistantId: \"custom-streaming\",\n    apiUrl: \"http://localhost:2024\",\n    onCustomEvent: handleCustomEvent,\n  });\n\n  return (\n    <div>\n      {/* Render progress cards */}\n      {Array.from(progressData.values()).map((data) => (\n        <div key={data.id} className=\"bg-neutral-800 rounded-lg p-4 mb-4\">\n          <div className=\"flex justify-between mb-2\">\n            <span className=\"text-sm text-white\">{data.message}</span>\n            <span className=\"text-xs text-neutral-400\">{data.progress}%</span>\n          </div>\n          <div className=\"w-full bg-neutral-700 rounded-full h-2\">\n            <div\n              className=\"bg-blue-500 h-2 rounded-full transition-all\"\n              style={{ width: `${data.progress}%` }}\n            />\n         ", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "     <span className=\"text-sm text-white\">{data.message}</span>\n            <span className=\"text-xs text-neutral-400\">{data.progress}%</span>\n          </div>\n          <div className=\"w-full bg-neutral-700 rounded-full h-2\">\n            <div\n              className=\"bg-blue-500 h-2 rounded-full transition-all\"\n              style={{ width: `${data.progress}%` }}\n            />\n          </div>\n        </div>\n      ))}\n    </div>\n  );\n}\n```\n\n</CodeGroup>\n:::\n\n<Card title=\"Try the custom streaming example\" icon=\"bolt\" href=\"https://github.com/langchain-ai/langgraphjs/tree/main/examples/ui-react/src/examples/custom-streaming\">\n  See a complete implementation of custom events with progress bars, status badges, and file operation cards in the `custom-streaming` example.\n</Card>\n\n## Event handling\n\nThe `useStream` hook provides callback options that give you access to different types of streaming events. You don't need to explicitly configure stream modes\u2014just pass callbacks for the event types you want to handle:\n\n:::js\n```tsx\nconst stream = useStream({\n  apiUrl: \"http://localhost:2024\",\n  assistantId: \"agent\",\n\n  // Handle state updates after each graph step\n  onUpdateEvent: (update, options) => {\n    console.log(\"Graph update:\", update);\n  },\n\n  // Handle custom events streamed from your graph\n  onCustomEvent: (event, options) => {\n    console.log(\"Custom event:\", event);\n  },\n\n  // Handle metadata events with run/thread info\n  onMetadataEvent: (metadata) => {\n    console.log(\"Run ID:\", metadata.run_id);\n    console.log(\"Thread ID:\", metadata.thread_id);\n  },\n\n  onError: (error) => {\n    console.error(\"Stream error:\", error);\n  },\n\n  onFinish: (state, options) => {\n    console.log(\"Stream finished with final state:\", state);\n  },\n});\n```\n:::\n\n### Available callbacks\n\n| Callback | Description | Stream mode |\n|----------|-------------|-------------|\n| `onUpdateEvent` | Called when a state update is received after each graph step | `updates` |\n| `onCustomEvent` | Called when a custom event is received from your graph | `custom` |\n| `onMetadataEvent` | Called with run and thread metadata | `metadata` |\n| `onError` | Called when an error occurs | - |\n| `onFinish` | Called when the stream completes | - |\n\n## Multi-agent streaming\n\nWhen working with multi-agent systems or graphs with multiple nodes, use message metadata to identify which node generated each message. This is particularly useful when multiple LLMs run in parallel and you", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": ") => {\n    console.log(\"Stream finished with final state:\", state);\n  },\n});\n```\n:::\n\n### Available callbacks\n\n| Callback | Description | Stream mode |\n|----------|-------------|-------------|\n| `onUpdateEvent` | Called when a state update is received after each graph step | `updates` |\n| `onCustomEvent` | Called when a custom event is received from your graph | `custom` |\n| `onMetadataEvent` | Called with run and thread metadata | `metadata` |\n| `onError` | Called when an error occurs | - |\n| `onFinish` | Called when the stream completes | - |\n\n## Multi-agent streaming\n\nWhen working with multi-agent systems or graphs with multiple nodes, use message metadata to identify which node generated each message. This is particularly useful when multiple LLMs run in parallel and you want to display their outputs with distinct visual styling.\n\n:::python\n<CodeGroup>\n\n```python agent.py\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import StateGraph, START, END, Send\nfrom langgraph.graph.state import CompiledStateGraph\nfrom langchain.messages import BaseMessage, AIMessage\nfrom typing import TypedDict, Annotated\nimport operator\n\n# Use different model instances for variety\nanalytical_model = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0.3)\ncreative_model = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0.9)\npractical_model = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0.5)\n\nclass State(TypedDict):\n    messages: Annotated[list[BaseMessage], operator.add]\n    topic: str\n    analytical_research: str\n    creative_research: str\n    practical_research: str\n\ndef fan_out_to_researchers(state: State) -> list[Send]:\n    return [\n        Send(\"researcher_analytical\", state),\n        Send(\"researcher_creative\", state),\n        Send(\"researcher_practical\", state),\n    ]\n\ndef dispatcher(state: State) -> dict:\n    last_message = state[\"messages\"][-1] if state[\"messages\"] else None\n    topic = last_message.content if last_message else \"\"\n    return {\"topic\": topic}\n\nasync def researcher_analytical(state: State) -> dict:\n    response = await analytical_model.ainvoke([\n        {\"role\": \"system\", \"content\": \"You are an analytical research expert.\"},\n        {\"role\": \"user\", \"content\": f\"Research: {state['topic']}\"},\n    ])\n    return {\n        \"analytical_research\": response.content,\n        \"messages\": [AIMessage(content=response.content, name=\"researcher_analytical\")],\n    }\n\n# Similar nodes for creative and practical researchers...\n\nworkflow = StateGraph(State)\nworkflow.add_node(\"dispatcher\", dispatcher)\nworkflow.", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "  return {\"topic\": topic}\n\nasync def researcher_analytical(state: State) -> dict:\n    response = await analytical_model.ainvoke([\n        {\"role\": \"system\", \"content\": \"You are an analytical research expert.\"},\n        {\"role\": \"user\", \"content\": f\"Research: {state['topic']}\"},\n    ])\n    return {\n        \"analytical_research\": response.content,\n        \"messages\": [AIMessage(content=response.content, name=\"researcher_analytical\")],\n    }\n\n# Similar nodes for creative and practical researchers...\n\nworkflow = StateGraph(State)\nworkflow.add_node(\"dispatcher\", dispatcher)\nworkflow.add_node(\"researcher_analytical\", researcher_analytical)\nworkflow.add_node(\"researcher_creative\", researcher_creative)\nworkflow.add_node(\"researcher_practical\", researcher_practical)\nworkflow.add_edge(START, \"dispatcher\")\nworkflow.add_conditional_edges(\"dispatcher\", fan_out_to_researchers)\nworkflow.add_edge(\"researcher_analytical\", END)\nworkflow.add_edge(\"researcher_creative\", END)\nworkflow.add_edge(\"researcher_practical\", END)\n\nagent: CompiledStateGraph = workflow.compile()\n```\n\n```tsx Chat.tsx\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\nimport type { AgentState } from \"./types\";\nimport { MessageBubble } from \"./MessageBubble\";\n\n// Node configuration for visual display\nconst NODE_CONFIG: Record<string, { label: string; color: string }> = {\n  researcher_analytical: { label: \"Analytical Research\", color: \"cyan\" },\n  researcher_creative: { label: \"Creative Research\", color: \"purple\" },\n  researcher_practical: { label: \"Practical Research\", color: \"emerald\" },\n};\n\nfunction MultiAgentChat() {\n  const stream = useStream<AgentState>({\n    assistantId: \"parallel-research\",\n    apiUrl: \"http://localhost:2024\",\n  });\n\n  return (\n    <div className=\"flex flex-col gap-4\">\n      {stream.messages.map((message, idx) => {\n        if (message.type !== \"ai\") {\n          return <MessageBubble key={message.id ?? idx} message={message} />;\n        }\n\n        const metadata = stream.getMessagesMetadata?.(message);\n        const nodeName =\n          (metadata?.streamMetadata?.langgraph_node as string) ||\n          (message as { name?: string }).name;\n\n        const config = nodeName ? NODE_CONFIG[nodeName] : null;\n\n        if (!", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "4\">\n      {stream.messages.map((message, idx) => {\n        if (message.type !== \"ai\") {\n          return <MessageBubble key={message.id ?? idx} message={message} />;\n        }\n\n        const metadata = stream.getMessagesMetadata?.(message);\n        const nodeName =\n          (metadata?.streamMetadata?.langgraph_node as string) ||\n          (message as { name?: string }).name;\n\n        const config = nodeName ? NODE_CONFIG[nodeName] : null;\n\n        if (!config) {\n          return <MessageBubble key={message.id ?? idx} message={message} />;\n        }\n\n        return (\n          <div\n            key={message.id ?? idx}\n            className={`bg-${config.color}-950/30 border border-${config.color}-500/30 rounded-xl p-4`}\n          >\n            <div className={`text-sm font-semibold text-${config.color}-400 mb-2`}>\n              {config.label}\n            </div>\n            <div className=\"text-neutral-200 whitespace-pre-wrap\">\n              {typeof message.content === \"string\" ? message.content : \"\"}\n            </div>\n          </div>\n        );\n      })}\n    </div>\n  );\n}\n```\n\n```typescript types.ts\nimport type { Message } from \"@langchain/langgraph-sdk\";\n\n// State matches your Python agent's State TypedDict\nexport interface AgentState {\n  messages: Message[];\n  topic: string;\n  analytical_research: string;\n  creative_research: string;\n  practical_research: string;\n}\n```\n\n</CodeGroup>\n:::\n\n:::js\n<CodeGroup>\n\n```tsx Chat.tsx\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\nimport type { agent } from \"./agent\";\nimport { MessageBubble } from \"./MessageBubble\";\n\n// Node configuration for visual display\nconst NODE_CONFIG: Record<string, { label: string; color: string }> = {\n  researcher_analytical: { label: \"Analytical Research\", color: \"cyan\" },\n  researcher_creative: { label: \"Creative Research\", color: \"purple\" },\n  researcher_practical: { label: \"Practical Research\", color: \"em", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": ";\n  analytical_research: string;\n  creative_research: string;\n  practical_research: string;\n}\n```\n\n</CodeGroup>\n:::\n\n:::js\n<CodeGroup>\n\n```tsx Chat.tsx\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\nimport type { agent } from \"./agent\";\nimport { MessageBubble } from \"./MessageBubble\";\n\n// Node configuration for visual display\nconst NODE_CONFIG: Record<string, { label: string; color: string }> = {\n  researcher_analytical: { label: \"Analytical Research\", color: \"cyan\" },\n  researcher_creative: { label: \"Creative Research\", color: \"purple\" },\n  researcher_practical: { label: \"Practical Research\", color: \"emerald\" },\n};\n\nfunction MultiAgentChat() {\n  const stream = useStream<typeof agent>({\n    assistantId: \"parallel-research\",\n    apiUrl: \"http://localhost:2024\",\n  });\n\n  return (\n    <div className=\"flex flex-col gap-4\">\n      {stream.messages.map((message, idx) => {\n        if (message.type !== \"ai\") {\n          return <MessageBubble key={message.id ?? idx} message={message} />;\n        }\n\n        // Get streaming metadata to identify the source node\n        const metadata = stream.getMessagesMetadata?.(message);\n        const nodeName =\n          (metadata?.streamMetadata?.langgraph_node as string) ||\n          (message as { name?: string }).name;\n\n        const config = nodeName ? NODE_CONFIG[nodeName] : null;\n\n        if (!config) {\n          return <MessageBubble key={message.id ?? idx} message={message} />;\n        }\n\n        return (\n          <div\n            key={message.id ?? idx}\n            className={`bg-${config.color}-950/30 border border-${config.color}-500/30 rounded-xl p-4`}\n          >\n            <div className={`text-sm font-semibold text-${config.color}-400 mb-2`}>\n              {config.label}\n            </div>\n            <div className=\"text-neutral-200 whitespace-pre-wrap\">\n              {typeof message.content === \"string\" ? message.content : \"\"}\n            </div>\n          </", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "color}-950/30 border border-${config.color}-500/30 rounded-xl p-4`}\n          >\n            <div className={`text-sm font-semibold text-${config.color}-400 mb-2`}>\n              {config.label}\n            </div>\n            <div className=\"text-neutral-200 whitespace-pre-wrap\">\n              {typeof message.content === \"string\" ? message.content : \"\"}\n            </div>\n          </div>\n        );\n      })}\n    </div>\n  );\n}\n```\n\n```typescript agent.ts\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport {\n  StateGraph,\n  START,\n  END,\n  Send,\n  StateSchema,\n  MessagesValue,\n  GraphNode,\n  ConditionalEdgeRouter,\n} from \"@langchain/langgraph\";\nimport { AIMessage } from \"@langchain/core/messages\";\nimport { z } from \"zod\";\n\n// Use different model instances for variety\nconst analyticalModel = new ChatOpenAI({ model: \"gpt-4.1-mini\", temperature: 0.3 });\nconst creativeModel = new ChatOpenAI({ model: \"gpt-4.1-mini\", temperature: 0.9 });\nconst practicalModel = new ChatOpenAI({ model: \"gpt-4.1-mini\", temperature: 0.5 });\n\n// Define the state schema\nconst StateAnnotation = new StateSchema({\n  messages: MessagesValue,\n  topic: z.string().default(\"\"),\n  analyticalResearch: z.string().default(\"\"),\n  creativeResearch: z.string().default(\"\"),\n  practicalResearch: z.string().default(\"\"),\n});\n\ntype State = typeof StateAnnotation.State;\n\n// Fan-out to parallel researchers\nconst fanOutToResearchers: ConditionalEdgeRouter<State> = (state) => {\n  return [\n    new Send(\"researcher_analytical\", state),\n    new Send(\"researcher_creative\", state),\n    new Send(\"researcher_practical\", state),\n  ];\n};\n\nconst dispatcherNode: GraphNode<State> = async (state) => {\n  const lastMessage = state.messages.at(-1);\n  const topic = typeof lastMessage?.content === \"string\" ? lastMessage.content : \"\";\n  return { topic };\n};\n\nconst analyticalResearcherNode: GraphNode<State> = async (state) => {\n  const response = await analyticalModel.invoke([\n    { role: \"system\", content: \"You are an analytical research expert. Focus on data and evidence.\" },\n    { role: \"user\", content: `Research: ${state.topic}` },\n  ]);\n  return {\n    analyticalResearch: response.content as string,\n    messages: [new AIMessage({ content: response.content as string, name: \"", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "_practical\", state),\n  ];\n};\n\nconst dispatcherNode: GraphNode<State> = async (state) => {\n  const lastMessage = state.messages.at(-1);\n  const topic = typeof lastMessage?.content === \"string\" ? lastMessage.content : \"\";\n  return { topic };\n};\n\nconst analyticalResearcherNode: GraphNode<State> = async (state) => {\n  const response = await analyticalModel.invoke([\n    { role: \"system\", content: \"You are an analytical research expert. Focus on data and evidence.\" },\n    { role: \"user\", content: `Research: ${state.topic}` },\n  ]);\n  return {\n    analyticalResearch: response.content as string,\n    messages: [new AIMessage({ content: response.content as string, name: \"researcher_analytical\" })],\n  };\n};\n\n// Similar nodes for creative and practical researchers...\n\n// Build the graph with parallel execution\nconst workflow = new StateGraph(StateAnnotation)\n  .addNode(\"dispatcher\", dispatcherNode)\n  .addNode(\"researcher_analytical\", analyticalResearcherNode)\n  .addNode(\"researcher_creative\", creativeResearcherNode)\n  .addNode(\"researcher_practical\", practicalResearcherNode)\n  .addEdge(START, \"dispatcher\")\n  .addConditionalEdges(\"dispatcher\", fanOutToResearchers)\n  .addEdge(\"researcher_analytical\", END)\n  .addEdge(\"researcher_creative\", END)\n  .addEdge(\"researcher_practical\", END);\n\nexport const agent = workflow.compile();\n```\n\n</CodeGroup>\n:::\n\n<Card title=\"Try the parallel research example\" icon=\"users\" href=\"https://github.com/langchain-ai/langgraphjs/tree/main/examples/ui-react/src/examples/parallel-research\">\n  See a complete implementation of multi-agent streaming with three parallel researchers and distinct visual styling in the `parallel-research` example.\n</Card>\n\n## Human-in-the-loop\n\nHandle interrupts when the agent requires human approval for tool execution. Learn more in the [How to handle interrupts](/oss/langgraph/interrupts#pause-using-interrupt) guide.\n\n:::python\n<CodeGroup>\n\n```python agent.py\nfrom langchain import create_agent, tool, human_in_the_loop_middleware\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.checkpoint.memory import MemorySaver\n\nmodel = ChatOpenAI(model=\"gpt-4.1-mini\")\n\n@tool\ndef send_email(to: str, subject: str, body: str) -> dict:\n    \"\"\"Send an email. Requires human approval.\"\"\"\n    return {\n        \"status\": \"success\",\n        \"content\": f'Email sent to {to} with subject \"{subject}\"',\n    }\n\n@tool\ndef delete_file(path: str) -> dict:\n    \"\"\"Delete a file. Requires human approval.\"\"\"\n    return {\"status\": \"success\", \"content\": f'File \"{path}\" deleted'}\n\n@tool\ndef read_file(path: str) ->", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "from langchain_openai import ChatOpenAI\nfrom langgraph.checkpoint.memory import MemorySaver\n\nmodel = ChatOpenAI(model=\"gpt-4.1-mini\")\n\n@tool\ndef send_email(to: str, subject: str, body: str) -> dict:\n    \"\"\"Send an email. Requires human approval.\"\"\"\n    return {\n        \"status\": \"success\",\n        \"content\": f'Email sent to {to} with subject \"{subject}\"',\n    }\n\n@tool\ndef delete_file(path: str) -> dict:\n    \"\"\"Delete a file. Requires human approval.\"\"\"\n    return {\"status\": \"success\", \"content\": f'File \"{path}\" deleted'}\n\n@tool\ndef read_file(path: str) -> dict:\n    \"\"\"Read file contents. No approval needed.\"\"\"\n    return {\"status\": \"success\", \"content\": f\"Contents of {path}...\"}\n\nagent = create_agent(\n    model=model,\n    tools=[send_email, delete_file, read_file],\n    middleware=[\n        human_in_the_loop_middleware(\n            interrupt_on={\n                \"send_email\": {\n                    \"allowed_decisions\": [\"approve\", \"edit\", \"reject\"],\n                    \"description\": \"\ud83d\udce7 Review email before sending\",\n                },\n                \"delete_file\": {\n                    \"allowed_decisions\": [\"approve\", \"reject\"],\n                    \"description\": \"\ud83d\uddd1\ufe0f Confirm file deletion\",\n                },\n                \"read_file\": False,  # Safe - auto-approved\n            }\n        ),\n    ],\n    checkpointer=MemorySaver(),\n)\n```\n\n```tsx Chat.tsx\nimport { useState } from \"react\";\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\nimport type { AgentState, HITLRequest, HITLResponse } from \"./types\";\nimport { MessageBubble } from \"./MessageBubble\";\n\nfunction HumanInTheLoopChat() {\n  const stream = useStream<AgentState, { InterruptType: HITLRequest }>({\n    assistantId: \"human-in-the-loop\",\n    apiUrl: \"http://localhost:2024\",\n  });\n\n  const [isProcessing, setIsProcessing] = useState(false);\n  const hitlRequest = stream.interrupt?.value as HITLRequest | undefined;\n\n  const handleApprove = async (index: number) => {\n", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "```\n\n```tsx Chat.tsx\nimport { useState } from \"react\";\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\nimport type { AgentState, HITLRequest, HITLResponse } from \"./types\";\nimport { MessageBubble } from \"./MessageBubble\";\n\nfunction HumanInTheLoopChat() {\n  const stream = useStream<AgentState, { InterruptType: HITLRequest }>({\n    assistantId: \"human-in-the-loop\",\n    apiUrl: \"http://localhost:2024\",\n  });\n\n  const [isProcessing, setIsProcessing] = useState(false);\n  const hitlRequest = stream.interrupt?.value as HITLRequest | undefined;\n\n  const handleApprove = async (index: number) => {\n    if (!hitlRequest) return;\n    setIsProcessing(true);\n\n    try {\n      const decisions: HITLResponse[\"decisions\"] =\n        hitlRequest.actionRequests.map((_, i) =>\n          i === index ? { type: \"approve\" } : { type: \"approve\" }\n        );\n\n      await stream.submit(null, {\n        command: { resume: { decisions } as HITLResponse },\n      });\n    } finally {\n      setIsProcessing(false);\n    }\n  };\n\n  const handleReject = async (index: number, reason: string) => {\n    if (!hitlRequest) return;\n    setIsProcessing(true);\n\n    try {\n      const decisions: HITLResponse[\"decisions\"] =\n        hitlRequest.actionRequests.map((_, i) =>\n          i === index\n            ? { type: \"reject\", message: reason }\n            : { type: \"reject\", message: \"Rejected along with other actions\" }\n        );\n\n      await stream.submit(null, {\n        command: { resume: { decisions } as HITLResponse },\n      });\n    } finally {\n      setIsProcessing(false);\n    }\n  };\n\n  return (\n    <div>\n      {stream.messages.map((message, idx) => (\n        <MessageBubble key={message.id ?? idx} message={message} />\n      ))}\n\n      {hitlRequest && hitlRequest.actionRequests.length > 0 && (\n        <div className=\"bg-amber-900/20 border border-amber-500/30 rounded-xl p-4 mt-4\">\n          <h3 className=\"text-amber-400 font-semibold mb-4\">\n            Action requires approval\n          </", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "Processing(false);\n    }\n  };\n\n  return (\n    <div>\n      {stream.messages.map((message, idx) => (\n        <MessageBubble key={message.id ?? idx} message={message} />\n      ))}\n\n      {hitlRequest && hitlRequest.actionRequests.length > 0 && (\n        <div className=\"bg-amber-900/20 border border-amber-500/30 rounded-xl p-4 mt-4\">\n          <h3 className=\"text-amber-400 font-semibold mb-4\">\n            Action requires approval\n          </h3>\n\n          {hitlRequest.actionRequests.map((action, idx) => (\n            <div key={idx} className=\"bg-neutral-900 rounded-lg p-4 mb-4 last:mb-0\">\n              <div className=\"text-sm font-mono text-white mb-2\">{action.name}</div>\n              <pre className=\"text-xs bg-black rounded p-2 mb-3 overflow-x-auto\">\n                {JSON.stringify(action.args, null, 2)}\n              </pre>\n              <div className=\"flex gap-2\">\n                <button\n                  onClick={() => handleApprove(idx)}\n                  disabled={isProcessing}\n                  className=\"px-3 py-1.5 bg-green-600 hover:bg-green-500 text-white text-sm rounded-lg\"\n                >\n                  Approve\n                </button>\n                <button\n                  onClick={() => handleReject(idx, \"User rejected\")}\n                  disabled={isProcessing}\n                  className=\"px-3 py-1.5 bg-red-600 hover:bg-red-500 text-white text-sm rounded-lg\"\n                >\n                  Reject\n                </button>\n   ", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "        </button>\n                <button\n                  onClick={() => handleReject(idx, \"User rejected\")}\n                  disabled={isProcessing}\n                  className=\"px-3 py-1.5 bg-red-600 hover:bg-red-500 text-white text-sm rounded-lg\"\n                >\n                  Reject\n                </button>\n              </div>\n            </div>\n          ))}\n        </div>\n      )}\n    </div>\n  );\n}\n```\n\n```typescript types.ts\nimport type { Message } from \"@langchain/langgraph-sdk\";\n\n// Tool call types matching your Python agent\nexport type SendEmailToolCall = {\n  name: \"send_email\";\n  args: { to: string; subject: string; body: string };\n  id?: string;\n};\n\nexport type DeleteFileToolCall = {\n  name: \"delete_file\";\n  args: { path: string };\n  id?: string;\n};\n\nexport type ReadFileToolCall = {\n  name: \"read_file\";\n  args: { path: string };\n  id?: string;\n};\n\nexport type AgentToolCalls = SendEmailToolCall | DeleteFileToolCall | ReadFileToolCall;\n\nexport interface AgentState {\n  messages: Message<AgentToolCalls>[];\n}\n\n// HITL types\nexport interface HITLRequest {\n  actionRequests: Array<{\n    name: string;\n    args: Record<string, unknown>;\n  }>;\n}\n\nexport interface HITLResponse {\n  decisions: Array<\n    | { type: \"approve\" }\n    | { type: \"reject\"; message: string }\n    | { type: \"edit\"; newArgs: Record<string, unknown> }\n  >;\n}\n```\n\n</CodeGroup>\n:::\n\n:::js\n<CodeGroup>\n\n```tsx Chat.tsx\nimport { useState } from \"react\";\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\nimport type { HITLRequest, HITLResponse } from \"langchain\";\nimport type { agent } from \"./agent\";\nimport { MessageBubble } from \"./MessageBubble\";\n\nfunction HumanInTheLoopChat() {\n  const stream = useStream<typeof agent, { InterruptType: HITLRequest }>({\n    assistantId: \"human-in-the-loop\",\n    apiUrl: \"http://localhost:2024\",\n  });\n\n  const [isProcessing, setIsProcessing] = useState(false);\n\n  // Type assertion for interrupt value\n  const hitlRequest = stream.inter", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "\n:::\n\n:::js\n<CodeGroup>\n\n```tsx Chat.tsx\nimport { useState } from \"react\";\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\nimport type { HITLRequest, HITLResponse } from \"langchain\";\nimport type { agent } from \"./agent\";\nimport { MessageBubble } from \"./MessageBubble\";\n\nfunction HumanInTheLoopChat() {\n  const stream = useStream<typeof agent, { InterruptType: HITLRequest }>({\n    assistantId: \"human-in-the-loop\",\n    apiUrl: \"http://localhost:2024\",\n  });\n\n  const [isProcessing, setIsProcessing] = useState(false);\n\n  // Type assertion for interrupt value\n  const hitlRequest = stream.interrupt?.value as HITLRequest | undefined;\n\n  const handleApprove = async (index: number) => {\n    if (!hitlRequest) return;\n    setIsProcessing(true);\n\n    try {\n      const decisions: HITLResponse[\"decisions\"] =\n        hitlRequest.actionRequests.map((_, i) =>\n          i === index ? { type: \"approve\" } : { type: \"approve\" }\n        );\n\n      await stream.submit(null, {\n        command: {\n          resume: { decisions } as HITLResponse,\n        },\n      });\n    } finally {\n      setIsProcessing(false);\n    }\n  };\n\n  const handleReject = async (index: number, reason: string) => {\n    if (!hitlRequest) return;\n    setIsProcessing(true);\n\n    try {\n      const decisions: HITLResponse[\"decisions\"] =\n        hitlRequest.actionRequests.map((_, i) =>\n          i === index\n            ? { type: \"reject\", message: reason }\n            : { type: \"reject\", message: \"Rejected along with other actions\" }\n        );\n\n      await stream.submit(null, {\n        command: {\n          resume: { decisions } as HITLResponse,\n        },\n      });\n    } finally {\n      setIsProcessing(false);\n    }\n  };\n\n  return (\n    <div>\n      {/* Render messages */}\n      {stream.messages.map((message, idx) => (\n        <MessageBubble key={message.id ?? idx} message={message} />\n      ))}\n\n      {/* Render approval UI when interrupted */}\n      {hitlRequest && hitlRequest.actionRequests.length > 0 && (\n  ", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "(null, {\n        command: {\n          resume: { decisions } as HITLResponse,\n        },\n      });\n    } finally {\n      setIsProcessing(false);\n    }\n  };\n\n  return (\n    <div>\n      {/* Render messages */}\n      {stream.messages.map((message, idx) => (\n        <MessageBubble key={message.id ?? idx} message={message} />\n      ))}\n\n      {/* Render approval UI when interrupted */}\n      {hitlRequest && hitlRequest.actionRequests.length > 0 && (\n        <div className=\"bg-amber-900/20 border border-amber-500/30 rounded-xl p-4 mt-4\">\n          <h3 className=\"text-amber-400 font-semibold mb-4\">\n            Action requires approval\n          </h3>\n\n          {hitlRequest.actionRequests.map((action, idx) => (\n            <div\n              key={idx}\n              className=\"bg-neutral-900 rounded-lg p-4 mb-4 last:mb-0\"\n            >\n              <div className=\"flex items-center gap-2 mb-2\">\n                <span className=\"text-sm font-mono text-white\">\n                  {action.name}\n                </span>\n              </div>\n\n              <pre className=\"text-xs bg-black rounded p-2 mb-3 overflow-x-auto\">\n                {JSON.stringify(action.args, null, 2)}\n              </pre>\n\n              <div className=\"flex gap-2\">\n                <button\n                  onClick={() => handleApprove(idx)}\n                  disabled={isProcessing}\n                  className=\"px-3 py-1.5 bg-green-600 hover:bg-green-700 text-white text-sm rounded disabled:opacity-50\"\n                >\n           ", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": ")}\n              </pre>\n\n              <div className=\"flex gap-2\">\n                <button\n                  onClick={() => handleApprove(idx)}\n                  disabled={isProcessing}\n                  className=\"px-3 py-1.5 bg-green-600 hover:bg-green-700 text-white text-sm rounded disabled:opacity-50\"\n                >\n                  Approve\n                </button>\n                <button\n                  onClick={() => handleReject(idx, \"User rejected\")}\n                  disabled={isProcessing}\n                  className=\"px-3 py-1.5 bg-red-600 hover:bg-red-700 text-white text-sm rounded disabled:opacity-50\"\n                >\n                  Reject\n                </button>\n              </div>\n            </div>\n          ))}\n        </div>\n      )}\n    </div>\n  );\n}\n```\n\n```typescript agent.ts\nimport { createAgent, tool, humanInTheLoopMiddleware } from \"langchain\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { MemorySaver } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst model = new ChatOpenAI({ model: \"gpt-4.1-mini\" });\n\n// Tool that requires human approval\nconst sendEmail = tool(\n  async ({ to, subject, body }) => {\n    return {\n      status: \"success\",\n      content: `Email sent to ${to} with subject \"${subject}\"`,\n    };\n  },\n  {\n    name: \"send_email\",\n    description: \"Send an email. Requires human approval.\",\n    schema: z.object({\n      to: z.string().describe(\"Recipient email address\"),\n      subject: z.string().describe(\"Email subject\"),\n      body: z.string().describe(\"Email body\"),\n    }),\n  }\n);\n\n// Tool that requires approval with limited options\nconst deleteFile = tool(\n  async ({ path }) => {\n    return { status:", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": " tool(\n  async ({ to, subject, body }) => {\n    return {\n      status: \"success\",\n      content: `Email sent to ${to} with subject \"${subject}\"`,\n    };\n  },\n  {\n    name: \"send_email\",\n    description: \"Send an email. Requires human approval.\",\n    schema: z.object({\n      to: z.string().describe(\"Recipient email address\"),\n      subject: z.string().describe(\"Email subject\"),\n      body: z.string().describe(\"Email body\"),\n    }),\n  }\n);\n\n// Tool that requires approval with limited options\nconst deleteFile = tool(\n  async ({ path }) => {\n    return { status: \"success\", content: `File \"${path}\" deleted` };\n  },\n  {\n    name: \"delete_file\",\n    description: \"Delete a file. Requires human approval.\",\n    schema: z.object({\n      path: z.string().describe(\"File path to delete\"),\n    }),\n  }\n);\n\n// Safe tool - no approval needed\nconst readFile = tool(\n  async ({ path }) => {\n    return { status: \"success\", content: `Contents of ${path}...` };\n  },\n  {\n    name: \"read_file\",\n    description: \"Read file contents. No approval needed.\",\n    schema: z.object({\n      path: z.string().describe(\"File path to read\"),\n    }),\n  }\n);\n\n// Create agent with HITL middleware\nexport const agent = createAgent({\n  model,\n  tools: [sendEmail, deleteFile, readFile],\n  middleware: [\n    humanInTheLoopMiddleware({\n      interruptOn: {\n        // Email requires all decision types\n        send_email: {\n          allowedDecisions: [\"approve\", \"edit\", \"reject\"],\n          description: \"\ud83d\udce7 Review email before sending\",\n        },\n        // Deletion only allows approve/reject\n        delete_file: {\n          allowedDecisions: [\"approve\", \"reject\"],\n          description: \"\ud83d\uddd1\ufe0f Confirm file deletion\",\n        },\n        // Reading is safe - auto-approved\n        read_file: false,\n      },\n    }),\n  ],\n  // Required for HITL - persists state across interrupts\n  checkpointer: new MemorySaver(),\n});\n```\n\n</CodeGroup>\n:::\n\n<Card title=\"Try the human-in-the-loop example\" icon=\"hand\" href=\"https://github.com/langchain-ai/langgraphjs/tree/main/examples/ui-react/src/examples/human-in-the-loop\">\n  See a complete implementation of approval workflows with approve, reject, and edit actions in the `", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": " \"reject\"],\n          description: \"\ud83d\uddd1\ufe0f Confirm file deletion\",\n        },\n        // Reading is safe - auto-approved\n        read_file: false,\n      },\n    }),\n  ],\n  // Required for HITL - persists state across interrupts\n  checkpointer: new MemorySaver(),\n});\n```\n\n</CodeGroup>\n:::\n\n<Card title=\"Try the human-in-the-loop example\" icon=\"hand\" href=\"https://github.com/langchain-ai/langgraphjs/tree/main/examples/ui-react/src/examples/human-in-the-loop\">\n  See a complete implementation of approval workflows with approve, reject, and edit actions in the `human-in-the-loop` example.\n</Card>\n\n## Reasoning models\n\n<Warning>\nExtended reasoning/thinking support is currently experimental. The streaming interface for reasoning tokens varies by provider (OpenAI vs. Anthropic) and may change as abstractions are developed.\n</Warning>\n\nWhen using models with extended reasoning capabilities (like OpenAI's reasoning models or Anthropic's extended thinking), the thinking process is embedded in the message content. You'll need to extract and display it separately.\n\n:::python\n<CodeGroup>\n\n```python agent.py\nfrom langchain import create_agent\nfrom langchain_openai import ChatOpenAI\n\n# Use a reasoning-capable model\n# For OpenAI: o1, o1-mini, o1-preview\n# For Anthropic: claude-sonnet-4-20250514 with extended thinking enabled\nmodel = ChatOpenAI(model=\"o1-mini\")\n\nagent = create_agent(\n    model=model,\n    tools=[],  # Reasoning models work best for complex reasoning tasks\n)\n```\n\n```tsx Chat.tsx\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\nimport type { AgentState } from \"./types\";\nimport { getReasoningFromMessage, getTextContent } from \"./utils\";\nimport { MessageBubble } from \"./MessageBubble\";\n\nfunction ReasoningChat() {\n  const stream = useStream<AgentState>({\n    assistantId: \"reasoning-agent\",\n    apiUrl: \"http://localhost:2024\",\n  });\n\n  return (\n    <div className=\"flex flex-col gap-4\">\n      {stream.messages.map((message, idx) => {\n        if (message.type === \"ai\") {\n          const reasoning = getReasoningFromMessage(message);\n          const textContent = getTextContent(message);\n\n          return (\n            <div key={message.id ?? idx}>\n              {reasoning && (\n                <div className=\"mb-4\">\n                  <div className=\"text-xs font-medium text-amber-400/80 mb-2\">\n  ", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "   {stream.messages.map((message, idx) => {\n        if (message.type === \"ai\") {\n          const reasoning = getReasoningFromMessage(message);\n          const textContent = getTextContent(message);\n\n          return (\n            <div key={message.id ?? idx}>\n              {reasoning && (\n                <div className=\"mb-4\">\n                  <div className=\"text-xs font-medium text-amber-400/80 mb-2\">\n                    Reasoning\n                  </div>\n                  <div className=\"bg-amber-950/50 border border-amber-500/20 rounded-2xl px-4 py-3\">\n                    <div className=\"text-sm text-amber-100/90 whitespace-pre-wrap\">\n                      {reasoning}\n                    </div>\n                  </div>\n                </div>\n              )}\n\n              {textContent && (\n                <div className=\"text-neutral-100 whitespace-pre-wrap\">\n                  {textContent}\n                </div>\n              )}\n            </div>\n          );\n        }\n\n        return <MessageBubble key={message.id ?? idx} message={message} />;\n      })}\n\n      {stream.isLoading && (\n        <div className=\"flex items-center gap-2 text-amber-400/70\">\n          <span className=\"text-sm\">Thinking...</span>\n        </div>\n      )}\n    </div>\n  );\n}\n```\n\n```typescript types.ts\nimport type { Message } from \"@langchain/langgraph-sdk\";\n\nexport interface AgentState {\n  messages: Message[];\n}\n```\n\n```typescript utils.ts\nimport type { Message, AIMessage } from \"@langchain/langgraph-sdk\";\n\n/**", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "id ?? idx} message={message} />;\n      })}\n\n      {stream.isLoading && (\n        <div className=\"flex items-center gap-2 text-amber-400/70\">\n          <span className=\"text-sm\">Thinking...</span>\n        </div>\n      )}\n    </div>\n  );\n}\n```\n\n```typescript types.ts\nimport type { Message } from \"@langchain/langgraph-sdk\";\n\nexport interface AgentState {\n  messages: Message[];\n}\n```\n\n```typescript utils.ts\nimport type { Message, AIMessage } from \"@langchain/langgraph-sdk\";\n\n/**\n * Extracts reasoning/thinking content from an AI message.\n * Supports both OpenAI reasoning and Anthropic extended thinking.\n */\nexport function getReasoningFromMessage(message: Message): string | undefined {\n  type MessageWithExtras = AIMessage & {\n    additional_kwargs?: {\n      reasoning?: {\n        summary?: Array<{ type: string; text: string }>;\n      };\n    };\n    contentBlocks?: Array<{ type: string; thinking?: string }>;\n  };\n\n  const msg = message as MessageWithExtras;\n\n  // Check for OpenAI reasoning in additional_kwargs\n  if (msg.additional_kwargs?.reasoning?.summary) {\n    const content = msg.additional_kwargs.reasoning.summary\n      .filter((item) => item.type === \"summary_text\")\n      .map((item) => item.text)\n      .join(\"\");\n    if (content.trim()) return content;\n  }\n\n  // Check for Anthropic thinking in contentBlocks\n  if (msg.contentBlocks?.length) {\n    const thinking = msg.contentBlocks\n      .filter((b) => b.type === \"thinking\" && b.thinking)\n      .map((b) => b.thinking)\n      .join(\"\\n\");\n    if (thinking) return thinking;\n  }\n\n  // Check for thinking in message.content array\n  if (Array.isArray(msg.content)) {\n    const thinking = msg.content\n      .filter((b): b is { type: \"thinking\"; thinking: string } =>\n        typeof b === \"object\" && b?.type === \"thinking\" && \"thinking\" in b\n      )\n      .map((b) => b.thinking)\n      .join(\"\\n\");\n    if (thinking) return thinking;\n  }\n\n  return undefined;\n}\n\n/**\n * Extracts text content from a message.\n */\nexport function getTextContent(message: Message): string {\n  if (typeof message.content === \"string\") return message.content;\n  if (Array.isArray(message.content)) {\n    return message.content\n      .filter((c): c is { type: \"text\"; text: string }", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "      .filter((b): b is { type: \"thinking\"; thinking: string } =>\n        typeof b === \"object\" && b?.type === \"thinking\" && \"thinking\" in b\n      )\n      .map((b) => b.thinking)\n      .join(\"\\n\");\n    if (thinking) return thinking;\n  }\n\n  return undefined;\n}\n\n/**\n * Extracts text content from a message.\n */\nexport function getTextContent(message: Message): string {\n  if (typeof message.content === \"string\") return message.content;\n  if (Array.isArray(message.content)) {\n    return message.content\n      .filter((c): c is { type: \"text\"; text: string } => c.type === \"text\")\n      .map((c) => c.text)\n      .join(\"\");\n  }\n  return \"\";\n}\n```\n\n</CodeGroup>\n:::\n\n:::js\n<CodeGroup>\n\n```tsx Chat.tsx\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\nimport type { Message } from \"@langchain/langgraph-sdk\";\nimport type { agent } from \"./agent\";\nimport { getReasoningFromMessage, getTextContent } from \"./utils\";\n\nfunction ReasoningChat() {\n  const stream = useStream<typeof agent>({\n    assistantId: \"reasoning-agent\",\n    apiUrl: \"http://localhost:2024\",\n  });\n\n  return (\n    <div className=\"flex flex-col gap-4\">\n      {stream.messages.map((message, idx) => {\n        if (message.type === \"ai\") {\n          const reasoning = getReasoningFromMessage(message);\n          const textContent = getTextContent(message);\n\n          return (\n            <div key={message.id ?? idx}>\n              {/* Render reasoning bubble if present */}\n              {reasoning && (\n                <div className=\"mb-4\">\n                  <div className=\"text-xs font-medium text-amber-400/80 mb-2\">\n                    Reasoning\n                  </div>\n                  <div className=\"bg-amber-950/50 border border-amber-500/20 rounded-2xl px-4 py-3\">\n                    <div className=\"text-sm text-amber-100/90 whitespace-pre-wrap\">\n                      {reasoning}", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "                <div className=\"text-xs font-medium text-amber-400/80 mb-2\">\n                    Reasoning\n                  </div>\n                  <div className=\"bg-amber-950/50 border border-amber-500/20 rounded-2xl px-4 py-3\">\n                    <div className=\"text-sm text-amber-100/90 whitespace-pre-wrap\">\n                      {reasoning}\n                    </div>\n                  </div>\n                </div>\n              )}\n\n              {/* Render text content */}\n              {textContent && (\n                <div className=\"text-neutral-100 whitespace-pre-wrap\">\n                  {textContent}\n                </div>\n              )}\n            </div>\n          );\n        }\n\n        return <MessageBubble key={message.id ?? idx} message={message} />;\n      })}\n\n      {stream.isLoading && (\n        <div className=\"flex items-center gap-2 text-amber-400/70\">\n          <span className=\"text-sm\">Thinking...</span>\n        </div>\n      )}\n    </div>\n  );\n}\n```\n\n```typescript utils.ts\nimport type { Message, AIMessage } from \"@langchain/langgraph-sdk\";\n\n/**\n * Extracts reasoning/thinking content from an AI message.\n * Supports both OpenAI reasoning (additional_kwargs.reasoning.summary)\n * and Anthropic extended thinking (content blocks with type \"thinking\").\n */\nexport function getReasoningFromMessage(message: Message): string | undefined {\n  type MessageWithExtras = AIMessage & {\n    additional_kwargs?: {\n      reasoning?: {\n        summary?: Array<{ type: string; text: string }>;\n      };\n    };\n    contentBlocks?: Array<{ type: string; thinking?: string }>;\n  };\n\n  const msg = message as MessageWithExtras;\n\n  // Check for OpenAI reasoning in additional_kwargs\n  if", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "essage } from \"@langchain/langgraph-sdk\";\n\n/**\n * Extracts reasoning/thinking content from an AI message.\n * Supports both OpenAI reasoning (additional_kwargs.reasoning.summary)\n * and Anthropic extended thinking (content blocks with type \"thinking\").\n */\nexport function getReasoningFromMessage(message: Message): string | undefined {\n  type MessageWithExtras = AIMessage & {\n    additional_kwargs?: {\n      reasoning?: {\n        summary?: Array<{ type: string; text: string }>;\n      };\n    };\n    contentBlocks?: Array<{ type: string; thinking?: string }>;\n  };\n\n  const msg = message as MessageWithExtras;\n\n  // Check for OpenAI reasoning in additional_kwargs\n  if (msg.additional_kwargs?.reasoning?.summary) {\n    const content = msg.additional_kwargs.reasoning.summary\n      .filter((item) => item.type === \"summary_text\")\n      .map((item) => item.text)\n      .join(\"\");\n\n    if (content.trim()) return content;\n  }\n\n  // Check for Anthropic thinking in contentBlocks\n  if (msg.contentBlocks?.length) {\n    const thinking = msg.contentBlocks\n      .filter((b) => b.type === \"thinking\" && b.thinking)\n      .map((b) => b.thinking)\n      .join(\"\\n\");\n\n    if (thinking) return thinking;\n  }\n\n  // Check for thinking in message.content array\n  if (Array.isArray(msg.content)) {\n    const thinking = msg.content\n      .filter((b): b is { type: \"thinking\"; thinking: string } =>\n        typeof b === \"object\" && b?.type === \"thinking\" && \"thinking\" in b\n      )\n      .map((b) => b.thinking)\n      .join(\"\\n\");\n\n    if (thinking) return thinking;\n  }\n\n  return undefined;\n}\n\n/**\n * Extracts text content from a message.\n */\nexport function getTextContent(message: Message): string {\n  if (typeof message.content === \"string\") return message.content;\n\n  if (Array.isArray(message.content)) {\n    return message.content\n      .filter((c): c is { type: \"text\"; text: string } => c.type === \"text\")\n      .map((c) => c.text)\n      .join(\"\");\n  }\n\n  return \"\";\n}\n```\n\n</CodeGroup>\n:::\n\n<Card title=\"Try the reasoning example\" icon=\"brain\" href=\"https://github.com/langchain-ai/langgraphjs/tree/main/examples/ui-react/src/examples/reasoning-agent\">\n  See a complete implementation of reasoning token display with OpenAI and Anthropic models in the `reasoning-agent` example.\n</Card>\n\n## Custom state types\n\nFor custom LangGraph applications, embed your tool call types in your state's", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "(message.content)) {\n    return message.content\n      .filter((c): c is { type: \"text\"; text: string } => c.type === \"text\")\n      .map((c) => c.text)\n      .join(\"\");\n  }\n\n  return \"\";\n}\n```\n\n</CodeGroup>\n:::\n\n<Card title=\"Try the reasoning example\" icon=\"brain\" href=\"https://github.com/langchain-ai/langgraphjs/tree/main/examples/ui-react/src/examples/reasoning-agent\">\n  See a complete implementation of reasoning token display with OpenAI and Anthropic models in the `reasoning-agent` example.\n</Card>\n\n## Custom state types\n\nFor custom LangGraph applications, embed your tool call types in your state's messages property.\n\n:::js\n```tsx\nimport { Message } from \"@langchain/langgraph-sdk\";\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\n\n// Define your tool call types as a discriminated union\ntype MyToolCalls =\n  | { name: \"search\"; args: { query: string }; id?: string }\n  | { name: \"calculate\"; args: { expression: string }; id?: string };\n\n// Embed tool call types in your state's messages\ninterface MyGraphState {\n  messages: Message<MyToolCalls>[];\n  context?: string;\n}\n\nfunction CustomGraphChat() {\n  const stream = useStream<MyGraphState>({\n    assistantId: \"my-graph\",\n    apiUrl: \"http://localhost:2024\",\n  });\n\n  // stream.values is typed as MyGraphState\n  // stream.toolCalls[0].call.name is typed as \"search\" | \"calculate\"\n}\n```\n\nYou can also specify additional type configuration for interrupts and configurable options:\n\n```tsx\ninterface MyGraphState {\n  messages: Message<MyToolCalls>[];\n}\n\nfunction CustomGraphChat() {\n  const stream = useStream<\n    MyGraphState,\n    {\n      InterruptType: { question: string };\n      ConfigurableType: { userId: string };\n    }\n  >({\n    assistantId: \"my-graph\",\n    apiUrl: \"http://localhost:2024\",\n  });\n\n  // stream.interrupt is typed as { question: string } | undefined\n}\n```\n:::\n\n## Custom transport\n\nFor custom API endpoints or non-standard deployments, use the `transport` option with `FetchStreamTransport` to connect to any streaming API.\n\n:::js\n```tsx\nimport { useMemo } from \"react\";\nimport { useStream, FetchStreamTransport } from \"@langchain/langgraph-sdk/react\";\n\nfunction CustomAPIChat({ apiKey }: { apiKey: string }) {\n  // Create transport with custom request handling\n  const transport = useMemo(() => {\n    return new FetchStreamTransport({\n      apiUrl: \"/api/my-agent\",\n      onRequest: async (url: string, init: RequestInit) => {\n        // Inject API key or other custom", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": " undefined\n}\n```\n:::\n\n## Custom transport\n\nFor custom API endpoints or non-standard deployments, use the `transport` option with `FetchStreamTransport` to connect to any streaming API.\n\n:::js\n```tsx\nimport { useMemo } from \"react\";\nimport { useStream, FetchStreamTransport } from \"@langchain/langgraph-sdk/react\";\n\nfunction CustomAPIChat({ apiKey }: { apiKey: string }) {\n  // Create transport with custom request handling\n  const transport = useMemo(() => {\n    return new FetchStreamTransport({\n      apiUrl: \"/api/my-agent\",\n      onRequest: async (url: string, init: RequestInit) => {\n        // Inject API key or other custom data into requests\n        const customBody = JSON.stringify({\n          ...(JSON.parse(init.body as string) || {}),\n          apiKey,\n        });\n\n        return {\n          ...init,\n          body: customBody,\n          headers: {\n            ...init.headers,\n            \"X-Custom-Header\": \"value\",\n          },\n        };\n      },\n    });\n  }, [apiKey]);\n\n  const stream = useStream({\n    transport,\n  });\n\n  // Use stream as normal\n  return (\n    <div>\n      {stream.messages.map((message, idx) => (\n        <MessageBubble key={message.id ?? idx} message={message} />\n      ))}\n    </div>\n  );\n}\n```\n:::\n\n## Related\n\n- [Streaming overview](/oss/langchain/streaming/overview) \u2014 Server-side streaming with LangChain agents\n- [useStream API Reference](https://reference.langchain.com/javascript/functions/_langchain_langgraph-sdk.react.useStream.html) \u2014 Full API documentation\n- [Agent Chat UI](/oss/langchain/ui) \u2014 Pre-built chat interface for LangGraph agents\n- [Human-in-the-loop](/oss/langchain/human-in-the-loop) \u2014 Configuring interrupts for human review\n- [Multi-agent systems](/oss/langchain/multi-agent) \u2014 Building agents with multiple LLMs\n", "metadata": {"source": "streaming/frontend.mdx"}}
{"text": "---\ntitle: INVALID_PROMPT_INPUT\n---\n\n{/* TODO: fix link when porting page */}\nOccurs when a [prompt template](https://github.com/langchain-ai/langchain/blob/v0.3/docs/docs/concepts/prompt_templates.mdx) received missing or invalid input variables.\n\n:::js\nOne unexpected way this can occur is if you add a JSON object directly into a prompt template:\n\n```typescript\nimport { PromptTemplate } from \"@langchain/core/prompts\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst prompt = PromptTemplate.fromTemplate(`You are a helpful assistant.\n\nHere is an example of how you should respond:\n\n{\n  \"firstName\": \"John\",\n  \"lastName\": \"Doe\",\n  \"age\": 21\n}\n\nNow, answer the following question:\n\n{question}`);\n```\n\nYou might think that the above prompt template should require a single input key named question, but the JSON object will be interpreted as an additional variable because the curly braces (`{`) are not escaped, and should be preceded by a second brace instead, like this:\n\n```typescript\nimport { PromptTemplate } from \"@langchain/core/prompts\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst prompt = PromptTemplate.fromTemplate(`You are a helpful assistant.\n\nHere is an example of how you should respond:\n\n{{\n  \"firstName\": \"John\",\n  \"lastName\": \"Doe\",\n  \"age\": 21\n}}\n\nNow, answer the following question:\n\n{question}`);\n```\n:::\n\n\n## Troubleshooting\n\nTo resolve this error, you can:\n\n1. Review your prompt template for correctness. When using f-string formats, ensure proper escaping of curly braces:\n   * Use `{{` for single braces in f-strings\n   * Use `{{{{` for double braces in f-strings\n2. When using `MessagesPlaceholder` components, confirm you're passing message arrays or message-like objects. If using shorthand tuples, wrap variable names in curly braces like `[\"placeholder\", \"{messages}\"]`\n3. Debug by examining actual inputs to your prompt template by using [LangSmith](/langsmith/home) or logging to verify they match expectations\n4. If sourcing prompts from LangChain [Prompt Hub](https://smith.langchain.com/prompts), isolate and test the prompt with sample inputs to ensure it functions as intended\n", "metadata": {"source": "errors/INVALID_PROMPT_INPUT.mdx"}}
{"text": "---\ntitle: MESSAGE_COERCION_FAILURE\n---\n\nThis error occurs when message objects don't conform to the expected format.\n\n:::python\n## Accepted message formats\n\nLangChain modules accept `MessageLikeRepresentation`, which is defined as:\n\n```python\nfrom typing import Union\n\nfrom langchain_core.prompts.chat import (\n    BaseChatPromptTemplate,\n    BaseMessage,\n    BaseMessagePromptTemplate,\n)\n\nMessageLikeRepresentation = Union[\n    Union[BaseMessagePromptTemplate, BaseMessage, BaseChatPromptTemplate],\n    tuple[\n        Union[str, type],\n        Union[str, list[dict], list[object]],\n    ],\n    str,\n]\n```\n\nThese include OpenAI style message objects (`{ role: \"user\", content: \"Hello world!\" }`), tuples, and plain strings (which are converted to @[`HumanMessage`] objects).\n\nIf a module receives a value outside of one of these formats, you will receive an error:\n\n```python\nfrom langchain_anthropic import ChatAnthropic\n\nuncoercible_message = {\"role\": \"HumanMessage\", \"random_field\": \"random value\"}\n\nmodel = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\n\nmodel.invoke([uncoercible_message])\n```\n\n```text\nValueError: Message dict must contain 'role' and 'content' keys, got {'role': 'HumanMessage', 'random_field': 'random value'}\n```\n:::\n:::js\nTODO: Add JS example\n:::\n\n## Troubleshooting\n\nTo resolve this error:\n\n1. **Ensure proper format**: All inputs to chat models must be an array of LangChain message classes or a supported message-like format\n2. Verify no unintended stringification or transformation occurs to your messages\n3. Examine the error's stack trace and add logging statements to inspect message objects before they're passed to the model\n", "metadata": {"source": "errors/MESSAGE_COERCION_FAILURE.mdx"}}
{"text": "---\ntitle: MODEL_RATE_LIMIT\n---\n\n<Note>\n    Currently only used in `langchainjs` (JavaScript/TypeScript).\n</Note>\n\nYou have hit the maximum number of requests that a model provider allows over a given time period and are being temporarily blocked.\n\nThis error occurs when you exceed the maximum number of requests permitted by your model provider within a specific timeframe, resulting in temporary blocking. The restriction is generally temporary and lifts after the limit resets.\n\n## Troubleshooting\n\nTo resolve this error, you can:\n\n1. **Implement Rate Limiting**: Deploy a rate limiter to regulate the frequency of requests sent to the model. See [rate limiting](/oss/langchain/models#rate-limiting) docs.\n2. **Implement Response Caching**: Use model response caching to reduce redundant requests when incoming queries are repetitive.\n3. **Use Multiple Providers**: Distribute requests across multiple providers if your application architecture supports this approach\n4. **Contact Your Provider**: Reach out to your model provider requesting an increase to your rate limits\n", "metadata": {"source": "errors/MODEL_RATE_LIMIT.mdx"}}
{"text": "---\ntitle: INVALID_TOOL_RESULTS\n---\n\n<Note>\n    Currently only used in `langchainjs` (JavaScript/TypeScript).\n</Note>\n\nThis error occurs when passing mismatched, insufficient, or excessive @[`ToolMessage`] objects to a model during tool calling operations.\n\nThe error stems from a fundamental requirement: an assistant message with `tool_calls` must be followed by tool messages responding to each `tool_call_id`.\n\nWhen a model returns an @[`AIMessage`] with tool calls, you must provide exactly one corresponding @[`ToolMessage`] for each tool call, with matching `tool_call_id` values.\n\n## Common causes\n\n* **Insufficient responses**: If a model requests two tool executions but you only provide one response message, the model rejects the incomplete message chain\n* **Duplicate responses**: Providing multiple @[`ToolMessage`] objects for the same tool call ID results in rejection, as does having unmatched IDs\n* **Orphaned tool messages**: Sending a @[`ToolMessage`] without a preceding @[`AIMessage`] containing tool calls violates protocol requirements\n\n:::python\nHere's an example of a problematic pattern:\n\n```python\n# Model requests two tool calls\nresponse_message.tool_calls  # Returns 2 calls\n\n# But only one ToolMessage provided\nchat_history.append(ToolMessage(\n    content=str(tool_response),\n    tool_call_id=tool_call.get(\"id\")\n))\n\nmodel_with_tools.invoke(chat_history)\n```\n:::\n\n:::js\nHere's an example of a problematic pattern:\n\n```javascript\n// Model requests two tool calls\nresponseMessage.tool_calls // Returns 2 calls\n\n// But only one ToolMessage provided\nchatHistory.push({\n  role: \"tool\",\n  content: toolResponse,\n  tool_call_id: responseMessage.tool_calls[0].id\n});\n\nawait modelWithTools.invoke(chatHistory); // Fails with INVALID_TOOL_RESULTS\n```\n:::\n\n## Troubleshooting\n\nTo resolve this error:\n\n* **Count matching pairs**: Ensure one @[`ToolMessage`] exists per tool call in the preceding @[`AIMessage`]\n* **Verify IDs**: Confirm each `ToolMessage.tool_call_id` matches an actual tool call identifier\n", "metadata": {"source": "errors/INVALID_TOOL_RESULTS.mdx"}}
{"text": "---\ntitle: MODEL_NOT_FOUND\n---\n\n<Note>\n    Currently only used in `langchainjs` (JavaScript/TypeScript).\n</Note>\n\nThe model name you have specified is not acknowledged by your provider.\n\n## Troubleshooting\n\nTo resolve this error:\n\n1. **Verify the model identifier**: Double check the model string you are passing in. Ensure the spelling and format are correct\n2. **Check proxy/wrapper configurations**: If you are using a proxy or other alternative host with a model wrapper, confirm that the permitted model names are not restricted or altered\n\nThe error typically stems from either a typo in the model name string itself or restrictions imposed by a proxy service or model wrapper between your code and the provider's API.\n", "metadata": {"source": "errors/MODEL_NOT_FOUND.mdx"}}
{"text": "---\ntitle: OUTPUT_PARSING_FAILURE\n---\n\nAn [output parser](https://reference.langchain.com/python/langchain_core/output_parsers/) was unable to handle model output as expected.\n\n<Note>\n    Some prebuilt constructs like legacy LangChain agents and chains may use output parsers internally, so you may see this error even if you're not visibly instantiating and using an output parser.\n</Note>\n\n## Troubleshooting\n\n- Consider using tool calling or other structured output techniques if possible without an output parser to reliably output parseable values.\n- Add more precise formatting instructions to your prompt.\n- If you are using a smaller or less capable model, try using a more capable one.\n", "metadata": {"source": "errors/OUTPUT_PARSING_FAILURE.mdx"}}
{"text": "---\ntitle: MODEL_AUTHENTICATION\n---\n\n<Note>\n    Currently only used in `langchainjs` (JavaScript/TypeScript).\n</Note>\n\nYour model provider is denying you access to their service.\n\nThis error typically occurs when there's an issue with your authentication credentials or API keys.\n\n## Troubleshooting\n\n* Confirm that your API key or authentication credentials are accurate and valid.\n* If using environment-based authentication, verify:\n    - The variable name is spelled correctly\n    - The variable contains an assigned value\n    - Third-party packages like `dotenv` haven't interfered with loading\n* If using a proxy or non-standard endpoint, make sure that your custom provider does not expect an alternative authentication scheme.\n* Bypass environment variable issues by passing credentials explicitly:\n\n:::python\n```python\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(api_key=\"YOUR_KEY_HERE\")\n```\n:::\n:::js\n```typescript\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst model = new ChatOpenAI({\n  apiKey: \"YOUR_KEY_HERE\",\n});\n```\n:::\n\n\n", "metadata": {"source": "errors/MODEL_AUTHENTICATION.mdx"}}
