from pathlib import Pathimport jsonfrom langchain_text_splitters import TokenTextSplitterfrom langchain_core.documents import Document# ---------- Paths ----------RAW_DIR = Path("data/raw/langchain")OUT_DIR = Path("data/processed/langchain")OUT_DIR.mkdir(parents=True, exist_ok=True)CHUNKS_FILE = OUT_DIR / "chunks.jsonl"# ---------- Step 1: Load ----------docs = []for file in RAW_DIR.rglob("*.mdx"):    try:        text = file.read_text(encoding="utf-8")        # Skip tiny files (navigation, stubs, etc.)        if len(text) < 200:            continue        metadata = {            "source": str(file.relative_to(RAW_DIR))        }        docs.append(            Document(                page_content=text,                metadata=metadata            )        )    except Exception as e:        print(f"Skipping {file}: {e}")print(f"Loaded {len(docs)} docs")# ---------- Step 2: Chunk ----------splitter = TokenTextSplitter(    chunk_size=800,    chunk_overlap=200)chunks = splitter.split_documents(docs)print(f"Created {len(chunks)} chunks")# ---------- Step 3: Cache ----------with open(CHUNKS_FILE, "w", encoding="utf-8") as f:    for chunk in chunks:        record = {            "text": chunk.page_content,            "metadata": chunk.metadata        }        f.write(json.dumps(record) + "\n")print(f"Saved chunks â†’ {CHUNKS_FILE}")print("Done.")